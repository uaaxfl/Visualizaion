2008.iwslt-evaluation.12,P07-2045,0,0.0131546,"d the evaluation results. 1. Introduction This paper describes the statistical machine translation system of CASIA (Institute of Automation, Chinese Academy of Sciences), which is used for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our s"
2008.iwslt-evaluation.12,W06-3119,0,0.027318,"nese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclusions. 2. System Overview Figure 1 depicts our system architecture. After the test data are preprocessed, they are passed into multiple translation systems respectively to produce an N-Best translation list, and then all the N-Best translations in the list a"
2008.iwslt-evaluation.12,J03-1002,0,0.00456062,"rs into Chinese words using the free software toolkit ICTCLAS3.01;  Transforming the SBC case into DBC case; For the English part of the training data and development data and test data, also two types of preprocessing are performed:  Tokenization of the English words: which separates the punctuations with the English words;  Transforming the uppercase into lowercase. 1 - 85 - http://www.nlp.org.cn Proceedings of IWSLT 2008, Hawaii - U.S.A. 2.2. Multiple translation systems 2.2.1. Three phrase-based SMT systems Phrase-based translation systems are usually modeled through a log-linear model [7]. In the log-linear model, given the sentence f (source language), the translating process is searching the translation e (target language) with the highest probability. The translation probability and the decision rule are given as Formula (1). M e* = arg max ∑ λm hm (e, f ) e (1) m =1 Where hm(e,f) is a feature function and λm Figure 2: Architecture of Bandore. is the weight of the feature. The entire λm are obtained by the minimum error rate training [8]. We use three phrase-based machine translation systems, Moses system (MOSES) [1], an in-home phrase-based system (PB) [2] and a sentence t"
2008.iwslt-evaluation.12,2005.eamt-1.36,0,0.242798,"translation systems 2.2.1. Three phrase-based SMT systems Phrase-based translation systems are usually modeled through a log-linear model [7]. In the log-linear model, given the sentence f (source language), the translating process is searching the translation e (target language) with the highest probability. The translation probability and the decision rule are given as Formula (1). M e* = arg max ∑ λm hm (e, f ) e (1) m =1 Where hm(e,f) is a feature function and λm Figure 2: Architecture of Bandore. is the weight of the feature. The entire λm are obtained by the minimum error rate training [8]. We use three phrase-based machine translation systems, Moses system (MOSES) [1], an in-home phrase-based system (PB) [2] and a sentence type-based reordering model (Bandore) [3]. The Moses decoder provided in the open source Moses package1 is run by the default parameters. We only train 3gram language model and extract phrase pairs no more than 10 words. Our in-home PB system’s word alignment is based on the training results of the GIZA++ 2 toolkit under the default parameters. We obtain word alignment by the method of grow-diag-final on the bi-directional word alignments of GIZA++. PB’s phr"
2008.iwslt-evaluation.12,I08-1066,0,0.0698379,"rted } , hi (O, A) is a feature, and λi is the weight of the feature. After reordering the Chinese sentences of training set and test set, we pass the reordered sentences into a phrase-based decoder such as Moses or PB decoder to get the final translation results. In our experiments Bandore uses Moses as its decoder. 2.2.2. Two formal syntax-based translation models Here we use two formal syntax-based translation models, a maximum entropy-based reordering model (MEBTG) [5] and a hierarchical phrase-based translation model (HPB) [4]. The system of MEBTG is realized in home according to [5] and [11]. In this model the prediction of relative orders of any two adjacent blocks is considered as a problem of classification. We extract reordering examples from the wordaligned training corpus and extract the following features from every two consecutive phrase pairs:  Lexical features: the last word of two source phrases or target phrases;  Collocation features: the combination of lexical features. With these features we train a MaxEnt classifier1. We extract phrase pairs using Och&apos;s algorithm [12]. The maximum length of source phrase is limited in 10 words. We use a CKY style decoder which l"
2008.iwslt-evaluation.12,J04-4002,0,0.032787,"based translation model (HPB) [4]. The system of MEBTG is realized in home according to [5] and [11]. In this model the prediction of relative orders of any two adjacent blocks is considered as a problem of classification. We extract reordering examples from the wordaligned training corpus and extract the following features from every two consecutive phrase pairs:  Lexical features: the last word of two source phrases or target phrases;  Collocation features: the combination of lexical features. With these features we train a MaxEnt classifier1. We extract phrase pairs using Och&apos;s algorithm [12]. The maximum length of source phrase is limited in 10 words. We use a CKY style decoder which limits the phrase table within 40 and the partial hypotheses is within 200. HPB translation engine is a re-implementation of David Chiang&apos;s hierarchical phrase translation model. Based on the union of the bidirectional alignment results of GIZA++, initial rules consistent with the alignment are extracted using Och&apos;s algorithm [12] and then rule subtraction is performed to obtain rules with no more than two non-terminals. Nullaligned words are allowed at the boundary of phrases. We set a limitation th"
2008.iwslt-evaluation.12,N04-1022,0,0.0689939,"et a limitation that initial rules are of no more than 10 words and other rules should have no more than 5 terminals and nonterminals. The decoder is CYK-style chart parser that maximizes the derivation probability. A 3-gram language model generated by SRILM is used in the cube-pruning process. The search space is pruned with a chart cell size limit 1 http://maxent.sourceforge.net/ Figure 3: System combination architecture. We collect the N-Best list translation hypotheses from each translation system in Section 2.2, and find a hypothesis as the alignment reference with the minimum Bayes risk [13]. We exploit word reordering alignment approaches to align all the hypotheses against the alignment reference and form a consensus alignment. Given N (N=3) translation hypotheses: please show me on this map . please on the map for me . show me on the map , please . when the first translation hypothesis is chosen as the alignment reference, the result of consensus alignment may look something like Figure 4, where “null” strings are used to accommodate insertions and deletions. 2 - 87 - http://www.cs.cmu.edu/~zollmann/samt Proceedings of IWSLT 2008, Hawaii - U.S.A. null null , please please plea"
2008.iwslt-evaluation.12,2007.iwslt-1.8,0,0.0239539,"Missing"
2008.iwslt-evaluation.12,P07-1002,0,0.0398822,"Missing"
2008.iwslt-evaluation.12,W06-3110,0,0.0625255,"Missing"
2008.iwslt-evaluation.12,C08-1137,1,0.735422,"ystem of CASIA (Institute of Automation, Chinese Academy of Sciences), which is used for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclus"
2008.iwslt-evaluation.12,J07-2003,0,0.651264,"e evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclusions. 2. System Overview Figure 1 depicts our system architecture. After the test data are"
2008.iwslt-evaluation.12,P06-1066,0,0.425706,"poken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclusions. 2. System Overview Figure 1 depicts our system architecture. After the test data are preprocessed, they are passed into multiple translation"
2008.iwslt-evaluation.12,2005.iwslt-1.16,0,\N,Missing
2008.iwslt-evaluation.12,2006.iwslt-evaluation.22,0,\N,Missing
2009.iwslt-evaluation.13,P07-2045,0,0.00656902,"Missing"
2009.iwslt-evaluation.13,W09-0424,0,0.0350416,"Missing"
2009.iwslt-evaluation.13,P06-1066,0,0.073844,"Missing"
2009.iwslt-evaluation.13,2008.amta-papers.10,1,0.900475,"Missing"
2009.iwslt-evaluation.13,C08-1137,1,0.888149,"Missing"
2009.iwslt-evaluation.13,P07-1040,0,0.133371,"Missing"
2009.iwslt-evaluation.13,2006.amta-papers.25,0,0.0477907,"Missing"
2009.iwslt-evaluation.13,P02-1040,0,0.085985,"Missing"
2009.iwslt-evaluation.13,J97-3002,0,0.020293,"Missing"
2009.iwslt-evaluation.13,N04-1022,0,0.192674,"Missing"
2009.iwslt-evaluation.13,P07-1003,0,0.0477973,"Missing"
2009.iwslt-evaluation.13,2008.iwslt-evaluation.18,0,\N,Missing
2009.iwslt-evaluation.13,2008.iwslt-evaluation.12,1,\N,Missing
2009.iwslt-evaluation.13,2006.iwslt-evaluation.4,0,\N,Missing
2009.iwslt-evaluation.13,P02-1038,0,\N,Missing
2009.iwslt-evaluation.13,2008.iwslt-evaluation.13,0,\N,Missing
2011.mtsummit-papers.29,J07-2003,0,0.477581,"Missing"
2011.mtsummit-papers.29,N09-1025,0,0.0737627,"Missing"
2011.mtsummit-papers.29,P10-1146,0,0.0331773,"to-tree rule. The tree-to-tree style SPMT algorithm used in our experiments can be described as follows: for each phrase pair, traverse the source and target parsing tree bottom up until it finds a node that subsumes the corresponding phrase respectively, then we can extract a rule whose roots are the nodes just found and the leaf nodes are the phrases. However, even with GHKM and SPMT rules, the rule coverage is still very low since tree-to-tree model requires that both source side and target side of its rule must be a subtree of the parsing tree. With this hard constraint (Liu et al., 2009; Chiang, 2010), the model would lose a large amount of bilingual phrases which are very useful to the translation process (DeNeefe et al., 2007). Eng Chn tree non-tree total Figure 1. An example of Chinese-English tree pair. 2.1 Limitations on Tree-to-tree Rule Extraction To extract all valid tree-to-tree rules, (Liu et al., 2009) extends the famous tree-to-string rule extraction algorithm GHKM (Galley et al., 2004) to their forest-based tree-to-tree model. However, only with GHKM rules, the rule coverage is very low1. As SPMT rules (Marcu et al., 2006) have proven to be a good complement to GHKM (DeNeefe e"
2011.mtsummit-papers.29,D07-1079,0,0.0557755,"air, traverse the source and target parsing tree bottom up until it finds a node that subsumes the corresponding phrase respectively, then we can extract a rule whose roots are the nodes just found and the leaf nodes are the phrases. However, even with GHKM and SPMT rules, the rule coverage is still very low since tree-to-tree model requires that both source side and target side of its rule must be a subtree of the parsing tree. With this hard constraint (Liu et al., 2009; Chiang, 2010), the model would lose a large amount of bilingual phrases which are very useful to the translation process (DeNeefe et al., 2007). Eng Chn tree non-tree total Figure 1. An example of Chinese-English tree pair. 2.1 Limitations on Tree-to-tree Rule Extraction To extract all valid tree-to-tree rules, (Liu et al., 2009) extends the famous tree-to-string rule extraction algorithm GHKM (Galley et al., 2004) to their forest-based tree-to-tree model. However, only with GHKM rules, the rule coverage is very low1. As SPMT rules (Marcu et al., 2006) have proven to be a good complement to GHKM (DeNeefe et al., 2007), we also extract full lexicalized SPMT rules to improve the rule coverage. 1 (Liu et al., 2009) investigate how many"
2011.mtsummit-papers.29,P05-1067,0,0.0942653,"ize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure respectively. Towards rule extraction, we found that in our training corpus th"
2011.mtsummit-papers.29,P03-2041,0,0.244471,"ation to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure respectively. Towards rule extraction, we found that in"
2011.mtsummit-papers.29,N04-1035,0,0.203798,"Missing"
2011.mtsummit-papers.29,P06-1121,0,0.104779,"oding. In this paper we propose two simple but effective approaches to overcome the constraints: utilizing fuzzy matching and category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons w"
2011.mtsummit-papers.29,P07-1019,0,0.10325,"Missing"
2011.mtsummit-papers.29,W06-3601,0,0.356544,"ting to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure"
2011.mtsummit-papers.29,D10-1014,0,0.0114776,"4. Rule(c) is an example of source phrase after transformation. When translating the tree structure, match the rule’s category with the head node and match the rule’s words with the terminal nodes of the structure. In the figure, if we do exact match between categories, rule(c) cannot be used yet. Normally, if we do exact match, rule(c) in Fig.4 will not be employed due to the mismatch between categories of rule and tree structure. Hence, to maximize the capacities of the source phrases, we utilize fuzzy matching method which has been successfully employed in hierarchical phrase-based model (Huang et al., 2010) and string-to-tree model (Zhang et al., 2011b) to match categories. With fuzzy matching method, we represent each SAMT-style syntactic category with a real-valued G vector F ( c ) using latent syntactic distribution. Due to the space limitation, here we ignore the details as we just follow the work of (Huang et al., 2010; Zhang et al., 2011b). Then the degree of syntactic similarity between two categories can be simply computed by dot-product: JG JG F ( c ) ⋅ F ( c &apos;) = ¦ f i ( c ) fi ( c &apos;) (2) 1≤ i ≤ n which yields a similarity score ranging from 0 (totally syntactic different) to 1 (totall"
2011.mtsummit-papers.29,P07-2045,0,0.00738743,"number of unmatchable nodes is reduced from 2990 to 2422. This is the contribution of fuzzy matching method of source phrase rules. With tree binarization, many unmatchable nodes are eliminated, as we can see, from 2990 to 2487, among which only 49 nodes are created by binarization. When we combine the two approaches, the number of unmatchable nodes decreases further (1957 unmatchable nodes), indicating that both bilingual phrases and binarization can help to alleviate the exact matching constraint and enlarge the search space. 4.3 Tree-to-tree vs. State-of-the-art Systems We also ran Moses (Koehn et al., 2007) with its default settings using the same data and obtained BLEU score of 32.35 and 30.03 on MT04 and MT05 respectively. Our best results are 34.50 and 31.37 on the two test sets which are significant better than Moses. 5 Conclusion and Future Work To overcome the limitations in rule extraction and decoding procedure of tree-to-tree model, this paper proposed two simple but effective approaches to integrate bilingual phrases and binarize the bilingual parsing trees. The experiments have shown that the approaches yield dramatic improvements over conventional tree-to-tree systems. Furthermore, o"
2011.mtsummit-papers.29,W04-3250,0,0.461991,"Missing"
2011.mtsummit-papers.29,P06-1077,0,0.767189,"d category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction an"
2011.mtsummit-papers.29,W06-1606,0,0.101091,"we propose two simple but effective approaches to overcome the constraints: utilizing fuzzy matching and category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree mode"
2011.mtsummit-papers.29,P03-1021,0,0.0344325,"Missing"
2011.mtsummit-papers.29,P02-1040,0,0.0821005,"Missing"
2011.mtsummit-papers.29,P06-1055,0,0.360707,"Missing"
2011.mtsummit-papers.29,P05-1034,0,0.177764,"Missing"
2011.mtsummit-papers.29,P08-1066,0,0.0400797,"e but effective approaches to overcome the constraints: utilizing fuzzy matching and category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfact"
2011.mtsummit-papers.29,D07-1078,0,0.337553,"Missing"
2011.mtsummit-papers.29,J10-2004,0,0.0977337,"Missing"
2011.mtsummit-papers.29,N10-1016,0,0.111775,"Missing"
2011.mtsummit-papers.29,P11-1084,0,0.188253,"Missing"
2011.mtsummit-papers.29,D09-1108,0,0.0237871,"Missing"
2011.mtsummit-papers.29,D11-1019,1,0.661239,"Missing"
2011.mtsummit-papers.29,2007.mtsummit-papers.71,0,0.141741,"Missing"
2011.mtsummit-papers.29,P08-1064,0,0.626595,"ments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure respectively. Towards rule extraction, we found that in our training corpus the bilingual phrases that tree-totree mod"
2011.mtsummit-papers.29,W06-3119,0,0.0851968,"ntax structure like treesequence based model. However, our analysis has shown that exact match would do harm to the translation quality. Thus instead of syntax structures, we decorate the source phrases with proper syntactic categories which have been proven to be necessary and effective for translation (Zhang et al., 2011b). When decoding with these source phrases, we ignore the internal structure of the subtree for translation and only match the rule’s category with root node of the subtree along with the matching between leaf nodes, just as shown in Fig.4. Here we utilize the SAMT grammar (Zollmann and Venugopal, 2006), with which each phrase can be associated with a corresponding syntactic category. For example, in Fig.1 the source span “ Ҫ Ӏ 䅼䆎 ℸџ” does not correspond to a subtree, 264 Figure 4. Rule(c) is an example of source phrase after transformation. When translating the tree structure, match the rule’s category with the head node and match the rule’s words with the terminal nodes of the structure. In the figure, if we do exact match between categories, rule(c) cannot be used yet. Normally, if we do exact match, rule(c) in Fig.4 will not be employed due to the mismatch between categories of rule and"
2011.mtsummit-papers.29,N06-1033,0,\N,Missing
2011.mtsummit-papers.29,P08-1023,0,\N,Missing
2011.mtsummit-papers.29,P09-1063,0,\N,Missing
2011.mtsummit-papers.29,W06-1628,0,\N,Missing
2020.aacl-main.1,W16-3415,0,0.0144565,"ed approach, Gonz´alez-Rubio et al. (2016); Cheng et al. (2016) introduce interaction methods that allow users to correct errors at arbitrary position in a machine hypothesis, while Weng et al. (2019) also preventing repeat mistakes by memorizing revision actions. Hokamp and Liu (2017) propose grid beam search to incorporate lexical constraints like words and phrases provided by human translators and force the constraints to appear in hypothesis. Recently, some researchers resort to more flexible interactions, which only require mouse click or touch actions. For example, Marie and Max (2015); Domingo et al. (2016) propose interactive translation methods which ask user to select correct or incorrect segments of a translation with mouse only. Similar to our work, Grangier and Auli (2018) propose a mouse based interactive method which allows users to simply mark the incorrect words in draft machine hypotheses and expect the system to generate refined translations. Herbig et al. (2019, 2020) propose a multi-modal interface for post-editors which takes pen, touch, and speech modalities into consideration. The protocol that given an initial translation to generate a refined translation, is also used in polis"
2020.aacl-main.1,J09-1002,0,0.748515,"Missing"
2020.aacl-main.1,2012.amta-papers.22,0,0.0154104,"tic post-editing (APE) task (Lagarda et al., 2009; Pal et al., 2016). The idea of multi-source encoder is also widely used in the field of APE research (Chatterjee et al., 2018, 2019). In human-machine interaction scenarios, the human feedback is used as extra information in polishing process. Related Work Post-editing is a pragmatic method that allows human translators to directly correct errors in draft machine translations (Simard et al., 2007). Comparing to purely manual translation, it achieves higher productivity while maintaining the human translation quality (Plitt and Masselot, 2010; Federico et al., 2012). Many notable works introduce different levels of human-machine interactions in post-editing. Barrachina et al. (2009) propose a prefix-based interactive method which enable users to correct the first translation error from left to right in each iteration. 6 Conclusion In this paper, we propose Touch Editing, a flexible and effective interaction approach which allows human translators to revise machine translation results via touch actions. The actions we introduce can be provided with gestures like tapping, panning, swiping or long pressing on touch screens to represent human editing intenti"
2020.aacl-main.1,D18-1048,0,0.0181626,"o select correct or incorrect segments of a translation with mouse only. Similar to our work, Grangier and Auli (2018) propose a mouse based interactive method which allows users to simply mark the incorrect words in draft machine hypotheses and expect the system to generate refined translations. Herbig et al. (2019, 2020) propose a multi-modal interface for post-editors which takes pen, touch, and speech modalities into consideration. The protocol that given an initial translation to generate a refined translation, is also used in polishing mechanism in machine translation (Xia et al., 2017; Geng et al., 2018) and automatic post-editing (APE) task (Lagarda et al., 2009; Pal et al., 2016). The idea of multi-source encoder is also widely used in the field of APE research (Chatterjee et al., 2018, 2019). In human-machine interaction scenarios, the human feedback is used as extra information in polishing process. Related Work Post-editing is a pragmatic method that allows human translators to directly correct errors in draft machine translations (Simard et al., 2007). Comparing to purely manual translation, it achieves higher productivity while maintaining the human translation quality (Plitt and Masse"
2020.aacl-main.1,K16-1020,0,0.0429466,"Missing"
2020.aacl-main.1,N18-1025,0,0.191393,"ssing at a particular position, and our method is expected to insert the correct word. To this end, we present a neural network model by augmenting Transformer (Vaswani et al., 2017) with an extra encoder for a hypothesis and its actions. Since it is impractical to manually annotate large-scale action dataset to train the model, we thereby adopt the algorithm of TER (Snover et al., 2006) to automatically extract actions from a draft hypothesis and its reference. To evaluate our method, we conduct simulated experiments on translation datasets the same as in other works (Denkowski et al., 2014; Grangier and Auli, 2018), The results demonstrate that our method can address the well-known challenging issues in machine translation including overWe propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on touch actions that users perform to indicate translation errors. We present a dual-encoder model to handle the actions and generate refined translations. To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions"
2020.aacl-main.1,2014.iwslt-evaluation.1,0,0.0223119,"Missing"
2020.aacl-main.1,D14-1130,0,0.186457,"We observe that the users with Touch Editing tends to correct an error for multiple times when the system cannot predict a word they want, while the users with keyboard input tends to modify more content of initial translation and spend more time on choosing words. We then conduct an unstructured interview on the usability of our method. The result of the interview shows that Touch Editing is convenient and intuitive but lack of ability of generating final accurate translation. It can be treated as a light-weight proofreading method, and suitable for Pre-Post-Editing (Marie and Max, 2015). 5 Green et al. (2014) implement a prefix-based interactive translation system and Huang et al. (2015) adopt the prefix constrained translation candidates into a novel input method for translators. Peris et al. (2017) further extend this idea to neural machine translation. The prefix-based protocol is inflexible since users have to follow the left-to-right order. To overcome the weakness of prefix-based approach, Gonz´alez-Rubio et al. (2016); Cheng et al. (2016) introduce interaction methods that allow users to correct errors at arbitrary position in a machine hypothesis, while Weng et al. (2019) also preventing r"
2020.aacl-main.1,W19-5402,0,0.0371464,"Missing"
2020.aacl-main.1,2020.acl-main.155,0,0.324106,"Missing"
2020.aacl-main.1,P16-2046,0,0.0266837,"Missing"
2020.aacl-main.1,P17-1141,0,0.285801,"to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–11 c December 4 - 7, 2020. 2020 Association for C"
2020.aacl-main.1,P18-2052,0,0.0167696,"our method significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Confe"
2020.aacl-main.1,D10-1092,0,0.0679881,"Missing"
2020.aacl-main.1,2016.amta-researchers.9,0,0.152262,"on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–11 c December 4 - 7, 2020."
2020.aacl-main.1,N09-2055,0,0.0869955,"Missing"
2020.aacl-main.1,2006.amta-papers.25,0,0.553195,"d by users through various of gestures on touch screen devices. By using these actions, our method is able to capture the editing intention from users to generate better translations: for instance, INSERTION indicates a word is missing at a particular position, and our method is expected to insert the correct word. To this end, we present a neural network model by augmenting Transformer (Vaswani et al., 2017) with an extra encoder for a hypothesis and its actions. Since it is impractical to manually annotate large-scale action dataset to train the model, we thereby adopt the algorithm of TER (Snover et al., 2006) to automatically extract actions from a draft hypothesis and its reference. To evaluate our method, we conduct simulated experiments on translation datasets the same as in other works (Denkowski et al., 2014; Grangier and Auli, 2018), The results demonstrate that our method can address the well-known challenging issues in machine translation including overWe propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on touch actions that users perform to indicate translation errors. We presen"
2020.aacl-main.1,D15-1166,0,0.0602627,"9.47 56.47 0.48 0.39 0.37 0.24 32.53 41.20 38.96 57.84 0.55 0.43 0.42 0.28 21.89 29.78 29.17 45.67 0.61 0.51 0.51 0.33 Model WMT’17 EN-ZH ZH-EN BLEU TER BLEU TER Table 1: Results of different systems measured in BLEU and TER. † denotes the results from Quick Edit. QuickEdit‡ is our reimplementation based on Transformer. Touch baseline is the result modified from initial hypothesis by deleting and reordering words. Touch Editing is our model trained with all actions described in Section 2.1. tains 4698 sentence pairs. For WMT’14 EnglishGerman dataset, we use the same data and preprocessing as (Luong et al., 2015). The dataset consists of 4.5M sentence pairs for training1 . We take newstest2013 for validation and newstest2014 for testing. For Chinese to English dataset, we use CWMT portion which is a subset of WMT’17 training data containing 9M sentence pairs. We validate on newsdev2017 and test on newstest2017. As for vocabulary, the English and German datasets are encoded using byte-pair encoding (Sennrich et al., 2015) with a shared vocabulary of 8k tokens for IWSLT’14 and 32k tokens for WMT’14. For Chinese to English dataset, the English vocabulary is set to 30k subwords, while the Chinese data is"
2020.aacl-main.1,D18-1458,0,0.0281639,"Missing"
2020.aacl-main.1,D15-1120,0,0.124853,"or TED talks (IWSLT). However in real world, data may be from any other domains. For model inconsistency, we use Transformer to build our training data while the translation model used 4.5 Discussion on Real Scenarios So far, the experiments we conducted are based on simulated human feedbacks, in which the actions are extracted from initial machine translation results and their corresponding references to simulate human editing actions. Thus in our simulated setting, the references are used in inference phase to simulate human behavior, as in other interaction methods (Denkowski et al., 2014; Marie and Max, 2015; Grangier and Auli, 2018). These experiments show that our method can significantly 7 improve the initial translation with similated actions. However, whether the actions are convenient to perform is a key point in real applications. To investigate the usability and applicable scenarios of our method, we implement a real mobile application on iPhone, in which the actions can be performed on multi-touch screens. For a given source sentence, the application provides an initial machine translation. The text area of translation can response to several gestures 4 : Tap indicated a missing word sho"
2020.aacl-main.1,W18-6304,0,0.0371598,"Missing"
2020.aacl-main.1,W18-6471,0,0.0192956,"- (3) Where P E∗ denote the action positional embedding matrixes in Figure 3. The learned action positional embedding is used in hypothesis encoder to replace the fixed sinusoids positional encoding in Transformer encoder. Next, the encoder adds the word embedding w and the action positional embedding p to obtain input embedding e = {w1 + p1 , · · · , wl + pl }. The following part of hypothesis encoder lies the same as Transformer encoder. P (yn |y<n , x, m(y0 ), a; θ). (2) n=1 As shown in Figure 2, the neural network model we developed is a dual encoder model based on Transformer similar to Tebbifakhr et al. (2018). Specifically, besides encoding the source sentence x with source encoder (the left part of Figure 2), our model additionally encodes A(y0 ) with an extra hypothesis encoder (the right part of Figure 2) and integrates the encoded representations into decoding network using dual multi-head attention. Decoding The output of hypothesis encoder, together with the output of source encoder, are fed into the decoder. To combine both of the encoders’ outputs, we apply dual multi-head attention in each layer of decoder: the attention sub-layer attends to both encoders’ outputs by performing multi-head"
2020.aacl-main.1,P16-1008,0,0.0205888,"ng, jjzhang, cqzong}@nlpr.ia.ac.cn, {redmondliu, donkeyhuang}@tencent.com Abstract forts, QuickEdit, in which users are asked to simply mark incorrect words in a translation hypothesis for one time in the hope that the system will change them. QuickEdit delivers appealing improvements on draft hypotheses while maintaining the flexibility of human-machine interaction. Unfortunately, only marking incorrect words is far from adequate: for example, it does not indicate the missing information beyond the original hypothesis, which is a typical issue called under-translation in machine translation (Tu et al., 2016). In this paper, we propose a novel one-time interaction method called Touch Editing, which is flexible for users and more adequate for a system to generate better translations. Inspired by human editing process, the proposed method relies on a series of touch-based actions including SUBSTITU TION , DELETION , INSERTION and REORDERING . These actions do not include lexical information and thus can be flexibly provided by users through various of gestures on touch screen devices. By using these actions, our method is able to capture the editing intention from users to generate better translatio"
2020.aacl-main.1,P16-1007,0,0.142257,"so conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–1"
2020.acl-main.121,D16-1250,0,0.0196286,"the mean value over the heads as follow: 1X h αt = αt (4) h h Translate. With the attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next, the lexicon translation probability P L (w1 ⇒ w2"
2020.acl-main.121,P18-1073,0,0.0188278,"on on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next, the lexicon translation probability P L (w1 ⇒ w2 ) is the average of source-to-target and target-to-source probabilities calculated through maximum lik"
2020.acl-main.121,N16-1012,0,0.0292305,"ach the cross-lingual system. Zhu et al. (2019) propose to acquire largescale datasets via a round-trip translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to bot"
2020.acl-main.121,P15-1166,0,0.0319216,"3,000 test pairs. Both the English-to-Chinese and Chinese-toEnglish test sets are manually corrected. 4.2 • TNCLS: It denotes the Transformer-based neural cross-lingual summarization system. The above methods only employ the cross-lingual summarization dataset, and we also compare our method with the following two methods (Zhu et al., 2019) that use extra datasets in other tasks. • CLSMS: It refers to the multi-task method, which simultaneously trains cross-lingual summarization and monolingual summarization. • CLSMT: It is the multi-task method which adopts the alternating training strategy (Dong et al., 2015) to train cross-lingual summarization and machine translation jointly. Experimental Settings We follow the setting of the vocabularies described in Zhu et al. (2019). In En2ZhSum, we surround each target sentence with tags “<t>” and “</t>”. If there is no special explanation, the limit on the number of translation candidate m in our models is set to 10. All the parameters are initialized via Xavier initialization method (Glorot and Bengio, 2010). We train our models using configuration transformer base (Vaswani et al., 2017), which contains a 6-layer encoder and a 6-layer decoder with 512-dime"
2020.acl-main.121,P19-1305,0,0.483224,"tion can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets. They incorporate machine translation and monolingual summarization into the training of cross-lingual summarization using multi-task learning to improve the summary quality with a quite promising performance. However, we find that there exist the following problems: (1) The multi-task methods adopt extra large-scale parallel data from other related tasks, suc"
2020.acl-main.121,N13-1073,0,0.0196664,"Since αth is a multi-head attention, we take the mean value over the heads as follow: 1X h αt = αt (4) h h Translate. With the attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next,"
2020.acl-main.121,P16-1154,0,0.0283382,"eural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lingual summaries, we introduce a novel model in this paper, which integrates the operation of attending, translating, and summarizing. 6 Conclusion and Future Work In this paper, we present a novel method consistent with the translation pattern in the pr"
2020.acl-main.121,C18-1121,1,0.846956,"uage model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lingual summaries, we introduce a novel model in this paper, which integrates the operation of attending, translating, and summarizing. 6 Conclusion and Future Work In this paper, we present a novel method consistent with the translation pattern in the process of obtaining"
2020.acl-main.121,W04-1013,0,0.0333936,"Missing"
2020.acl-main.121,D16-1031,0,0.0156619,"summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lin"
2020.acl-main.121,K16-1028,0,0.0293629,"translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network"
2020.acl-main.121,orasan-chiorean-2008-evaluation,0,0.266832,"me words in the summary are translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets."
2020.acl-main.121,N19-1204,0,0.452155,"le also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets. They incorporate machine translation and monolingual summarization into the training of cross-l"
2020.acl-main.121,D15-1044,0,0.042495,"the target-side counterparts. Recently, end-to-end methods have been applied to cross-lingual summarization. Due to the lack of supervised training data, Ayana et al. (2018) and Duan et al. (2019) focus on zero-shot training methods that use machine translation or monolingual summarization or both to teach the cross-lingual system. Zhu et al. (2019) propose to acquire largescale datasets via a round-trip translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a po"
2020.acl-main.121,P17-1099,0,0.236716,"s from the translation candidates of the source text. The final distribution is obtained by the weighted sum (weighed by ptrans ) of the neural distribution PN and the translation distribution PT . Best viewed in color. method. It first attends to some source words, then obtains the translation candidates of them, and finally generates words from the translation candidates or the neural distribution. Our proposed method is a hybrid between Transformer and an additional translation layer, which is depicted in Figure 2 and described as follows. Attend. Inspired by the pointer-generator network (See et al., 2017), we employ the encoderdecoder attention distribution αth (the last layer) to help focus on some salient words in the source text. Since αth is a multi-head attention, we take the mean value over the heads as follow: 1X h αt = αt (4) h h Translate. With the attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bili"
2020.acl-main.121,P16-1162,0,0.060767,"t “subword-subword” and “word-character” segmentation granularities in Zh2En and En2Zh models, respectively. Unit pmacro trans pmicro trans rmacro rmicro Zh2En Zh2En sw-sw w-w 21.41 21.17 20.71 20.46 21.86 21.90 21.00 21.05 En2Zh w-c 14.91 14.84 14.27 14.05 Model Unit RG-1 RG-2 RG-L MVS Task TNCLS w-w sw-sw 37.70 38.85 21.15 21.93 34.05 35.05 19.43 19.07 ATS-A w-w sw-sw 39.65 40.68 23.79 24.12 36.05 36.97 22.06 22.15 Table 5: Results of models on Zh2EnSum with different segmentation granularities. Unit represents the granularity combination of text units. w and sw denote “word” and “subword” (Sennrich et al., 2016), respectively. The improvement of all ATS models over TNCLS is statistically significant (p < 0.01). more aggressive value 1. The results are presented in Table 4. In Zh2En experiment, the ATS-A (m=5) performs best while ATS-A (m=1) performs comparably with ATS-A (m=10). In En2Zh experiment, the ATS-A (m=5) performs comparably with ATSA (m=10) while the performance drops a bit when m=1. The above results illustrate that (1) A slightly larger m enables the model to learn when to search for translation candidates from the source words and which ones to choose, leading to improve the quality of"
2020.acl-main.121,D16-1112,0,0.0204318,"acquire largescale datasets via a round-trip translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization"
2020.acl-main.121,P11-1155,0,0.271514,"English. Wan et al. (2010) apply the summarizethen-translate scheme to English-to-Chinese crosslingual summarization, which extracts English sentences considering both the informativeness and translation quality of sentences and automatically translates the English summary into Chinese. They also argue that summarize-then-translate is better, since it can alleviate both the computational expense of translating sentences and sentence extraction errors caused by incorrect translations. There have been some researches focusing on improving cross-lingual summarization with bilingual information. Wan (2011) translates the English document into Chinese and extracts sentences based on the original English sentences and Chinese translation. Yao et al. (2015) propose a compressive method which calculates the sentence scores based on the aligned bilingual phrases obtained by machine translation service and performs compression via deleting redundant or poorly translated phrases. Zhang et al. (2016) introduce an abstractive method that constructs a pool of bilingual concepts represented by the bilingual elements of the source-side predicate-argument structures and the target-side counterparts. Recentl"
2020.acl-main.121,P10-1094,0,0.8245,"translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets. They incorporate m"
2020.acl-main.121,D15-1012,0,0.278288,"Missing"
2020.acl-main.121,P17-1179,0,0.0313971,"attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next, the lexicon translation probability P L (w1 ⇒ w2 ) is the average of source-to-target and target-to-source probabilities calcula"
2020.acl-main.121,D19-1053,0,0.0357714,"Missing"
2020.acl-main.121,P17-1101,0,0.0131077,"ng multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lingual summaries, we introduce a novel model in this paper, which integrates th"
2020.acl-main.121,D19-1302,1,0.889956,"or from the translation candidates of source words. Experimental results on Chinese-toEnglish and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art. 1 10 Input (Chinese) 10 Output (English) —Foshan —young couple —train tickets —10 yuan —detained —migrant workers Translation Table Foshan young couple was detained for charging 10 yuan for buying train tickets online for migrant workers Figure 1: An example of the translation pattern in a sample extracted from Zh2EnSum (Zhu et al., 2019) which is a Chinese-to-English cross-lingual summarization dataset. It shows that some words in the summary are translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarizati"
2020.autosimtrans-1.4,J82-2005,0,0.674208,"Missing"
2020.autosimtrans-1.4,W17-4123,0,0.30461,"slightly lower than 24 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 24–29 c July 10, 2020. 2020 Association for Computational Linguistics Softmax mechanism (Vaswani et al., 2017). Softmax N N Feed-Forward Feed-Forward Enc-NAD Cross-Attention Enc-AD Cross-Attention Feed-Forward Position Attention NAD-AD Cross-Attention Self-Attention Unmask SelfAttention Mask SelfAttention Source Embedding Copied Source Embedding Target Embedding 2.2 We initialize the non-autoregressive decoder inputs using copied source inputs from the encoder side by the fertility mechanism (Gu et al., 2017). For each layer in non-autoregressive decoder, the lowest sublayer is the unmasked multi-head self-attention network, and it also uses residual connections around each of the sublayers, followed by layer normalization. N z1l = LN(z l−1 + MHAtt(z l−1 , z l−1 , z l−1 )) (2) Figure 2: The extended Transformer translation model that exploits global information produced by NAT. We omit the residual connection and layer normalization in each sub-layer for simplicity. The second sub-layer is a positional attention. We follow (Gu et al., 2017) and use the positional encoding p as both query and key"
2020.autosimtrans-1.4,D16-1139,0,0.0595642,"Missing"
2020.autosimtrans-1.4,D18-1149,0,0.198734,"en the input and output languages: sl3 = LN(sl2 + MHAtt(sl2 , hN , hN )) e hl = LN(hl−1 + MHAtt(hl−1 , hl−1 , hl−1 )) (1) hl = LN(e hl + FFN(e hl )) sl = LN(sl3 + FFN(sl3 )) 2.4 where the superscript l indicates layer depth, hl denotes the source hidden state of l-th layer, LN is layer normalization, FFN means feed-forward networks, and MHAtt denotes the multi-head attention (7) Training and Inference Given a set of training examples {x(z) , y (z) }Z z=1 , the training algorithm aims to find the model parameters that maximize the likelihood of the training 25 # System 1 2 3 (Gu et al., 2017) (Lee et al., 2018) (Wang et al., 2019) 4 5 6 7 (Wu et al., 2016) (Gehring et al., 2017) (Vaswani et al., 2017) (Xia et al., 2017) 8 9 10 this work Architecture En⇒De Existing NAT Systems NAT 17.35 NAT-IR (adaptive) 18.91 NAT-AR 20.61 Existing AT Systems Google-NMT 24.60 ConvS2S 26.36 Transformer 27.30 Deliberate Network 27.56 Our NMT Systems Transformer 27.06 NAT 21.25 Our Model 27.65↑ En⇒Ro De⇒En 26.22 - 23.89 33.18 33.95 32.28 26.60 33.17⇑ 32.87 27.06 34.01⇑ Table 1: Comparing with existing NMT systems on WMT14 En⇒De, WMT16 En⇒Ro, and IWSLT14 De⇒En test sets. “↑/⇑” indicates statistically significant (p<0.05/"
2020.autosimtrans-1.4,D18-1336,0,0.0285778,"Missing"
2020.autosimtrans-1.4,P05-1066,0,0.366946,"Missing"
2020.autosimtrans-1.4,P02-1040,0,0.108208,"l as Vaswani et al. (2017), whose encoder and decoder both have 6 layers, 8 attention-heads, and 512 hidden sizes. We follow Gu et al. (2017) to use the same small Transformer setting for IWSLT14 because of its smaller dataset. For evaluation, we use argmax decoding for NAD, and beam search with a beam size of k=4 and length penalty α=0.6 for AD. We also re-implement and compare with deliberate network (Xia et al., 2017) based on strong Transformer, which adopts the two-pass decoding method and uses the autoregressive decoding manner for the first decoder. Experiments We use 4-gram NIST BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. 3.1 Model Settings Datasets We conduct experiments on three widely used public machine translation corpora: WMT14 English-German2 (En⇒De), WMT16 EnglishRomanian3 (En⇒Ro), and IWSLT14 GermanEnglish4 (De⇒En), whose training sets consist of 4.5M, 600K, 153K sentence pairs, respectively. We employ 37K, 40K, and 10K shared BPE (Sennrich et al., 2016) tokens for En⇒De, En⇒Ro, and De⇒en respectively. For En⇒De, 2 http://www.statmt.org/wmt14/translation-task.html http://www.statmt.org/wmt16/translatio"
2020.autosimtrans-1.4,P16-1162,0,0.0917784,"Missing"
2020.autosimtrans-1.4,D18-1048,0,0.0481522,"Missing"
2020.coling-main.318,D15-1076,0,0.0286831,"subsection but based on the real examples of the Ti . The outer-phase method is slower than the inner-phase one, but higher compression quality can be expected. 4 Experimental Setup 4.1 Tasks and Datasets Following the setting of Sun et al. (2020), we select five different tasks from decaNLP. We use Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)) for question answering task, Stanford Sentiment Treebank (SST, Radford et al. (2017)) for sentiment analysis, WikiSQL (Zhong et al. (2017)) for Semantic Parsing, English Wizard of Oz (WOZ) for task-oriented dialogue, and QA-SRL (He et al., 2015) for semantic role labeling. As shown in Table 1, the samples of these tasks are framed into the scheme of SQuAD by decaNLP. The setting of continually learning tasks of different types is challenging. To conduct a fair evaluation, we also compare models on learning tasks of the same type but from different domains in a sequence. We follow de Masson d’Autume et al. (2019)’s setting to use Zhang et al. (2015)’s collection of five text classification tasks, as briefed in Table 2. 4.2 Baselines We include the following baselines in the evaluation. All the baselines are based on GPT2 with 12 hidde"
2020.coling-main.318,D16-1264,0,0.0286672,"task Ti . In this compression phase, the distillation is not only for transferring knowledge across tasks, but also for producing a compressed copy that mimics the behavior of the initial model. So we use the distillation methods proposed in the previous subsection but based on the real examples of the Ti . The outer-phase method is slower than the inner-phase one, but higher compression quality can be expected. 4 Experimental Setup 4.1 Tasks and Datasets Following the setting of Sun et al. (2020), we select five different tasks from decaNLP. We use Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)) for question answering task, Stanford Sentiment Treebank (SST, Radford et al. (2017)) for sentiment analysis, WikiSQL (Zhong et al. (2017)) for Semantic Parsing, English Wizard of Oz (WOZ) for task-oriented dialogue, and QA-SRL (He et al., 2015) for semantic role labeling. As shown in Table 1, the samples of these tasks are framed into the scheme of SQuAD by decaNLP. The setting of continually learning tasks of different types is challenging. To conduct a fair evaluation, we also compare models on learning tasks of the same type but from different domains in a sequence. We follow de Masson d"
2020.coling-main.318,D18-1173,1,0.818923,"i et al., 2019). However, the currently dominant paradigm of machine language learning is still training a model on a static dataset to achieve satisfactory performance on that particular task (Wang et al., 2018; Ostapenko et al., 2019). Most of these methods, especially the deep neural network-based ones, do not fare well in the continual learning scenarios. In continual learning, a model is required to fit on a stream of tasks where data distribution may not be uniform. For example, a network fitted on a first task tends to forget how to perform on it after trained on a sequential new task (Sun et al., 2018; Shin et al., 2017). This problem, namely catastrophic forgetting, poses a severe challenge in building a general language intelligent system with lifelong learning capacity. Efforts have been made in recent years to overcome the catastrophic forgetting of deep neural networks. There are mainly two stretches of methods, differentiated by the way of isolating and reusing the accumulated knowledge. One stretch of method works by reproducing the data distribution of witnessed tasks. Some members of this family select and store informative samples in an explicit memory module. The memorized sampl"
2020.coling-main.318,D18-1011,1,0.834186,"d effective in results, promoting the empirical application of continual language learning. We also hope that DnR could contribute to building human-level language intelligence that is no longer bothered by catastrophic forgetting. 2 Related Work Tackling new tasks without necessarily losing the knowledge learned in previous tasks is one fundamental requirement for a human-like intelligent system (Parisi et al., 2019). However, the currently dominant paradigm of machine language learning is still training a model on a static dataset to achieve satisfactory performance on that particular task (Wang et al., 2018; Ostapenko et al., 2019). Most of these methods, especially the deep neural network-based ones, do not fare well in the continual learning scenarios. In continual learning, a model is required to fit on a stream of tasks where data distribution may not be uniform. For example, a network fitted on a first task tends to forget how to perform on it after trained on a sequential new task (Sun et al., 2018; Shin et al., 2017). This problem, namely catastrophic forgetting, poses a severe challenge in building a general language intelligent system with lifelong learning capacity. Efforts have been m"
2020.coling-main.397,D16-1162,0,0.0187155,"entities in the sentences. Moussallem et al. (2019) exploit the entity linking to disambiguate the entities found in a sentence. While these studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel s"
2020.coling-main.397,P16-1160,0,0.021606,"affects on PER, PLC, Reason, and Anatomy. We think there may be two reasons to cause this phenomenon: i) During translating these entities, KG is unnecessary7 . ii) The KG we utilize in this paper does not cover the useful knowledge and semantic information which can improve the translation of these entities. 6.3 Comparison on Different Granularities In this paper, we split the entities into sub-entity granularity. Actually, besides the sub-entity granularity, we also evaluate the other fine granularities: hybrid word-character granularity (Luong and Manning, 2016) and character granularity (Chung et al., 2016). The results are reported in Table 5. The results show that in both Transformer and our proposed multi-task method, the sub-entity granularity can produce better results than hybrid word-character granularity and character granularity. 7 Take the entity PER as an example, assuming that the neural model tends to translate a person’s name, while the KG always contains knowledge on his/her occupation, age or education, etc. Intuitively, this knowledge is not benefit for the translation of a person’s name. 4502 Model TransE TransH Transformer(sub-entity) Transformer(sub-entity)+MT Appeared Head E"
2020.coling-main.397,P19-1294,0,0.0660377,"dling the entities. 1 Introduction Neural machine translation (NMT) based on the encoder-decoder architecture becomes a new state-ofthe-art approach due to its distributed representation and end-to-end learning (Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). During translation, the translation quality of the entities in a sentence has a great influence on the translation quality of the whole sentence. However, translating these entities remains challenging (Moussallem et al., 2019) and various methods are proposed to improve the translation of entities (Zhang and Zong, 2016; Dinu et al., 2019; Ugawa et al., 2018; Wang et al., 2019). Among them, some methods aim at incorporating the knowledge graph (KG) to utilize their structured knowledge on entities and improve the entity translation. These studies utilize KG to enhance the semantic representing of entities in a sentence (Moussallem et al., 2019; Lu et al., 2018) or extract the important semantic vectors with KG (Shi et al., 2016). Although great efforts have been made to incorporate KG into NMT, we find the existing methods have the following two problems: Knowledge Under-utilization: Given a KG (denoted by K) and a parallel se"
2020.coling-main.397,D17-1146,0,0.0175299,"l. (2019) exploit the entity linking to disambiguate the entities found in a sentence. While these studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity"
2020.coling-main.397,N18-2081,0,0.0475228,"Missing"
2020.coling-main.397,P19-1581,0,0.0186415,"studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to improve the semantic represent of sub-entity and parameter"
2020.coling-main.397,P19-1352,0,0.013042,"on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to improve the semantic represent of sub-entity and parameters in encoder or de"
2020.coling-main.397,P16-1100,0,0.121946,"U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on the K ∩ D entities, the other three subsets are ignored. Consequently, much knowledge on the other three subsets in KG is wasted. Granularity Mismatch: The current KG methods, such as knowledge embedding methods (Bordes et al., 2013; Wang et al., 2014) and knowledge reasoning methods (Xiong et al., 2017), always utilize the entity as the basic granularity. While the NMT models utilize the sub-word (Sennrich et al., 2016) or hybrid word-character (Luong and Manning, 2016) as the translation granularity. This granularity mismatch between KG and NMT makes the current KG methods different to be utilized into NMT. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 4495 Proceedings of the 28th International Conference on Computational Linguistics, pages 4495–4505 Barcelona, Spain (Online), December 8-13, 2020 yansuan anfei latong yansuananfeilatong yansuanyipu component yansuanbenbing paitong latong （盐酸苯丙哌酮） （盐酸依普拉酮） anfeitaiming type (安非他命) U − (K yaopin (药品) D) D −K"
2020.coling-main.397,D15-1166,0,0.29435,"cally, we first split the entities in KG and sentence pairs into sub-entity granularity by using joint BPE. Then we utilize the multi-task learning to combine the machine translation task and knowledge reasoning task. The extensive experiments on various translation tasks have demonstrated that our method significantly outperforms the baseline models in both translation quality and handling the entities. 1 Introduction Neural machine translation (NMT) based on the encoder-decoder architecture becomes a new state-ofthe-art approach due to its distributed representation and end-to-end learning (Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). During translation, the translation quality of the entities in a sentence has a great influence on the translation quality of the whole sentence. However, translating these entities remains challenging (Moussallem et al., 2019) and various methods are proposed to improve the translation of entities (Zhang and Zong, 2016; Dinu et al., 2019; Ugawa et al., 2018; Wang et al., 2019). Among them, some methods aim at incorporating the knowledge graph (KG) to utilize their structured knowledge on entities and improve the entity translation. These studies"
2020.coling-main.397,P02-1040,0,0.109756,"sentence pairs on tourism as development set, and 2000 other sentence pairs as test set. The CN⇒UY translation are extracted from CCMT-19 dataset. The RO⇒EN translation are extracted from TED dataset. The statistics of training pairs and KGs are shown in Table 1. Training and Evaluation Details. We implement the NMT model based on the THUMT toolkit6 (Zhang et al., 2017). We use the “base” version parameters of the Transformer model. On all translation tasks, we use the BPE (Sennrich et al., 2016) method to merge 30K steps. We evaluate the final translation quality with case-insensitive BLEU (Papineni et al., 2002) for all translation tasks. 2 http://www.openkg.cn/dataset/cndbpedia http://www.openkg.cn/dataset/symptom-in-chinese 4 The target KG in medical and tourism KG is filtered by retaining the triples which contain the pre-defined key words. 5 http://www.openkg.cn/dataset/tourist-attraction 6 https://github.com/THUNLP-MT/THUMT 3 4499 Task CH⇒EN CH⇒UY RO⇒EN Domain General Medical Tourism General General Source KG 3.3M 0.41M 0.16M 3.3M - Target KG 2.4M 0.29M 0.28M 2.4M Pair 2.01M 0.22M 0.44M Dev/Test 919/6146 2000/2000 2000/2000 914/1678 1166/1160 Table 1: The statistics of the training data. Column"
2020.coling-main.397,P16-1162,0,0.737095,"n D; 3) K −D entities, which only appear in K; 4) U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on the K ∩ D entities, the other three subsets are ignored. Consequently, much knowledge on the other three subsets in KG is wasted. Granularity Mismatch: The current KG methods, such as knowledge embedding methods (Bordes et al., 2013; Wang et al., 2014) and knowledge reasoning methods (Xiong et al., 2017), always utilize the entity as the basic granularity. While the NMT models utilize the sub-word (Sennrich et al., 2016) or hybrid word-character (Luong and Manning, 2016) as the translation granularity. This granularity mismatch between KG and NMT makes the current KG methods different to be utilized into NMT. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 4495 Proceedings of the 28th International Conference on Computational Linguistics, pages 4495–4505 Barcelona, Spain (Online), December 8-13, 2020 yansuan anfei latong yansuananfeilatong yansuanyipu component yansuanbenbing paitong latong （盐酸苯丙哌酮） （盐酸依普拉酮） a"
2020.coling-main.397,P16-1212,0,0.0823736,"y of the whole sentence. However, translating these entities remains challenging (Moussallem et al., 2019) and various methods are proposed to improve the translation of entities (Zhang and Zong, 2016; Dinu et al., 2019; Ugawa et al., 2018; Wang et al., 2019). Among them, some methods aim at incorporating the knowledge graph (KG) to utilize their structured knowledge on entities and improve the entity translation. These studies utilize KG to enhance the semantic representing of entities in a sentence (Moussallem et al., 2019; Lu et al., 2018) or extract the important semantic vectors with KG (Shi et al., 2016). Although great efforts have been made to incorporate KG into NMT, we find the existing methods have the following two problems: Knowledge Under-utilization: Given a KG (denoted by K) and a parallel sentence pair dataset (denoted by D), the full entity set U can be divided into four subsets as shown in Fig. 1 (a): 1) K ∩ D entities, which appear in both K and D; 2) D −K entities, which only appear in D; 3) K −D entities, which only appear in K; 4) U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on"
2020.coling-main.397,2020.acl-main.325,0,0.0161976,"ies. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to improve the semantic represent of sub-entity and parameters in encoder or decoder. The proposed met"
2020.coling-main.397,D17-1060,0,0.168589,"bsets as shown in Fig. 1 (a): 1) K ∩ D entities, which appear in both K and D; 2) D −K entities, which only appear in D; 3) K −D entities, which only appear in K; 4) U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on the K ∩ D entities, the other three subsets are ignored. Consequently, much knowledge on the other three subsets in KG is wasted. Granularity Mismatch: The current KG methods, such as knowledge embedding methods (Bordes et al., 2013; Wang et al., 2014) and knowledge reasoning methods (Xiong et al., 2017), always utilize the entity as the basic granularity. While the NMT models utilize the sub-word (Sennrich et al., 2016) or hybrid word-character (Luong and Manning, 2016) as the translation granularity. This granularity mismatch between KG and NMT makes the current KG methods different to be utilized into NMT. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 4495 Proceedings of the 28th International Conference on Computational Linguistics, pages 4495–4505 Barcelona, Spain (Online), December 8-1"
2020.coling-main.397,2020.amta-research.11,0,0.0774918,"Missing"
2020.coling-main.397,D18-1036,1,0.844092,"disambiguate the entities found in a sentence. While these studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to"
2020.coling-main.496,W18-6402,0,0.0186341,"e highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely explored in machine translation (MT) (Calixto et al., 2017; Caglayan et al., 2017; Helcl et al., 2018; Gr¨onroos et al., 2018) in recent years, and the performances of their models surpass text-only models (Barrault et al., 2018). The main difference between multimodal MT and multimodal text summarization is: the model for MT is required to convert the same semantics from the input sentence and the paired image to the output, while for summarization, the model is expected to select the important information from the input. Li et al. (2018a) propose a hierarchical attention model for the multimodal sentence summarization task, while the image is not involved in the process of text encoding. Obviously, it will be easier for the decoder to generate This work is licensed under a Creative Commons Attribution 4.0 Internatio"
2020.coling-main.496,W17-4746,0,0.0458668,"Missing"
2020.coling-main.496,D17-1105,0,0.0235329,"ect-level visual sGate Figure 2: The framework of our model. We design visual selective gates including global-level, gridlevel and object-level visual gates to select salient encoding information. We also integrate the textual and visual selective gates to construct multimodal selective gates. In this figure, the summary is generated with the text-based decoder, and we also apply multimodal selective gates to the multimodal-based decoder (Li et al., 2018a) in our work. 2.2 Multimodal Seq2seq Models Libovick´y and Helcl (2017) propose multi-source seq2seq learning with hierarchical attention. Calixto and Liu (2017) use images as source words to improve translation quality. Delbrouck and Dupont (2017) adjust various attention for the visual modality. Calixto et al. (2017) propose attention mechanisms for textual and visual modalities and combine them to decode target words. Narayan et al. (2017) develop extractive summarization with side information including images and captions. Zhu et al. (2018), Chen and Zhuge (2018) and Zhu et al. (2020) propose to generate multimodal summary for multimodal news document. Li et al. (2018a) first introduce the multimodal sentence summarization task, and they propose a"
2020.coling-main.496,P17-1175,0,0.389415,"rce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely explored in machine translation (MT) (Calixto et al., 2017; Caglayan et al., 2017; Helcl et al., 2018; Gr¨onroos et al., 2018) in recent years, and the performances of their models surpass text-only models (Barrault et al., 2018). The main difference between multimodal MT and multimodal text summarization is: the model for MT is required to convert the same semantics from the input sentence and the paired image to the output, while for summarization, the model is expected to select the important information from the input. Li et al. (2018a) propose a hierarchical attention model for the multimodal sentence summarization task, while the image is not i"
2020.coling-main.496,D18-1438,0,0.0891796,"; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely expl"
2020.coling-main.496,N16-1012,0,0.201591,"highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015)"
2020.coling-main.496,N18-2097,0,0.0120218,"e between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi"
2020.coling-main.496,D17-1095,0,0.0234216,"ctive gates including global-level, gridlevel and object-level visual gates to select salient encoding information. We also integrate the textual and visual selective gates to construct multimodal selective gates. In this figure, the summary is generated with the text-based decoder, and we also apply multimodal selective gates to the multimodal-based decoder (Li et al., 2018a) in our work. 2.2 Multimodal Seq2seq Models Libovick´y and Helcl (2017) propose multi-source seq2seq learning with hierarchical attention. Calixto and Liu (2017) use images as source words to improve translation quality. Delbrouck and Dupont (2017) adjust various attention for the visual modality. Calixto et al. (2017) propose attention mechanisms for textual and visual modalities and combine them to decode target words. Narayan et al. (2017) develop extractive summarization with side information including images and captions. Zhu et al. (2018), Chen and Zhuge (2018) and Zhu et al. (2020) propose to generate multimodal summary for multimodal news document. Li et al. (2018a) first introduce the multimodal sentence summarization task, and they propose a hierarchical attention model, which can pay different attention to image patches, word"
2020.coling-main.496,D19-1301,0,0.0122215,"ploy a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi Attention h1＇ sGate1 → yt-1 sGate1 sGate2 MLP MLP max-pooling sGateN ... ... hi MLP hi Object-level visu"
2020.coling-main.496,W18-6439,0,0.0553968,"Missing"
2020.coling-main.496,P16-1154,0,0.024788,"the important information from the source text. • We propose a visual-guided modality regularization module to encourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao"
2020.coling-main.496,P16-1014,0,0.0210833,"text. • We propose a visual-guided modality regularization module to encourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the pr"
2020.coling-main.496,W18-6441,0,0.0326239,"Missing"
2020.coling-main.496,N16-1082,0,0.0779981,"Missing"
2020.coling-main.496,D17-1222,0,0.0734802,"curately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Nar"
2020.coling-main.496,C18-1121,1,0.321043,"select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image wi"
2020.coling-main.496,P17-2031,0,0.0523496,"Missing"
2020.coling-main.496,W04-1013,0,0.0202043,"on samples. 4.2 Experimental Settings We set word embedding size to 300 and GRU hidden state size to 512. We use the full source and target vocabularies collected from the training data, which have 36,916 source words and 26,168 target words, respectively. The mini-batch size is 64, and beam search size is 10. Adam optimizer is applied with the learning rate of 0.0005, momentum parameters β1 = 0.9 and β1 = 0.999, and  = 10−8 . We use dropout (Srivastava et al., 2014) with probability of 0.2 and gradient clipping (Pascanu et al., 2013) with range [−1, 1]. During training, we test the ROUGE-2 (Lin, 2004) F1-score on the validation set for every 5,000 batches, and we halve the learning rate if the score drops for 5 consecutive testings. 5660 Methods RG-1 RG-2 RG-L Non-Selective T-Selective Global V-Selective Object V-Selective Grid V-Selective 44.53 (± 0.11) 44.81 (± 0.14) 45.31 (± 0.15) 44.83 (± 0.12) 45.11 (± 0.16) 22.67 (± 0.12) 23.13 (± 0.13) 23.39 (± 0.13) 23.28 (± 0.13) 23.33 (± 0.15) 41.91 (± 0.09) 41.88 (± 0.11) 42.48 (± 0.11) 42.03 (± 0.12) 42.21 (± 0.12) T + Global V-Selective T + Object V-Selective T + Grid V-Selective T + Grid V-Selective + MR 45.51 (± 0.13) 45.33 (± 0.14) 45.58 (±"
2020.coling-main.496,P17-2100,0,0.0165761,"advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations"
2020.coling-main.496,W04-3252,0,0.0347575,"overlapping words, except for stop-words, between the input sentence and the reference summary as the ground-truth keywords). Following Zhou et al. (2017), we use the method of Li et al. (2016) to calculate the contribution of the selective gates to the final summary. Considering that the average word count of the summaries in the validation set is eight, we take the words with top-8 contribution values as the activated keywords by the selective mechanisms. Table 3 shows the results for keyword extraction. Our models with selective encoding perform better than unsupervised TextRank algorithm (Mihalcea and Tarau, 2004) (we also take the top-8 scored words as the keywords), and Multimodal Selective show advantages over Textual Selective. We further train a BiLSTM-CRF model (Huang et al., 2015) using sentence-keyword samples of the multimodal sentence summarization dataset, and the keyword extraction result for BiLSTM-CRF is better than our model, indicating a promising prospect for further development of the selective mechanism. In the future, we will dedicate our efforts to explore whether the selective gate can benefit from supervision signals of a special keyword extractor. A feasible research direction m"
2020.coling-main.496,K16-1028,0,0.0569473,"Missing"
2020.coling-main.496,D15-1044,0,0.34564,"ary to capture the highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015;"
2020.coling-main.496,P17-1099,0,0.210618,"ourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the inpu"
2020.coling-main.496,C18-1146,0,0.0135528,"(2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate"
2020.coling-main.496,P17-1108,0,0.0123044,"fy the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017;"
2020.coling-main.496,P19-1207,0,0.0147737,"generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi Attention h1＇ sGate1 → yt-1 sGate1 sGate2 MLP MLP max-pooling sGateN ... ... hi MLP hi Object-level visual sGate Figure 2: The framework of our model. We design visual selective gates including global-level, grid"
2020.coling-main.496,P17-1101,0,0.258595,"ion of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018;"
2020.coling-main.496,D18-1400,0,0.0969879,"corporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ..."
2020.coling-main.496,D18-1448,1,0.820738,"Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learni"
2020.emnlp-main.175,C18-1305,1,0.834672,"value > 0) of generating the ground truth sentence Y = {y1 , · · · , yT }, and utilize a monotone decreasing function to get the final reward bounded in 0 ∼ 1 as follows: 1 r(g) = e−g = e− T PT t=1 gt (5) A high value of the reward means that it is easy to generate the ground truth. Therefore, the selected context sentences should be encouraged. Conversely, if a reward is low, generating the ground truth with the selected context would cost a lot, so the selection is discouraged. 3.3.3 Self-Critical Training We train the whole model with the self-critical training method (Rennie et al., 2017; Bai et al., 2018). The goal of RL training is to minimize the negative expected reward. And in practice, the loss is usually approximated with a single sample u from the policy P as follows: Lrl = −Eu∼P [r(u)] ≈ −r(u), u ∼ P (6) The self-critical training introduces a baseline reward r(u0 ) to reduce the variance of the gradient, where u0 is obtained by the inference algorithm at test time. The final gradient is estimated by: ∇Lrl = (r(u) − r(u0 ))∇logP (u) (7) Specifically, we denote the trainable parameters of the context scorer and DocNMT by ω and θ, respectively. For each source sentence X, we select a set"
2020.emnlp-main.175,N18-1118,0,0.347023,"ring the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train two DocNMT models and test them using various context settings1 . During the test, we obtain dynamic context sentences that achieve the best BLEU scores by traversing all the context combinations for each source sentence. Compared wit"
2020.emnlp-main.175,P18-1063,0,0.0263756,"rrer, 2017), and select context sentences that yield highest forced back-translation probability. However, the method cannot optimize DocNMT model at training phase, and requires backtranslation model at inference phrase. Maruf et al. (2019) sharpen the attention weights between the source and context sentences through the sparse2249 max function, and implicitly select context with high attention weights. Nevertheless, the method lacks direct supervision over context selection, and it cannot cover the situation where context is not needed. Inspired by the extractive-abstractive summarization (Chen and Bansal, 2018), our approach is different from above DocNMT methods. Our approach can explicitly select dynamic size (that can be 0) of context sentences for the translation of different source sentences. 7 Conclusion and Future Work We propose a dynamic selection method to choose variable sizes of context sentences for documentlevel translation. The candidate context sentences are scored and selected by two proposed strategies. We train the whole model via reinforcement learning, and design a novel reward to encourage the selection of useful context sentences. When applied to existing DocNMT models, our ap"
2020.emnlp-main.175,D14-1179,0,0.016645,"Missing"
2020.emnlp-main.175,N19-1423,0,0.0148332,"ording to the selection probability in Pselect , we can obtain useful context sentences for the translation task. To select context dynamically, we add a special empty sentence “hN ON i” into the candidate context set, which stands for the situation that translates a source sentence without any context. As a result, we select those context sentences …… Candidate Context Sentences Reward DocNMT Source Sentence Sample As Figure 1 shows, we obtain the representation of context sentences for scoring. Inspired by the popular pre-training language models such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), we produce one instance by concatenating the source sentence with a context sentence, and adding a special symbol “hDCSi” at the beginning and a separator token “hSEP i” in between. The instance is fed into a stack of L1 Transformer encoder layers. We believe the special symbol “hDCSi” can encode the information of source-context sentence pairs well by the self-attention. For a candidate context sentence z ∈ Z, its hidden state of “hDCSi” after L1 layers is extracted as the input to L2 Transformer encoder layers to model the dependencies among context sentences. We denote the hidden state af"
2020.emnlp-main.175,W19-5321,0,0.0290954,"n (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train t"
2020.emnlp-main.175,D19-1294,0,0.0887846,"el neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train two DocNMT models and test them using various context settings1 . During the test, we obtain dynamic context sentences that achieve the best BLEU scores by traversing all the context combinations for each source sentence. Compared with the fixed size context (row 1 and 2), dynamic context (row 3 and 4)"
2020.emnlp-main.175,D19-6503,0,0.0877178,"Missing"
2020.emnlp-main.175,W19-6616,0,0.0142413,", 2018; Xiong et al., 2019; Voita et al., 2019b,a). However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets (Wang et al., 2017; Miculicich et al., 2018; Zhang et al., 2018; Yang et al., 2019; Voita et al., 2018; Xu et al., 2020) , or full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Kang and Zong, 2020; Zheng et al., 2020). They ignore the individualized needs for context when translating different source sentences. Some works have noticed that not all context is useful (Jean and Cho, 2019; Kim et al., 2019). Kimura et al. (2019) explore the context selection in the single-encoder framework (Tiedemann and Scherrer, 2017), and select context sentences that yield highest forced back-translation probability. However, the method cannot optimize DocNMT model at training phase, and requires backtranslation model at inference phrase. Maruf et al. (2019) sharpen the attention weights between the source and context sentences through the sparse2249 max function, and implicitly select context with high attention weights. Nevertheless, the method lacks direct supervision over context selection, and it cannot cover the situation w"
2020.emnlp-main.175,C18-1050,0,0.125614,"l machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown"
2020.emnlp-main.175,D15-1166,0,0.202437,"Missing"
2020.emnlp-main.175,2020.acl-main.321,0,0.666884,"eat progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train two DocNMT models a"
2020.emnlp-main.175,P18-1118,0,0.206941,"xt size or scope to be fixed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attention weights lacks guidance, and they cannot handle the situation where the source sente"
2020.emnlp-main.175,N19-1313,0,0.463049,"n strategies. The details of Transformer layers are shown in the right dotted box. 2 Document-level Machine Translation A standard DocNMT system generally translates a source sentence X = {x1 , · · · , xI } to a target sentence Y = {y1 , · · · , yT } with the aid of contextual information Z that is usually a subset of the candidate context set Z. The model is trained to minimize the negative log-likelihood as: Lmle = − T X logP (yt |y&lt;t , X, Z; θ) (1) t=1 Different granularity (word or sentence) and different sources (source-side or target-side) of contextual information Z have been explored. Maruf et al. (2019) divide the candidate context set Z into two cases: offline where the context comes from the entire document, and online that only uses the past context. In this paper, we mainly focus on a general scenario, where DocNMT translates sentences with the online source-side context sentences. 3 Dynamic Context Selection Our approach translates a source sentence X in the document in two steps. First, we select the appropriate context sentences for the translation of X via the selection module. Independent of DocNMT module, this step is conducted before the context encoding in DocNMT module. The core"
2020.emnlp-main.175,W18-6307,0,0.0444008,"Missing"
2020.emnlp-main.175,P02-1040,0,0.10748,"2019). 4.2 Models We compare our approach with the following methods: 1) S ENT N MT (Vaswani et al., 2017) is a standard sentence-level Transformer model using the “base” version parameters. 2) TDNMT (Zhang et al., 2018) introduces the contextual information by adding attention sub-layers at each encoder and decoder layer. We use 2 previous consecutive context sentences as they suggested. 3) HAN (Miculicich et al., 2018) uses 3 previous sentences as 3 https://wit3.fbk.eu/mt.php?release= 2017-01-trnted 4 http://data.statmt.org/ news-commentary/v14 Results and Analysis Main Results We use BLEU (Papineni et al., 2002) score to evaluate the translation quality. Considering the memory limitation and complex sampling space, we select dynamic context from previous six sentences. Table 10 shows the performance of models utilizing different context settings. We always keep the same setting for training and test. Comparison with Fixed Context Methods. The performance of DocNMT models with fixed context is shown in row 2∼5. Row 2 and 3 follow the context settings in the published papers. It can be found that using more context sentences indiscriminately (row 4 and 5) does not bring significant BLEU improvement. In"
2020.emnlp-main.175,P16-1162,0,0.321796,"Missing"
2020.emnlp-main.175,D19-1168,0,0.410747,"xed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attention weights lacks guidance, and they cannot handle the situation where the source sentences achieve the b"
2020.emnlp-main.175,W17-4811,0,0.123918,"ngs are consistent. Introduction Although neural machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences"
2020.emnlp-main.175,Q18-1029,0,0.547054,"on Although neural machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires differe"
2020.emnlp-main.175,P16-1008,0,0.0488321,"Missing"
2020.emnlp-main.175,D19-1081,0,0.344414,"Missing"
2020.emnlp-main.175,P19-1116,0,0.225925,"Missing"
2020.emnlp-main.175,P18-1117,0,0.25425,"Experiments indicate that only the limited context sentences are really useful, and they change with source sentences. Majority of existing DocNMT models set the context size or scope to be fixed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the mo"
2020.emnlp-main.175,D19-1164,0,0.26443,"ey change with source sentences. Majority of existing DocNMT models set the context size or scope to be fixed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attent"
2020.emnlp-main.175,P17-4012,0,0.0864033,"ampled context Z. context. We adopt the “HAN encoder + HAN decoder” strategy that adds a hierarchical network on the top of the last encoder and decoder layer to model sentence-level and word-level contextual information. 4) SAN (Maruf et al., 2019) utilizes all context in the entire document by calculating the sentence-level and word-level weights. It focuses on relevant context sentences through the sparsemax function. We choose the model that integrates the online context into encoder with “sparse-soft H-Attention”. We implement our approach and baseline methods based on the toolkit THUMT (Zhang et al., 2017). The parameters are the “base” version of the original Transformer (Vaswani et al., 2017). The d1 and d2 in Eq. 2 are 512 and 256, respectively. We set the layers of L1 = 2 and L2 = 2. The effect of layer depth of context scorer and more implementation details are shown in the appendix. 5 5.1 4 4.1 Experiment Datasets We evaluate our approach on different domains of Chinese-English (Zh→En) and English-German (En→De) datasets. The corpora statistics are listed in Table 9. For TED Talks in IWSLT173 , we use dev-2010 as the development set, and tst2010∼2013 as the test set for both Zh→En and En→"
2020.emnlp-main.175,D18-1049,0,0.281493,"Missing"
2020.emnlp-main.175,D17-1301,0,0.273126,"test context settings are consistent. Introduction Although neural machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation"
2020.emnlp-main.175,D16-1160,1,0.891947,"Missing"
2020.emnlp-main.175,P19-1117,1,0.843124,"simple way of selecting target-side context bears the risk of missing selection, the accuracy of some phenomena does not change significantly. Table 7 has shown that our approach can select useful target-side context in most cases. And the selection mechanism can make the model focus more on the useful context to resolve the discourse phenomena. 6 Related Work Standard neural machine translation methods usually focus on the sentence-level translation (Cho et al., 2014; Bahdanau et al., 2015; Zhang and Zong, 2015; Luong et al., 2015; Tu et al., 2016; Zhang and Zong, 2016; Vaswani et al., 2017; Wang et al., 2019; Zhou et al., 2019; Zhao et al., 2020). As a contrast, document-level neural machine translation methods mainly pay attention to how to utilize the cross-sentence context. Researchers propose various context-aware networks to utilize contextual information to improve the performance of DocNMT models on the translation quality (Jean et al., 2017; Tu et al., 2018; Kuang et al., 2018) or discourse phenomena (Bawden et al., 2018; Xiong et al., 2019; Voita et al., 2019b,a). However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets (Wang et al.,"
2020.emnlp-main.175,D18-1397,0,0.0207271,"eline reward r(Z ∗ ) obtained by the current best policy (i.e., learned selection strategies), the method encourages model to explore more useful context (i.e., sampled context) that yields higher reward than the current best (i.e., selected context). 2245 Datasets TED Zh→En News TED En→De News Europarl Training 0.23M 0.31M 0.21M 0.33M 1.67M Dev 0.88K 2.00K 0.89K 3.00K 3.59K Test 4.68K 3.98K 4.70K 3.00K 5.14K Table 2: Dataset statistics in the number of sentences. For DocNMT module, we can combine the MLE objective (Eq. 1) and RL objective (Eq. 6) together to stabilize the training procedure (Wu et al., 2018) through a balance factor α as follows: ˆ θ) + (1 − α) ∗ Lrl (θ) L(θ) = α ∗ Lmle (Y |X, Z, (9) We introduce the RL objective into DocNMT module so that the model can make better use of the selected context. The final RL gradient of DocNMT is calculated by: ˆ − r(Z ∗ ))∇θ logPθ (Yˆ |X, Z) ˆ ∇θ Lrl (θ) = (r(Z) (10) where Yˆ is a sequence generated by current Docˆ NMT model with the sampled context Z. context. We adopt the “HAN encoder + HAN decoder” strategy that adds a hierarchical network on the top of the last encoder and decoder layer to model sentence-level and word-level contextual informa"
2020.emnlp-main.175,Q19-1006,1,0.830299,"ting target-side context bears the risk of missing selection, the accuracy of some phenomena does not change significantly. Table 7 has shown that our approach can select useful target-side context in most cases. And the selection mechanism can make the model focus more on the useful context to resolve the discourse phenomena. 6 Related Work Standard neural machine translation methods usually focus on the sentence-level translation (Cho et al., 2014; Bahdanau et al., 2015; Zhang and Zong, 2015; Luong et al., 2015; Tu et al., 2016; Zhang and Zong, 2016; Vaswani et al., 2017; Wang et al., 2019; Zhou et al., 2019; Zhao et al., 2020). As a contrast, document-level neural machine translation methods mainly pay attention to how to utilize the cross-sentence context. Researchers propose various context-aware networks to utilize contextual information to improve the performance of DocNMT models on the translation quality (Jean et al., 2017; Tu et al., 2018; Kuang et al., 2018) or discourse phenomena (Bawden et al., 2018; Xiong et al., 2019; Voita et al., 2019b,a). However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets (Wang et al., 2017; Miculicich et"
2020.iwslt-1.15,W17-4712,0,0.0183908,"ain 6.1M and 16.4M monolingual sentences for Japanese and Chinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel."
2020.iwslt-1.15,C18-1111,0,0.0183138,"s for Japanese and Chinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel. Since the later is mainly source docume"
2020.iwslt-1.15,N13-1073,0,0.0374842,"ethods are used to implement escape character transformation and text normalization as pre-processing. For UNK processing, we find some of the numbers can not be well translated by model and we replace these UNKs with the numbers in source sentence. Otherwise, we remove the UNK symbols. 3.2 Parallel Data Filtering The following methods are applied to further filter the parallel sentence pairs. We remove sentences longer than 50 and select the parallel sentences where the length ratio (Ja/Zh) is between 0.53 and 2.90. We then calculate word alignment of each sentence pair by using fast align4 (Dyer et al., 2013). The percentage of aligned words and alignment perplexities are used as the metric where the thresholds are set as 0.4 and −30 respectively. Through the above filtering procedure, the number of the remaining data is reduced from 20.9M to 15.7M, as shown in Table 2. 3.3 Monolingual Data Filtering It is proven that back-translation is a simple but effective approach to enhance the translation quality 4 https://github.com/fxsjy/jieba 133 https://github.com/clab/fast_align Figure 4: The domain data processing steps, including the NMT model trained on general domain data, the NMT models fine tuned"
2020.iwslt-1.15,D16-1139,0,0.0251575,"anslation direction. Recent work (Edunov et al., 2018) has shown that different methods of generating pseudo corpus made discrepant influence on translation performance. Edunov et al. (2018) indicated that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam search or greedy search. We adopt the back-translation script from fairseq2 and generate back-translated data with sampling for both translation directions. 2.3 Knowledge Distillation The goal of knowledge distillation is to deliver a student model that matches the accuracy of a teacher model (Kim and Rush, 2016). Prior work (Yang et al., 2018) demonstrates that student model can surpass the accuracy of the teacher model. In our experiments, we adopt sequence-level knowledge distillation method and investigate four different teacher models to boost the translation quality of student model. S2T+L2R Teacher Model: We translate the source sentences of the parallel data into target language using our source-to-target (briefly, S2T) system described in Section 2.1 with left-to-right (briefly, L2R) manner. S2T+R2L Teacher Model: We translate the source sentences of the parallel data into target language usi"
2020.iwslt-1.15,W18-6304,0,0.0143967,"performance on development data with different model settings and different data processing techniques. 1 Introduction Neural machine translation(NMT) has been introduced and made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Zhou et al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al., 2018) and Dynamic Convolutions (Wu et al., 2019) to investigate the performances of advanced model variations. For the implementation, we extend the latest release of Fairseq1 (Ott et al., 2019). 1 https://github.com/pytorch/fairseq 2 System Description Figure 1 depict"
2020.iwslt-1.15,W17-3204,0,0.0246073,"3 https://github.com/clab/fast_align Figure 4: The domain data processing steps, including the NMT model trained on general domain data, the NMT models fine tuned on specific domain, the domain classification and weighted ensemble in the decoding stage. Filtering Methods original remove illegal filter by length filter by LM Ja 941,297,925 10,078,827 8,175,157 6,128,443 Domain Wiki Spoken News Zh 928,670,666 32,644,917 30,415,964 16,374,195 3.4 Domain Data Processing Although the amount of provided training data is large enough, it is a noise set of web data built from multiple domain sources. Koehn and Knowles (2017) have demonstrated that the NMT model performs poorly when the test domain does not match 5 https://github.com/kpu/kenlm Web 4,006,232 9,534,754 2,444,884 Table 4: Statistics of domain data. Existing indicates existing parallel which is used to train the domain classifier, while Web means web crawled parallel in which the domain labels are predicted by the classifier. Table 3: The number of the remaining monolingual sentences for Japanese and Chinese after each filtering operation. as described in Section 2.2. To achieve that, we extract the high-quality monolingual sentences from the provided"
2020.iwslt-1.15,D15-1166,0,0.0604003,"em is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques. 1 Introduction Neural machine translation(NMT) has been introduced and made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Zhou et al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation"
2020.iwslt-1.15,N19-4009,0,0.0145701,"ty due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al., 2018) and Dynamic Convolutions (Wu et al., 2019) to investigate the performances of advanced model variations. For the implementation, we extend the latest release of Fairseq1 (Ott et al., 2019). 1 https://github.com/pytorch/fairseq 2 System Description Figure 1 depicts the whole process of our submission system, in which we pre-process the provided data and train our advanced Transformer models on the bilingual data together with synthetic corpora from back-translation and knowledge distillation. With domain classification and fine tuning techniques, we obtain multiple models for ensemble strategy and post-processing. In this section, we will introduce each process step in detail. 2.1 NMT Baseline In this work, we build our model based on the powerful Transformer (Vaswani et al., 20"
2020.iwslt-1.15,P16-1009,0,0.104076,"to an output. Specifically, we can multiply query Qi by key Ki to obtain an attention weight matrix, which is then multiplied by value Vi for each token to obtain the self-attention token representation. As shown in Figure 3, we compute the matrix of outputs as: QK T Attention(Q, K, V ) = Softmax( √ )V dk (2) where dk is the dimension of the key. For the sake of brevity, we refer the reader to Vaswani et al. (2017) for more details. 131 2.2 Back-Translation Back-translation is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system (Sennrich et al., 2016a; Zhang and Zong, 2016). Especially for low-resource language tasks, it is indispensable to augment the training data by mixing the pseudo corpus with the parallel part. Back-translation first trains an intermediate target-to-source system that is used to translate monolingual target data into additional synthetic parallel data. This data is used in conjunction with human translated bitext data to train the desired source-to-target system. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We"
2020.iwslt-1.15,P16-1162,0,0.386197,"to an output. Specifically, we can multiply query Qi by key Ki to obtain an attention weight matrix, which is then multiplied by value Vi for each token to obtain the self-attention token representation. As shown in Figure 3, we compute the matrix of outputs as: QK T Attention(Q, K, V ) = Softmax( √ )V dk (2) where dk is the dimension of the key. For the sake of brevity, we refer the reader to Vaswani et al. (2017) for more details. 131 2.2 Back-Translation Back-translation is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system (Sennrich et al., 2016a; Zhang and Zong, 2016). Especially for low-resource language tasks, it is indispensable to augment the training data by mixing the pseudo corpus with the parallel part. Back-translation first trains an intermediate target-to-source system that is used to translate monolingual target data into additional synthetic parallel data. This data is used in conjunction with human translated bitext data to train the desired source-to-target system. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We"
2020.iwslt-1.15,N18-2074,0,0.169049,"al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al., 2018) and Dynamic Convolutions (Wu et al., 2019) to investigate the performances of advanced model variations. For the implementation, we extend the latest release of Fairseq1 (Ott et al., 2019). 1 https://github.com/pytorch/fairseq 2 System Description Figure 1 depicts the whole process of our submission system, in which we pre-process the provided data and train our advanced Transformer models on the bilingual data together with synthetic corpora from back-translation and knowledge distillation. With domain classification and fine tuning techniques, we obtain multiple models for ensemble strategy"
2020.iwslt-1.15,D18-1458,0,0.0285615,"Missing"
2020.iwslt-1.15,P17-2089,0,0.0184314,"onolingual sentences for Japanese and Chinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel. Since the later is"
2020.iwslt-1.15,D16-1160,1,0.675141,"ly, we can multiply query Qi by key Ki to obtain an attention weight matrix, which is then multiplied by value Vi for each token to obtain the self-attention token representation. As shown in Figure 3, we compute the matrix of outputs as: QK T Attention(Q, K, V ) = Softmax( √ )V dk (2) where dk is the dimension of the key. For the sake of brevity, we refer the reader to Vaswani et al. (2017) for more details. 131 2.2 Back-Translation Back-translation is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system (Sennrich et al., 2016a; Zhang and Zong, 2016). Especially for low-resource language tasks, it is indispensable to augment the training data by mixing the pseudo corpus with the parallel part. Back-translation first trains an intermediate target-to-source system that is used to translate monolingual target data into additional synthetic parallel data. This data is used in conjunction with human translated bitext data to train the desired source-to-target system. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We trained a n-gram based"
2020.iwslt-1.15,C18-1269,0,0.0176612,"hinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel. Since the later is mainly source document index for each sente"
2020.iwslt-1.15,P17-2060,1,0.83251,"r model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques. 1 Introduction Neural machine translation(NMT) has been introduced and made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Zhou et al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al"
2021.acl-long.103,D16-1162,0,0.0226188,"ethods, in various text classification tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain,"
2021.acl-long.103,Q19-1004,0,0.0275735,"yers except the 1-st layer on Zh⇒En task. These findings prove that each attention layer plays a different role in the decoding process. The low layers generally grasp information from various inputs, while the high layers look for some particular words tied to the model predictions. 6 Related Work The attention mechanism is first introduced to augment vanilla recurrent network (Bahdanau et al., 2015; Luong et al., 2015), which are then the backbone of state-of-the-art Transformer (Vaswani et al., 2017) for NMT. It yields better performance and provides a window into how a model is operating (Belinkov and Glass, 2019; Du et al., 2020). This section reviews the recent researches on analyzing and improving attention mechanisms. The Attention Debate Many recent studies have spawned interest in whether attention weights faithfully represent each input token’s responsibility for model prediction. Serrano and Smith flip the model’s decision by permuting some small attention weights, with high-weighted components not being the reason for the decision. Some work (Jain and Wallace, 2019; Vashishth et al., 2019) find a weak correlation between attention scores and other well-ground feature importance metrics, speci"
2021.acl-long.103,2016.amta-researchers.10,0,0.0184399,"tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the a"
2021.acl-long.103,N16-1000,0,0.226226,"Missing"
2021.acl-long.103,D19-1453,0,0.0238084,"nsformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NMT performance. 7 Conclusion In this pa"
2021.acl-long.103,N19-1357,0,0.119117,"to the current translation are expected to receive more attention. However, many studies doubt whether highlyattended inputs have a large impact on the model outputs. On the one hand, erasing the representations accorded high attention weights do not necessarily lead to a performance decrease (Serrano and † 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 deaths _ [before] Introduction ∗ 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 <EOS> in _ [after] Smith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020). On the other hand, Jain and Wallace (2019) state that attention weights are inconsistent with other feature importance metrics in text classification tasks. It further proves that attention mechanisms are incapable of precisely identifying decisive inputs for each prediction, which would result in wrong-translation or over-translation in NMT (Tu et al., 2016). We take Figure 1 as an example. After producing the target-side word “deaths”, attention mechanisms wrongly attribute most attention to the “hEOSi”, making parts of the source sentence untranslated. In this paper, we propose to calibrate the vanilla attention mechanism by focusi"
2021.acl-long.103,P07-2045,0,0.0145057,"man (En⇒De), WMT16 English-Romanian (En⇔Ro), WMT17 English-Finnish (En⇔Fi) and English-Latvian (En⇔Lv). 1291 Source LDC1 WMT142 WMT173 WMT164 Lang. Zh⇒En En⇒De En⇒Lv Lv⇒En En⇒Fi Fi⇒En En⇒Ro Ro⇒En Train 2.09M 4.54M Dev. 878 3000 Test 4789 3003 Vocab. 32k 37k 4.46M 2003 2001 37k 2.63M 3000 3002 32k 0.61M 1999 1999 32k Model GNMT (Wu et al., 2016)‡ Conv (Gehring et al., 2017)‡ AttIsAll (Vaswani et al., 2017)‡ (Feng et al., 2020)‡ (Weng et al., 2020)‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (Papineni et al., 2002) on the true-case output to score the performance. For En⇔Ro, we use newsdev2016 and newstest2016 as development and test sets. For En⇔Lv and En⇔Fi,"
2021.acl-long.103,N16-1082,0,0.0346824,"which simulates the process of perturbation. Formally, let mt be a mask at t-th step. The perturbed attention weight apt is calculated as: apt = mt at + (1 − mt ) µ0 often presented as communicating the relative importance of inputs. However, recent work has cautioned against whether the inputs accorded high attention weights decide the model outputs (Jain and Wallace, 2019). Our analysis examines the correlation with attention weights and feature importance metrics in NMT to test if the attention mechanisms focus on the decisive inputs. We apply gradient-based methods (Simonyan et al., 2014; Li et al., 2016) to measure the importance of each contextual representation hi for model output yt : τit = |∇hi p(yt |x1:n )| (2) We train a baseline Transformer model on NIST Zh⇒En dataset and extract the averaged attention weights over heads. Figure 2 reports the statistics of Kendall-τ correlation for each attention layer, where the observed correlation is all modest (0 indicates no correlation, while 1 implies perfect concordance). The inconsistency with feature importance metrics reveals that the high-attention inputs are not always responsible for the model prediction. It further motivates us whether w"
2021.acl-long.103,C16-1291,0,0.0216138,"sent the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results"
2021.acl-long.103,D15-1166,0,0.0570323,"ry across layers. For En⇒De translation, the calibrated attention weights are more uniform at 1-3 layers and more focused at 4-6 layers, while the attention weights become more focused for all layers except the 1-st layer on Zh⇒En task. These findings prove that each attention layer plays a different role in the decoding process. The low layers generally grasp information from various inputs, while the high layers look for some particular words tied to the model predictions. 6 Related Work The attention mechanism is first introduced to augment vanilla recurrent network (Bahdanau et al., 2015; Luong et al., 2015), which are then the backbone of state-of-the-art Transformer (Vaswani et al., 2017) for NMT. It yields better performance and provides a window into how a model is operating (Belinkov and Glass, 2019; Du et al., 2020). This section reviews the recent researches on analyzing and improving attention mechanisms. The Attention Debate Many recent studies have spawned interest in whether attention weights faithfully represent each input token’s responsibility for model prediction. Serrano and Smith flip the model’s decision by permuting some small attention weights, with high-weighted components no"
2021.acl-long.103,D16-1249,0,0.0120831,"on analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NM"
2021.acl-long.103,2020.acl-main.387,0,0.0690076,"nputs, where the ones that are most relevant to the current translation are expected to receive more attention. However, many studies doubt whether highlyattended inputs have a large impact on the model outputs. On the one hand, erasing the representations accorded high attention weights do not necessarily lead to a performance decrease (Serrano and † 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 deaths _ [before] Introduction ∗ 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 <EOS> in _ [after] Smith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020). On the other hand, Jain and Wallace (2019) state that attention weights are inconsistent with other feature importance metrics in text classification tasks. It further proves that attention mechanisms are incapable of precisely identifying decisive inputs for each prediction, which would result in wrong-translation or over-translation in NMT (Tu et al., 2016). We take Figure 1 as an example. After producing the target-side word “deaths”, attention mechanisms wrongly attribute most attention to the “hEOSi”, making parts of the source sentence untranslated. In this paper, we propose to calibra"
2021.acl-long.103,P02-1040,0,0.110384,")‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (Papineni et al., 2002) on the true-case output to score the performance. For En⇔Ro, we use newsdev2016 and newstest2016 as development and test sets. For En⇔Lv and En⇔Fi, newsdev2017 and newstest2017 are validation set and test set. See Table 1 for statistics of the data. 4.2 TEST 24.61 25.16 27.3 27.55 27.7 27.37 27.38 28.1∗ 27.75 Settings We implement the described models with fairseq5 toolkit for training and evaluating. We experiment with Transformer Base (Vaswani et al., 2017): hidden size dmodel = 512, 6 encoder and decoder layers, 8 attention heads and 2048 feed-forward innerlayer dimension. The dropout rate"
2021.acl-long.103,W18-5431,0,0.0183701,"and Wallace, 2019; Vashishth et al., 2019) find a weak correlation between attention scores and other well-ground feature importance metrics, specially gradient-based and leave-one-out methods, in various text classification tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indic"
2021.acl-long.103,P16-1162,0,0.0617801,"T17 English-Finnish (En⇔Fi) and English-Latvian (En⇔Lv). 1291 Source LDC1 WMT142 WMT173 WMT164 Lang. Zh⇒En En⇒De En⇒Lv Lv⇒En En⇒Fi Fi⇒En En⇒Ro Ro⇒En Train 2.09M 4.54M Dev. 878 3000 Test 4789 3003 Vocab. 32k 37k 4.46M 2003 2001 37k 2.63M 3000 3002 32k 0.61M 1999 1999 32k Model GNMT (Wu et al., 2016)‡ Conv (Gehring et al., 2017)‡ AttIsAll (Vaswani et al., 2017)‡ (Feng et al., 2020)‡ (Weng et al., 2020)‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (Papineni et al., 2002) on the true-case output to score the performance. For En⇔Ro, we use newsdev2016 and newstest2016 as development and test sets. For En⇔Lv and En⇔Fi, newsdev2017 and newstest2017 are validation set a"
2021.acl-long.103,P19-1282,0,0.0266268,"Missing"
2021.acl-long.103,W19-4807,0,0.0443838,"Missing"
2021.acl-long.103,P16-1008,0,0.0193538,"郊 雪 亡 通 断 deaths _ [before] Introduction ∗ 远 连 日 大 多 人 死 交 中 郊 雪 亡 通 断 <EOS> in _ [after] Smith, 2019), which can be attributed to that unimportant words (e.g., punctuations) are frequently assigned with high attention weights (Mohankumar et al., 2020). On the other hand, Jain and Wallace (2019) state that attention weights are inconsistent with other feature importance metrics in text classification tasks. It further proves that attention mechanisms are incapable of precisely identifying decisive inputs for each prediction, which would result in wrong-translation or over-translation in NMT (Tu et al., 2016). We take Figure 1 as an example. After producing the target-side word “deaths”, attention mechanisms wrongly attribute most attention to the “hEOSi”, making parts of the source sentence untranslated. In this paper, we propose to calibrate the vanilla attention mechanism by focusing more on key in1288 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1288–1298 August 1–6, 2021. ©2021 Association for Computational Linguistics puts. To test what inputs affect the model predicti"
2021.acl-long.103,2020.repl4nlp-1.17,0,0.0172795,"be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ importance (Kitada and Iyatomi, 2020; Tutek and Snajder, 2020; Mohankumar et al., 2020) which is designed for analysis with no significant performance gain, while our methods incorporate the analytical results to enhance the NMT performance. 7 Conclusion In this paper, we present a mask perturbation model to automatically discover the decisive inputs for the model prediction. We propose three methods to calibrate the attention mechanism by focusing on the discovered vital inputs. Extensive experimental results show that our approaches obtain significant improvements over the state-of-the-art system. Analytical results indicate that our proposed methods"
2021.acl-long.103,W19-4808,0,0.018581,"t al., 2019) find a weak correlation between attention scores and other well-ground feature importance metrics, specially gradient-based and leave-one-out methods, in various text classification tasks. We also present the correlation analysis in the lessdiscussed Transformer-based NMT and reach a similar conclusion. As opposed to the critiques of regarding attention weights as explanation, Wiegreffe and Pinter claim that the trained attention mechanisms do learn something meaningful about the relationship between inputs and outputs, such as syntactic information (Raganato and Tiedemann, 2018; Vig and Belinkov, 2019; Pham et al., 2019). Can Attention be improved? There is plenty of work on supervising attention weights with lexical probabilities (Arthur et al., 2016), word alignment (Chen et al., 2016; Liu et al., 2016; Mi et al., 2016; Cohn et al., 2016; Garg et al., 2019; Feng et al., 2020), human rationales (Strout et al., 2019) and sparsity regularization (Zhang et al., 2019). Unlike them, we never introduce any external knowledge but highlight the inputs whose removal would significantly decrease Transformer’s performance. Another work line aims to make attention better indicative of the inputs’ imp"
2021.acl-long.103,2021.acl-long.91,0,0.0670852,"Missing"
2021.acl-long.103,P19-1580,0,0.0210165,"Q , W K } represents the parameters of mask perturbation model. The first term indicates that the perturbation operation aims to harm the translation quality. The second one serves as a penalty term to encourage most of the mask to be turned off (perturb inputs as few as possible). Lc (θm ) = k1 − mt k2 (6) The perturbation extent is determined by the hyperparameter α. Notably, earlier studies employ masks and “deletion game” as the analytical tools to explore the importance of each attention head (Fong and Vedaldi, 2017) or the contributions of the pixels in the figure to the model outputs (Voita et al., 2019). However, we extend to probing the inputs’ contributions to the model prediction in NMT and further use the masks to calibrate the attention mechanisms based on the analytical results. 1290 K Q Attention Mechanisms NMT decoder Attention Calibration Network (ACN) Original （?） Attention Weights mask （?） Calibrated （?? ） Attention Weights Better performance after calibration Worse performance after perturbation Perturbed （?? ） Attention Weights Mask Perturbation Model Figure 3: The overview of the framework. The mask perturbation model is trained to perturb the attention weights of decisive inpu"
2021.acl-long.103,2020.emnlp-main.212,0,0.145882,"an accessible measurement of the inputs’ contributions to the model predictions. 4 Experiments We evaluate our method in LDC Chinese-English (Zh⇒En), WMT14 English-German (En⇒De), WMT16 English-Romanian (En⇔Ro), WMT17 English-Finnish (En⇔Fi) and English-Latvian (En⇔Lv). 1291 Source LDC1 WMT142 WMT173 WMT164 Lang. Zh⇒En En⇒De En⇒Lv Lv⇒En En⇒Fi Fi⇒En En⇒Ro Ro⇒En Train 2.09M 4.54M Dev. 878 3000 Test 4789 3003 Vocab. 32k 37k 4.46M 2003 2001 37k 2.63M 3000 3002 32k 0.61M 1999 1999 32k Model GNMT (Wu et al., 2016)‡ Conv (Gehring et al., 2017)‡ AttIsAll (Vaswani et al., 2017)‡ (Feng et al., 2020)‡ (Weng et al., 2020)‡ Our Implemented Baseline Fixed Ours Anneal Gate Table 1: Statistics of the datasets. 4.1 Dataset We tokenize the corpora using a script from Moses (Koehn et al., 2007). Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a join vocabulary except for Zh⇒En where the source and target languages are separately encoded. For Zh⇒En, we remove the sentences of more than 50 words. We use NIST 2002 as validation set, NIST 2003-2006 as the testbed. For En⇒De, newstest2013 and newstest2014 are set as validation and test sets. We use the standard 4-gram BLEU (P"
2021.acl-long.103,D19-1002,0,0.0400659,"Missing"
2021.acl-short.65,P18-1008,0,0.0290905,"assigns larger training weights to tokens with higher BMI, so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity. Experimental results on WMT14 English-to-German and WMT19 Chinese-to-English demonstrate the superiority of our approach compared with the Transformer baseline and previous token-level adaptive training approaches. Further analyses confirm that our method can improve the lexical diversity. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018; Meng and Zhang, 2019; Zhang et al., 2019; Yan et al., 2020; Liu et al., 2021) has achieved remarkable success. As a data-driven model, the performance of NMT depends on training corpus. Balanced training data is a crucial factor in building a superior model. ∗ This work was done when Yangyifan Xu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. However, natural languages conform to the Zipf’s law (Zipf, 1949), the frequencies of words exhibit the long tail characteristics, which brings an imbalance in the distribu"
2021.acl-short.65,2020.aacl-main.25,0,0.0828327,"Missing"
2021.acl-short.65,2020.emnlp-main.76,1,0.674679,"Missing"
2021.acl-short.65,D19-1088,0,0.0219702,"l competence increases (Wan et al., 2020). Second, previous studies only use monolingual word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks, e.g., machine translation. The mapping between bilingualism is a more appropriate indicator. As shown in Table 1, word frequency of pleasing and bearings are both 847. Corresponding to Chinese, pleasing has multiple mappings, while bearings is relatively single. The more multivariate the mapping is, the less confidence in predicting the target word given the source context. He et al. (2019) also confirm this view that words with multiple mappings contribute more to the BLEU score. To tackle the above issues, we propose bilingual mutual information (BMI), which has two characteristics: 1) BMI measures the learning difficulty for each target token by considering the strength of association between it and the source sentence; 2) for each target token, BMI can dynamically adjust according to the context. BMI-based adaptive training can dynamically adjust the learning granularity on tokens. Easy tokens are updated with coarse granularity while difficult tokens are updated with 511 Pr"
2021.acl-short.65,W19-6622,0,0.0330324,"Missing"
2021.acl-short.65,2020.emnlp-main.80,0,0.0330367,"ings an imbalance in the distribution of words in training corpora. Some studies (Jiang et al., 2019; Gu et al., 2020) assign different training weights to target tokens according to their frequencies. These approaches alleviate the token imbalance problem and indicate that tokens should be treated differently during training. However, there are two issues in existing approaches. First, these approaches believe that lowfrequency words are not sufficiently trained and thus amplify the weight of them. Nevertheless, low-frequency tokens are not always difficult as the model competence increases (Wan et al., 2020). Second, previous studies only use monolingual word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks, e.g., machine translation. The mapping between bilingualism is a more appropriate indicator. As shown in Table 1, word frequency of pleasing and bearings are both 847. Corresponding to Chinese, pleasing has multiple mappings, while bearings is relatively single. The more multivariate the mapping is, the less confidence in predicting the target word given the source context. He et al. (2019) also confirm this view t"
2021.acl-short.65,2020.emnlp-main.77,1,0.857232,"Missing"
2021.acl-short.65,W04-3250,0,0.357853,"Missing"
2021.acl-short.65,P17-4012,0,0.0902858,"on and report the BLEU scores on newstest2014. ZH-EN. The training data is from WMT19 which consists of 20.5M sentence pairs. The number of merge operations in byte pair encoding (BPE) is set to 32K for both source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 4k and 2k sentences, respectively. BMI-based Objective We calculate the token-level weight by scaling BMI and adjusting the lower limit as follows: wj = S · BMI(x, yj ) + B. Experiments 4.2 Systems Transformer. We implement our approach with the open source toolkit THUMT (Zhang et al., 2017) and strictly follow the setting of TransformerBase in (Vaswani et al., 2017). Exponential (Gu et al., 2020). This method adds an additional training weights to lowfrequency target tokens: wj = A · e−T ·Count(yj ) + 1. (4) The two hyperparameters S (scale) and B (base) influence the magnitude of change and the lower limit, respectively. 513 (5) Chi-Square (Gu et al., 2020). The weighting function of this method is similar to the form of chi-square distribution wj = A · Count2 (yj )e−T ·Count(yj ) + 1. (6) B 1.0 BMI 0.9 0.8 0.7 S 0.05 0.10 0.15 0.20 0.25 0.30 0.15 0.20 0.25 0.15 0.20 0.25 0.15"
2021.acl-short.65,P19-1426,1,0.87636,"Missing"
2021.acl-short.65,P16-1162,0,0.0591186,"icult tokens, the model has a higher tolerance because their translation errors may not be absolute. As a result, the loss is small due to the small weight and the difficult tokens are always updated in a fine-grained way. 4 We evaluate our method on the Transformer (Vaswani et al., 2017) and conduct experiments on two widely-studied NMT tasks, WMT14 English-to-German (En-De) and WMT19 Chineseto-English (Zh-En). 4.1 Data Preparation EN-DE. The training data consists of 4.5M sentence pairs from WMT14. Each word in the corpus has been segmented into subword units using byte pair encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. The vocabulary is shared among source and target languages. We select newstest2013 for validation and report the BLEU scores on newstest2014. ZH-EN. The training data is from WMT19 which consists of 20.5M sentence pairs. The number of merge operations in byte pair encoding (BPE) is set to 32K for both source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 4k and 2k sentences, respectively. BMI-based Objective We calculate the token-level weight by scaling BMI and adjusting the lower limit as follows: wj"
2021.emnlp-main.365,2020.lrec-1.58,0,0.0615961,"Missing"
2021.emnlp-main.365,P18-1063,0,0.0180648,"order until the length of the summary reaches the limit. SummaRuNNer (Nallapati et al., 2017): A supervised extractive summarization method by scoring each utterance using RNN. Here, we use the key utterance indexes as extractive labels. BERTExt (Liu and Lapata, 2019): This method scores each utterance in dialogue by ﬁnetuning on the pretrained BERT (Devlin et al., 2019) model. Extractive labels are the same as SummaRuNNer. We also implement some abstractive methods: PGN (See et al., 2017): An RNN-based seq2seq model using source word copy mechanism and attention coverage mechanism. Fast-RL (Chen and Bansal, 2018): This method ﬁrst extracts important sentences and then compresses them into summaries. The whole model is at last jointly trained by reinforcement learning. BERTAbs (Liu and Lapata, 2019): It uses pretrained BERT as the encoder and a transformerbased network as the decoder to summarize. TDS+SATM (Zou et al., 2021b): It is similar to Fast-RL but uses BERT and transformer structure as the extractive model and abstractive model. Besides, it also introduces a topic model to enhance summary generation. For all abstractive methods containing the extractive process, such as Fast-RL and TDS+SATM, we"
2021.emnlp-main.365,W04-3247,0,0.126744,"ng to generate different kinds of summaries separately and relax the structural requirements2 . 4.2 Summarization Models In this section, we will introduce some widely-used extractive and abstractive summarization models on dialogue summarization. We also enhance some of the models using our special annotations. The extractive methods include: LONGEST: As the longer utterances in the dialogue may contain more useful information, we sort the utterances by their lengths and extract the top k longest utterances as the summary. Number k is decided by the maximum summary length limit. LexPageRank (Erkan and Radev, 2004): This method ranks dialogue utterances by PageRank algorithm and extracts utterances in order until the length of the summary reaches the limit. SummaRuNNer (Nallapati et al., 2017): A supervised extractive summarization method by scoring each utterance using RNN. Here, we use the key utterance indexes as extractive labels. BERTExt (Liu and Lapata, 2019): This method scores each utterance in dialogue by ﬁnetuning on the pretrained BERT (Devlin et al., 2019) model. Extractive labels are the same as SummaRuNNer. We also implement some abstractive methods: PGN (See et al., 2017): An RNN-based se"
2021.emnlp-main.365,D19-5409,0,0.0853486,"rmance beacquire the user’s problems and previous service comes much worse when analyzing the perforprogress, ﬁguring out the solved and unsolved probmance on role-oriented summaries and topic lems. Besides, role-oriented and structural sumstructures. We hope that this study could benchmark Chinese dialogue summarization maries are also valuable for other dialogue domains and beneﬁt further studies. such as debating and court trials. Although several dialogue summarization 1 Introduction datasets have been proposed recently (McCowan Text summarization aims to compress a long in- et al., 2005; Gliwa et al., 2019; Zou et al., 2021a,b), put text and generate a condensed summary (Zong none of them adds dialogue features (i.e., different et al., 2021). It can help people capture the gist of speakers’ roles or topic structure) in summaries, a long document quickly. Traditional summariza- limiting the application of these datasets. Theretion tasks mainly focus on news reports (Nallapati fore, we aim to construct a ﬁne-grained Chinese et al., 2016; Narayan et al., 2018; Zhu et al., 2018). dataset for Customer Service domain Dialogue However, as the communication tools become con- Summarization (CSDS). venie"
2021.emnlp-main.365,2020.inlg-1.23,0,0.0243619,"Missing"
2021.emnlp-main.365,2020.findings-emnlp.335,0,0.0200117,"herent summary. First, we present automatic evaluation metric results of different models in Table 3. In general, we observe that abstractive methods perform better than extractive methods with a large margin. Among extractive methods, SummaRunner achieves the best results, indicating the effectiveness of supervised utterance index labels. As for abstractive methods, Fast-RL and Fast-RL* perform best on almost all metrics except for ROUGEL of the overall summary, where the PGN method obtains a better result. Transformer-based methods perform worse mainly because of relatively small data size (Joshi et al., 2020). It is worth noticing that enhanced methods (Fast-RL*, TDS+SATM*) are usually better than their original version on the 6 Dataset Difﬁculties overall summary and the user summary. This highlights the effect of the key utterance indexes even In this section, we want to analyze the difﬁculties just used as supervised signals, as it can reﬂect of CSDS further. According to the ﬁne-grained which utterance is more critical for summarization. features in CSDS and the challenges mentioned in By comparing with the same model in different Section 3.6, we raise the following two questions. tasks, we ﬁn"
2021.emnlp-main.365,N18-1149,0,0.0639934,"Missing"
2021.emnlp-main.365,W02-0406,0,0.315851,"gher quality. We will leave it to future work. as supervised signals only in the training process. We name them as Fast-RL* and TDS+SATM*. Besides, all extractive methods are restricted to generate summaries less than a limited length, which is 84 for the overall summary, 38 for the user summary, and 49 for the agent summary. They are set according to the average length of reference summaries. More experimental settings are given in Appendix E. 4.3 Evaluation Metrics We employ ﬁve widely used automatic metrics to evaluate the above methods. The automatic metrics3 include: ROUGE-based methods (Lin and Hovy, 2002): Widely used metrics by measuring the overlap of n-grams between two texts. Here we choose ROUGE-2 and ROUGE-L for comparison. BLEU (Papineni et al., 2002): Another n-gram overlap metric by considering up to 4-grams. BERTScore (Zhang et al., 2020): It measures the word overlap between two texts according to contextual BERT embeddings. MoverScore (Zhao et al., 2019): It measures the semantic distance between two texts according to pretrained embeddings. Here we use BERT embedding as well. As for human evaluation metrics, we try to evaluate the quality of summaries at a ﬁne-grained topic level."
2021.emnlp-main.365,K16-1028,0,0.0794084,"Missing"
2021.emnlp-main.365,D18-1206,0,0.130882,"alogue summarization 1 Introduction datasets have been proposed recently (McCowan Text summarization aims to compress a long in- et al., 2005; Gliwa et al., 2019; Zou et al., 2021a,b), put text and generate a condensed summary (Zong none of them adds dialogue features (i.e., different et al., 2021). It can help people capture the gist of speakers’ roles or topic structure) in summaries, a long document quickly. Traditional summariza- limiting the application of these datasets. Theretion tasks mainly focus on news reports (Nallapati fore, we aim to construct a ﬁne-grained Chinese et al., 2016; Narayan et al., 2018; Zhu et al., 2018). dataset for Customer Service domain Dialogue However, as the communication tools become con- Summarization (CSDS). venient, enormous information is presented in a To achieve this goal, we employ Questionconversational format, such as meeting records, Answer (QA) pairs as the annotation format since ∗ Corresponding author. it is the basic granularity in a customer service di4436 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4436–4451 c November 7–11, 2021. 2021 Association for Computational Linguistics Fine-grained Annotation"
2021.emnlp-main.365,P02-1040,0,0.110507,"xtractive methods are restricted to generate summaries less than a limited length, which is 84 for the overall summary, 38 for the user summary, and 49 for the agent summary. They are set according to the average length of reference summaries. More experimental settings are given in Appendix E. 4.3 Evaluation Metrics We employ ﬁve widely used automatic metrics to evaluate the above methods. The automatic metrics3 include: ROUGE-based methods (Lin and Hovy, 2002): Widely used metrics by measuring the overlap of n-grams between two texts. Here we choose ROUGE-2 and ROUGE-L for comparison. BLEU (Papineni et al., 2002): Another n-gram overlap metric by considering up to 4-grams. BERTScore (Zhang et al., 2020): It measures the word overlap between two texts according to contextual BERT embeddings. MoverScore (Zhao et al., 2019): It measures the semantic distance between two texts according to pretrained embeddings. Here we use BERT embedding as well. As for human evaluation metrics, we try to evaluate the quality of summaries at a ﬁne-grained topic level. First, we split the ground truth summaries and the summaries generated by models into different topic segments4 . Then we evaluate the summary quality for"
2021.emnlp-main.365,2020.acl-main.459,0,0.0295048,"apers (Kang et al., 2018). Different from document summarization, dialogue summarization aims to summarize a conversation into a narrative text. McCowan et al. (2005); Janin et al. (2003) provide two datasets for dialogue summarization at the earliest, and the task of their data is to summarize a meeting transcript into a few short sentences. Zhong et al. (2021) incorporate them and propose a query-based summarization task. Gliwa et al. (2019) propose an English daily conversation summarization dataset on ﬁctitious dialogues, providing a new daily chatting scenario. Other datasets provided by Rameshkumar and Bailey (2020), Duan et al. (2019) and Zhu et al. (2021) also show the potential of dialogue summarization in other scenarios. As for Chinese dialogue summarization datasets, Song et al. (2020) construct a medical dialogue summarization dataset, where most of the summaries are extractive and relatively easy to generate. Almost all the above datasets only provide an overall summary for each dialogue without further annotations. In the customer service domain, Zou et al. (2021b) provide a related dialogue summarization dataset, which is the most similar to our work. However, their dataset only contains an ove"
2021.emnlp-main.365,P17-1099,0,0.0371487,"exPageRank (Erkan and Radev, 2004): This method ranks dialogue utterances by PageRank algorithm and extracts utterances in order until the length of the summary reaches the limit. SummaRuNNer (Nallapati et al., 2017): A supervised extractive summarization method by scoring each utterance using RNN. Here, we use the key utterance indexes as extractive labels. BERTExt (Liu and Lapata, 2019): This method scores each utterance in dialogue by ﬁnetuning on the pretrained BERT (Devlin et al., 2019) model. Extractive labels are the same as SummaRuNNer. We also implement some abstractive methods: PGN (See et al., 2017): An RNN-based seq2seq model using source word copy mechanism and attention coverage mechanism. Fast-RL (Chen and Bansal, 2018): This method ﬁrst extracts important sentences and then compresses them into summaries. The whole model is at last jointly trained by reinforcement learning. BERTAbs (Liu and Lapata, 2019): It uses pretrained BERT as the encoder and a transformerbased network as the decoder to summarize. TDS+SATM (Zou et al., 2021b): It is similar to Fast-RL but uses BERT and transformer structure as the extractive model and abstractive model. Besides, it also introduces a topic model"
2021.emnlp-main.365,2020.coling-main.63,0,0.0366529,"ovide two datasets for dialogue summarization at the earliest, and the task of their data is to summarize a meeting transcript into a few short sentences. Zhong et al. (2021) incorporate them and propose a query-based summarization task. Gliwa et al. (2019) propose an English daily conversation summarization dataset on ﬁctitious dialogues, providing a new daily chatting scenario. Other datasets provided by Rameshkumar and Bailey (2020), Duan et al. (2019) and Zhu et al. (2021) also show the potential of dialogue summarization in other scenarios. As for Chinese dialogue summarization datasets, Song et al. (2020) construct a medical dialogue summarization dataset, where most of the summaries are extractive and relatively easy to generate. Almost all the above datasets only provide an overall summary for each dialogue without further annotations. In the customer service domain, Zou et al. (2021b) provide a related dialogue summarization dataset, which is the most similar to our work. However, their dataset only contains an overall summary from the agent’s perspective for each dialogue. Besides, their publicly available data are difﬁcult to analyze since all the sentences are given by word indexes. On t"
2021.emnlp-main.365,D19-1053,0,0.0157651,"gth of reference summaries. More experimental settings are given in Appendix E. 4.3 Evaluation Metrics We employ ﬁve widely used automatic metrics to evaluate the above methods. The automatic metrics3 include: ROUGE-based methods (Lin and Hovy, 2002): Widely used metrics by measuring the overlap of n-grams between two texts. Here we choose ROUGE-2 and ROUGE-L for comparison. BLEU (Papineni et al., 2002): Another n-gram overlap metric by considering up to 4-grams. BERTScore (Zhang et al., 2020): It measures the word overlap between two texts according to contextual BERT embeddings. MoverScore (Zhao et al., 2019): It measures the semantic distance between two texts according to pretrained embeddings. Here we use BERT embedding as well. As for human evaluation metrics, we try to evaluate the quality of summaries at a ﬁne-grained topic level. First, we split the ground truth summaries and the summaries generated by models into different topic segments4 . Then we evaluate the summary quality for each segment in the following three aspects: informativeness, non-redundancy, ﬂuency5 . These three aspects are frequently used in the summarization community (Zhu et al., 2019; Fabbri et al., 2021), and we also"
2021.emnlp-main.365,2021.naacl-main.474,0,0.0411465,"marization, dialogue summarization aims to summarize a conversation into a narrative text. McCowan et al. (2005); Janin et al. (2003) provide two datasets for dialogue summarization at the earliest, and the task of their data is to summarize a meeting transcript into a few short sentences. Zhong et al. (2021) incorporate them and propose a query-based summarization task. Gliwa et al. (2019) propose an English daily conversation summarization dataset on ﬁctitious dialogues, providing a new daily chatting scenario. Other datasets provided by Rameshkumar and Bailey (2020), Duan et al. (2019) and Zhu et al. (2021) also show the potential of dialogue summarization in other scenarios. As for Chinese dialogue summarization datasets, Song et al. (2020) construct a medical dialogue summarization dataset, where most of the summaries are extractive and relatively easy to generate. Almost all the above datasets only provide an overall summary for each dialogue without further annotations. In the customer service domain, Zou et al. (2021b) provide a related dialogue summarization dataset, which is the most similar to our work. However, their dataset only contains an overall summary from the agent’s perspective"
2021.findings-emnlp.92,W18-6402,0,0.0314119,"Missing"
2021.findings-emnlp.92,W17-4746,0,0.0426111,"Missing"
2021.findings-emnlp.92,P17-1175,0,0.0347155,"Missing"
2021.findings-emnlp.92,P19-1642,0,0.158365,"ti30k Adam optimizer with an initial learning rate of 0.002. We set the minibatch size to 40. Models are 1 In our experiments, we use “hen_sosi ” as the language selected based on BLEU4 (Papineni et al., 2002) identification token for English reconstruction decoding and “hde_sosi ” for German translation decoding. results of the translation task on the validation data. 1070 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR NMT 35.9 (0.1) 54.9 (0.1) 28.8 (0.6) 49.5 (0.2) 25.9 (1.0) 45.7 (0.7) pRCNNs (Huang et al., 2016) DATT (Calixto et al., 2017) Imagination (Elliott and Kádár, 2017) VMMTC (Calixto et al., 2019) VMMTF (Calixto et al., 2019) 36.5 (0.8) 36.5 36.8 (0.8) 37.5 (0.3) 37.7 (0.4) 54.1 (0.7) 55.0 55.8 (0.4) 55.7 (0.1) 56.0 (0.1) 26.1 (6.6) 30.0 (0.3) 45.4 (7.3) 49.9 (0.3) 21.8 (5.6) 25.5 (0.5) 41.2 (6.3) 44.8 (0.2) word Test2016 EMMTSR EMMTSS EMMTT 37.8 (0.2) 38.0 (0.5) 36.3 (0.5) 56.1 (0.2) 56.2 (0.2) 55.0 (0.1) 30.1 (0.7) 30.3 (0.5) 28.4 (0.1) 50.3 (0.1) 50.1 (0.1) 48.6 (0.2) 27.0 (0.1) 26.1 (0.7) 25.3 (0.1) 46.4 (0.2) 45.6 (0.7) 44.3 (0.4) phrase RNN-based EMMTSR EMMTSS EMMTT 38.0 (0.1) 37.8 (0.1) 36.8 (0.1) 56.5 (0.3) 56.1 (0.2) 55.0 (0.4) 30.2 (0.8) 30.5 (0.5) 29.4 (0.2) 50.3 (0.4) 50.1"
2021.findings-emnlp.92,D17-1095,0,0.0386098,"Missing"
2021.findings-emnlp.92,W14-3348,0,0.0125568,"ms. RNN-based models: Other Settings We train our models by randomly selecting from the translation task and the reconstruction task. The parameter w is the probability of updating the translation model in the current minibatch. It is set according to the ratio of the amount of data used in the translation task and the reconstruction task. For the Multi30K dataset, we set 0.5 to keep the balance between two tasks. we report mean and standard deviation over 3 independent runs for all models. Finally, we evaluate translation quality using the metrics of BLEU4 (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). 1071 • NMT: It is the text-only RNN-based attentional NMT system (Luong et al., 2015) with default setting. • pRCNNs (Huang et al., 2016): Visual objects are respectively encoded with the source sentence. In the decoding phase, the decoder chooses to attend mostly to the relevant words in the sequence encoded with the relevant visual object. • DATT (Calixto et al., 2017): It is an NMT model with a doubly attentive decoder. One of the decoders attends to the relevant region of the image to help to predict a word. • Imagination (Elliott and Kádár, 2017): It is an NMT model with an auxiliary ta"
2021.findings-emnlp.92,W17-4718,0,0.0164199,"rmore, our in-depth analysis shows how vilearning approach for multi-modal machine translasual information improves translation. tion (EMMT). Different from sentence-level crossmodal semantics fusion approaches, our approach 1 Introduction aims to augment the entity representation explicitly. Multi-modal machine translation (MMT) aims at We frame the entity-level cross-modal learning apimproving the translation performance with the proach as a reconstruction task that reconstructs help of visual information such as image (Specia the original textual sentence from a degraded multiet al., 2016; Elliott et al., 2017; Barrault et al., 2018; modal input (Lewis et al., 2020). The multi-modal Zhang et al., 2020). The assumption behind this input is a mixture of a degraded sentence and reis that images consist of relatively complete infor- lated visual objects. The degraded sentence is mation compared with textual description and can generated by erasing the visually depictable entity provide complementary knowledge to guide trans- words as done by (Caglayan et al., 2019) and filling lation (Elliott et al., 2016). the erased position with corresponding visual obPrevious studies mainly focus on integrating the"
2021.findings-emnlp.92,W16-3210,0,0.0564879,"Missing"
2021.findings-emnlp.92,I17-1014,0,0.333768,"the encoding stage and the linguistic feature space in the decoding stage. The reconstruction model is trained to minimise the negative log-likelihood function: 2.3 Multi-task Framework As illustrated in Figure 2, the architecture of the reconstruction model is basically the same as the translation model. The objective function of translation model is also similar to LR (θ, ψ): LT (θ, ϕ) = − N X log p(yi |y&lt;i , X) (3) i where ϕ is the decoder parameters of the translation model. To combine the reconstruction task with the translation task, we mix their objective function with the parameter w (Elliott and Kádár, 2017): L(θ, ϕ, ψ) = wLT (θ, ϕ) + (1 − w)LR (θ, ψ) (4) LR (θ, ψ) = − N X log p(xi |x&lt;i , XM M ) (1) i where XM M is XM M w or XM M p , θ is the parameters of the shared encoder, and ψ is the parameters of the reconstruction decoder. We also consider reconstructing the target language text Y = {y0 , y1 , . . . , yM }. As shown in Figure 2, the black dash line points to decoder of the translation model. To reconstruct the target text Y , we modify the reconstruction objective function to: LR (θ, ψ) = − M X j log p(yj |y&lt;j , XM M ) where w is the probability of updating translation model parameters in"
2021.findings-emnlp.92,W16-2360,0,0.130593,"to 0. The RNN-based models are trained with the tively. We also evaluate our model in the Multi30k Adam optimizer with an initial learning rate of 0.002. We set the minibatch size to 40. Models are 1 In our experiments, we use “hen_sosi ” as the language selected based on BLEU4 (Papineni et al., 2002) identification token for English reconstruction decoding and “hde_sosi ” for German translation decoding. results of the translation task on the validation data. 1070 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR NMT 35.9 (0.1) 54.9 (0.1) 28.8 (0.6) 49.5 (0.2) 25.9 (1.0) 45.7 (0.7) pRCNNs (Huang et al., 2016) DATT (Calixto et al., 2017) Imagination (Elliott and Kádár, 2017) VMMTC (Calixto et al., 2019) VMMTF (Calixto et al., 2019) 36.5 (0.8) 36.5 36.8 (0.8) 37.5 (0.3) 37.7 (0.4) 54.1 (0.7) 55.0 55.8 (0.4) 55.7 (0.1) 56.0 (0.1) 26.1 (6.6) 30.0 (0.3) 45.4 (7.3) 49.9 (0.3) 21.8 (5.6) 25.5 (0.5) 41.2 (6.3) 44.8 (0.2) word Test2016 EMMTSR EMMTSS EMMTT 37.8 (0.2) 38.0 (0.5) 36.3 (0.5) 56.1 (0.2) 56.2 (0.2) 55.0 (0.1) 30.1 (0.7) 30.3 (0.5) 28.4 (0.1) 50.3 (0.1) 50.1 (0.1) 48.6 (0.2) 27.0 (0.1) 26.1 (0.7) 25.3 (0.1) 46.4 (0.2) 45.6 (0.7) 44.3 (0.4) phrase RNN-based EMMTSR EMMTSS EMMTT 38.0 (0.1) 37.8 (0.1"
2021.findings-emnlp.92,P19-1653,0,0.413649,"liott and Kádár, ies leverage the visual information through in2017; Zhou et al., 2018). Some works leverage the tegrating the global image features as auxiliary spatial information in the decoding stage by attendinput or decoding by attending to relevant loing to relevant local regions of the image (Calixto cal regions of the image. However, this kind of usage of visual information makes it difet al., 2017; Caglayan et al., 2017, 2018; Libovický ficult to figure out how the visual modality and Helcl, 2017; Libovický et al., 2018; Yao and helps and why it works. Inspired by the findWan, 2020; Ive et al., 2019). ings of (Caglayan et al., 2019) that entities are However, these sentence-level approaches which most informative in the image, we propose an implicitly incorporate image features make it exexplicit entity-level cross-modal learning aptremely difficult to figure out how visual features proach that aims to augment the entity representation. Specifically, the approach is framed affect the representation of source-side sentences as a reconstruction task that reconstructs the or the decision when generating a target-side word. original textural input from multi-modal input Furthermore, results f"
2021.findings-emnlp.92,N13-1073,0,0.0213821,"xpected to obtain more increment for the entity words. It is represented Table 4: Adversarial evaluation and ablation study reas lowering the increment difference besults on Multi30K 2016 test set. The best results are bold, and the worst are underlined. tween the other words and the entity words. The measurement is based on the sentence-level As pointed out by previous studies that noise is translation results of various MMT models. To the major part of visual features in the image-to- get the word-level translation, we employ the fasttext task. It is necessary to find out whether our align (Dyer et al., 2013) toolkit which aligns tokens model can eliminate noise and learn useful infor- from source-side to target-side and concatenates mation from visual features. We suppose that our the training set and the test set to train better alignmodels benefit from the visual object information, ments. The aligned target-side words are considthe multi-task scheme, and the de-noising ability. ered to be the translation of the source-side words. To investigate the effectiveness of these compo- We take the alignment results of reference paralnents, we conduct several experiments to compare lel data as the corr"
2021.findings-emnlp.92,2020.acl-main.703,0,0.023739,"for multi-modal machine translasual information improves translation. tion (EMMT). Different from sentence-level crossmodal semantics fusion approaches, our approach 1 Introduction aims to augment the entity representation explicitly. Multi-modal machine translation (MMT) aims at We frame the entity-level cross-modal learning apimproving the translation performance with the proach as a reconstruction task that reconstructs help of visual information such as image (Specia the original textual sentence from a degraded multiet al., 2016; Elliott et al., 2017; Barrault et al., 2018; modal input (Lewis et al., 2020). The multi-modal Zhang et al., 2020). The assumption behind this input is a mixture of a degraded sentence and reis that images consist of relatively complete infor- lated visual objects. The degraded sentence is mation compared with textual description and can generated by erasing the visually depictable entity provide complementary knowledge to guide trans- words as done by (Caglayan et al., 2019) and filling lation (Elliott et al., 2016). the erased position with corresponding visual obPrevious studies mainly focus on integrating the jects. Reconstructed from this kind of input, entity vis"
2021.findings-emnlp.92,D18-1329,0,0.0689474,"s of (Caglayan et al., 2019) that entities are However, these sentence-level approaches which most informative in the image, we propose an implicitly incorporate image features make it exexplicit entity-level cross-modal learning aptremely difficult to figure out how visual features proach that aims to augment the entity representation. Specifically, the approach is framed affect the representation of source-side sentences as a reconstruction task that reconstructs the or the decision when generating a target-side word. original textural input from multi-modal input Furthermore, results from (Elliott, 2018) have in which entities are replaced with visual feashown that visual information maybe not the reatures. Then, a multi-task framework is emson why MMT models were promoted, and it is ployed to combine the translation task and the observed that irrelevant images can improve transreconstruction task to make full use of crosslation unexpectedly. modal entity representation learning. The extensive experiments demonstrate that our apInspired by the findings of (Caglayan et al., proach can achieve comparable or even better 2019) that entities are most informative in the performance than state-of-th"
2021.findings-emnlp.92,P17-2031,0,0.0469028,"Missing"
2021.findings-emnlp.92,W18-6326,0,0.0306031,"Missing"
2021.findings-emnlp.92,J82-2005,0,0.650276,"Missing"
2021.findings-emnlp.92,D15-1166,0,0.0666437,"ation task and the reconstruction task. The parameter w is the probability of updating the translation model in the current minibatch. It is set according to the ratio of the amount of data used in the translation task and the reconstruction task. For the Multi30K dataset, we set 0.5 to keep the balance between two tasks. we report mean and standard deviation over 3 independent runs for all models. Finally, we evaluate translation quality using the metrics of BLEU4 (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). 1071 • NMT: It is the text-only RNN-based attentional NMT system (Luong et al., 2015) with default setting. • pRCNNs (Huang et al., 2016): Visual objects are respectively encoded with the source sentence. In the decoding phase, the decoder chooses to attend mostly to the relevant words in the sequence encoded with the relevant visual object. • DATT (Calixto et al., 2017): It is an NMT model with a doubly attentive decoder. One of the decoders attends to the relevant region of the image to help to predict a word. • Imagination (Elliott and Kádár, 2017): It is an NMT model with an auxiliary task that imagines the image from the source sentence description. • VMMT (Calixto et al."
2021.findings-emnlp.92,P02-1040,0,0.114208,"one trans- the decoder, and the attention layer. All model parameters are initialized sampling from a uniform lated German description. Multi30k was split into distribution u(−0.1, +0.1) and bias vectors are set three parts: training, validation, and test, containing 29,000, 1,014, and 1,000 pairs of sentences respec- to 0. The RNN-based models are trained with the tively. We also evaluate our model in the Multi30k Adam optimizer with an initial learning rate of 0.002. We set the minibatch size to 40. Models are 1 In our experiments, we use “hen_sosi ” as the language selected based on BLEU4 (Papineni et al., 2002) identification token for English reconstruction decoding and “hde_sosi ” for German translation decoding. results of the translation task on the validation data. 1070 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR NMT 35.9 (0.1) 54.9 (0.1) 28.8 (0.6) 49.5 (0.2) 25.9 (1.0) 45.7 (0.7) pRCNNs (Huang et al., 2016) DATT (Calixto et al., 2017) Imagination (Elliott and Kádár, 2017) VMMTC (Calixto et al., 2019) VMMTF (Calixto et al., 2019) 36.5 (0.8) 36.5 36.8 (0.8) 37.5 (0.3) 37.7 (0.4) 54.1 (0.7) 55.0 55.8 (0.4) 55.7 (0.1) 56.0 (0.1) 26.1 (6.6) 30.0 (0.3) 45.4 (7.3) 49.9 (0.3) 21.8 (5.6) 25.5"
2021.findings-emnlp.92,W16-2346,0,0.0462908,"Missing"
2021.findings-emnlp.92,D18-1011,1,0.816307,"focused on fusing the multi-modal information into the sentence-level semantics (Huang et al., 2016; Calixto and Liu, 2017; Calixto et al., 2017; Libovický and Helcl, 2017; Delbrouck and Dupont, 2017) in the RNN-based architecture (Bahdanau et al., 2015). Besides above approaches, Toyama et al. (2016); Calixto et al. (2019) proposed to apply latent variables as the unified semantic representations. Ive et al. (2019) proposed a translate-and-refine approach to generate a good translation from the first draft by making better use of the target language and visual context. There are also works (Wang et al., 2018a,b; Zhao et al., 2020) show that extra modality information Results of Our Multi-task Models As shown in Figure 3, neither adversarial models (“ro” is useful in a more fine-grained way. Recently, Yin et al. (2020) proposed a fineand “rw”) nor MLM models get stable lower grained method that employs a graph-based multidifferences. No evidence was found that the modal fusion encoder to fuse image and source text de-noising ability or the MLM of our multi-task scheme was a guarantee for helping the transla- in the entity level. The input sentence and image are represented as a unified graph. The"
2021.findings-emnlp.92,2020.acl-main.273,0,0.19187,"port the mean and the standard deviation over 3 independent runs. Best overall results are bold. The training procedure is halted if the model does not improve BLEU4 scores on the validation set for 10 epochs. We translate test data on the last saved model. Transformer-based Model For Transformerbased models, we set it up with a 128D word embedding layer and 256D hidden size. The embedding layer is shared between source and target vocabularies. Both the encoder and the decoder have Ld = 4 layers, and the number of heads is 4. We set the dropout to 0.2 which gets a similar baseline model with (Yin et al., 2020). Adam optimizer is applied in the same way with the original transformer model (Vaswani et al., 2017). Each training batch contained 2,000 source tokens and corresponding target sentences and images. The training was halted after 80,000 steps. All above Transformer-based settings are basically the same as the set up in the publication of (Yin et al., 2020) which we will compare with. 4 Experimental Results 4.1 Baselines We compare the proposed models against the following MMT systems. RNN-based models: Other Settings We train our models by randomly selecting from the translation task and the"
2021.findings-emnlp.92,Q14-1006,0,0.14725,"Missing"
C08-1137,C04-1073,0,0.0585709,"n et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2"
C08-1137,W06-1609,0,0.0665215,"ld be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewri"
C08-1137,D07-1077,0,0.435576,"osed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2007) used a parser to get the syntactic t"
C08-1137,P07-1091,0,0.0130776,"l was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2007) used a parser to get the syntactic t"
C08-1137,P03-1054,0,0.00678349,"Missing"
C08-1137,P05-1033,0,0.020392,"sting reordering approaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatic"
C08-1137,P06-1066,0,0.107323,"pproaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewri"
C08-1137,J04-4002,0,0.0732557,"Missing"
C08-1137,N03-1017,0,0.00581239,"model. However, reordering is always a key issue in the decoding process. A number of models have been developed to deal with the problem of reordering. The existing reordering approaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord"
C08-1137,W07-0401,0,0.0694085,"Missing"
C08-1137,P02-1038,0,\N,Missing
C08-1137,P05-1066,0,\N,Missing
C12-1185,W11-2136,0,0.0914387,"them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work is different from the existing work in the following aspects: (1) we induce PAS transformation rules to model the interrelation between source-side PAS and its target counterpart; (2) we utilize multiple SRL results to alleviate the negative impact of bad PASs; (3) we design a CKY algorithm to translate the entire PAS according to the target-side-like PAS. The algorithm can be easily inte"
C12-1185,J04-4004,0,0.0311547,"ture is used to evaluate which TP is more appropriate for the specific SP 6 . The two features indicate the distribution of bilingual PASs from two different angles, which will be helpful for the decoder to choose effective PAS transformation rules. 3 PAS-based Translation Framework In the PAS acquisition step, we perform SRL on each test sentence with a monolingual SRL system. To alleviate the negative impact of SRL errors, we use multiple SRL results. We provide the monolingual SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). FIGURE 4(a) shows an example of multiple SRL results. In the transformation step, we match the multiple SRL results with PAS transformation rules and convert them to target-side-like PASs. Then in the translation step, we decode the PAS based on these target-side-like PASs. FIGURE 4 – Multiple SRL results and the final mathcing result of the example sentence. 3.1 PAS Transformation In this section, we describe how to match the multiple SRL results with PAS transformation rules and transform them to target-side-like PASs. We design Algorithm 1 to"
C12-1185,J93-2003,0,0.02612,"transformation rules; (3) Translation: first translate the predicate and arguments of PAS and then adopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a ne"
C12-1185,J07-2003,0,0.0825815,"decision by voting with the abovementioned five parse trees. After attachment, some PASs may be identical to each other, such as the matching result M2 and M3 of FIGURE 4(b). We only retain the one whose matching score is larger. FIGURE 5 – An example of gap word attachment using parse tree. 3.3 PAS Translation In the translation step, we translate each source element by a traditional translation method. Then we combine these candidates to translate the entire PAS based on the target-side-like PAS, just as FIGURE 1 shows. Intuitively, the combination can be operated directly by cube pruning (Chiang, 2007). However, since the source elements are translated independently and many source elements’ spans are very short, numerous phrase translation rules are ignored during translation. This fact leads to a narrow decoding space and poor translation accuracy. To alleviate this problem, we design a CKY-style decoding algorithm for each target-side-like PAS. FIGURE 6 – An example of our CKY-style decoding algorithm for target-side-like PAS. In this example, only one path is generated for the final span 3-12. In practice, there can be many paths. In the CKY-style decoding algorithm, we organize the sou"
C12-1185,P03-2041,0,0.116531,"hat depicts the relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are"
C12-1185,2007.tmi-papers.10,0,0.440117,"Missing"
C12-1185,P06-1121,0,0.0316988,"KY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-"
C12-1185,W11-1012,0,0.183153,"ignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work is different from the existing work in the following aspects: (1) we induce PAS transformation rules to model the interrelation between source-side PAS and its target counterpart; (2) we utilize multiple SRL results to alleviate the negative impact of bad PASs; (3) we design a CKY algorithm to translate the entire PAS according to the target-side-like PAS. The algorithm can be easily integrated with any CKY-based decoder to generate better translation hypotheses. Conclusion and Pers"
C12-1185,P03-1054,0,0.00598437,"is more appropriate for the specific SP 6 . The two features indicate the distribution of bilingual PASs from two different angles, which will be helpful for the decoder to choose effective PAS transformation rules. 3 PAS-based Translation Framework In the PAS acquisition step, we perform SRL on each test sentence with a monolingual SRL system. To alleviate the negative impact of SRL errors, we use multiple SRL results. We provide the monolingual SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). FIGURE 4(a) shows an example of multiple SRL results. In the transformation step, we match the multiple SRL results with PAS transformation rules and convert them to target-side-like PASs. Then in the translation step, we decode the PAS based on these target-side-like PASs. FIGURE 4 – Multiple SRL results and the final mathcing result of the example sentence. 3.1 PAS Transformation In this section, we describe how to match the multiple SRL results with PAS transformation rules and transform them to target-side-like PASs. We design Algorithm 1 to achieve our purpose. First, we look for the pr"
C12-1185,N03-1017,0,0.0174728,"translate the predicate and arguments of PAS and then adopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the s"
C12-1185,W04-3250,0,0.0513644,"Missing"
C12-1185,P07-2045,0,0.00708231,"Missing"
C12-1185,2006.iwslt-evaluation.11,0,0.0653863,"TG) system segments the sentence based on its PAS. Since a correct PAS denotes the skeleton structure of the sentence, it performs both reasonable sentence segmentation and better global phrase reordering for translation. Furthermore, in the second example, our PASbased method successfully recognizes the [AM-TMP] argument “2005年” and move it to the end of sentence. However, the BTG system only performs translation without any reordering. 6 Related Work Previous work utilizing PAS in SMT can be roughly categorized into three directions. One direction is to do pre-processing or post-processing. Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted s"
C12-1185,W08-0308,0,0.217448,"ed semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work is different from the existing work in the following aspects: (1) we induce PAS transformation rules to model the interrelation between source-side PAS and its target counterpart; (2) we utilize multiple SRL results to alleviate the negative impact of bad PASs; (3"
C12-1185,C10-1081,0,0.397582,"nslation without any reordering. 6 Related Work Previous work utilizing PAS in SMT can be roughly categorized into three directions. One direction is to do pre-processing or post-processing. Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles a"
C12-1185,P06-1077,0,0.138834,"orithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure"
C12-1185,W06-1606,0,0.187065,"e the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure (PAS) is a structure"
C12-1185,P03-1021,0,0.022423,"Missing"
C12-1185,J04-4002,0,0.143747,"cate and arguments of PAS and then adopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of language"
C12-1185,P02-1040,0,0.0851425,"Missing"
C12-1185,P06-1055,0,0.0205813,"Missing"
C12-1185,D07-1078,0,0.0166676,"respectively. We compare these two ancestor nodes and attach the gap word to the element whose corresponding ancestor node is lower in the parse tree. For example in FIGURE 5, the common ancestor node of word “减税” and [A1] is node NP11,12, while it is node VP10,12 for [Pred]. Hence, we attach word “减税” to [A1] and transform the PAS1 to PAS2 in FIGURE 5. In practice, it is common that the neighboring left and right elements get the same ancestor node. This is because a father node can dominate many children nodes in parse trees. To address this problem, we employ the head binarization method (Wang et al., 2007) to binarize the parse trees. We average the five probabilities given by the parse trees as this probability. 3025 We make the final attachment decision by voting with the abovementioned five parse trees. After attachment, some PASs may be identical to each other, such as the matching result M2 and M3 of FIGURE 4(b). We only retain the one whose matching score is larger. FIGURE 5 – An example of gap word attachment using parse tree. 3.3 PAS Translation In the translation step, we translate each source element by a traditional translation method. Then we combine these candidates to translate th"
C12-1185,2009.eamt-1.30,0,0.538197,"eaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure (PAS) is a structure that depicts the relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: per"
C12-1185,N09-2004,0,0.477147,"eaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure (PAS) is a structure that depicts the relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: per"
C12-1185,W11-1003,0,0.268493,"Missing"
C12-1185,I11-1004,0,0.0730523,"based on its PAS. Since a correct PAS denotes the skeleton structure of the sentence, it performs both reasonable sentence segmentation and better global phrase reordering for translation. Furthermore, in the second example, our PASbased method successfully recognizes the [AM-TMP] argument “2005年” and move it to the end of sentence. However, the BTG system only performs translation without any reordering. 6 Related Work Previous work utilizing PAS in SMT can be roughly categorized into three directions. One direction is to do pre-processing or post-processing. Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to t"
C12-1185,P06-1066,0,0.158022,"Missing"
C12-1185,P12-1095,0,0.320708,"Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work i"
C12-1185,J08-2004,0,0.148475,"Missing"
C12-1185,2011.mtsummit-papers.29,1,0.85492,"Missing"
C12-1185,D10-1043,0,0.190317,"e relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are extracted from the r"
C12-1185,D11-1019,1,0.863801,"Missing"
C12-1185,Y09-2016,1,0.908229,"Missing"
C12-1185,D10-1030,1,0.89398,"ding translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are extracted from the result of bilingual semantic role labeling (Zhuang and Zong, 2010b). Here, target-side-like PAS denotes a list of general non-terminals in target language order, where a non-terminal aligns to a source element. Henceforward, we use source elements to denote the predicate and arguments of sourceside PAS (similarly for target elements). Translation: just as FIGURE 1 shows, this step is further divided into two parts: (a) element translation is to translate each source element respectively; (b) translation by global reordering is to combine the translation candidates of source elements to translate the entire PAS based on the target-side-like PAS. This method"
C12-1185,C10-1153,1,0.392016,"ding translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are extracted from the result of bilingual semantic role labeling (Zhuang and Zong, 2010b). Here, target-side-like PAS denotes a list of general non-terminals in target language order, where a non-terminal aligns to a source element. Henceforward, we use source elements to denote the predicate and arguments of sourceside PAS (similarly for target elements). Translation: just as FIGURE 1 shows, this step is further divided into two parts: (a) element translation is to translate each source element respectively; (b) translation by global reordering is to combine the translation candidates of source elements to translate the entire PAS based on the target-side-like PAS. This method"
C12-1186,N10-1028,0,0.126723,"Missing"
C12-1186,P09-1088,0,0.0849466,"Missing"
C12-1186,N10-1015,0,0.0527055,"nto packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on both parsing and word alignment. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Our work is different from theirs in that we are pursuing better unsupervised tree structures for better translation performance. As a whole, compared with previous works, our unsupervised trees are generated fully depending on word alignment. Therefore, by using our tree structures, the incompatibility problem between tree structures a"
C12-1186,D08-1092,0,0.0482246,"ed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on both parsing and word alignment. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Our work is different from theirs in that we are pursuing better unsupervised tree structures for better translation performance. As a whole, compared with previous works, our unsupervised trees are generated fully depending on word alignment. Therefore, by using our tree structures, the incompatibility problem"
C12-1186,D12-1079,0,0.0811569,"cording to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on bot"
C12-1186,P05-1033,0,0.452574,"Missing"
C12-1186,J07-2003,0,0.193766,"rees in the state-of-the-art string-to-tree translation system. 2 Related Work Our work focuses on inducing effective unsupervised tree structures, and meanwhile, resolving the incompatibility problem between tree structures and word alignment for tree-based translation. Several researchers have studied unsupervised tree structure induction for different objectives. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammar (SCFG) from a parallel corpus. The obtained SCFG grammar is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Denero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled tree structures for syntactic pre-reordering. Different from above works, we concentrate on producing effective and labeled unsupervised trees for tree-based translation models. Moreover, since most of the current tree-based translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two d"
C12-1186,N09-1025,0,0.0427139,"Missing"
C12-1186,D09-1037,0,0.179081,"Missing"
C12-1186,W06-1628,0,0.194541,"Missing"
C12-1186,D07-1079,0,0.0335265,"Missing"
C12-1186,P07-1003,0,0.0645151,"word alignment 6 . For example, in FIGURE 2, node PRP…RB’s span is {we meet again} and it dominates span {我 们 再次 见面} at the other side. These two spans are consistent with word alignment. Therefore, node PRP…RB is a frontier node. Our frontier node assumption makes sense in tree-based translation model. This is because with the purpose of achieving better rule coverage, we tend to extract small minimal rules as many as possible and generate larger rules by composing them. Maximizing the number of frontier nodes supports this goal, while producing many interior (non-frontier) nodes hinders it (DeNero and Klein, 2007). Hence, in the forest constructor, we follow this assumption and only consider the tree structures with the largest number of frontier nodes. Denero and Uszkoreit (2011) utilized a similar heuristic to construct their unlabeled trees. They required that all spans in their trees must align continuously to the other side. Unlike their heuristic, our frontier node assumption only maximizes the number of frontier nodes. The interior nodes are also permitted in the tree structure, which is more flexible and appropriate for constructing forests. Today we meet again , jin-tian wo-men zai-ci jian-mia"
C12-1186,D11-1018,0,0.0888056,"Missing"
C12-1186,P05-1067,0,0.0684606,"significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees. In the existing tree-ba"
C12-1186,P03-2041,0,0.0968446,"pervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees."
C12-1186,P06-1121,0,0.456171,"ey are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence w"
C12-1186,N04-1035,0,0.70857,"plify the space, we take advantage of the following assumption during forest construction: Frontier Node Assumption: The more frontier nodes the tree structure has, the more reasonable it is for translation. The heuristic values in the constraints are chosen by a series of survey and experiments on a well‐aligned corpus. ？ ！} and {. , : ; ? !} as split anchors for Chinese and English, respectively. We take the position before and after the punctuations as split point candidates. We use {。 ， 3041 Frontier nodes are utilized to factor a tree structure into several fragments for rule extraction (Galley et al., 2004). Formally, a frontier node is a node that meets the following constraint: the span of the node and its dominated span at the other side are consistent with word alignment 6 . For example, in FIGURE 2, node PRP…RB’s span is {we meet again} and it dominates span {我 们 再次 见面} at the other side. These two spans are consistent with word alignment. Therefore, node PRP…RB is a frontier node. Our frontier node assumption makes sense in tree-based translation model. This is because with the purpose of achieving better rule coverage, we tend to extract small minimal rules as many as possible and generat"
C12-1186,N04-1014,0,0.109625,"in each derivation d in the set of all derivations D. That is p (t ei , f i , ai )    p ( r ) D rd To get the derivation set D, we employ the algorithm of Mi et al., (2008a) to transform our induced packed forests into synchronous derivation forests. Practically, in order to reduce the complexity of the derivation forest, we only utilize the minimal STSG translation rules extracted by the method of Galley et al., (2004) and Mi et al., (2008b) to construct derivation forests 11 . Using the synchronous derivation forests, the rule probabilities are estimated by the insideoutside algorithm (Graehl and Knight, 2004). Here, leaf(r) and root(r) denote the leaf nonterminals and root node of rule r respectively. The inside and outside probabilities of forest node N are defined as follows, p IN ( N )  pOUT ( N )    rR ( N )    p IN ( N l )   p (r )  N l  leaf ( r )      p IN ( N l )   p ( r )  pOUT ( root ( r ))   r : N leaf ( r )  N l  leaf ( r ) { N }  where R(N) denotes the set of matched rules rooted at node N. Therefore, the process of EM algorithm is shown as follows: )n the triple, te refers to the target tree structures, f denotes the source language sentences, and a is t"
C12-1186,P07-1019,0,0.113058,"Missing"
C12-1186,W06-3601,0,0.456543,"tring-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees"
C12-1186,N03-1017,0,0.0727375,"Missing"
C12-1186,W04-3250,0,0.0277905,"aining about 7.1 million Chinese words and 9.2 million English words. We generate the final symmetric word alignment using GIZA++ and the grow-diag-final-and balance strategy. We train a 5-gram language model on the target part of the training corpus and the Xinhua portion of English Gigaword corpus. We use the NIST MT 2003 evaluation data as the development set, and adopt NIST MT04 and MT05 as the test set. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed using the re-sampling approach (Koehn, 2004). Our baseline system is an in-house string-to-tree system (named s2t) based on Galley et al. (2006) and Marcu et al. (2006). The English side of the training corpus is parsed with Berkeley parser (Petrov et al., 2006). We extract the minimal GHKM rules (Galley et al., 2004) and the rules of SPMT Model 1 (Marcu et al., 2006) with phrases up to length L=5 on the source side. Then we extract the composed rules by composing two or three adjacent minimal GHKM rules (Galley et al., 2006). The beam size of the decoder is set as 500. We further implement head binarization on the English parse trees a"
C12-1186,P07-2045,0,0.00886089,"Missing"
C12-1186,P09-4007,0,0.0141116,"., 2006). We extract the minimal GHKM rules (Galley et al., 2004) and the rules of SPMT Model 1 (Marcu et al., 2006) with phrases up to length L=5 on the source side. Then we extract the composed rules by composing two or three adjacent minimal GHKM rules (Galley et al., 2006). The beam size of the decoder is set as 500. We further implement head binarization on the English parse trees and apply the achieved binary trees to another string-to-tree system (abbreviated as s2t-hb) with the same settings of s2t. In addition, we also run the state-of-the-art hierarchical phrase-based system Joshua (Li et al., 2009) for comparison. For inducing our unsupervised tree structures, we use Urheen 12 to get the POS tags of the English corpus. Just as we described in section 3.1.1, we reuse GIZA++ and the grow-diag-final-and strategy to re-align words based on the sub-sentence pairs and then combine the alignment result together to get a new word alignment for the whole sentence pair. We perform the EM algorithm to capture the final tree structures by 20 iterations. Then we build a string-to-tree system using our induced unsupervised tree structures (abbreviated as s2t-IT). Different from the above http://www.o"
C12-1186,D12-1078,0,0.112585,"d method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on both parsing and word alignment. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Our work is different from theirs in that we are pursuing better unsupervised tree structures for better translation performance. As a whole, compared with previous works, our unsupervised trees are generated fully depending on word alignment. Therefore, by using our tree structures, the incompatibility problem between tree structures and word alignment can be well resolved. 3 Packed Forest Generation In this section, we introduce how to compress all the reasonable tree structures into a packed forest for the given flat se"
C12-1186,P06-1077,0,0.348814,"e shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a"
C12-1186,P09-1063,0,0.442132,"trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees. In the existing tree-based translation models, parse trees are essential to extrac"
C12-1186,W06-1606,0,0.335873,"l for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule e"
C12-1186,D08-1022,0,0.0389908,"Missing"
C12-1186,P08-1023,0,0.169254,"supervised trees for tree-based translation models. Moreover, since most of the current tree-based translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the al"
C12-1186,P03-1021,0,0.0851695,"Missing"
C12-1186,P02-1040,0,0.0905556,"Missing"
C12-1186,P06-1055,0,0.0901048,"n the target part of the training corpus and the Xinhua portion of English Gigaword corpus. We use the NIST MT 2003 evaluation data as the development set, and adopt NIST MT04 and MT05 as the test set. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed using the re-sampling approach (Koehn, 2004). Our baseline system is an in-house string-to-tree system (named s2t) based on Galley et al. (2006) and Marcu et al. (2006). The English side of the training corpus is parsed with Berkeley parser (Petrov et al., 2006). We extract the minimal GHKM rules (Galley et al., 2004) and the rules of SPMT Model 1 (Marcu et al., 2006) with phrases up to length L=5 on the source side. Then we extract the composed rules by composing two or three adjacent minimal GHKM rules (Galley et al., 2006). The beam size of the decoder is set as 500. We further implement head binarization on the English parse trees and apply the achieved binary trees to another string-to-tree system (abbreviated as s2t-hb) with the same settings of s2t. In addition, we also run the state-of-the-art hierarchical phrase-based system Joshua (Li et al"
C12-1186,P05-1034,0,0.129712,"rimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree tra"
C12-1186,P08-1066,0,0.0823393,"n between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE"
C12-1186,P09-1009,0,0.0444789,"Missing"
C12-1186,D07-1078,0,0.315679,"to induce unlabeled tree structures for syntactic pre-reordering. Different from above works, we concentrate on producing effective and labeled unsupervised trees for tree-based translation models. Moreover, since most of the current tree-based translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective"
C12-1186,J10-2004,0,0.114521,"Missing"
C12-1186,P06-1066,0,0.0309837,"ng yi da bu different . xiang-tong le (a) ROOT . NN... 0 , 1 NN...RB 2 .4 3 CC...JJ PRP...RB NN+PRP VBP+RB 0 NN Today jin-tian 今天 1 PRP 2 we wo-men 我们 VBP meet zai-ci 再次 3 RB 4 again , CC DT NN VBZ but the situation is jian-mian , qing-xing , 见面 情形 yi 已 da 大 RB JJ quite different . bu xiang-tong le 。 不 相同 了。 (b) FIGURE 3 – (a) An example of bilingual sentence segmentation. (b) The ultimate packed forest of the example sentence pair in (a). 3.2 Node Labeling To create packed forests for sentences, a problem that must be resolved is how to label the forest nodes without any syntactic knowledge. Xiong et al. (2006) showed that the boundary word of a phrase is a very effective indicator for phrase reordering. Zollmann and Vogel (2011) labeled hierarchical rules with word classes of boundary words and achieved better translation A node’s dominated span at the other side refers to the minimum continuous span covering all the words that are reachable from the node via word alignment. Two spans are consistent with word alignment means that words in one span only align to words in the other span via word alignment, and vice versa. 3042 performance. Inspired by their work, we combine word classes of boundary w"
C12-1186,2011.mtsummit-papers.29,1,0.680435,"Missing"
C12-1186,P11-1084,0,0.441569,"translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to"
C12-1186,N06-1033,0,0.178101,"Missing"
C12-1186,P09-1020,0,0.1385,"Missing"
C12-1186,D11-1019,1,0.914813,"translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to"
C12-1186,Y09-2016,1,0.891605,"Missing"
C12-1186,2007.mtsummit-papers.71,0,0.814658,"Missing"
C12-1186,P08-1064,0,0.157759,"e system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees. In the existing tree-based translation models, parse trees are"
C12-1186,W06-3119,0,0.171084,"Missing"
C12-1186,P11-1001,0,0.109517,"ay jin-tian 今天 1 PRP 2 we wo-men 我们 VBP meet zai-ci 再次 3 RB 4 again , CC DT NN VBZ but the situation is jian-mian , qing-xing , 见面 情形 yi 已 da 大 RB JJ quite different . bu xiang-tong le 。 不 相同 了。 (b) FIGURE 3 – (a) An example of bilingual sentence segmentation. (b) The ultimate packed forest of the example sentence pair in (a). 3.2 Node Labeling To create packed forests for sentences, a problem that must be resolved is how to label the forest nodes without any syntactic knowledge. Xiong et al. (2006) showed that the boundary word of a phrase is a very effective indicator for phrase reordering. Zollmann and Vogel (2011) labeled hierarchical rules with word classes of boundary words and achieved better translation A node’s dominated span at the other side refers to the minimum continuous span covering all the words that are reachable from the node via word alignment. Two spans are consistent with word alignment means that words in one span only align to words in the other span via word alignment, and vice versa. 3042 performance. Inspired by their work, we combine word classes of boundary words to label forest nodes. We divide the non-leaf forest nodes into three groups: one-word node, dominating only one wor"
C12-1186,N10-1016,0,\N,Missing
C16-1020,J07-4004,0,0.0911108,"Missing"
C16-1020,J07-3004,0,0.0324898,"Missing"
C16-1020,N16-1030,0,0.03397,"each token in a sequence. Typical examples include part-of-speech (POS) tagging and combinatory category grammar (CCG) supertagging. A regular feature of sequential tagging is that the input tokens in a sequence cannot be assumed to be independent since the same token in different contexts can be assigned to different tags. Therefore, the classifier should have memories to remember the contexts to make a correct prediction. Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al., 2015; Vaswani et al., 2016; Lample et al., 2016). The horizontal hierarchy of LSTMs with bidirectional processing can remember the long-range dependencies without affecting the short-term storage. Although the models have a deep horizontal hierarchy (the depth is the same as the sequence length), the vertical hierarchy is often shallow, which may not be efficient at representing each token. Stacked LSTMs are deep in both directions, but become harder to train due to the feed-forward structure of stacked layers. Skip connections (or shortcut connections) enable unimpeded information flow by adding direct connections across different layers ("
C16-1020,Q14-1026,0,0.0296003,"Missing"
C16-1020,N16-1026,0,0.0273145,"Missing"
C16-1020,J93-2004,0,0.0548993,"Missing"
C16-1020,D14-1162,0,0.0802863,"Missing"
C16-1020,P11-2009,0,0.0467262,"Missing"
C16-1020,N16-1027,0,0.0450828,"ng discrete labels to each token in a sequence. Typical examples include part-of-speech (POS) tagging and combinatory category grammar (CCG) supertagging. A regular feature of sequential tagging is that the input tokens in a sequence cannot be assumed to be independent since the same token in different contexts can be assigned to different tags. Therefore, the classifier should have memories to remember the contexts to make a correct prediction. Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al., 2015; Vaswani et al., 2016; Lample et al., 2016). The horizontal hierarchy of LSTMs with bidirectional processing can remember the long-range dependencies without affecting the short-term storage. Although the models have a deep horizontal hierarchy (the depth is the same as the sequence length), the vertical hierarchy is often shallow, which may not be efficient at representing each token. Stacked LSTMs are deep in both directions, but become harder to train due to the feed-forward structure of stacked layers. Skip connections (or shortcut connections) enable unimpeded information flow by adding direct connections acr"
C16-1020,N16-1025,0,0.0389185,"Missing"
C16-1073,C14-1008,0,0.0427566,"Missing"
C16-1073,D15-1298,0,0.048117,"Missing"
C16-1073,P13-1045,0,0.0235155,"ontext. Experimental results on similarity task and analogy task show that the word representations learned by the proposed method outperform the competitive baselines. 1 Introduction Different from traditional one-hot sparse vector representation, word embeddings are dense and lowdimensional. Due to natural advantage in word similarity computation, word embeddings are useful in a variety of applications, such as information retrieval (Uddin et al., 2013; Ganguly et al., 2015), sentiment analysis (Santos et al., 2014; Nguyen et al., 2015), question answering (Tellex et al., 2003) and parsing (Socher et al., 2013). Researchers learn word embeddings in various ways. Matrix Factorization methods for generating dense word embeddings have been used for years (Lund et al., 1996). While in recent years, neural network and deep learning have become popular approaches for learning word embeddings since Bengio et al. (2003) introduced feed forward neural network into traditional n-gram language models. For example, Collobert and Weston proposed a new objective function to learn word embeddings and improved word embeddings’ quality (Collobert et al., 2008). Huang et al. (2012) presented a new neural network arch"
C16-1073,P12-1092,0,0.535088,"(Tellex et al., 2003) and parsing (Socher et al., 2013). Researchers learn word embeddings in various ways. Matrix Factorization methods for generating dense word embeddings have been used for years (Lund et al., 1996). While in recent years, neural network and deep learning have become popular approaches for learning word embeddings since Bengio et al. (2003) introduced feed forward neural network into traditional n-gram language models. For example, Collobert and Weston proposed a new objective function to learn word embeddings and improved word embeddings’ quality (Collobert et al., 2008). Huang et al. (2012) presented a new neural network architecture which incorporated both local and global document context, and offered an impressive result. The word2vec toolkit developed by (Mikolov et al., 2013a; Mikolov et al., 2013b) implemented both Skip-gram and CBOW models which could provide high-quality word embeddings. In spite of many successful applications, most word embeddings have a common problem that each word is represented by a single vector, subsequently ignoring polysemy. For example, the word bank cannot have high cosine similarity with the word river and money at the same time since these"
C16-1073,D14-1113,0,0.462299,"e models have been proposed to alleviate the problem caused by the polysemy and homonymy. Guo et al. (2014) took advantages of bilingual resources and affinity propagation clustering algorithm to learn multiple embeddings corresponding with multiple word senses. Due to the limitation of bilingual resources, they couldn’t train their model on large scale corpus. Huang et al. (2012) pre-clustered the contexts of a word into K classes, and then learned K embeddings per word. K was a predefined value that would make matters confusion because different words usually had different number of senses. Neelakantan et al. (2014) shifted clusters into the training progress and ∗ The authors contributed equally to this paper This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 762 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 762–771, Osaka, Japan, December 11-17 2016. proposed a non-parametric clustering model which could dynamically generate new cluster center by similarity threshold λ and thus could learn different number of embeddings per word type. Alt"
C16-1073,N10-1013,0,0.0620306,"Missing"
C18-1121,H05-1079,0,0.0828249,"rence is considerable, leading to high ROUGE scores, the summary is invalid. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1430 Proceedings of the 27th International Conference on Computational Linguistics, pages 1430–1441 Santa Fe, New Mexico, USA, August 20-26, 2018. We argue that correctness is an essential requirement for summarization systems, while most existing systems ignore it. Generally, a correct summary is semantically entailed by the source sentence, thus we believe entailment1 (Bos and Markert, 2005) knowledge is beneficial to avoid producing contradictory or unrelated information in the summary. To incorporate entailment knowledge into abstractive summarization models, we propose in this work an entailment-aware encoder and an entailment-aware decoder. We share the encoder of the summarization generation system with the entailment recognition system, so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships. Furthermore, we propose an entailment Reward Augmented Maximum Likelihood (RAML) (Norouzi et al., 2016) training that encourages the"
C18-1121,D15-1075,0,0.0406554,"MLT 65 21.8 60 21.4 55 1 All words Figure 2: Average count of novel words (words that do not appear in the article). Seq2seq model generates more novel words, but less words are in the reference compared to our model. Source 1 (a) Words in the reference Reference Seq2seq Seq2seq + MTL + ERAML Source 2 Reference Seq2seq Seq2seq + MTL + ERAML Source 3 Reference Seq2seq Seq2seq + MTL + ERAML α 50 5 10 20 30 40 50 60 70 80 90 100 5 10 20 30 40 50 60 70 80 90 100 21 0 α (b) Figure 3: The performance of (a) summarization generation on Gigaword validation set and (b) entailment recognition on SNLI (Bowman et al., 2015) validation set with different task batch switches (α). brazilian stocks rose , led by consumer stocks , after the government said it would n’t impose restraints on consumer credit brazil stocks rise after government rules out credit restraints brazil stocks rise on consumer credit concerns brazil stocks rise after government says it wo n’t impose credit restrictions authorities have denied neo-nazi groups permission to stage a demonstration next week in the austrian capital , where skinheads planned to gather on the ##th anniversary of nazi germany ’s surrender ending world war ii in europe ."
C18-1121,P16-1046,0,0.0295625,"tion, where the proposal distribution is Hamming distance sampling2 . We define entailment reward s(x, y, y∗ ) as follows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch"
C18-1121,N16-1012,0,0.422367,"rence, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and sum"
C18-1121,P16-1154,0,0.0501788,"t summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder."
C18-1121,P16-1014,0,0.0343157,"an et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solv"
C18-1121,S14-1010,0,0.0768699,"odel to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descriptions. To the best of our knowledge, our work is the first to directly explore the correctness of summary without any preprocessing. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has used textual entailment recognition to reduce redundancy for extractive summarization task. Our work is partially inspired by the models of Pasunuru et al. (2017) with following differences: Pasunuru et al. (2017) model the entailment task as the seq2seq generation problem and enforce sharing of the same decoder between summarization and entailment. However, the entailment task is more reasonable to be considered as a multi-label classification problem rather than a generation problem. We thus design a multi-task learning framework in which the summarization generation task shares the sam"
C18-1121,D17-1114,1,0.833514,". We define entailment reward s(x, y, y∗ ) as follows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate"
C18-1121,D17-1222,0,0.0326185,". We define entailment reward s(x, y, y∗ ) as follows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate"
C18-1121,D15-1166,0,0.0732554,"ABS. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They use an attentive CNN encoder and neural network language model decoder to summarize sentence. ABS+. Rush et al. (2015) further tune ABS model on DUC 2003 dataset, then test on DUC 2004 test set. CAs2s. Chopra et al. (2016) extend the ABS model with a convolutional encoder and RNN decoder, which performs better than the ABS model. Feats2s. Nallapati et al. (2016) use a full RNN seq2seq model and add some lexical features to enhance the encoder, including POS, NER tags and TF-IDF values. Luong-NMT. Luong et al. (2015) propose a neural machine translation model with two-layer LSTMs for the encoder-decoder. Seq2seq. This is a standard seq2seq model with attention mechanism. Seq2seq + MTL. This is our proposed model with entailment-aware encoder, which applies a multi-task learning (MTL) framework to seq2seq model. Seq2seq + MTL (Share decoder). Pasunuru et al. (2017) propose a multi-task learning (MTL) framework in which the decoder is shared for summarization generation and entailment generation task. Seq2seq + ERAML. This is our proposed model with entailment-aware decoder, which conducts an Entailment Rew"
C18-1121,P17-2100,0,0.0464098,"neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descrip"
C18-1121,W13-2117,0,0.0719189,"selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descriptions. To the best of our knowledge, our work is the first to directly explore the correctness of summary without any preprocessing. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has used textual entailment recognition to reduce redundancy for extractive summarization task. Our work is partially inspired by the models of Pasunuru et al. (2017) with following differences: Pasunuru et al. (2017) model the entailment task as the seq2seq generation problem and enforce sharing of the same decoder between summarization and entailment. However, the entailment task is more reasonable to be considered as a multi-label classification problem rather than a generation problem. We thus design a multi-task learning framework in which the summarization generatio"
C18-1121,P16-2022,0,0.0260826,"ation model is trained by minimizing negative log-likelihood loss as in Equation 4. 3.2.3 Matching-based Entailment Inference Model To infer entailment relation, input sentence pairs from the entailment recognition corpus are fed into sentence encoder to obtain hidden representation (hu1 , · · · , hun ) and (hv1 , · · · , hvn ), respectively. Then, → − ← − → − ← − the sentence pairs are encoded as vectors u = [ h un ; h u1 ] and v = [ h vn ; h v1 ], respectively. Next, the absolute difference and the element-wise product for the tuple [u, v] are concatenated with the original vectors u and v (Mou et al., 2016) as follows: q = [|u − v|; u ∗ v; u; v] (13) We then feed q into a 3-layer multilayer perceptron (MLP) classifier. The 3-class softmax output layer is on top of MLP. The entailment recognition model is trained by minimizing cross-entropy loss. 3.2.4 Multi-Task Learning (MTL) In our multi-task setup, we share the encoder parameters of both the tasks, as shown in Figure 1(a). Traditional MTL considers equal contribution for all tasks. In our model, two tasks are significantly different. The summary generation task is much more complicated than entailment recognition, leading to different learnin"
C18-1121,K16-1028,0,0.459457,"2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017)"
C18-1121,D16-1244,0,0.0921424,"Missing"
C18-1121,W17-4504,0,0.64199,", two tasks are significantly different. The summary generation task is much more complicated than entailment recognition, leading to different learning difficulties and convergence rates. Therefore, summarization generation is regarded as the main task and entailment recognition as the auxiliary task, and our goal is to optimize the main task with assistance of auxiliary task. To this end, we optimize the two loss functions alternatively during training. Let α be the number of mini-batches of training for entailment recognition after 100 mini-batches of training for summarization generation (Pasunuru et al., 2017). We adopt α = 10 and performance with different α is discussed in Section 6.6.3. 3.3 Entailment-aware Decoder In order to encourage the decoder of the summarization system to produce summary entailed by the source sentence, we apply an entailment-aware decoder by entailment RAML training (Norouzi et al., 2016). 3.3.1 Reward Augmented Maximum Likelihood (RAML) Training RAML provides a computationally efficient approach to optimize task-specific reward (loss) directly. In our work, we apply RAML to incorporate entailment-based reward into our summarization model, as shown in Figure 1(b). The RA"
C18-1121,D15-1044,0,0.755145,"lows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to"
C18-1121,D16-1112,0,0.0661736,"Missing"
C18-1121,P17-1101,0,0.743107,"pati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descriptions. To the best of our knowledge, our work is the first to directly explore the correctness of summary without any preprocessing. Some previo"
C18-1305,N13-1073,0,0.0244006,"Missing"
C18-1305,P16-1162,0,0.105057,"stripped off before entering the general translator. We then align the words between the source sentence and the translated sentence, in order to add back slot labels into the translated sentence. This is an indirect slot transferring approach. Some limitations of the naive translation approach come from both translation and alignment. There are plenty of slot values like song names and contact names in the original SLU corpus. Many of these words are out-of-vocabulary (OOV) for the translation model. Although the translation model can handle these OOV words with sub-word modeling techniques (Sennrich et al., 2016), there are still many slot values remain to be OOV or mistranslated. For example, the Chinese name “白晓霞” in Table 1 is literally translated to “white sunshine”. Furthermore, wrong translations might also result in wrong alignments, which will yield inaccurate positions for slot labels. 3.2 Token-added Translation To make the translator be aware of slots, we then propose a token-added translation approach. This approach uses some special tokens to mark the segmentation boundary for the slot value in the source sentence. These special tokens are common in both the source vocabulary and target v"
C18-1305,stepanov-etal-2014-development,0,0.0286835,"2. However, bilingual in-domain data is scarce and costly, making it difficult to deliver both quality and low cost. Jabaian et al. (2010; 2013) propose two language transferring schemes. One is transferring source language annotation indirectly through word alignment. The other one is forcing the SMT model to translate the segmentation and slot labels simultaneously. The authors report that the indirect alignment gives the best performance. However, they also point out that distant language pairs suffer severely in word alignment. Finally rather than relying on automatic machine translation, Stepanov et al. (2014) prefer using human professional translation services. In Stepanov et al. (2017), they extend their work via crowdsourcing for semantic annotation. 3598 Methods Naive Translation Token-added Translation Class-based / SCRT Source input 我想打个电话给白晓霞 我想打个电话给（ a 白晓霞 ） 我想打个电话给 $contact name Translation result I would like to make a call to telephone number of white sunshine I would like to make a call to ( a white sunshine ) ’s telephone number please I would love to make a call to $contact name ’s number please Table 1: Translation examples by different translation systems. 3 Translation Systems Thi"
D11-1019,P05-1033,0,0.152867,"ple, our proposed system moves the prepositional phrase at an early 212 date after the sibling verb phrase. It is more reasonable compared with the baseline system s2t. In the third example, the proposed system FT2ETDeepSim successfully recognizes the Chinese long prepositional phrase 在 与 中国 总理 温家宝 举行 峰 会 后 发布 的联合 声明 中 and short verb phrase 说, and obtains the correct phrase reordering at last. 7 Related Work Several studies have tried to incorporate source or target syntax into translation models in a fuzzy manner. Zollmann and Venugopal (2006) augment the hierarchical string-to-string rules (Chiang, 2005) with target-side syntax. They annotate the target side of each string-to-string rule using SAMT-style syntactic categories and aim to generate the output more syntactically. Zhang et al. (2010) base their approach on tree-to-string models, and generate grammatical output more reliably with the help of tree-to-tree sequence rules. Neither of them builds target syntactic trees using target syntax, however. Thus they can be viewed as integrating target syntax in a fuzzy manner. By contrast, we base our approach on a string-to-tree model which does construct target syntactic trees during decoding"
D11-1019,J07-2003,0,0.916312,"annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the transla"
D11-1019,P10-1146,0,0.317476,"curacy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and Liu (2010) proposed to enhance the tree-to-string model with target dependency structures (as a language model). In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax. 204 Proceedings of the 2011 Conference on Empirical Methods in Natural Language"
D11-1019,N09-1025,0,0.422919,"ive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as sh"
D11-1019,P05-1067,0,0.192745,"rom bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and Liu (2010) proposed to enhance the tree-to-string model with target dependency structures (as a language model). In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax. 2"
D11-1019,N04-1035,0,0.370764,"e-syntax-decorated string-to-tree rule as a fuzzy-tree to exact-tree rule. We first briefly review issues of string-to-tree rule extraction; then we discuss how to augment the string-to-tree rules to yield fuzzy-tree to exact-tree rules. Figure 1: Two alternative derivations for a sample string-to-tree translation. The rules used are listed on the right. The target yield of the tree with solid lines is hussein and terrorist networks established relations. The target yield of the tree with dotted lines is hussein established relations with terrorist networks. 2.1 String-to-Tree Rule Extraction Galley et al. (2004) proposed the GHKM algorithm for extracting (minimal) string-to-tree translation rules from a triple (f, et, a), where f is the sourcelanguage sentence, et is a target-language parse tree whose yield e is the translation of f, and a is the set of word alignments between e and f. The basic idea of GHKM is to obtain the set of minimally-sized translation rules which can explain the mappings between source string and target parse tree. The minimal string-to-tree rules are extracted in three steps: (1) frontier set computation; (2) fragmentation; and (3) extraction. The frontier set (FS) is the se"
D11-1019,P06-1121,0,0.915213,"slation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well"
D11-1019,D10-1063,0,0.0605581,"Missing"
D11-1019,P07-1019,0,0.051178,"is an augmentation of the string-to-tree model. In the baseline string-to-tree model, the decoder searches for the optimal derivation d * that parses a source string f into a target tree et from all possible derivations D: d D  3 d  R  d |f  (8)    01 12  match   13  unmatch  Translation Model and Decoding d *  arg max 1 log pLM   d    2   d  (7) (5) The 0-1 matching 4 is triggered only when we set   01  1 . The other two fuzzy matching algorithms are triggered in a similar way. During decoding, we use a CKY-style parser with beam search and cube-pruning (Huang and Chiang, 2007) to decode the new source sentences. 6 6.1 Experiments Experimental Setup where the first element is a language model score in which   d  is the target yield of derivation d ; the second element is the translation length penalty; the third element is used to control the derivation length; and the last element is a translation score that includes six features: The experiments are conducted on Chinese-toEnglish translation, with training data consisting of about 19 million English words and 17 million Chinese words 5. We performed bidirectional word alignment using GIZA++, and employed the gr"
D11-1019,W06-3601,0,0.591091,"ntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up ove"
D11-1019,D10-1014,0,0.285965,"le will be 7.2e-6. Similar to 0-1 matching, likelihood matching will serve as an additional model feature representing the compatibility between categories and rules. 3.3 Deep Similarity Matching Considering the two algorithms above, we can see that the purpose of fuzzy matching is in fact to calculate a similarity. 0-1 matching assigns similarity 1 for exact matches and 0 for mismatch, while likelihood matching directly utilizes the likelihood to measure the similarity. Going one step further, we adopt a measure of deep similarity, computed using latent distributions of syntactic categories. Huang et al. (2010) proposed this method to compute the similarity between two syntactic tag sequences, used to impose soft syntactic constraints in hierarchical phrase-based models. Analogously, we borrow this idea to calculate the similarity between two SAMT-style syntactic categories, and then apply it to calculate the degree of matching between a translation rule and the syntactic category of a test source string for purposes of fuzzy matching. We call this procedure deep similarity matching. Instead of directly using the SAMT-style syntactic categories, we represent each category by a real-valued feature ve"
D11-1019,P07-2045,0,0.00802438,"Missing"
D11-1019,W04-3250,0,0.373957,"tem ourselves according to (Galley et al., 2006; Marcu et al., 2006). We extracted minimal GHKM rules and the rules of SPMT Model 1 with source language phrases up to length L=4. We further extracted composed rules by composing two or three minimal GHKM rules. We also ran the stateof-the-art hierarchical phrase-based system Joshua (Li et al., 2009) for comparison. In all systems, we set the beam size to 200. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed using the re-sampling approach (Koehn, 2004). 6.2 Results Table 1 shows the translation results on development and test sets. First, we investigate the performance of the strong baseline string-to-tree model (s2t for short). As the table shows, s2t outperforms the hierarchical phrase-based system Joshua by more than 1.0 BLEU point in all translation tasks. This result verifies the superiority of the baseline string-to-tree model. With the s2t system providing a baseline, we further study the effectiveness of our sourcesyntax-augmented string-to-tree system with fuzzy-tree to exact-tree rules (we use FT2ET to denote our proposed system)."
D11-1019,W09-0424,0,0.0407669,"Missing"
D11-1019,P06-1077,0,0.420049,"via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it"
D11-1019,W06-1606,0,0.705211,"zy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful"
D11-1019,P08-1114,0,0.342219,"syntactic categories. Since we need to deal with the potential problem that the rule is hit by the test string but the syntactic category of the test string is not in the category set of the rule’s source side, we apply the m-estimate of probability (Mitchell, 1997) to calculate a smoothed likelihood 0-1 Matching 0-1 matching is a straightforward approach that rewards rules whose source syntactic category exactly matches the syntactic category of the test string and punishes mismatches. It has mainly been employed in hierarchical phrase-based models for integrating source or both-side syntax (Marton and Resnik, 2008; Chiang et al., 2009; Chiang, 2010). Since it is verified to be very effective in hierarchical models, we borrow this idea in our source-syntax-augmented string-to-tree translation. In 0-1 matching, the rule’s source side must contain only one syntactic category, but a rule may have been decorated with more than one syntactic category on the source side. Thus we have to choose the most reliable category and discard the others. Here, we select the one with the highest frequency. For example, the tag P in the rule 和 P : 6, CC : 4  IN  with  appears more frequently, so the final rule used i"
D11-1019,P10-1145,0,0.0189445,"., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and Liu (2010) proposed to enhance the tree-to-string model with target dependency structures (as a language model). In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax. 204 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 204–215, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics First, we give a motivatin"
D11-1019,P03-1021,0,0.0349121,"rized rules. It should be noted that our strategy makes the annotated binarized rules equivalent to the original rule. 210 symmetric word alignment. We parsed both sides of the parallel text with the Berkeley parser (Petrov et al., 2006) and trained a 5-gram language model with the target part of the bilingual data and the Xinhua portion of the English Gigaword corpus. For tuning and testing, we use NIST MT evaluation data for Chinese-to-English from 2003 to 2006 (MT03 to MT06). The development data set comes from MT06 in which sentences with more than 20 words are removed to speed up MERT 6 (Och, 2003). The test set includes MT03 to MT05. We implemented the baseline string-to-tree system ourselves according to (Galley et al., 2006; Marcu et al., 2006). We extracted minimal GHKM rules and the rules of SPMT Model 1 with source language phrases up to length L=4. We further extracted composed rules by composing two or three minimal GHKM rules. We also ran the stateof-the-art hierarchical phrase-based system Joshua (Li et al., 2009) for comparison. In all systems, we set the beam size to 200. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length pena"
D11-1019,P06-1055,0,0.214001,"the unmatch_count is redundant. In practice, since the derivation may include glue rules which are not scored by fuzzy matching. Thus, ""unmatch_count + match_count + glue_rule_number = derivation_length"". 5 LDC catalog number: LDC2002E18, LDC2003E14, LDC2003E07, LDC2004T07 and LDC2005T06. We call it heuristic because there may be other syntactic annotation strategies for the binarized rules. It should be noted that our strategy makes the annotated binarized rules equivalent to the original rule. 210 symmetric word alignment. We parsed both sides of the parallel text with the Berkeley parser (Petrov et al., 2006) and trained a 5-gram language model with the target part of the bilingual data and the Xinhua portion of the English Gigaword corpus. For tuning and testing, we use NIST MT evaluation data for Chinese-to-English from 2003 to 2006 (MT03 to MT06). The development data set comes from MT06 in which sentences with more than 20 words are removed to speed up MERT 6 (Och, 2003). The test set includes MT03 to MT05. We implemented the baseline string-to-tree system ourselves according to (Galley et al., 2006; Marcu et al., 2006). We extracted minimal GHKM rules and the rules of SPMT Model 1 with source"
D11-1019,P05-1034,0,0.434154,"Missing"
D11-1019,P08-1066,0,0.153032,"syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving trans"
D11-1019,P10-1076,0,0.0113477,"statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al.,"
D11-1019,N06-1033,0,0.168469,"source-side parsed, word-aligned parallel corpus. The EM algorithm is employed to induce the parameters. We simply follow the algorithm of (Huang et al., 2010), except that we replace the tag sequence with SAMT-style syntactic categories. 4 Rule Binarization In the baseline string-to-tree model, the rules are not in Chomsky Normal Form. There are several ways to ensure cubic-time decoding. One way is to prune the extracted rules using a scope-3 grammar and do SCFG decoding without binarization (Hopkins and Lengmead, 2010). The other, and most popular way is to binarize the translation rules (Zhang et al., 2006). We adopt the latter approach for efficient decoding with integrated ngram language models since this binarization technique has been well studied in string-to-tree translation. However, when the rules’ source string is decorated with syntax (fuzzy-tree to exact-tree rules), how should we binarize these rules? We use the rule rn in Figure 2 for illustration: rn : x2 x0 x1  PP *VP  VP  x0 : VB x1 : NP x2 : PP  . Without regarding the source-side syntax, we obtain the following two binarized rules: B1: B2 :  x2 x0*1  VP x0*1 : Vx0 * x1 x2 : PP  x0 x1  Vx0 * x1  x0 : VB x1 : NP  Since"
D11-1019,P09-1020,0,0.0848052,"et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and L"
D11-1019,W06-3119,0,0.852615,"te its source string? 2) Given the source-annotated string-to-tree rules, how should we match these rules according to the test source tree during decoding? 3) How should we binarize the sourceannotated string-to-tree rules for efficient decoding? For the first problem, one may require the source side of a string-to-tree rule to be a constituent. However, such excessive constraints will exclude many good string-to-tree rules whose source strings are not constituents. Inspired by Chiang (2010), we adopt a fuzzy way to label 205 every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). This method leads to a one-to-one correspondence between the new rules and the string-to-tree rules. We will detail our fuzzy labeling method in Section 2. For the second problem, it appears simple and intuitive to match rules by requiring a rule’s source syntactic category to be the same as the category of the test string. However, this hard constraint will greatly narrow the search space during decoding. Continuing to pursue the fuzzy methodology, we adopt a fuzzy matching procedure to enable matching of all the rules whose source strings match the test string, and then determine the degre"
D11-1019,D10-1043,0,\N,Missing
D11-1019,P08-1023,0,\N,Missing
D11-1019,P09-1063,0,\N,Missing
D11-1019,W06-1628,0,\N,Missing
D16-1160,P16-1185,0,0.634478,"(SMT) (Koehn et al., 2007; Chiang, 2007), Gulcehre et al. (2015) and Sennrich et al. (2015) attempt to enhance the decoder network model of NMT by incorporating the targetside monolingual data so as to boost the translation fluency. They report promising improvements by using the target-side monolingual data. In contrast, the source-side monolingual data is not fully explored. Luong et al. (2015a) adopt a simple autoencoder or skip-thought method (Kiros et al., 2015) to exploit the source-side monolingual data, but no significant BLEU gains are reported. Note that, in parallel to our efforts, Cheng et al. (2016b) have explored the usage of both source and target monolingual data using a similar semi-supervised reconstruction method, in which two NMTs are employed. One translates the source-side monolingual data into target translations, and the other reconstructs the source-side monolingual data from the target translations. In this work, we investigate the usage of the source-side large-scale monolingual data in NMT and aim at greatly enhancing its encoder network so that we can obtain high quality context vector representations. To achieve this goal, we propose two 1535 Proceedings of the 2016 Con"
D16-1160,P15-1166,0,0.021441,"ale monolingual data. Sennrich et al. (2015) further propose a new approach to use targetside monolingual data. They generate the synthetic bilingual data by translating the target monolingual sentences to source language sentences and retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual"
D16-1160,N16-1101,0,0.0958203,"Missing"
D16-1160,P15-1001,0,0.0725684,"n quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT. Gulcehre et al. (2015) first investigate the targetside monolingual data in NMT. They propose shallow and deep fusion methods to enhance the decoder network by training a big language model on targetside large-scale mono"
D16-1160,D13-1176,0,0.0933825,"aches to make full use of the sourceside monolingual data in NMT. The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT. 1 Introduction Neural Machine Translation (NMT) following the encoder-decoder architecture proposed by (Kalchbrenner and Blunsom, 2013; Cho et al., 2014) has become the novel paradigm and obtained state-ofthe-art translation quality for several language pairs, such as English-to-French and English-to-German (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015). This endto-end NMT typically consists of two recurrent neural networks. The encoder network maps the source Currently, most NMT methods utilize only the sentence aligned parallel corpus for model training, which limits the capacity of the model. Recently, inspired by the successful application of target monolingual data in convent"
D16-1160,P07-2045,0,0.0444293,"digm and obtained state-ofthe-art translation quality for several language pairs, such as English-to-French and English-to-German (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015). This endto-end NMT typically consists of two recurrent neural networks. The encoder network maps the source Currently, most NMT methods utilize only the sentence aligned parallel corpus for model training, which limits the capacity of the model. Recently, inspired by the successful application of target monolingual data in conventional statistical machine translation (SMT) (Koehn et al., 2007; Chiang, 2007), Gulcehre et al. (2015) and Sennrich et al. (2015) attempt to enhance the decoder network model of NMT by incorporating the targetside monolingual data so as to boost the translation fluency. They report promising improvements by using the target-side monolingual data. In contrast, the source-side monolingual data is not fully explored. Luong et al. (2015a) adopt a simple autoencoder or skip-thought method (Kiros et al., 2015) to exploit the source-side monolingual data, but no significant BLEU gains are reported. Note that, in parallel to our efforts, Cheng et al. (2016b) have"
D16-1160,D15-1166,0,0.679943,"using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT. 1 Introduction Neural Machine Translation (NMT) following the encoder-decoder architecture proposed by (Kalchbrenner and Blunsom, 2013; Cho et al., 2014) has become the novel paradigm and obtained state-ofthe-art translation quality for several language pairs, such as English-to-French and English-to-German (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015). This endto-end NMT typically consists of two recurrent neural networks. The encoder network maps the source Currently, most NMT methods utilize only the sentence aligned parallel corpus for model training, which limits the capacity of the model. Recently, inspired by the successful application of target monolingual data in conventional statistical machine translation (SMT) (Koehn et al., 2007; Chiang, 2007), Gulcehre et al. (2015) and Sennrich et al. (2015) attempt to enhance the decoder network model of NMT by incorporating the targetside monolingual data so as to b"
D16-1160,P15-1002,0,0.0633985,"Missing"
D16-1160,D16-1096,0,0.0201568,"since large-scale training data can make the parameters of the encoder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT"
D16-1160,D16-1249,0,0.041127,"since large-scale training data can make the parameters of the encoder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT"
D16-1160,P16-2021,0,0.0134864,"d retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt"
D16-1160,P02-1040,0,0.124744,"Missing"
D16-1160,P06-1055,0,0.0199578,"Missing"
D16-1160,P16-1159,0,0.0774709,"oder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT. Gulcehre et al. (2015) first investigate the targetside monolingual"
D16-1160,P16-5005,0,0.017437,"data are much smaller. It is reasonable since large-scale training data can make the parameters of the encoder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side mono"
D16-1160,P07-1004,0,0.155635,"monolingual data into target translations, and the other reconstructs the source-side monolingual data from the target translations. In this work, we investigate the usage of the source-side large-scale monolingual data in NMT and aim at greatly enhancing its encoder network so that we can obtain high quality context vector representations. To achieve this goal, we propose two 1535 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics approaches. Inspired by (Ueffing et al., 2007; Wu et al., 2008) handling source-side monolingual corpus in SMT and (Sennrich et al., 2015) exploiting target-side monolingual data in NMT, the first approach adopts the self-learning algorithm to generate adequate synthetic parallel data for NMT training. In this method, we first build the baseline machine translation system with the available aligned sentence pairs, and then obtain more synthetic parallel data by translating the source-side monolingual sentences with the baseline system. The proposed second approach applies the multitask learning framework to predict the target translation"
D16-1160,D07-1077,0,0.0085868,"influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning framework to take full advantage of the source-side monolingual data. Luong et al. (2015a) also investigate the source-side monolingual data in the multi-task learning framework, in which a simple autoencoder or skip-thought vectors are employed to model the monolingual data. Our sentence"
D16-1160,K16-1004,0,0.0199067,"d retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt"
D16-1160,C16-1127,0,0.0138546,"Missing"
D16-1160,C08-1125,1,0.265167,"on model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning framework to take full advantage of the source-side monolingual data. Luong et al. (2015a) also investigate the source-side monolingual data in the multi-task learning framework, in which a simple autoencoder or skip-thought vectors are employed to model the monolingual data. Our sentence reordering model is"
D16-1160,P14-1011,1,0.454717,"Missing"
D16-1160,D16-1163,0,0.0662772,"n NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning f"
D16-1160,N16-1004,0,0.0398445,"sentences and retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt"
D16-1160,P16-1008,0,\N,Missing
D16-1160,J07-2003,0,\N,Missing
D17-1029,D15-1092,0,0.37527,"represents (1) contributions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chin"
D17-1029,D15-1177,0,0.0216389,"ibutions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chinese characters contain rich in"
D17-1029,N13-1092,0,0.0268,"Missing"
D17-1029,P16-1020,0,0.021302,"ts are cosine similarities between embeddings of a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character level informations have also been investigated in English recently (Hashimoto and Tsuruoka, 2016; Rei et al., 2016; Miyamoto and Cho, 2016). It should be noted that these studies are not focus on learning sentence embeddings. In this paper, we explore word internal structures to learn generic sentence representations, and propose a mixed character-word architecture which can be integrated into various sentence composition models. In the proposed architecture, a mask gate is employed to model the relation among characters in a word, and pooling mechanism is leveraged to model the contributions of the atomic and compositional word embeddings to the final word representations. Experiments o"
D17-1029,P14-1006,0,0.0292836,"he atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chinese characters contain rich information and are capable of indicating semantic meanings of words. As illustrated in"
D17-1029,N16-1162,0,0.0463897,"Missing"
D17-1029,P15-1162,0,0.0632206,"Missing"
D17-1029,P14-1062,0,0.038149,"“机(machine)” and “场(field)”. The color depth represents (1) contributions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence represen"
D17-1029,D16-1157,0,0.141464,"te to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chinese characters contain rich information and are capable of indicating semantic meanings of words. As illustrated in Figure 1, the internal structures of Chinese words express two characteristics: (1) Each character in a word contribute differently to the compositional word meaning (Wong et al., 2009) such as the"
D17-1029,N16-1119,0,0.0717882,"Processing, pages 298–303 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics beddings. The atomic word representation is calculated by projecting word level inputs into a highdimensional space by a look up table, while the compositional word representation is computed as a gated composition of character representations: The word internal structures have been proven to be useful for Chinese word representations. Chen et al. (2015b) proposes a character-enhanced word representation model by adding the averaged character embeddings to the word embedding. Xu et al. (2016) extends this work by using weighted character embeddings. The weights are cosine similarities between embeddings of a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character"
D17-1029,C10-1142,0,0.0573697,"Missing"
D17-1029,D16-1209,0,0.0204481,"a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character level informations have also been investigated in English recently (Hashimoto and Tsuruoka, 2016; Rei et al., 2016; Miyamoto and Cho, 2016). It should be noted that these studies are not focus on learning sentence embeddings. In this paper, we explore word internal structures to learn generic sentence representations, and propose a mixed character-word architecture which can be integrated into various sentence composition models. In the proposed architecture, a mask gate is employed to model the relation among characters in a word, and pooling mechanism is leveraged to model the contributions of the atomic and compositional word embeddings to the final word representations. Experiments on sentence similarity (as well as word simi"
D17-1029,C16-1030,0,0.0248623,"ween embeddings of a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character level informations have also been investigated in English recently (Hashimoto and Tsuruoka, 2016; Rei et al., 2016; Miyamoto and Cho, 2016). It should be noted that these studies are not focus on learning sentence embeddings. In this paper, we explore word internal structures to learn generic sentence representations, and propose a mixed character-word architecture which can be integrated into various sentence composition models. In the proposed architecture, a mask gate is employed to model the relation among characters in a word, and pooling mechanism is leveraged to model the contributions of the atomic and compositional word embeddings to the final word representations. Experiments on sentence similar"
D17-1029,P15-1150,0,0.0280064,". The color depth represents (1) contributions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contra"
D17-1114,J02-3001,0,0.0228951,"image vector vi are mapped to a joint space by a two-branch neural network as follows:  x = W2 · f (W1 · vs + bs ) (6) y = V2 · f (V1 · vi + bi ) Note that the images in Flickr30K are similar to our task. However, the image descriptions are much simpler than the text in news, so the model trained on Flickr30K cannot be directly used for our task. For example, some of the information contained in the news, such as the time and location of events, cannot be directly reflected by images. To solve this problem, we simplify each sentence and speech transcription based on semantic role labelling (Gildea and Jurafsky, 2002), in which each predicate indicates an event and the arguments express the relevant information of this event. ARG0 denotes the agent of the event, and ARG1 denotes the action. The assumption is that the concepts including agent, predicate and action compose the body of the event, so we extract “ARG0+predicate+ARG1” as the simplified sentence that is used to match the images. It is worth noting that there may be multiple predicateargument structures for one sentence and we extract all of them. After the text-image matching model is trained and the sentences are simplified, for each textimage p"
D17-1114,P03-1054,0,0.0133063,"Missing"
D17-1114,P03-1056,0,0.0149749,"Missing"
D17-1114,N03-1020,0,0.232444,"model uses generated image captions to match the text; i.e., if the similarity between a generated image caption and a sentence exceeds the threshold Ttext , the image and the sentence match. Image alignment. The images are aligned to the text in the following ways: The images in a document are aligned to all the sentences in this document and the key-frames in a shot are aligned to all the speech transcriptions in this shot. Image match. The texts are matched with images using the approach introduced in Section 3.4. 4.3 4.4 Multi-modal Summarization Evaluation We use the ROUGE-1.5.5 toolkit (Lin and Hovy, 2003) to evaluate the output summaries. This evaluation metric measures the summary quality by matching n-grams between generated summary and reference summary. Table 3 and Table 4 show the averaged ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) F-scores regarding to the three reference summaries for each topic in English and Chinese. For the results of the English MMS, from the first three lines in Table 3 we can see that when summarizing without visual information, the method with guidance strategies performs slightly better than do the first two methods. Because Rouge mainly measures word ov"
D17-1114,N10-1134,0,0.0443431,"summary score obtained by Equation 8 and Mc is the summary score obtained by Equation 9. The aim of Ms and Mc is to balance the aspects of salience and coverage for images. λs , and λm are determined by testing on development set. Note that to guaranteed monotone of F, λs , and λm should be lower than the minimum salience score of sentences. To further improve non-redundancy, we make sure that similarity between any pair of sentences in the summary is lower than Ttext . Equations 8,9 and 10 are all monotone submodular functions under the budget constraint. Thus, we apply the greedy algorithm (Lin and Bilmes, 2010) guaranteeing near-optimization to solve the problem. Experiment 4.1 Dataset There is no benchmark dataset for MMS. We construct a dataset as follows. We select 50 news topics in the most recent five years, 25 in English and 25 in Chinese. We set 5 topics for each language as a development set. For each topic, we collect 20 documents within the same period using Google News search6 and 5-10 videos in CCTV.com7 and Youtube8 . More details of the corpus are illustrated in Table 1. Some examples of news topics are provided Table 2. We employ 10 graduate students to write reference summaries after"
D17-1114,D14-1041,1,0.851946,"+ audio + guide Image caption Image caption match Image alignment Image match 0.422 0.422 0.440 0.435 0.429 0.409 0.442 0.114 0.109 0.117 0.111 0.115 0.082 0.133 0.166 0.164 0.171 0.167 0.166 0.082 0.187 Table 3: Experimental results (F-score) for English MMS. Implementation Details We perform sentence9 and word tokenization, and all the Chinese sentences are segmented by Stanford Chinese Word Segmenter (Tseng et al., 2005). We apply Stanford CoreNLP toolkit (Levy and D. Manning, 2003; Klein and D. Manning, 2003) to perform lexical parsing and use semantic role labelling approach proposed by Yang and Zong (2014). We use 300-dimension skipgram English word embeddings which are publicly available10 . Given that text-image matching model and image caption generation model are trained in English, to create summaries in Chinese, we first translate the Chinese text into English via Google Translation11 and then conduct text and image matching. Method Method R-1 R-2 R-SU4 Text only Text + audio Text + audio + guide Image caption match Image alignment Image match 0.409 0.407 0.411 0.381 0.368 0.414 0.113 0.111 0.115 0.092 0.096 0.125 0.167 0.166 0.173 0.149 0.143 0.173 Table 4: Experimental results (F-score)"
D17-1114,N06-2046,0,0.168436,"• To select the representative sentences, we consider four criteria that are jointly optimized by the budgeted maximization of submodular functions. • We introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our system can take advantage of multi-modal information and outperforms other baseline methods. 2 2.1 Related Work Multi-document Summarization Multi-document summarization (MDS) attempts to extract important information for a set of documents related to a topic to generate a short summary. Graph based methods (Mihalcea and Tarau, 2004; Wan and Yang, 2006; Zhang et al., 2016) are commonly used. LexRank (Erkan and Radev, 2011) first builds a graph of the documents, in which each node represents a sentence and the edges represent the relationship between sentences. Then, the importance of each sentence is computed through an iterative random walk. 2.2 Multi-modal Summarization In recent years, much work has been done to summarize meeting recordings, sport videos, movies, pictorial storylines and social multimedia. Erol et al. (2003) aim to create important segments of a meeting recording based on audio, text and visual activity analysis. Tjondro"
D17-1114,N16-1008,0,0.328886,"Multi-modal summarization (MMS) can provide users with textual summaries that can help acquire the gist of multimedia data in a short time, without reading documents or watching videos from beginning to end. 1 http://www.nlpr.ia.ac.cn/cip/jjzhang.htm The existing applications related to MMS include meeting record summarization (Erol et al., 2003; Gross et al., 2000), sport video summarization (Tjondronegoro et al., 2011; Hasan et al., 2013), movie summarization (Evangelopoulos et al., 2013; Mademlis et al., 2016), pictorial storyline summarization (Wang et al., 2012), timeline summarization (Wang et al., 2016b) and social multimedia summarization (Del Fabro et al., 2012; Bian et al., 2013; Schinas et al., 2015; Bian et al., 2015; Shah et al., 2015, 2016). When summarizing meeting recordings, sport videos and movies, such videos consist of synchronized voice, visual and captions. For the summarization of pictorial storylines, the input is a set of images with text descriptions. None of these applications focus on summarizing multimedia data that contain asynchronous information about general topics. In this paper, as shown in Figure 1, we propose an approach to a generate textual summary from a set"
D17-1114,Q14-1006,0,\N,Missing
D18-1011,C16-1175,0,0.0358277,"Missing"
D18-1011,D16-1235,0,0.0265375,"Missing"
D18-1011,N09-1003,0,0.0607332,"Missing"
D18-1011,Q17-1002,0,0.0187394,"ignore the associations between modalities, and thus lack the ability of information transferring between modalities. Consequently they cannot handle words without perceptual information. Second, they integrate textual and perceptual representations with simple concatenation, which is insufficient to effectively fuse information from various modalities. Third, they typically treat the representations from different modalities equally. This is inconsistent with many psychological findings that information from different modalities contributes differently to the meaning of words (Paivio, 1990; Anderson et al., 2017). In this work, we introduce the associative multichannel autoencoder (AMA), a novel multimodal word representation model that addresses all the above issues. Our model is built upon the stacked autoencoder (Bengio et al., 2007) to learn semantic representations by integrating textual and perceptual inputs. Inspired by the re-constructive and associative nature of human memory, we propose two associative memory modules as extensions. One is to learn associations between modalities (e.g., associations between textual and visual features), so as to reconstruct corresponding perceptual informatio"
D18-1011,D14-1032,0,0.0226054,"he fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing perceptual information, associating related concepts and grounding word symbols to external environment. 2 2.1 Multimodal Models 2.2.1 Jointly training models A class of models extends Latent Dirichlet Allocation (Blei et al., 2003) to jointly learn topic distributions from words and perceptual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013). For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. Kiela and Clark (2015) adopt the MMSkip to learn multimodal vectors with auditory perceptual inputs. These methods can implicitly propagate perceptual information to word representations and at the same time learn multimod"
D18-1011,Q14-1023,0,0.300461,"tion and associated words is triggered and mediated by a linguistic input. The learned cross-modality mapping and reconstruction of associated words are inspired by the human mental model of associations between different modalities and related concepts. Moreover, word meaning is tied to both linguistic and physical environment, and relies differently on each modality in2.2.2 Separate training models The simplest approach is concatenation which fuses textual and visual vectors by concatenating them. It has been proven to be effective in learning multimodal representations (Bruni et al., 2014; Hill et al., 2014; Collell et al., 2017). Variations of this method employ transformation and dimension reduction on the concatenation result, including application of singular value decomposition (SVD) (Bruni et al., 2014) or canonical correlation analysis (CCA) (Hill et al., 2014). There is also work using deep learning methods to project different modality inputs into a common 116 word2vec image2vec Multimo representa ... ... ... sound2vec There's nothing that In case youyou cheers need, upwe've quite as collected fast as the a cute cutest small dog doing dogsomething breeds to lift your peculiar. mood. dog"
D18-1011,D14-1005,0,0.0223777,"ut of the textual vectors, output of visual or auditory vectors, and is trained with SGD for 100 epochs. We initialize the network biases as zeros and network weights with He-initialisation (He et al., 2015). The best parameters of AMA-M model are 2 hidden layers, with textual channel size of 300, 250 and 150, visual/auditory channel size of 128, 5.2 Baseline Multimodal Models Most of existing multimodal models only utilize textual and visual modalities. For fair comparison, we re-implement several representative systems with our own textual and visual vectors. The Concatenation (CONC) model (Kiela and Bottou, 2014) is simple concatenation of normalized textual and visual vectors. The Mapping (Collell et al., 2017) and Ridge (Hill et al., 2014) models first learn a mapping matrix from textual to visual modality using feed-forward neural network and ridge regression respectively. After applying the mapping function on the textual vectors, they obtain the predicted visual vectors for all words in textual vocabulary. Then they concatenate the normalized textual and predicted visual vectors to get multimodal word representations. The SVD (Bruni et al., 2014) and CCA (Hill et al., 2014) models first concatena"
D18-1011,D15-1293,0,0.0199505,"ual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013). For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. Kiela and Clark (2015) adopt the MMSkip to learn multimodal vectors with auditory perceptual inputs. These methods can implicitly propagate perceptual information to word representations and at the same time learn multimodal representations. However, they utilize raw text corpus in which words having perceptual information account for a small portion. This weakens the effect of introducing perceptual information and consequently leads to the slight improvement of textual vectors. Background and Related Work Cognitive Grounding A large body of research evidences that human semantic memory is inherently re-constructi"
D18-1011,D14-1162,0,0.0964741,"ix benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models. 1 Introduction Representing the meaning of a word is a prerequisite to solve many linguistic and non-linguistic problems, such as retrieving words with the same meaning, finding the most relevant images or sounds of a word and so on. In recent years we have seen a surge of interest in building computational models that represent word meanings from patterns of word co-occurrence in corpora (Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Clark, 2015; Wang et al., 2018b). However, word meaning is also tied to the physical world. Many behavioral studies suggest that human semantic representation is grounded in the external environment and sensorimotor experience (Landau et al., 1998; Barsalou, 2008). This has led to the development of multimodal representation models that utilize both textual and perceptual information (e.g., images, sounds). As evidenced by a range of evaluations (Andrews et al., 2009; Bruni et al., 2014; Silberer • We present a novel associative multichannel autoencoder for multimodal word representation, wh"
D18-1011,D13-1115,0,0.0357832,"Missing"
D18-1011,D12-1130,0,0.0176076,"emory in humans, suggesting that rich information contained in human cognitive processing can be used to enhance NLP models. Furthermore, our results shed light on the fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing perceptual information, associating related concepts and grounding word symbols to external environment. 2 2.1 Multimodal Models 2.2.1 Jointly training models A class of models extends Latent Dirichlet Allocation (Blei et al., 2003) to jointly learn topic distributions from words and perceptual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013). For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. Kiela and Clark (2015) adopt the MMSkip to learn multimoda"
D18-1011,P14-1068,0,0.0601988,"(Bruni et al., 2014) or canonical correlation analysis (CCA) (Hill et al., 2014). There is also work using deep learning methods to project different modality inputs into a common 116 word2vec image2vec Multimo representa ... ... ... sound2vec There's nothing that In case youyou cheers need, upwe've quite as collected fast as the a cute cutest small dog doing dogsomething breeds to lift your peculiar. mood. dog Figure 1: Architecture of the multichannel autoencoder with inputs of textual, visual and auditory sources. Our model extends the unimodal and bimodal autoencoder (Ngiam et al., 2011; Silberer and Lapata, 2014) to induce semantic representations integrating textual, visual and auditory information. As shown in Figure 1, our model first transforms input textual vector xt , visual vector xv and auditory vector xa to hidden representations: ht = g(Wt xt + bt ) hv = g(Wv xv + bv ) (1) ha = g(Wa xa + ba ). Then the hidden representations are concatenated together and mapped to a common space: Associative Multichannel Autoencoder hm = g(Wm [ht ; hv ; ha ] + bm ). (2) The model is trained to reconstruct the hidden representations of the three modalities from the multimodal representation hm : We first prov"
D18-1036,D14-1179,0,0.0198241,"Missing"
D18-1036,D17-1148,0,0.0190034,"ublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pairs into NMT to improve the translation quality. There are"
D18-1036,N13-1073,0,0.0374239,"(hm 1 , h2 , ..., hT x ) by using m stacked Long Short Term Memory (LSTM) layers (Hochreiter and Schmidhuber, 1997) . hm j is the hidden state of the top layer in encoder. The bottom layer of encoder is a bi-direction LSTM layer to collect the context from the left side and right side. The decoder generates one target word at a time by computing pN i (yi |y<i , C) as follows: (1) where zei is the attention output: zei = tanh(Wz [zim ; ci ]) aij hm i j=1 Neural Machine Translation pN i (yi |y<i , C) = sof tmax(Wyi zei + bs ) Tx X 2 The word alignments A is extracted using the fast-align tool (Dyer et al., 2013) on the bilingual training data with both source-to-target and target-to-source directions. (2) 392 Arthur: alc percent in fo Baseline+ME 30 percent i 30 percent in fourth quarter of last year source sentence y1 y2 y3 input NMT model output probability of each N gold target word P i (y)i 0.80 0.18 0.02 ... 0.35 0.34 0.31 ... 0.75 0.20 0.05 ... alignment y1 y2 y3 x1 x3 Source: ae chengzhan Reference: last year gr Baseline: he in fourth q Arthur: al percent in Baseline+M 30 percent alignments NMT model If PNi (y)i word satisfied the exception criterion and x j aligns to y i x1 x j is an exceptio"
D18-1036,D17-1146,0,0.0730537,"lesome words. The extensive experiments on Chineseto-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words. 1 Figure 1: The NMT model produces a wrong translation for the low-frequency word “aerkat”. While introducing an external lexicon table without contextual information, the model incorrectly translates the ambiguous word “chengzhang” into “growth”. that the low-frequency words can be represented by frequent subword sequences. Arthur et al. (2016) and Feng et al. (2017) try to incorporate a translation lexicon into NMT in order to obtain the correct translation of low-frequency words. However, the former method still faces the lowfrequency problem of subwords. And the latter one has a drawback that they use lexicons without considering specific contexts. Fig. 1 shows an example, in which “aerkate” is an infrequent word and the baseline NMT incorrectly translates it into a pronoun “he”. Incorporation of bilingual lexicon rectifies the mistake but wrongly converts “chengzhang” into an incorrect target word “growth” since an entry “(chengzhang, growth)” in the"
D18-1036,D16-1162,0,0.547059,"ectly translate the troublesome words. The extensive experiments on Chineseto-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words. 1 Figure 1: The NMT model produces a wrong translation for the low-frequency word “aerkat”. While introducing an external lexicon table without contextual information, the model incorrectly translates the ambiguous word “chengzhang” into “growth”. that the low-frequency words can be represented by frequent subword sequences. Arthur et al. (2016) and Feng et al. (2017) try to incorporate a translation lexicon into NMT in order to obtain the correct translation of low-frequency words. However, the former method still faces the lowfrequency problem of subwords. And the latter one has a drawback that they use lexicons without considering specific contexts. Fig. 1 shows an example, in which “aerkate” is an infrequent word and the baseline NMT incorrectly translates it into a pronoun “he”. Incorporation of bilingual lexicon rectifies the mistake but wrongly converts “chengzhang” into an incorrect target word “growth” since an entry “(cheng"
D18-1036,P15-1001,0,0.062259,"st all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN . To make it efficient, given the aligned training data D =  (d) (d) |D| X ,Y , we keep θN unchanged and opd=1 M timize θ by maximizing the following objective function. 2) Arthur: It is the state-of-the-art method which incorporates"
D18-1036,D07-1007,0,0.0487669,"inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also"
D18-1036,P16-1100,0,0.0442821,"the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later,"
D18-1036,1983.tc-1.13,0,0.743972,"Missing"
D18-1036,D15-1166,0,0.347565,"esome words can be correctly translated. The contributions are listed as follows: 1) We are the first to define and handle the troublesome words in neural machine translation. 2) We propose to memorize not only the bilingual lexicons but also their contexts with a contextual memory. 3) We design a dynamic approach to correctly translate the troublesome words by combining the contextual memory and the NMT model. 2 ci = (3) where ai,j is the attention weight: m hm j zi ai,j = P m m j hj zi (4) where zim is the hidden state of the top layer in decoder. More detailed introduction can be found in (Luong et al., 2015). Notation. In this paper, we denote the whole |VS | source vocabulary by VS = {sm }m=1 and target |VT | vocabulary by VT = {tn }n=1 , where sm is the source word and tn is the target word. We denote a source sentence by X and a target sentence by Y . Each source word in X is denoted by xj . Each target word in Y is denoted by yi . Accordingly, a target word can be denoted not only by tn , but also by yi . This does not contradict. tn means this target word is the nth word in vocabulary VT , and yi means this target word is the ith word in sentence Y . Similarly, we denote a source word by sm"
D18-1036,D16-1096,0,0.059563,"that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pairs into NMT to imp"
D18-1036,P13-1111,1,0.846978,"ng Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. C"
D18-1036,P02-1040,0,0.101068,"liable. Thus we design the dynamic weight λi according to the exception rate r(xj ): λi = sigmoid(βγ ∗ γi ) γi = Tx X ai,j ∗ r(xj ) sentence pairs whose length exceeds 100. We run a total of 20 iterations for all translation tasks. We test all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN ."
D18-1036,W17-4702,0,0.0202248,"Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NM"
D18-1036,D16-1160,1,0.858302,"., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017"
D18-1036,P16-1162,0,0.809658,"achine translation (NMT) based on the encoder-decoder architecture becomes the new state-of-the-art due to distributed representation and end-to-end learning (Cho et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). However, the current NMT is a global model that maximizes the performance on the overall data and has problems in handling low-frequency words and ambiguous words1 , we refer these words as troublesome words and define them in Section 3.1. Some previous work attempt to tackle the translation problem of low-frequency words. Sennrich et al. (2016) propose to decompose the words into subwords which are used as translation units so 1 In this work, we consider a source word is ambiguous if it has multiple translations with high entropy of probability distribution. 391 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 391–400 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ci can be calculated as follows: address them. Our method first investigates different strategies to define the troublesome words. Then, these words and their contexts in the t"
D18-1036,P16-5005,0,0.0197893,"owfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pair"
D18-1036,P17-2060,1,0.804071,"s mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate"
D18-1036,N16-1004,0,0.0190829,"ranslation tasks. We test all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN . To make it efficient, given the aligned training data D =  (d) (d) |D| X ,Y , we keep θN unchanged and opd=1 M timize θ by maximizing the following objective function. 2) Arthur: It is the state-of-the-art method"
D18-1036,H05-1097,0,0.0707789,"s follows: Neural Turing Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could allevi"
D18-1036,D17-1149,0,0.0883335,"n cases (Deterio) when rectifying the troublesome words. As a comparison, we also count the total rectification and deterioration numbers of Arthur(test). The results are reported in Table 5. These results show that our method could rectify more words (51 vs. 70) with less deterioration (17 vs. 11) than Arthur(test). 6 Unit 7 Related Work The related work can be divided into three categories and we describe each of them as follows: Neural Turing Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into small"
D18-1036,I17-1039,1,0.885788,"Missing"
D18-1326,C18-1263,0,0.155278,"an share more hidden units and languages with a great difference (En/Ja) may share less hidden units. 6 Related Work In this work, we explore the balancing problem of shared and unique parameters, and attempt to 2958 incorporate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper"
D18-1326,P17-1176,0,0.0323628,"porate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed three effective strategies to improve the universal one-to-many multilingual translation, including special label initialization, language-dependent positional embedding and a new parameter-sharing"
D18-1326,P15-1166,0,0.109787,"s attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al. (2016) present an"
D18-1326,N16-1101,0,0.0872011,"uages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al. (2016) present an appealing approach in which a universal encoder-decoder framew"
D18-1326,D16-1026,0,0.110205,"Missing"
D18-1326,N18-1032,0,0.0544922,"h different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed three effective strategies to improve the universal one-to-many multilingual translation, including special label initialization, language-dependent positional embedding and a new parameter-sharing mechanism. The empirical experiments on four language pairs demo"
D18-1326,C18-1054,0,0.0880812,"ts and languages with a great difference (En/Ja) may share less hidden units. 6 Related Work In this work, we explore the balancing problem of shared and unique parameters, and attempt to 2958 incorporate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed th"
D18-1326,W18-6309,0,0.349927,"onding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al. (2016) present an appealing approach in which a universal encoder-decoder framework is designed for manyto-one, many-to-many and one-to-many multilingual translation tasks. The network model is compact and the model size does not grow as the number of languages increases. However, Johnson et al. (2017) observe that only the many-toone paradigm"
D18-1326,P02-1040,0,0.102327,"ional hidden representations. During training, each mini-batch on one GPU contains a set of sentence pairs with roughly 3,072 source and 3,072 target tokens. We use Adam optimizer (Kingma and Ba, 2014) with β1 =0.9, β2 =0.98, and =10−9 . For our model, we train for 400,000 steps on one machine with 8 NVIDIA Tesla M40 GPUs. 5 Results and Analysis We show the results of one-to-many translation experiments using our proposed strategies. The translation performance is evaluated by case-insensitive BLEU4 for Zh→En translation, character-level BLEU5 for Zh→Ja translation, and case-sensitive BLEU4 (Papineni et al., 2002) for En→De/Fr translation task. 5.1 4 Experiments Settings In this section, we test the proposed methods on two one-to-many translation tasks, including (i) Chinese→English/Japanese in general domain, and (ii) English→French/German in WMT14 Our Strategies vs. Baseline Table 1 reports the main translation results of Zh→En/Ja and En→De/Fr translation tasks. We conduct universal one-to-many translation using 2957 1 2 http://www.statmt.org/wmt14/translation-task.html https://github.com/tensorflow/tensor2tensor Zh→En Methods Zh→Ja En→De En→Fr MT03 MT04 MT05 MT06 Ave test test test Indiv 43.59 43.95"
D18-1326,P17-2060,1,0.837479,"e than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong"
D18-1326,N16-1004,0,0.120711,"mework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al."
D18-1326,D16-1163,0,0.0524242,"tempt to 2958 incorporate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed three effective strategies to improve the universal one-to-many multilingual translation, including special label initialization, language-dependent positional embedding and a new"
D18-1326,P16-1009,0,0.0349212,"onstrate that our proposed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios"
D18-1326,I17-1039,1,0.87635,"trategies can achieve comparable or even better performance than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multi"
D18-1326,1983.tc-1.13,0,0.554923,"Missing"
D18-1326,D16-1160,1,0.864526,"sed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-"
D18-1415,D17-1234,0,0.0451927,"Missing"
D18-1415,D17-1260,0,0.0158729,"ssociation for Computational Linguistics dialog features based on new ontology. Besides, new system actions may be added to deal with new user actions. The network architecture of the new system and the original one will be different. The new system can not inherit the parameters from the old one directly. It will make the original dialog manager model invalid. Therefore, developers have to retrain the new system by interacting with users from scratch. Though there are many methods to train a RL-based dialog manager efficiently (Su et al., 2016a, 2017; Lipton et al., 2017; Chang et al., 2017; Chen et al., 2017), the unmaintainable RL-based dialog systems will still be put on the shelf in real-world applications (Paek and Pieraccini, 2008; Paek, 2006). To alleviate this problem, we propose a teacherstudent framework to maintain the RL-based dialog manager without training from scratch. The idea is to transfer the knowledge of existing resources to a new dialog manager. Specifically, after the system is deployed, if developers find some intents and slots missing before, they can define a few simple dialog rules to handle such situations. For example, under the condition shown in Fig. 1, a reasonable s"
D18-1415,P17-1045,0,0.016006,"we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology has changed drastically. Shah et al. (2016) proposed to integrate turnlevel feedback with a task-level reward signal to"
D18-1415,P84-1044,0,0.416399,"Missing"
D18-1415,P16-1228,0,0.0315951,"t ots1 otu encoding encoding s atu Inform(area=Beihai) at 1 Request(area) Victorian Germy Do Eat otdb encoding 10 Godear Holiday Database Figure 2: An overview of the RL-based dialog manager used in our work3 . In the last turn, the system inquires “Where do you want to go?”. In current turn, the user input is “Find a restaurant in Beihai.”. a large highly regularized model into a smaller model. The knowledge which can be transferred has not been restricted to models. Stewart and Ermon (2017) proposed to distill the physics and domain knowledge to train neural networks without labeled data. Hu et al. (2016) enabled a neural network to learn simultaneously from labeled instances as well as logic rules. Zhang et al. (2017) integrated multiple prior knowledge sources into neural machine translation using posterior regularization. Our experiments are based on such insights. Through defining appropriate regularization terms, we can distill different knowledge (e.g., trained model or prior knowledge) to a new designed model, alleviating the need for new labeled data or expensive interaction environments. 3 RL-based Dialog Manager Before going to the details of our method, we provide some background on"
D18-1415,I17-1074,0,0.277254,"can be incrementally extended once developers find new intents and slots that are not taken into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial"
D18-1415,W17-5518,0,0.0341068,"Missing"
D18-1415,P16-1230,0,0.0561139,"Missing"
D18-1415,P17-1062,0,0.355141,"n into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology has changed drastically. Shah et al. (2016) proposed to integrate turnlevel fee"
D18-1415,D17-1237,0,0.234987,"lly extended once developers find new intents and slots that are not taken into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology ha"
D18-1415,P17-1139,0,0.0197507,"10 Godear Holiday Database Figure 2: An overview of the RL-based dialog manager used in our work3 . In the last turn, the system inquires “Where do you want to go?”. In current turn, the user input is “Find a restaurant in Beihai.”. a large highly regularized model into a smaller model. The knowledge which can be transferred has not been restricted to models. Stewart and Ermon (2017) proposed to distill the physics and domain knowledge to train neural networks without labeled data. Hu et al. (2016) enabled a neural network to learn simultaneously from labeled instances as well as logic rules. Zhang et al. (2017) integrated multiple prior knowledge sources into neural machine translation using posterior regularization. Our experiments are based on such insights. Through defining appropriate regularization terms, we can distill different knowledge (e.g., trained model or prior knowledge) to a new designed model, alleviating the need for new labeled data or expensive interaction environments. 3 RL-based Dialog Manager Before going to the details of our method, we provide some background on the RL-based dialog manager in this section. Fig. 2 shows an overview of such dialog manager. We describe each of t"
D18-1415,W16-3601,0,0.0710311,"evelopers find new intents and slots that are not taken into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology has changed drastically. Sha"
D18-1448,N18-1150,0,0.0169755,"score despite the mediocre performance in three individual metrics. The MAXsim score of ATG is much higher than ATL and HAN. It shows the global features can help to learn better image-text alignments. Since GR itself makes use of the image-caption pairs, it is natural to get a high image-text relevance score. Our proposed multimodal attention models all achieves higher performance than the extractive baseline GR, which further indicate the effectiveness of our models. 5 Related Work Different from text summarization (Wan and Yang, 2006; Rush et al., 2015; Zhu et al., 2017; See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018), Multimodal Summarization is a task to generate a condensed text summary or a few keyframes to help acquire the gist of multimedia data. One of the most significant advantages of the task is that it does not rely solely on text information, but it can also utilize the rich visual content from the images. In recent years, much work has focused on multimodal summarization. Evangelopoulos et al. (2013) detect salient events in a movie based on the saliency of individual features for aural, visual, and linguistic representations. Li et al. (2017) generate the text summary fr"
D18-1448,D17-1114,1,0.881109,"lience and intermodality relevance. The experimental results show the effectiveness of MMAE. 1 summarize Researchers have discovered the fossilized remains of a small, lizard- like creature that is the missing ancestral link … Figure 1: The illustration of our proposed task – Multimodal Summarization with Multimodal Output (MSMO). The image can help better understand the text in the red font. Introduction Text summarization is to extract the important information from source documents. With the increase of multimedia data on the internet, some researchers (Li et al., 2016b; Shah et al., 2016; Li et al., 2017) focus on multimodal summarization in recent years. Existing experiments (Li et al., 2017, 2018a) have proven that, compared to text summarization, multimodal summarization can improve the quality of generated summary by using information in visual modality. However, the output of existing multimodal summarization systems is usually represented in a single modality, such as textual or visual (Li et al., 2017; Evangelopoulos et al., 2013; Mademlis et al., 2016). In this paper, we argue that multimodal output1 is necessary for the following three reasons: 1) It is much easier and faster 1 Tiny w"
D18-1448,W12-2601,0,0.0306175,"erence with that in model output, m2 is obtained by comparing the image set in reference with the image in model output, and m3 considers the image-text similarity in model output. To learn MMAE, we choose three simple methods to fit y with human judgment scores. These methods include Linear Regression (LR), and two nonlinear methods: Logistic Regression (Logis), and Multilayer Perceptron (MLP). 3.1 Salience of Text ROUGE (Lin, 2004b) is widely used to automatically assess the quality of text summarization systems. It has been shown that ROUGE correlates well with human judgments (Lin, 2004a; Owczarzak et al., 2012; Over and Yen, 2004). Therefore, we directly apply ROUGE to assess the salience of the text units. 3.2 Salience of Image We propose a metric, namely, image precision (IP), to measure the salience of image. The image precision is defined as follows: IP = |{ref img } ∩ {recimg }| |{recimg }| (18) where ref img , recimg denote reference images and recommended images by MSMO systems respectively. The reasons for this metric are as follows. A good summary should have good coverage of the events for both texts and images. The image in the output should be closely related to the events. So we formul"
D18-1448,D15-1044,0,0.125061,". Surprisingly, the model ATG achieves the highest MMAE score despite the mediocre performance in three individual metrics. The MAXsim score of ATG is much higher than ATL and HAN. It shows the global features can help to learn better image-text alignments. Since GR itself makes use of the image-caption pairs, it is natural to get a high image-text relevance score. Our proposed multimodal attention models all achieves higher performance than the extractive baseline GR, which further indicate the effectiveness of our models. 5 Related Work Different from text summarization (Wan and Yang, 2006; Rush et al., 2015; Zhu et al., 2017; See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018), Multimodal Summarization is a task to generate a condensed text summary or a few keyframes to help acquire the gist of multimedia data. One of the most significant advantages of the task is that it does not rely solely on text information, but it can also utilize the rich visual content from the images. In recent years, much work has focused on multimodal summarization. Evangelopoulos et al. (2013) detect salient events in a movie based on the saliency of individual features for aural, visual, and linguistic"
D18-1448,P17-1099,0,0.744619,"ur dataset has been released to the public, which can be found in http://www.nlpr.ia.ac.cn/cip/ jjzhang.htm. 3 http://www.robots.ox.ac.uk/˜vgg/ research/very_deep decoder, which is a unidirectional LSTM, makes use of information from two modalities to generate the text summary and select the most relevant image according to visual coverage vector. Our text encoder and summary decoder are based on pointer-generator network which we will describe in Sec. 2.2. We then describe image encoder and multimodal attention layer in our multimodal attention model (Sec. 2.3). 2.2 Pointer-Generator Network See et al. (2017) propose a pointer-generator network which allows both copying words from the source text and generating words from a fixed vocabulary, achieving the best performance on CNN/Daily mail dataset. Their model consists of an encoder (a single-layer bidirectional LSTM) and an attentive decoder (a unidirectional LSTM). The encoder maps the article to a sequence of encoder hidden states hi . During decoding, the decoder receives the embedding of the previous word and reaches a new decoder state st . Then the context vector ct is computed by the attention mechanism (Bahdanau et al., 2015; Luong et al."
D18-1448,D15-1166,0,0.405232,"t al. (2017) propose a pointer-generator network which allows both copying words from the source text and generating words from a fixed vocabulary, achieving the best performance on CNN/Daily mail dataset. Their model consists of an encoder (a single-layer bidirectional LSTM) and an attentive decoder (a unidirectional LSTM). The encoder maps the article to a sequence of encoder hidden states hi . During decoding, the decoder receives the embedding of the previous word and reaches a new decoder state st . Then the context vector ct is computed by the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) as calculated in Eq. 1 and 2. To alleviate the problem of repetition, See et al. (2017) maintain a coverage vector cov t , which is the sum of attention distributions over all previous decoding timestepsP(initialized to zero vector at timestep 0): cov t = t−1 αt˜. The coverage vector is used as t˜=0 an extra input to the attention vector (Eq. 1) and is also used to calculate the coverage loss (Eq. 6). Next, the attention distribution is used to calculate the context vector as follows. eti = v T tanh(Wh hi + Ws st + Wc cov t ) t t α = softmax(e ) X αit hi ct = (1) (2) (3) i The important part"
D18-1448,N06-2046,0,0.0242991,"61 articles are left. Surprisingly, the model ATG achieves the highest MMAE score despite the mediocre performance in three individual metrics. The MAXsim score of ATG is much higher than ATL and HAN. It shows the global features can help to learn better image-text alignments. Since GR itself makes use of the image-caption pairs, it is natural to get a high image-text relevance score. Our proposed multimodal attention models all achieves higher performance than the extractive baseline GR, which further indicate the effectiveness of our models. 5 Related Work Different from text summarization (Wan and Yang, 2006; Rush et al., 2015; Zhu et al., 2017; See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018), Multimodal Summarization is a task to generate a condensed text summary or a few keyframes to help acquire the gist of multimedia data. One of the most significant advantages of the task is that it does not rely solely on text information, but it can also utilize the rich visual content from the images. In recent years, much work has focused on multimodal summarization. Evangelopoulos et al. (2013) detect salient events in a movie based on the saliency of individual features for aural, visu"
D18-1448,K16-1028,0,0.0426672,"than text summaries. It shows that users prefer this way of presenting information. It also confirms our motivation for MSMO. 4.3 Comparison with Text Summarization Our user satisfaction test in Sec. 4.2 is done in an ideal situation, comparing the text reference with the pictorial reference. To show the effectiveness of our model, we also compare our model with text summarization from ROUGE and human judgment scores. We compare several abstractive summarization methods with our multimodal summarization methods. PGC7 (See et al., 2017) refers to the pointer-generator network (Sec. 2.2). AED (Nallapati et al., 2016) uses an attentional encoder-decoder framework and adds some linguistic features such as POS, named-entities, and TF-IDF into the encoder. We also implement a seq2seq model with attention (S2S+attn). To compare the multimodal output with our multimodal model, we propose an extractive method 7 https://github.com/abisee/ pointer-generator based on GuideRank (GR) (Li et al., 2016a, 2018b). GuideRank applies LexRank (Erkan and Radev, 2004) with guidance strategy. In this strategy, captions recommend the sentences related to them. The rankings of sentences and captions are obtained through GR; we e"
D18-1448,N16-1008,0,0.315353,"tput (text) (more details can be found in Sec. 4.2). 3) Images help users to grasp events while texts provide more details related to the events. Thus the images and text can complement each other, assisting users to gain a more visualized understanding of events (Bian et al., 2013). We give an example in Fig. 1 to illustrate this phenomenon. For the output with only the text summary, user will be confused about the description of “four-legged creatures”; while with a relevant image, user will have a clearer understanding of the text. In recent years, some researchers(Bian et al., 2013, 2015; Wang et al., 2016) focus on incorporating multimedia contents into the output of summarization which all treat the image-text pair as a basic summarization unit. But in our work, our input comes from a document and a collection of images where there is no alignment between texts and images. So our biggest challenge is how to bridge the semantic gaps between texts and images. Based on the above discussion, in this work, we propose a novel task which we refer to as Multimodal Summarization with Multimodal Output (MSMO). To explore this task, we focus on the simplicity, we first consider only one image) and a piec"
D18-1448,Q14-1006,0,0.027754,"re, we regard the image-text relevance as one of metrics to measure the quality of the pictorial summary. We consider using visual-semantic embedding (Faghri et al., 2018; Wang et al., 2018) to calculate the cosine similarity between visual feature and textual feature, which we use as image-text relevance. Visual-semantic embedding has been widely used in cross-modal retrieval (Kiros et al., 2014) and image captioning (Karpathy and Fei-Fei, 2015). We apply VSE0 model of Faghri et al. (2018), which achieves state-of-the-art performance for image-caption retrieval task on the Flickr30K dataset (Young et al., 2014). The difference is that instead of training a CNN model to encode the image, we use the pretrained VGG19 to extract global features. The text is encoded by a unidirectional Gated Recurrent Unit (GRU) to a sequence of vector representations. Then we apply the max-over-time pooling (Collobert et al., 2011) to get a single vector representation. Next, the visual features and text features are projected to a joint semantic space by two feed-forward neural networks. The whole network is trained using a max-margin loss: X L= max(β − s(i, c) + s(i, cˆ), 0) our evaluation metrics, we calculate the co"
D19-1185,C18-1107,0,0.0287291,"Missing"
D19-1185,P17-1162,0,0.0182911,"tions in our task are selecting nodes in the KG to generate questions. Thus, the structured information is important in our task. Besides, some works also try to model structured information in dialogue systems. For example, Peng et al. (2017) used hierarchical reinforcement learn1769 ing (Vezhnevets et al., 2017; Kulkarni et al., 2016; Florensa et al., 2017) to design multi-domain dialogue management. Chen et al. (2018) used graph neural networks (Battaglia et al., 2018; Li et al., 2015; Scarselli et al., 2009; Niepert et al., 2016) to improve the sample-efficiency of reinforcement learning. He et al. (2017) used DynoNet to incorporate structured information in the collaborative dialogue setting. Compared with them, our method is a combination of the graph neural networks and hierarchical reinforcement learning, and experiments prove that they both work in the novel dialogue task. 7 Conclusion This paper proposes to detect identity fraud automatically via dialogue interactions. To achieve this goal, we present structured dialogue management to explore anti-fraud dialogue strategies based on a KG with reinforcement learning and a heuristic user simulator to evaluate our systems. Experiments have s"
D19-1185,I17-1074,0,0.0716819,"Missing"
D19-1185,D17-1237,0,0.0173809,"tem. Our work is also related to task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Li et al., 2017; Gaˇsi´c et al., 2011; Wang et al., 2018, 2019). Existing systems have mainly focused on slot-filling tasks (e.g., booking a hotel). In such tasks, a set of system actions can be pre-defined based on the business logic and slots. In contrast, the system actions in our task are selecting nodes in the KG to generate questions. Thus, the structured information is important in our task. Besides, some works also try to model structured information in dialogue systems. For example, Peng et al. (2017) used hierarchical reinforcement learn1769 ing (Vezhnevets et al., 2017; Kulkarni et al., 2016; Florensa et al., 2017) to design multi-domain dialogue management. Chen et al. (2018) used graph neural networks (Battaglia et al., 2018; Li et al., 2015; Scarselli et al., 2009; Niepert et al., 2016) to improve the sample-efficiency of reinforcement learning. He et al. (2017) used DynoNet to incorporate structured information in the collaborative dialogue setting. Compared with them, our method is a combination of the graph neural networks and hierarchical reinforcement learning, and experiments pr"
D19-1185,D15-1281,0,0.0549047,"Missing"
D19-1185,P19-1361,1,0.850777,"Missing"
D19-1185,D18-1415,1,0.830374,"an et al., 2018; Graciarena et al., 2006) or both (Krishnamurthy et al., 2018; P´erez-Rosas et al., 2015) to train a classification model. In their work, the definition of deception is telling a lie. Besides, existing work requires labeled data, which is often hard to get. In contrast, we focus on detecting identity fraud through multi-turn interactions and use reinforcement learning to explore the anti-fraud policy without any labeled data. Dialogue System. Our work is also related to task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Li et al., 2017; Gaˇsi´c et al., 2011; Wang et al., 2018, 2019). Existing systems have mainly focused on slot-filling tasks (e.g., booking a hotel). In such tasks, a set of system actions can be pre-defined based on the business logic and slots. In contrast, the system actions in our task are selecting nodes in the KG to generate questions. Thus, the structured information is important in our task. Besides, some works also try to model structured information in dialogue systems. For example, Peng et al. (2017) used hierarchical reinforcement learn1769 ing (Vezhnevets et al., 2017; Kulkarni et al., 2016; Florensa et al., 2017) to design multi-domain"
D19-1185,E17-1042,0,0.0769615,"Missing"
D19-1185,P17-1062,0,0.0317656,") πtw (va |Et (vp )) ∝ exp (Ww [Et (vp ), Et (va )] + bw ) πtw (dw |Et (vp )) ∝ exp (Ww [Et (vp ), E(dw )] + bw ) (4) where {Wm , Ww , bm , bw , E(dm ), E(dw )} are parameters, Et (vu ) and Et (vp ) are dialogue states of the manager and worker in the t-th turn, E(dm ) is the encoding of the manager’s terminal action which has the same dimension as Et (vp ), and E(dw ) is the encoding of the worker’s terminal action which has the same dimension as Et (va ). Besides, to prevent the two agents from making decisions in haste, domain rules are applied to their dialogue policies by “Action Mask” (Williams et al., 2017). Specifically, domain rules are defined as follows. First, only after all or at least three answer nodes related to a worker have been explored can the worker make the decision. Second, only after all workers have made decisions or at least one worker’s decision is “Fraud” can the manager make the final decision. 4 4.1 Training Reward Function We expect the system can give correct decisions about applicants within minimum turns. Thus, at the end of each dialogue, the manager receives m for correct decision, or a a positive reward rcrt m negative reward −rwrg for wrong decision. If the manager"
D19-1302,P15-1166,0,0.035152,"stance, En2ZhSum dataset contains a total of 370,687 documents with corresponding summaries in both MS-Decoder gray , 94 , had … Rod gray was (2) where dk is the dimension of the key. Finally, the output values are concatenated and projected by a feed-forward layer to get final values: Attention(QWiQ , KWiK , V CLS-Decoder (1) N X (1) (1) logP(yt |y&lt;t , x; θ) + (2) N X t=1 (2) (2) logP(yt |y&lt;t , x; θ) t=1 (4) where y (1) and y (2) are the outputs of two tasks. CLS+MT. Since CLS input-output pairs are different from MT input-output pairs, we consider adopting the alternating training strategy (Dong et al., 2015), which optimizes each task for a fixed number of mini-batches before switching to the next task, to jointly train CLS and MT. For MT task, we employ 2.08M6 sentence pairs from LDC corpora with CLS dataset to train CLS+MT. 4 Experiments 4.1 Experimental Settings For English, we apply two different granularities of segmentation, i.e., words and subwords (Sennrich et al., 2016). We lowercase all English characters. We truncate the input to 200 words and the output to 120 words (150 characters for Chinese output) . For Chinese, we employ three different 6 LDC2000T50, LDC2002L27, LDC2002T01, LDC20"
D19-1302,D17-1222,0,0.036895,"Missing"
D19-1302,P19-1305,0,0.25288,"terparts. The final summary is generated by maximizing both the salience and translation quality of the PAS elements. However, all these researches belong to the pipeline paradigm which not only relies heavily on hand-crafted features but also causes error propagation. End-to-end deep learning has proven to be able to alleviate these two problems, while it has been absent due to the lack of largescale training data. Recently, Ayana et al. (2018) present zero-shot cross-lingual headline generation based on existing parallel corpora of translation and monolingual headline generation. Similarly, Duan et al. (2019) propose to use monolingual abstractive sentence summarization system to teach zero-shot cross-lingual abstractive sentence summarization on both summary word generation and attention. Although great efforts have been made in cross-lingual summarization, how to automatically build a high-quality large-scale cross-lingual summarization dataset remains unexplored. In this paper, we focus on English-to-Chinese and Chinese-to-English CLS and try to automatically construct two large-scale corpora respectively. In addition, based on the two corpora, we perform several end-to-end training methods not"
D19-1302,W04-1013,0,0.0392245,"nsformer-based NCLS models where the input and output are different granularities combinations of units. CLS+MS: It refers to the multi-task NCLS model which accepts an input text and simultaneously performs text generation for both CLS and MS tasks and calculates the total losses. CLS+MT: It trains CLS and MT tasks via alternating training strategy. Specifically, we optimize the CLS task in a mini-batch, and we optimize the MT task in the next mini-batch. 4.3 Experimental Results and Analysis Comparison between NCLS with baselines. We evaluate different models with the standard ROUGE metric (Lin, 2004), reporting the F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L. The results are presented in Table 4. 7 https://translate.google.com/ The parameter for ROUGE script here is “-c 95 -r 1000 -n 2 -a”. 3058 8 Model Unit En2ZhSum En2ZhSum* Zh2EnSum Zh2EnSum* RG1-RG2-RGL(↑) RG1-RG2-RGL(↑) RG1-RG2-RGL(↑) RG1-RG2-RGL(↑) TETran – 26.12-10.59-23.21 26.15-10.60-23.24 22.81- 7.17-18.55 23.09- 7.33-18.74 GETran – 28.17-11.38-25.75 28.19-11.40-25.77 24.03- 8.91-19.92 24.34- 9.14-20.13 TLTran c-c w-w sw-sw – 30.20-12.20-27.02 – – 30.22-12.20-27.04 – 32.85-15.34-29.21 31.11-13.23-27.55 33.64-15.58-29.74 33.01-15"
D19-1302,W00-0405,0,0.14307,"2003) translate the Hindi document to English and then generate the English headline for it. Ouyang et al. (2019) present a robust abstractive summarization system for low resource languages where no summarization corpora are currently available. They train a neural abstractive summarization model on noisy English documents and clean English reference summaries. Then the model can learn to produce fluent summaries from disfluent inputs, which allows generating summaries for translated documents. Orasan and Chiorean (2008) summarize the Romanian news with the maximal marginal relevance method (Goldstein et al., 2000) and produce the English summaries for English speakers. Wan et al. (2010) adopt the late translation scheme for the task of English-to-Chinese CLS. They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary. The above researches only make use of the information from only one language side. Some methods have been proposed to improve CLS with bilingual information. Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside"
D19-1302,P16-1154,0,0.0724329,"Missing"
D19-1302,D15-1229,0,0.426112,"cale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we introduce a novel approach to directly address the lack of data. Specifically, we propose a simple yet effective round-trip translation strategy to obtain cross-lingual documentsummary pairs from existing monolingual summarization datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015). More details can be found in Section 2 below. Based on the dataset that we have constructed, we propose end-to-end models on cross-lingual summarization, which we refer to as Neural CrossLingual Summarization (NCLS). Furthermore, we consider improving CLS with two related tasks: MS and MT. We incorporate the training process of MS and MT into that of CLS under the multitask learning framework (Caruana, 1997). Experimental results demonstrate that NCLS achieves remarkable improvement over traditional pipeline paradigm. In addition, both MS and MT can significantly help to produce better summa"
D19-1302,orasan-chiorean-2008-evaluation,0,0.889347,"here: http://www. nlpr.ia.ac.cn/cip/dataset.htm. 1 Introduction Given a document in one source language, crosslingual summarization aims to produce a summary in a different target language, which can help people efficiently acquire the gist of an article in a foreign language. Traditional approaches to CLS are based on the pipeline paradigm, which either first translates the original document into target language and then summarizes the translated document (Leuski et al., 2003) or first summarizes the original document and then translates the ∗ summary into target language (Lim et al., 2004; Orasan and Chiorean, 2008; Wan et al., 2010). However, the current machine translation (MT) is not perfect, which results in the error propagation problem. Although end-to-end deep learning has made great progress in natural language processing, no one has yet applied it to CLS due to the lack of large-scale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we i"
D19-1302,N19-1204,0,0.272835,"ent. The CLS+MS summary matches the gold summary better. The flaws of both of them are that they do not reflect the “scale” in the original text. In conclusion, our methods can produce more accurate summaries than baselines. 5 Related Work Cross-lingual summarization has been proposed to present the most salient information of a source document in a different language, which is very important in the field of multilingual information processing. Most of the existing methods handle the task of CLS via simply applying two typical translation schemes, i.e., early translation (Leuski et al., 2003; Ouyang et al., 2019) and late translation (Orasan and Chiorean, 2008; Wan et al., 2010). The early translation scheme first translates the original document into target language and then generates the summary of the translated document. The late translation scheme first summarizes the original document into a summary in the source language and then translates it into target language. 3061 Leuski et al. (2003) translate the Hindi document to English and then generate the English headline for it. Ouyang et al. (2019) present a robust abstractive summarization system for low resource languages where no summarization"
D19-1302,P17-1099,0,0.189683,"Missing"
D19-1302,P16-1162,0,0.0613843,"1 (2) (2) logP(yt |y&lt;t , x; θ) t=1 (4) where y (1) and y (2) are the outputs of two tasks. CLS+MT. Since CLS input-output pairs are different from MT input-output pairs, we consider adopting the alternating training strategy (Dong et al., 2015), which optimizes each task for a fixed number of mini-batches before switching to the next task, to jointly train CLS and MT. For MT task, we employ 2.08M6 sentence pairs from LDC corpora with CLS dataset to train CLS+MT. 4 Experiments 4.1 Experimental Settings For English, we apply two different granularities of segmentation, i.e., words and subwords (Sennrich et al., 2016). We lowercase all English characters. We truncate the input to 200 words and the output to 120 words (150 characters for Chinese output) . For Chinese, we employ three different 6 LDC2000T50, LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07 3057 Model Gu et al. (2016) Li et al. (2017) Transformer ROUGE-1 ROUGE-2 ROUGE-L 35.00 36.99 39.71 22.30 24.15 27.45 32.00 34.21 37.13 Model See et al. (2017) Transformer Table 2: Performance of our implemented transformerbased monolingual summarization model on LCSTS. granularities of segmentation: characters, words, and"
D19-1302,P11-1155,0,0.51022,"orean (2008) summarize the Romanian news with the maximal marginal relevance method (Goldstein et al., 2000) and produce the English summaries for English speakers. Wan et al. (2010) adopt the late translation scheme for the task of English-to-Chinese CLS. They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary. The above researches only make use of the information from only one language side. Some methods have been proposed to improve CLS with bilingual information. Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside information in the task of English-to-Chinese CLS. Inspired by the phrase-based translation models, Yao et al. (2015) introduce a compressive CLS, which simultaneously performs sentence selection and compression. They calculate the sentence scores based on the aligned bilingual phrases obtained by MT service and perform compression via deleting redundant or poorly translated phrases. Zhang et al. (2016) propose an abstractive CLS which constructs a pool of bilingual concepts represented by the bil"
D19-1302,P10-1094,0,0.838602,"ac.cn/cip/dataset.htm. 1 Introduction Given a document in one source language, crosslingual summarization aims to produce a summary in a different target language, which can help people efficiently acquire the gist of an article in a foreign language. Traditional approaches to CLS are based on the pipeline paradigm, which either first translates the original document into target language and then summarizes the translated document (Leuski et al., 2003) or first summarizes the original document and then translates the ∗ summary into target language (Lim et al., 2004; Orasan and Chiorean, 2008; Wan et al., 2010). However, the current machine translation (MT) is not perfect, which results in the error propagation problem. Although end-to-end deep learning has made great progress in natural language processing, no one has yet applied it to CLS due to the lack of large-scale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we introduce a novel ap"
D19-1302,D15-1012,0,0.375044,"n scheme for the task of English-to-Chinese CLS. They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary. The above researches only make use of the information from only one language side. Some methods have been proposed to improve CLS with bilingual information. Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside information in the task of English-to-Chinese CLS. Inspired by the phrase-based translation models, Yao et al. (2015) introduce a compressive CLS, which simultaneously performs sentence selection and compression. They calculate the sentence scores based on the aligned bilingual phrases obtained by MT service and perform compression via deleting redundant or poorly translated phrases. Zhang et al. (2016) propose an abstractive CLS which constructs a pool of bilingual concepts represented by the bilingual elements of the source-side predicate-argument structures (PAS) and the target-side counterparts. The final summary is generated by maximizing both the salience and translation quality of the PAS elements. Ho"
D19-1302,D18-1448,1,0.906264,"he lack of large-scale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we introduce a novel approach to directly address the lack of data. Specifically, we propose a simple yet effective round-trip translation strategy to obtain cross-lingual documentsummary pairs from existing monolingual summarization datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015). More details can be found in Section 2 below. Based on the dataset that we have constructed, we propose end-to-end models on cross-lingual summarization, which we refer to as Neural CrossLingual Summarization (NCLS). Furthermore, we consider improving CLS with two related tasks: MS and MT. We incorporate the training process of MS and MT into that of CLS under the multitask learning framework (Caruana, 1997). Experimental results demonstrate that NCLS achieves remarkable improvement over traditional pipeline paradigm. In addition, both MS and MT can significantly help to pr"
D19-1330,P15-1166,0,0.0443813,"redicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model. 1 Chinese Neural Machine Translation (NMT) based on the encoder-decoder framework has significantly improved translation quality due to its powerful endto-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of"
D19-1330,N16-1101,0,0.0597358,"demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model. 1 Chinese Neural Machine Translation (NMT) based on the encoder-decoder framework has significantly improved translation quality due to its powerful endto-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when"
D19-1330,P07-2045,0,0.0070878,"respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as devlopment set and test set respectively. Besides, we also perform En→De/Fr translation in large scale WMT142 datasets. We use newstest2014 as test set. En→Zh/Ja: For this translation task, the training sets of En→Zh and En→Ja consist of 231K, 223K sentence pairs. We tokenize the English sentences using a script from Moses (Koehn et al., 2007), and we segment Chinese and Japanese data by jieba3 and mecab4 . We use BPE method (Sennrich et al., 2016b) to encode the source side sentences and the combination of target side sentences respectively and limit the vocabularies of both sides to the most frequent 10k tokens. En→De/Fr: We conduct this translation task on two different settings. One setting is using training set of IWSLT datasets which contains 206K sentence pairs for En→De and 233K sentence pairs for En→Fr. We follow the common practice to tokenize and lowercase all words. Sentences are encoded using BPE, which has a shared vo"
D19-1330,N16-1046,0,0.0605969,"Missing"
D19-1330,W18-6309,0,0.0131386,"g (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentence"
D19-1330,D18-1103,0,0.0205996,"l., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentences, in which two targe"
D19-1330,P02-1040,0,0.103586,"t sides. We use Adam optimizer (Kingma and Ba, 2014) with β1 =0.9, β2 =0.98, and =10−9 . For decoding, we set beam size to be k = 4 and length penalty α = 0.6. All our methods are trained and tested on single Nvidia P40 GPU. We investigate the impact of different λs in our synchronous attention model. As shown in Table 1, when λ=0.1, the translation results perform best on development set for both En→Zh/Ja and En→De/Fr tasks, and we will use this setting in the subsequent experiments. 5 Results and Analysis The translation performance of IWSLT datasets is evaluated by case-insensitive BLEU4 (Papineni et al., 2002) for En→De/Fr task and characterlevel BLEU5 for En→Zh/Ja task. For WMT14 datasets, we calculate the case-sensitive BLEU4 the same as previous work. In our experiments, the NMT models trained on individual language pair are denoted by Indiv. 5.1 Results on IWSLT Table 2 shows the main translation results of En→Zh/Ja and En→De/Fr on IWSLT datasets. We also conduct a typical one-to-many translation adopting Johnson et al. (2017) method on Transformer as our another baseline model, referred to Multi. Compared with Indiv, we can see that Multi achieves better results on all cases, which can be attr"
D19-1330,P16-1009,0,0.0332944,"data D = (x1, y 1, y 2?) ∪ (x2, y 1?, y 2 ) , which can be used to train our synchronous translation model mentioned above. |yi | Õ log P(yi2 |x)) (5) i=1 When calculating P(yi1 |x), except for the context from source side x, our synchronous method 1 as condiemploys not only previous reference y&lt;i tion, but the previous context of the other decoder 2 . The calculation process of P(y 2 |x) reference y&lt;i i is similar. However, the practical situation is that the triple data is limited and hard to be collected. In this work, we construct the trilingual training corpus by data augmenting method (Sennrich et al., 2016a; Zhang and Zong, 2016). To achieve this, we first learn two independent translation models Model-1 and Model-2 on the bilingual training data (x1, y 1 ) and (x2, y 2 ) separately. Then, Model-1 and Model-2 are employed to decode the input sentences x2 and x1 , resulting in pseudo training data (x2, y 1?) and (x1, y 2?), respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as de"
D19-1330,P16-1162,0,0.0488093,"data D = (x1, y 1, y 2?) ∪ (x2, y 1?, y 2 ) , which can be used to train our synchronous translation model mentioned above. |yi | Õ log P(yi2 |x)) (5) i=1 When calculating P(yi1 |x), except for the context from source side x, our synchronous method 1 as condiemploys not only previous reference y&lt;i tion, but the previous context of the other decoder 2 . The calculation process of P(y 2 |x) reference y&lt;i i is similar. However, the practical situation is that the triple data is limited and hard to be collected. In this work, we construct the trilingual training corpus by data augmenting method (Sennrich et al., 2016a; Zhang and Zong, 2016). To achieve this, we first learn two independent translation models Model-1 and Model-2 on the bilingual training data (x1, y 1 ) and (x2, y 2 ) separately. Then, Model-1 and Model-2 are employed to decode the input sentences x2 and x1 , resulting in pseudo training data (x2, y 1?) and (x1, y 2?), respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as de"
D19-1330,D18-1326,1,0.779143,"l., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentences, in which two targets can interact with each other. Introductio"
D19-1330,P19-1117,1,0.710862,"ements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model. 1 Chinese Neural Machine Translation (NMT) based on the encoder-decoder framework has significantly improved translation quality due to its powerful endto-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5"
D19-1330,D16-1160,1,0.852653,") ∪ (x2, y 1?, y 2 ) , which can be used to train our synchronous translation model mentioned above. |yi | Õ log P(yi2 |x)) (5) i=1 When calculating P(yi1 |x), except for the context from source side x, our synchronous method 1 as condiemploys not only previous reference y&lt;i tion, but the previous context of the other decoder 2 . The calculation process of P(y 2 |x) reference y&lt;i i is similar. However, the practical situation is that the triple data is limited and hard to be collected. In this work, we construct the trilingual training corpus by data augmenting method (Sennrich et al., 2016a; Zhang and Zong, 2016). To achieve this, we first learn two independent translation models Model-1 and Model-2 on the bilingual training data (x1, y 1 ) and (x2, y 2 ) separately. Then, Model-1 and Model-2 are employed to decode the input sentences x2 and x1 , resulting in pseudo training data (x2, y 1?) and (x1, y 2?), respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as devlopment set and test se"
D19-1330,Q19-1006,1,0.681102,"where x is source sentence, y 1, y 2 are target sentences corresponding to two different languages. At time-step i, we have generated the first i − 1 tokens of language-1 y 1 and the first i − 1 tokens of language-2 y 2 . Then both languages predictions can be utilized together with source sentence to generate tokens yi1 and yi2 . This interaction between two languages is realized by synchronous attention model, which will be detailed in the following subsection. It should be noted that the two language sentences can be generated in different directions (Liu et al., 2016; Zhang et al., 2018; Zhou et al., 2019), which means language-1 can be produced in leftto-right (L2R) manner while language-2 in rightto-left (R2L) manner. We will analyze the effect of different decoding manners in Sec. 5.2. 3.2 Synchronous Attention Model Synchronous attention model (SyncAtt) is shown in Figure 2, in which inputs of two decoders contain queries (Q1, Q2 ), keys (K1, K2 ), and values 0 (V1, V2 ) separately. The new hidden states (Hi ) can be computed by our proposed synchronous atten3351 tion as follows: 0 H1 =SyncAtt (Q1, [K1 ; K2 ], [V1 ; V2 ]) 0 H2 =SyncAtt (Q2, [K1 ; K2 ], [V1 ; V2 ]) (3) where synchronous atte"
D19-1330,N16-1004,0,0.0662303,"sentences respectively and limit the vocabularies of both sides to the most frequent 10k tokens. En→De/Fr: We conduct this translation task on two different settings. One setting is using training set of IWSLT datasets which contains 206K sentence pairs for En→De and 233K sentence pairs for En→Fr. We follow the common practice to tokenize and lowercase all words. Sentences are encoded using BPE, which has a shared vocabulary of 10K tokens. At last, we construct pseudo triple data by the method described in Sec. 3.3. For the other setting, we extract the trilingual subset in WMT14 inspired by Zoph and Knight (2016), which includes about 2.43M sentence triples. We use 37K shared BPE tokens as vocabulary. 4.2 Training Details We implement our synchronous translation based on the tensor2tensor5 library. We train our models using the configuration transformer base adopted 3352 1 https://wit3.fbk.eu 2 http://www.statmt.org/wmt14/translation-task.html 3 https://github.com/fxsjy/jieba 4 http://taku910.github.io/mecab 5 https://github.com/tensorflow/tensor2tensor Hyperparamter (λ) 0.1 0.2 0.3 0.4 0.5 En-De/Fr En-De En-Fr 30.95 43.01 30.77 42.99 30.55 42.99 29.60 42.52 29.19 41.87 En-Zh/Ja En-Zh En-Ja 16.33 18.8"
D19-1330,D18-1039,0,0.0154149,"al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentences, in which two targets can interact with eac"
I17-1039,D16-1026,0,0.387584,"model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i=0 j=0 Objective Function (n) (n) a(θ)i,j × b ai,j ("
I17-1039,P07-2045,0,0.00651849,"radient decent algorithm. We set learning rate to 0.1 at the beginning and halve the threshold while the perplexity increases on the development set. Dropout is applied to our model, and the rate is set to 0.2. For Table 1: The statistics of monolingual dataset on the LDC corpus. 4.2 Training Details Data Preparing and Preprocessing Considering the fact that the amount of manually annotated phrase pairs is not enough, in order to imitate the environment of experiment, we extract phrase pairs from parallel corpora automatically to make up for the shortage of quantity. To do this, we use Moses (Koehn et al., 2007) in its training step to learn a phrase table from LDC corpus, which includes 0.63M sentence pairs. In order to simulate the experiment as far as possible, we adopt three strategies to filter low quality 1 388 https://github.com/isi-nlp/ZophRNN # System MT03 MT04 MT05 MT06 Ave 1 2 3 4 5 Phrase NMT Model Partially Aligned Model(MUL) Partially Aligned Model(MSE) Partially Aligned Model(MSE) + LimitedVocab Phrase NMT model + LimitedVocab 3.64 3.80 5.11 6.63 3.78 4.25 4.37 5.04 6.81 4.33 3.55 3.75 4.26 5.59 3.63 3.77 4.24 4.95 5.77 3.94 3.80 4.04 4.84 6.20 3.92 Table 2: Translation results (BLEU s"
I17-1039,P17-1176,0,0.0316832,"easily, and high quality phrase pairs can be obtained using some effective methods (Zhang et al., 2014). To learn a good NMT Pivot-based Scenario Pivot-based scenario assumes that there only exists source-pivot and pivot-target parallel corpora, which can be used to train source-to-pivot and pivot-to-target translation models. Cheng et al. (2017) propose to translate source language into pivot language, and then the pivot language will be translated into target language. According to the fact that parallel sentences should have close probabilities of generating a sentence in a third language, Chen et al. (2017) construct a TeacherStudent framework, in which existing pivot-target NMT model guides the learning process of the source-target model. 6.2 Monolingual Data Scenario Multilingual Scenario In multilingual scenario, there exists multiple language pairs but no source-target sentence pairs. Johnson et al. (2016) use parallel corpora of multiple languages to train a universal NMT model. This universal model learns translation knowledge from multiple different languages, which makes zero-shot translation feasible. Firat et al. (2016) present a multi-way, multilingual model to resolve the zero-resour"
I17-1039,C16-1291,0,0.0203739,"y2 and y3 . Therefore, the decoder in our model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i=0 j=0 Objective Func"
I17-1039,P16-1185,0,0.0288224,"able translation result is still in need by incorporating additional data resource. G¨ulc¸ehre et al. (2015) propose to incorporate target-side corpora as a language model. Sennrich et al. (2016a) attempt to enhance the decoder network model of NMT by incorporating the target-side monolingual data. Luong et at. (2016) explore the sequence autoencoders and skip-thought vectors method to exploit the monolingual data of source language. Zhang and Zong (2016) propose two approaches, self-training algorithm and multitask learning framework, to incorporate sourceside monolingual data. Besides that, Cheng et al. (2016) have explored the usage of both source and target monolingual data using a semisupervised method to reconstruct both source and target side monolingual language, where two NMT frameworks will be used. Above methods are designed for different scenarios, and their work can achieve great results on these scenarios. However, when in the scenario we propose in this work, that is we only have monolingual sentences and some phrase pairs, their methods are hard to be utilized to train an NMT model. Under this scenario, monolingual data can be acquired easily, and high quality phrase pairs can be obta"
I17-1039,P06-1077,0,0.0400899,"l method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned sentences) with the Introduction Neural machine translation (NMT) proposed by Kalchbrenner et al.(2013), Sutskever et al.(2014) and Cho et al.(2014) has achieved significant progress in recent years. Different from traditional statistical machine translation(SMT) (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Zhai et al., 2012) which contains multiple separately tuned components, NMT builds an end-to-end framework to model the whole translation process. For several language pairs, NMT is reaching significantly better translation performance than SMT (Luong et al., 2015b; Wu et al., 2016). In general, in order to obtain an NMT model 384 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 384–393, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP monolingual data and phrase pairs. Then we can utilize these partially aligned sentences to train an"
I17-1039,C12-1186,1,0.831605,"an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned sentences) with the Introduction Neural machine translation (NMT) proposed by Kalchbrenner et al.(2013), Sutskever et al.(2014) and Cho et al.(2014) has achieved significant progress in recent years. Different from traditional statistical machine translation(SMT) (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Zhai et al., 2012) which contains multiple separately tuned components, NMT builds an end-to-end framework to model the whole translation process. For several language pairs, NMT is reaching significantly better translation performance than SMT (Luong et al., 2015b; Wu et al., 2016). In general, in order to obtain an NMT model 384 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 384–393, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP monolingual data and phrase pairs. Then we can utilize these partially aligned sentences to train an NMT model. Figure 1"
I17-1039,P14-1011,1,0.83923,"urce and target monolingual data using a semisupervised method to reconstruct both source and target side monolingual language, where two NMT frameworks will be used. Above methods are designed for different scenarios, and their work can achieve great results on these scenarios. However, when in the scenario we propose in this work, that is we only have monolingual sentences and some phrase pairs, their methods are hard to be utilized to train an NMT model. Under this scenario, monolingual data can be acquired easily, and high quality phrase pairs can be obtained using some effective methods (Zhang et al., 2014). To learn a good NMT Pivot-based Scenario Pivot-based scenario assumes that there only exists source-pivot and pivot-target parallel corpora, which can be used to train source-to-pivot and pivot-to-target translation models. Cheng et al. (2017) propose to translate source language into pivot language, and then the pivot language will be translated into target language. According to the fact that parallel sentences should have close probabilities of generating a sentence in a third language, Chen et al. (2017) construct a TeacherStudent framework, in which existing pivot-target NMT model guide"
I17-1039,D15-1166,0,0.649257,"these sentences as partially aligned sentences) with the Introduction Neural machine translation (NMT) proposed by Kalchbrenner et al.(2013), Sutskever et al.(2014) and Cho et al.(2014) has achieved significant progress in recent years. Different from traditional statistical machine translation(SMT) (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Zhai et al., 2012) which contains multiple separately tuned components, NMT builds an end-to-end framework to model the whole translation process. For several language pairs, NMT is reaching significantly better translation performance than SMT (Luong et al., 2015b; Wu et al., 2016). In general, in order to obtain an NMT model 384 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 384–393, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP monolingual data and phrase pairs. Then we can utilize these partially aligned sentences to train an NMT model. Figure 1 shows an example of our data. Source sentence and target sentence are not fully aligned but contain two translation fragments: (”外交部发言人”, ”foreign ministry deputy”) and (”在 例 行 的 记 者 招 待 会 上 说”, ”speaking at a regular press”). Intuitively, these"
I17-1039,D16-1160,1,0.931466,"sentence are not parallel but they include two parallel parts (highlight in blue and red respectively). of great translation quality, we usually need largescale parallel data. Unfortunately, the large-scale parallel data is always insufficient in many domains and language pairs. Without sufficient parallel sentence pairs, NMT tends to learn poor estimates on low-count events. Actually, there have been some effective methods to deal with the situation of translating language pairs with limited resource under different scenarios (Johnson et al., 2016; Cheng et al., 2017; Sennrich et al., 2016a; Zhang and Zong, 2016). In this paper, we address a new translation scenario in which we do not have any parallel sentences but have massive monolingual corpora and phrase pairs. The previous methods are hard to be used to learn an NMT model under this situation. In this paper, we propose a novel method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned sentences) with the Intro"
I17-1039,P15-1002,0,0.054861,"Missing"
I17-1039,P17-2060,1,0.819411,"ts and +13.21 BLEU points respectively. When using more than 60K sentence pairs, we still get a relatively high promotion of translation quality. However, the promotion is not very remarkable as Row1-3 reveal in Table 4. We can see when the number of parallel corpora is 100K(Row 5), the improvement over NMT Model is +3.95 BLEU points, which indicates that as the size of parallel corpora increases, the improvement of fine-tuning model is decreasing. 8.21 8 #Sent. #Word Vocab 6 Related Work Most of existing work in neural machine translation focus on integrating SMT strategies (He et al., 2016; Zhou et al., 2017; Wang et al., 2017; Shen et al., 2015), handling rare words (Li et al., 2016; Sennrich et al., 2016b; Luong et al., 2015b) and designing the better framework (Tu et al., 2016; Luong et al., 2015a; Meng et al., 2016). As for translation scenarios, training NMT model under Effect of Adding Small Parallel Corpus We concern that when we have a tiny parallel corpus, whether the small scale parallel corpus can boost the translation performance of the partially aligned method. Here, we fine-tune the partially 390 #Sent. Method MT03 MT04 MT05 MT06 Ave 20K NMT Model Partially Aligned Model(MSE) + Para"
I17-1039,C16-1205,0,0.0150104,"le 4. We can see when the number of parallel corpora is 100K(Row 5), the improvement over NMT Model is +3.95 BLEU points, which indicates that as the size of parallel corpora increases, the improvement of fine-tuning model is decreasing. 8.21 8 #Sent. #Word Vocab 6 Related Work Most of existing work in neural machine translation focus on integrating SMT strategies (He et al., 2016; Zhou et al., 2017; Wang et al., 2017; Shen et al., 2015), handling rare words (Li et al., 2016; Sennrich et al., 2016b; Luong et al., 2015b) and designing the better framework (Tu et al., 2016; Luong et al., 2015a; Meng et al., 2016). As for translation scenarios, training NMT model under Effect of Adding Small Parallel Corpus We concern that when we have a tiny parallel corpus, whether the small scale parallel corpus can boost the translation performance of the partially aligned method. Here, we fine-tune the partially 390 #Sent. Method MT03 MT04 MT05 MT06 Ave 20K NMT Model Partially Aligned Model(MSE) + Para 1.60 12.36 1.22 15.07 1.05 11.64 1.70 14.61 1.39 13.42 40K NMT Model Partially Aligned Model(MSE) + Para 1.87 14.12 2.00 17.84 1.47 13.66 2.24 17.26 1.90 15.72 60K NMT Model Partially Aligned Model(MSE) + Para 3.72"
I17-1039,D16-1249,0,0.228359,"e decoder outputs y2 and y3 . Therefore, the decoder in our model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i="
I17-1039,P16-2021,0,0.271354,"model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i=0 j=0 Objective Function (n) (n) a(θ)i,j × b ai,j ("
I17-1039,P02-1040,0,0.100406,"ade up of all the target words of the special phrase pairs whose corresponding source words belong to the source sentence. 4 Experiment In this section, we perform the experiment on Chinese-English translation tasks to test our method. 4.1 Dataset We evaluate our approach on large-scale monolingual data set from LDC corpus, which includes 13M Chinese sentences and 10M English sentences. Table 1 shows the detailed statistics of our training data. To test our model, we use NIST 2003(MT03) as development set, and NIST 20042006(MT04-06) as test set. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl. Corpus monolingual #Sent. #Word Vocab Chinese 13.33M 327.10M 1.83M English 10.03M 276.07M 1.07M 4.3 We build our described method based on the Zoph RNN toolkit1 written in C++/CUDA. Both encoder and decoder consist of two stacked LSTM layers. We set minibatch size to 128. The word embedding dimension of both source and target sides is 1000, and the dimensions of hidden layers unit is set to 1000. In our baseline model, we limit the vocabulary of both source and target languages to 30K most frequent words, and other words are replaced by a special symbol “"
I17-1039,P16-1009,0,0.475568,"rce sentence and target sentence are not parallel but they include two parallel parts (highlight in blue and red respectively). of great translation quality, we usually need largescale parallel data. Unfortunately, the large-scale parallel data is always insufficient in many domains and language pairs. Without sufficient parallel sentence pairs, NMT tends to learn poor estimates on low-count events. Actually, there have been some effective methods to deal with the situation of translating language pairs with limited resource under different scenarios (Johnson et al., 2016; Cheng et al., 2017; Sennrich et al., 2016a; Zhang and Zong, 2016). In this paper, we address a new translation scenario in which we do not have any parallel sentences but have massive monolingual corpora and phrase pairs. The previous methods are hard to be used to learn an NMT model under this situation. In this paper, we propose a novel method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned s"
I17-1039,P16-1162,0,0.715196,"rce sentence and target sentence are not parallel but they include two parallel parts (highlight in blue and red respectively). of great translation quality, we usually need largescale parallel data. Unfortunately, the large-scale parallel data is always insufficient in many domains and language pairs. Without sufficient parallel sentence pairs, NMT tends to learn poor estimates on low-count events. Actually, there have been some effective methods to deal with the situation of translating language pairs with limited resource under different scenarios (Johnson et al., 2016; Cheng et al., 2017; Sennrich et al., 2016a; Zhang and Zong, 2016). In this paper, we address a new translation scenario in which we do not have any parallel sentences but have massive monolingual corpora and phrase pairs. The previous methods are hard to be used to learn an NMT model under this situation. In this paper, we propose a novel method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned s"
I17-1039,P16-5005,0,0.027134,"ery remarkable as Row1-3 reveal in Table 4. We can see when the number of parallel corpora is 100K(Row 5), the improvement over NMT Model is +3.95 BLEU points, which indicates that as the size of parallel corpora increases, the improvement of fine-tuning model is decreasing. 8.21 8 #Sent. #Word Vocab 6 Related Work Most of existing work in neural machine translation focus on integrating SMT strategies (He et al., 2016; Zhou et al., 2017; Wang et al., 2017; Shen et al., 2015), handling rare words (Li et al., 2016; Sennrich et al., 2016b; Luong et al., 2015b) and designing the better framework (Tu et al., 2016; Luong et al., 2015a; Meng et al., 2016). As for translation scenarios, training NMT model under Effect of Adding Small Parallel Corpus We concern that when we have a tiny parallel corpus, whether the small scale parallel corpus can boost the translation performance of the partially aligned method. Here, we fine-tune the partially 390 #Sent. Method MT03 MT04 MT05 MT06 Ave 20K NMT Model Partially Aligned Model(MSE) + Para 1.60 12.36 1.22 15.07 1.05 11.64 1.70 14.61 1.39 13.42 40K NMT Model Partially Aligned Model(MSE) + Para 1.87 14.12 2.00 17.84 1.47 13.66 2.24 17.26 1.90 15.72 60K NMT Model"
I17-1039,1983.tc-1.13,0,0.765242,"Missing"
I17-1039,D13-1176,0,\N,Missing
I17-1039,P05-1033,0,\N,Missing
I17-1039,N03-1017,0,\N,Missing
K16-2003,prasad-etal-2008-penn,0,0.402218,"Missing"
K16-2003,K15-2002,0,0.171723,"ng and Xue, 2012), punctuation marks play a significant role in Chinese discourse. Fortunately, CDTB has annotated those punctuations that may indicate discourse relations. Inspired by the above phenomena, we design our system by fully considering these Chinese characteristics. Besides the training data, we simply use skipgram neural word embeddings provided by the CoNLL-2016 organizers to replace words in some features. 3 Argument Extraction 3.1.1 Connective Identification A classifier is trained to recognize connectives. The features are chosen by referring to the best system in CoNLL-2015 (Wang and Lan, 2015). Zhou and Xue (2012) found that a discourse connective is almost always accompanied by punctuations, which help us to design the features. The features we used are as follows: System Architecture Zhou and Xue (2015) pointed out that discourse connectives and punctuation marks in Chinese can serve as anchors, which are clues of discourse relations. This opinion encourages us to treat explicit and non-explicit relations similarly. Therefore, the explicit and non-explicit parsers share the same • Lexical features: candidate itself, number of the candidate words, POS of the candidate, POS of the"
K16-2003,I11-1170,0,0.0354422,"Missing"
K16-2003,K16-2001,0,0.0684866,"Missing"
K16-2003,Q15-1024,0,0.0283668,"Missing"
K16-2003,D14-1008,0,0.0232098,"ions are divided into two types, explicit or non-explicit, depending on whether connectives exist or not. A complete discourse relation contains two discourse units called Argument1 (Arg1) and Argument2 (Arg2). An end-to-end parser usually consists of some components, such as discourse connective identification, argument extraction, explicit sense classification and implicit sense classification. Pitler and Nenkova (2009) used syntactic features to disambiguate explicit discourse connectives. For argument extraction, Lin et al. (2014) used a tree subtraction algorithm to extract arguments and Kong et al. (2014) proposed a • We implement a complete end-to-end PDTBstyle discourse parser for Chinese. • We design a uniform framework to recog27 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 27–32, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics framework shown in figure 1. We divide the shallow discourse parsing into four subtasks: anchor identification, argument extraction, sense classification and argument relabeling nize both explicit and non-explicit relations together. • We utilize an effective seed-expansion approa"
K16-2003,P12-1083,0,0.0201937,"from several words to sentences even to paragraphs. But in general, the span is in one sentence and the clauses split by punctuations can be regarded as the minimum constituent units. 3.1 Anchor Identification A full text is scanned to pick out the anchor candidate set. Then, a binary classifier is designed to check whether each candidate is anchor or not. The explicit connective candidate set is generated by matching the text with our connective dictionary. The non-explicit punctuation candidate set consists of all punctuations except for quotes, parentheses, and pause marks. • As shown in (Yang and Xue, 2012), punctuation marks play a significant role in Chinese discourse. Fortunately, CDTB has annotated those punctuations that may indicate discourse relations. Inspired by the above phenomena, we design our system by fully considering these Chinese characteristics. Besides the training data, we simply use skipgram neural word embeddings provided by the CoNLL-2016 organizers to replace words in some features. 3 Argument Extraction 3.1.1 Connective Identification A classifier is trained to recognize connectives. The features are chosen by referring to the best system in CoNLL-2015 (Wang and Lan, 201"
K16-2003,D09-1036,0,0.222568,"se Parser with Adaptation to Explicit and Non-explicit Relation Recognition Xiaomian Kang1,2 , Haoran Li1,2 , Long Zhou1,2 , Jiajun Zhang1,2 , Chengqing Zong1,2,3 1 National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences 2 The University of Chinese Academy of Sciences, Beijing, China 3 CAS Center for Excellence in Brain Science and Intelligence Technology {xiaomian.kang,haoran.li,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract constituent-based approach to solve it. Recent researches mainly focus on the implicit sense classification. In this subtask, Lin et al. (2009) and Rutherford and Xue (2014) explored rich features such as word-pairs, dependency rules, production rules and Brown cluster pairs. Some studies (Rutherford and Xue, 2015) paid attention to the data expansion. Neural network approaches (Ji and Eisenstein, 2015; Zhang et al., 2015) were also applied to improve the classification performance. Lin et al. (2014) implemented a full endto-end PDTB parser and Wang and Lan (2015) built a more refined system in the CoNLL-2015 Shared Task. In contrast to English, there are limited studies on Chinese discourse parsing (Huang and Chen, 2011; Zong, 2013;"
K16-2003,D15-1266,0,0.0292035,"Missing"
K16-2003,P12-1008,0,0.572677,"es, production rules and Brown cluster pairs. Some studies (Rutherford and Xue, 2015) paid attention to the data expansion. Neural network approaches (Ji and Eisenstein, 2015; Zhang et al., 2015) were also applied to improve the classification performance. Lin et al. (2014) implemented a full endto-end PDTB parser and Wang and Lan (2015) built a more refined system in the CoNLL-2015 Shared Task. In contrast to English, there are limited studies on Chinese discourse parsing (Huang and Chen, 2011; Zong, 2013; Tu et al., 2014). One of the main reasons is the shortage of Chinese discourse corpus. Zhou and Xue (2012) annotated a PDTBstyle Chinese Discourse TreeBank (CDTB), which is the data for Chinese shallow discourse parsing. In this paper, we describe our approaches to implement the Chinese shallow discourse parser which is participated in the CoNLL-2016 Shared Task (Xue et al., 2016). In view of some typical characteristics in CDTB (Section 2), we adopt and extend the state-of-the-art English parser in CoNLL-2015 (Wang and Lan, 2015). A unified framework for both explicit and non-explicit parsing is built and a seed-expansion approach is utilized for argument extraction. Some useful features are sele"
K16-2003,P09-2004,0,0.0298595,"08), discourse parsing has drawn more and more attention. The PDTB-style parser puts emphasis on shallow discourse parsing, which annotates a piece of text with a set of discourse relations. The relations are divided into two types, explicit or non-explicit, depending on whether connectives exist or not. A complete discourse relation contains two discourse units called Argument1 (Arg1) and Argument2 (Arg2). An end-to-end parser usually consists of some components, such as discourse connective identification, argument extraction, explicit sense classification and implicit sense classification. Pitler and Nenkova (2009) used syntactic features to disambiguate explicit discourse connectives. For argument extraction, Lin et al. (2014) used a tree subtraction algorithm to extract arguments and Kong et al. (2014) proposed a • We implement a complete end-to-end PDTBstyle discourse parser for Chinese. • We design a uniform framework to recog27 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 27–32, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics framework shown in figure 1. We divide the shallow discourse parsing into four subtasks:"
K16-2003,E14-1068,0,\N,Missing
K16-2003,N15-1081,0,\N,Missing
L16-1159,P02-1040,0,0.111085,"Missing"
L16-1159,A00-2002,0,0.236834,"Missing"
L16-1159,D08-1035,0,\N,Missing
L18-1143,D16-1162,0,0.0186183,"om the above methods, we treat this problem with another perspective, as we observe that the words need to be reordered during translation are more likely to be ignored by NMT. Thus we exploit the pre-ordering for NMT to alleviate this problem. Exploiting techniques in SMT for NMT. Our work is also inspired by the works which incorporating the techniques in SMT to NMT. The earlier related work is conducted on the SMT framework, which is deeply discussed in the reviewed paper (Zhang and Zong, 2015). Here, we only focus on the work which combines the SMT and NMT on NMT framework. Specifically, (Arthur et al., 2016) incorporates word translation table in attention part to adjust the final loss. (Zhang and Zong, 2016) moves forward further by incorporating a bilingual dictionaries in NMT. (Stahlberg et al., 2016) and (He et al., 2016) rescore word candidates with SMT features. (G¨ulc¸ehre et al., 2015) improves the beam search with language model. (Zhou et al., 2017) proposes a neural combination model to fuse the NMT translation results and SMT translation results. (Wang et al., 2017) improves the NMT system with the SMT recommendations. (Zhang et al., 2014) proposes bilingually-constrained recursive aut"
L18-1143,D14-1179,0,0.0511682,"Missing"
L18-1143,N16-1102,0,0.0260283,"m to pre-order the source sentences. Note that the word order of the target sentence does not change. 3.2. Position Embedding As mentioned before, the most noticeable feature of preordering is that it can make the word order in source more consistent with the word order in target. Intuitively, monotone translation is preferred. That is to say the words in the similar positions between the source and target sentences are more likely to be translation pairs. Thus, we further enhance the pre-ordering model with the position embedding to encourage monotone translation. Actually, previous studies (Cohn et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) have shown that the position information is effective for NMT, and these studies are all based on the following assumption: Assumption: a word at a given relative position j in the source (whose length is denoted as J) is more likely to align to a word at a similar relative position i in the target (whose length is denoted as I), i.e. Jj ≈ Ii . Obviously, pre-ordering can make more words satisfy this assumption. We design the procedure as follows: We first randomly generate the respective position embedding matrix for the source and target position"
L18-1143,P05-1066,0,0.290685,"C) = p(yi |y<i , ci ) = sof tmax(Wyi zei + bs ) (2) where Wy is an embedding matrix containing row vectors of the target words and zei is the attention output: zei = tanh(Wc [zim ; ci ]) (3) The attention model calculates ci as the weighted sum of the source-side context vectors: ci = Tx X ai,j hm i (4) j=1 Where aij can be computed by exp(ei,j ) ai,j = PT x k=1 exp(ei,k ) (5) ei,j = vaT tanh(Wa zi + Ua hj ) (6) and zik is computed using the following formula: k zjk = LST M (zj−1 , zjk−1 ) 3. (7) Exploiting Pre-Ordering for NMT In SMT, pre-ordering is a commonly used pre-processing technique (Collins et al., 2005; Zhang and Zong, 2009; Genzel, 2010; Hitschler et al., 2016), which makes the word order of a source sentence closer to that of a target sentence. This technology was originally proposed to alleviate the weakness of reordering in classical phrase-based SMT (Koehn et al., 2003). As SMT always penalizes the cases that move target phrases far away from their corresponding source positions. Fig. 1 shows an example of pre-ordering, in which when translating the original source sentence, the words in red and words in blue need to exchange their positions. With the pre-ordering, the word order in th"
L18-1143,C10-1043,0,0.433597,"ning (18 times) is the case that the sub-sentences in source are totally dropped. From these statistics, we think that the first kind of under-translation, i.e. words need to be reordered are ignored, is a major problem affecting the final translation quality. Considering the fact that source words requiring reordering during translation are more likely to be ignored by the NMT model, we propose to exploit the pre-ordering approach which is commonly used in Statistical Machine Translation (SMT). The pre-ordering can make the word order of a source sentence closer to that of a target sentence (Genzel, 2010; Hitschler et al., 2016). We first pre-order the source sentences to approximate the target language word order. We then further combine the pre-ordering model with the position embedding strategy to enhance the monotone translation. Finally, to overcome the over-translation problem, we augment our model with the coverage mechanism. In this paper, we make the following contributions: 1) Through error analysis, we find that under-translation occurs more frequently than over-translation in NMT and source words that need reordering are more likely to be missed. We propose a pre-ordering approach"
L18-1143,J82-2005,0,0.699175,"Missing"
L18-1143,D13-1176,0,0.10621,"Missing"
L18-1143,N03-1017,0,0.0499309,": ci = Tx X ai,j hm i (4) j=1 Where aij can be computed by exp(ei,j ) ai,j = PT x k=1 exp(ei,k ) (5) ei,j = vaT tanh(Wa zi + Ua hj ) (6) and zik is computed using the following formula: k zjk = LST M (zj−1 , zjk−1 ) 3. (7) Exploiting Pre-Ordering for NMT In SMT, pre-ordering is a commonly used pre-processing technique (Collins et al., 2005; Zhang and Zong, 2009; Genzel, 2010; Hitschler et al., 2016), which makes the word order of a source sentence closer to that of a target sentence. This technology was originally proposed to alleviate the weakness of reordering in classical phrase-based SMT (Koehn et al., 2003). As SMT always penalizes the cases that move target phrases far away from their corresponding source positions. Fig. 1 shows an example of pre-ordering, in which when translating the original source sentence, the words in red and words in blue need to exchange their positions. With the pre-ordering, the word order in this source sentence is adjusted to the word order in reference. When translating the pre-ordered source sentence, the translation system does not need to reorder the source words. Since we find that the source words should to be reordered during translation are more likely to be"
L18-1143,P07-2045,0,0.0133189,"per-parameters we used in Otedama are set as follows: window size is set to 3, matching feature is 10, and the max waiting time is 30 minute. The others are set to the default values. More details can be found in (Hitschler et al., 2016). 4.4. (9) k=1 ei,j =vaT tanh(Wa zi + Ua hj + where Ci−1,j is the coverage vector of source word xj before time i, and Va is the weight matrix for coverage vector. Translation Methods In the experiments, we compare our approaches with other models, and we list all the translation methods as follows: 1) Moses: It is the state-of-the-art phrase-based SMT system (Koehn et al., 2007). Our system is built using the default settings. 2) Baseline: It is the baseline attention-based NMT system (Luong et al., 2015; Zoph and Knight, 2016). 2 LDC2000T50, LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07. 3 https://github.com/isi-nlp/ZophRNN. We extend this toolkit with global attention, and change the attention model to the way shown in Eq. 6. 4 https://github.com/StatNLP/otedama. 895 3) +Pre-Ordering: It is the NMT system which only uses the pre-ordering approach. 4) +Position: It is the NMT system which only employs the position embedding. 5) +"
L18-1143,D15-1166,0,0.090334,"Missing"
L18-1143,D16-1096,0,0.0998901,"Missing"
L18-1143,P02-1040,0,0.100887,"Missing"
L18-1143,P16-2049,0,0.039992,"Missing"
L18-1143,P16-5005,0,0.037963,"Missing"
L18-1143,Y09-2016,1,0.757731,"= sof tmax(Wyi zei + bs ) (2) where Wy is an embedding matrix containing row vectors of the target words and zei is the attention output: zei = tanh(Wc [zim ; ci ]) (3) The attention model calculates ci as the weighted sum of the source-side context vectors: ci = Tx X ai,j hm i (4) j=1 Where aij can be computed by exp(ei,j ) ai,j = PT x k=1 exp(ei,k ) (5) ei,j = vaT tanh(Wa zi + Ua hj ) (6) and zik is computed using the following formula: k zjk = LST M (zj−1 , zjk−1 ) 3. (7) Exploiting Pre-Ordering for NMT In SMT, pre-ordering is a commonly used pre-processing technique (Collins et al., 2005; Zhang and Zong, 2009; Genzel, 2010; Hitschler et al., 2016), which makes the word order of a source sentence closer to that of a target sentence. This technology was originally proposed to alleviate the weakness of reordering in classical phrase-based SMT (Koehn et al., 2003). As SMT always penalizes the cases that move target phrases far away from their corresponding source positions. Fig. 1 shows an example of pre-ordering, in which when translating the original source sentence, the words in red and words in blue need to exchange their positions. With the pre-ordering, the word order in this source sentence is"
L18-1143,P14-1011,1,0.820879,"SMT and NMT on NMT framework. Specifically, (Arthur et al., 2016) incorporates word translation table in attention part to adjust the final loss. (Zhang and Zong, 2016) moves forward further by incorporating a bilingual dictionaries in NMT. (Stahlberg et al., 2016) and (He et al., 2016) rescore word candidates with SMT features. (G¨ulc¸ehre et al., 2015) improves the beam search with language model. (Zhou et al., 2017) proposes a neural combination model to fuse the NMT translation results and SMT translation results. (Wang et al., 2017) improves the NMT system with the SMT recommendations. (Zhang et al., 2014) proposes bilingually-constrained recursive auto-encoders to learn phrase embeddings, which can distinguish the phrases with different semantic meanings. (Tang et al., 2016) explores the possibility to incorporate phrase memory into NMT, in which the decoder can generate a sequence of multiple words all at once. In this work, we exploit another new technique in SMT, preordering, to NMT to improve the translation performance. 7. Conclusions and Future Work We have exploited the pre-ordering approach to alleviate the under-translation problem in NMT. Specifically, we preorder the source sentence"
L18-1143,P17-2060,1,0.765969,"SMT to NMT. The earlier related work is conducted on the SMT framework, which is deeply discussed in the reviewed paper (Zhang and Zong, 2015). Here, we only focus on the work which combines the SMT and NMT on NMT framework. Specifically, (Arthur et al., 2016) incorporates word translation table in attention part to adjust the final loss. (Zhang and Zong, 2016) moves forward further by incorporating a bilingual dictionaries in NMT. (Stahlberg et al., 2016) and (He et al., 2016) rescore word candidates with SMT features. (G¨ulc¸ehre et al., 2015) improves the beam search with language model. (Zhou et al., 2017) proposes a neural combination model to fuse the NMT translation results and SMT translation results. (Wang et al., 2017) improves the NMT system with the SMT recommendations. (Zhang et al., 2014) proposes bilingually-constrained recursive auto-encoders to learn phrase embeddings, which can distinguish the phrases with different semantic meanings. (Tang et al., 2016) explores the possibility to incorporate phrase memory into NMT, in which the decoder can generate a sequence of multiple words all at once. In this work, we exploit another new technique in SMT, preordering, to NMT to improve the"
L18-1143,N16-1004,0,0.0182704,"ers are set to the default values. More details can be found in (Hitschler et al., 2016). 4.4. (9) k=1 ei,j =vaT tanh(Wa zi + Ua hj + where Ci−1,j is the coverage vector of source word xj before time i, and Va is the weight matrix for coverage vector. Translation Methods In the experiments, we compare our approaches with other models, and we list all the translation methods as follows: 1) Moses: It is the state-of-the-art phrase-based SMT system (Koehn et al., 2007). Our system is built using the default settings. 2) Baseline: It is the baseline attention-based NMT system (Luong et al., 2015; Zoph and Knight, 2016). 2 LDC2000T50, LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07. 3 https://github.com/isi-nlp/ZophRNN. We extend this toolkit with global attention, and change the attention model to the way shown in Eq. 6. 4 https://github.com/StatNLP/otedama. 895 3) +Pre-Ordering: It is the NMT system which only uses the pre-ordering approach. 4) +Position: It is the NMT system which only employs the position embedding. 5) +Pre-Ordering+Position: It is the NMT system using both pre-ordering and position embedding together. 6) +Coverage: It is the NMT system with the coverag"
L18-1146,2013.mtsummit-papers.5,0,0.0226409,"ong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model for each testing sentence. This is accompl"
L18-1146,P16-1185,0,0.0199226,"the encoder-decoder architecture to do sequence to sequence mapping. At the same time, Sutskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translatio"
L18-1146,D14-1179,0,0.0966249,"Missing"
L18-1146,P14-1129,0,0.0360782,"bone, it can produce a correct translation for the testing sentence with a minor modification. Whereas the baseline skips the source word ”以色列” (Israel) and translates the source word ”秘书长” (secretary - general) twice. In the lower example, we can only find a not so similar sentence to the testing one, with a similarity score of 0.31. However, the sentence pair found in the example can remind the model how to translate the phrase “方括号”, whose translation is missing in the baseline system. 6. Related Work After a few pioneer work in exploring neural features in SMT systems (Zhang et al., 2014; Devlin et al., 2014), NMT quickly become the dominant approach for machine translation. Kalchbrenner and Blunsom (2013) and Cho et al. (2014) first propose to use the encoder-decoder architecture to do sequence to sequence mapping. At the same time, Sutskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabula"
L18-1146,P15-1166,0,0.068233,"Missing"
L18-1146,P16-1227,0,0.0141056,"-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need careful"
L18-1146,P15-1001,0,0.15154,"uction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is another challenge for nowadays deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepancy between training"
L18-1146,D13-1176,0,0.446148,"In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available. Keywords: Neural machine translation, online learning, sentence similarity 1. Introduction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is an"
L18-1146,2010.jec-1.4,0,0.0908634,"et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model fo"
L18-1146,2014.amta-researchers.19,0,0.0322218,"nd multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model for each testing sentence. This is accomplished by two-stage training. An gener"
L18-1146,D12-1037,0,0.144875,"nism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity le"
L18-1146,D15-1166,0,0.335593,"similarity 1. Introduction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is another challenge for nowadays deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepan"
L18-1146,P15-1002,0,0.105525,"similarity 1. Introduction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is another challenge for nowadays deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepan"
L18-1146,P11-1124,0,0.128975,"Missing"
L18-1146,1983.tc-1.13,0,0.672139,"Missing"
L18-1146,P02-1040,0,0.100924,"phrase in (s,t) i=1 The translation probability of each training sentence pair is calculated offline with the general network parameters. We don’t use the phrase pairs as training data to fine-tune the network parameters. There are two reasons. First, context information is not available for choosing the proper phrase translation. Second, training on phrase pairs will harm the recurrent weights of the network, because they are not complete sentences1 . 5. Experiments We evaluate our proposed method on the Chinese to English translation task. Translation quality is measured by the BLEU metric (Papineni et al., 2002). 5.1. Datasets We conduct experiments on two datasets. One is on the United Nations Parallel Corpus2 , which is composed of official records and other parliamentary documents of the United Nations. Since this data is from a narrow domain, it is relatively easy to find similar sentences for many testing sentences. The training data contains 1M sentence pairs extracted from the corpus, and the testing data contains 5 groups of sentence pairs, with 200 sentence pairs in each group. The most similar3 sentence we can find for the sentences in each group falls into the similarity range of 0-0.2, 0."
L18-1146,J82-2005,0,0.715304,"Missing"
L18-1146,P13-1002,1,0.90906,"and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model for each testing sentence. This is accomplished by two-stage"
L18-1146,D16-1137,0,0.0224659,"deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepancy between training and testing, the authors propose to moderately replace gold reference with generated output during training. Wiseman and Rush (2016) take a similar approach and regard training as beam search optimization. However, no matter how the network parameters are learnt, they are fixed after the training is finished in all current NMT practice. And the same model is applied to every testing sentence. A potential issue of this practice is that a neural network needs to be able to compress all translation knowledge into a fixed set of parameters, which is very hard in reality. So we propose to learn a specific model for each testing sentence by paying more attention to those related sentences. In particular, we propose a learning on"
L18-1146,P14-1011,1,0.792564,"e. Based on the backbone, it can produce a correct translation for the testing sentence with a minor modification. Whereas the baseline skips the source word ”以色列” (Israel) and translates the source word ”秘书长” (secretary - general) twice. In the lower example, we can only find a not so similar sentence to the testing one, with a similarity score of 0.31. However, the sentence pair found in the example can remind the model how to translate the phrase “方括号”, whose translation is missing in the baseline system. 6. Related Work After a few pioneer work in exploring neural features in SMT systems (Zhang et al., 2014; Devlin et al., 2014), NMT quickly become the dominant approach for machine translation. Kalchbrenner and Blunsom (2013) and Cho et al. (2014) first propose to use the encoder-decoder architecture to do sequence to sequence mapping. At the same time, Sutskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability"
L18-1146,N16-1004,0,0.0213779,"tskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013"
P13-1111,J04-4004,0,0.0118426,"English Gigaword corpus and target part of the training data. The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). We perform SRL on the source part of the training set, development set and test set by the Chinese SRL system used in (Zhuang and Zong, 2010b). To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). Therefore, at last, we can get 5 SRL result for each sentence. For the training set, we use these SRL results to do rule extraction respectively. We combine the obtained rules together to get a combined rule set. We discard the rules with fewer than 5 appearances. Using this set, we can train our MEPD model directly. As to translation, we match the 5 SRL results with transformation rules respectively, and then apply the resulting target-side-like PASs for decoding. As we mentioned in section 2.3, we use the state-of-the-art BTG system to translat"
P13-1111,J07-2003,0,0.19644,"Missing"
P13-1111,2007.tmi-papers.6,0,0.0208573,"), we also use PASTR to construct a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable impr"
P13-1111,D07-1007,0,0.0657799,"), we also use PASTR to construct a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable impr"
P13-1111,P07-1005,0,0.0498005,"a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable improvements on translation quality."
P13-1111,P03-2041,0,0.0185004,") 中国 和 俄罗斯 是 两个 大国 ， 应 … [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 ] [X2] [ ] X3 being two major countries , China and Russia should … (b) 奥运村 的 位置 对 运动员 是 最好的 [ A0 ]1 [Pred]2 [ A1 ]3 Introduction Predicate-argument structure (PAS) depicts the relationship between a predicate and its associated arguments, which indicates the skeleton structure of a sentence on semantic level. Basically, PAS agrees much better between two languages than syntax structure (Fung et al., 2006; Wu and Fung, 2009b). Considering that current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS is really a better representation of a sentence pair to model the bilingual structure mapping. However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. For example, in Figure 1, (a) and (b) carry the same source-side PAS <[A0]1 [Pred( 是 )]2 [A1]3> for Chinese predicate “ 是 ”. However, in Figure 1(a), the corresponding target-side-like PAS is <[X1] [X2] [X3]>, while in ] [ X1 [X2] [ X3 ] the location of the olympic village is the best for athletes (c) Figure 1. An example of"
P13-1111,2007.tmi-papers.10,0,0.0527801,"Missing"
P13-1111,W11-1012,0,0.0288647,"Missing"
P13-1111,P03-1021,0,0.012381,"to substitute PASTR for building a PAS-based translation system directly. We use “IC-PASTR” to denote this system. In addition, since our method of rule extraction is different from (Zhai et al., 2012), we also use PASTR to construct a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better"
P13-1111,J04-4002,0,0.205085,"Missing"
P13-1111,P03-1054,0,0.00365332,"t part of the training data. The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). We perform SRL on the source part of the training set, development set and test set by the Chinese SRL system used in (Zhuang and Zong, 2010b). To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). Therefore, at last, we can get 5 SRL result for each sentence. For the training set, we use these SRL results to do rule extraction respectively. We combine the obtained rules together to get a combined rule set. We discard the rules with fewer than 5 appearances. Using this set, we can train our MEPD model directly. As to translation, we match the 5 SRL results with transformation rules respectively, and then apply the resulting target-side-like PASs for decoding. As we mentioned in section 2.3, we use the state-of-the-art BTG system to translate the non-PAS spans. source-side PAS counts nu"
P13-1111,N03-1017,0,0.0162886,"Missing"
P13-1111,W04-3250,0,0.0739352,"s. The LDC category number : LDC2000T50, LDC2002E18, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34. 1131 whose lengths are among 10 and 30 words. Finally, the development set includes 595 sentences from NIST MT03 and the test set contains 1,786 sentences from NIST MT04 and MT05. We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and target part of the training data. The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). We perform SRL on the source part of the training set, development set and test set by the Chinese SRL system used in (Zhuang and Zong, 2010b). To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). Therefore, at last, we can get 5 SRL result for each sentence. For the training set, we use these SRL results to do rule extraction respectively. We combine the obtained rules together"
P13-1111,P07-2045,0,0.0048156,"the source and target predicate are “是(shi)” and “is” respectively. The predicate feature is thus “PredF=是(shi)+is”. The target predicate is determined by: t -pred = arg max p (t j |s -pred ) j∈t_range ( PAS ) is 3 where s-pred is the source predicate and t-pred the corresponding target predicate. http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm l 1130 t_range(PAS) refers to the target range covering all the words that are reachable from the PAS via word alignment. tj refers to the jth word in t_range(PAS). The utilized lexical translation probabilities are from the toolkit in Moses (Koehn et al., 2007).  Syntax Features. These features include st(Ei), i.e., the highest syntax tag for each argument, and fst(PAS) which is the lowest father node of sp in the parse tree. For example, for the rule shown in Figure 4(b), syntax features are st([A0]1)=NP, st([A1]4)=CP, and fst(PAS)=IP respectively. Using these features, we can train the MEPD model. We set the Gaussian prior to 1.0 and perform 100 iterations of the L-BFGS algorithm for each MEPD model. At last, we build 160 and 215 different MEPD classifiers, respectively, for the PASTRs and IC-PASTRs. Note that since the training procedure of maxi"
P13-1111,2006.iwslt-evaluation.11,0,0.0706908,"Missing"
P13-1111,W08-0308,0,0.0296806,"Missing"
P13-1111,C10-1081,0,0.0731141,"Missing"
P13-1111,D08-1010,0,0.545389,"-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable improvements on translation quality. Our work differs from the above work in the following two aspects: 1) in our work, we focus on the problem of disambiguates on PAS; 2) we define two kinds of PAS ambiguities: role ambiguity and gap ambiguity. 3) towards the two different ambiguities, we design two specific metho"
P13-1111,P06-1077,0,0.0472421,"Missing"
P13-1111,W06-1606,0,0.0418659,"Missing"
P13-1111,P02-1040,0,0.0863687,"Missing"
P13-1111,P06-1055,0,0.0324144,"Missing"
P13-1111,2009.eamt-1.30,0,0.0390937,"rovements on translation quality. 1 防洪 是 首要 的 任务 [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 X3 ] ] [ X2 ] [ flood prevention is the primary mission (a) 中国 和 俄罗斯 是 两个 大国 ， 应 … [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 ] [X2] [ ] X3 being two major countries , China and Russia should … (b) 奥运村 的 位置 对 运动员 是 最好的 [ A0 ]1 [Pred]2 [ A1 ]3 Introduction Predicate-argument structure (PAS) depicts the relationship between a predicate and its associated arguments, which indicates the skeleton structure of a sentence on semantic level. Basically, PAS agrees much better between two languages than syntax structure (Fung et al., 2006; Wu and Fung, 2009b). Considering that current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS is really a better representation of a sentence pair to model the bilingual structure mapping. However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. For example, in Figure 1, (a) and (b) carry the same source-side PAS <[A0]1 [Pred( 是 )]2 [A1]3> for Chinese predicate “ 是 ”. However, in Figure 1(a), the corresponding target-side-like PAS is <[X1"
P13-1111,N09-2004,0,0.0491553,"rovements on translation quality. 1 防洪 是 首要 的 任务 [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 X3 ] ] [ X2 ] [ flood prevention is the primary mission (a) 中国 和 俄罗斯 是 两个 大国 ， 应 … [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 ] [X2] [ ] X3 being two major countries , China and Russia should … (b) 奥运村 的 位置 对 运动员 是 最好的 [ A0 ]1 [Pred]2 [ A1 ]3 Introduction Predicate-argument structure (PAS) depicts the relationship between a predicate and its associated arguments, which indicates the skeleton structure of a sentence on semantic level. Basically, PAS agrees much better between two languages than syntax structure (Fung et al., 2006; Wu and Fung, 2009b). Considering that current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS is really a better representation of a sentence pair to model the bilingual structure mapping. However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. For example, in Figure 1, (a) and (b) carry the same source-side PAS <[A0]1 [Pred( 是 )]2 [A1]3> for Chinese predicate “ 是 ”. However, in Figure 1(a), the corresponding target-side-like PAS is <[X1"
P13-1111,W11-1003,0,0.0238699,"Missing"
P13-1111,I11-1004,0,0.127938,"Missing"
P13-1111,P06-1066,0,0.0351635,"lgorithm to decode the entire sentence. In the algorithm, they organized the space of translation candidates into a hypergraph. For the span covered by PAS (PAS span), a multiplebranch hyperedge is employed to connect it to the PAS’s elements. For the span not covered by PAS (non-PAS span), the decoder considers all the possible binary segmentations of it and utilizes binary hyperedges to link them. 1128 During translation, the decoder fills the spans with translation candidates in a bottom-up manner. For the PAS span, the PAS-based translation framework is adopted. Otherwise, the BTG system (Xiong et al., 2006) is used. When the span covers the whole sentence, we get the final translation result. Obviously, PAS ambiguities are not considered in this framework at all. The targetside-like PAS is selected only according to the language model and translation probabilities, without considering any context information of PAS. Consequently, it would be difficult for the decoder to distinguish the source-side PAS from different context. This harms the translation quality. Thus to overcome this problem, we design two novel methods to cope with the PAS ambiguities: inside-context integration and a maximum ent"
P13-1111,P12-1095,0,0.08467,"first converted into target-side-like PASs by PAS transformation rules, and then perform translation based on the obtained target-side-like PASs. 2.1  Pred means the predicate where the rule is extracted.  SP denotes the list of source elements in source language order.  TP refers to the target-side-like PAS, i.e., a list of general non-terminals in target language order. Sentence Decoding with the PAS-based translation framework Sometimes, the source sentence cannot be fully covered by the PAS, especially when there are several predicates. Thus to translate the whole sentence, Zhai et al. (2012) further designed an algorithm to decode the entire sentence. In the algorithm, they organized the space of translation candidates into a hypergraph. For the span covered by PAS (PAS span), a multiplebranch hyperedge is employed to connect it to the PAS’s elements. For the span not covered by PAS (non-PAS span), the decoder considers all the possible binary segmentations of it and utilizes binary hyperedges to link them. 1128 During translation, the decoder fills the spans with translation candidates in a bottom-up manner. For the PAS span, the PAS-based translation framework is adopted. Other"
P13-1111,J08-2004,0,0.0576515,"Missing"
P13-1111,C12-1185,1,0.523176,"Missing"
P13-1111,C10-1153,1,0.937944,"the PAS in Figure 3; (b) The extracted IC-PASTR from (a). Using the IC-PASs, we look for the aligned target span for each element of the IC-PAS. We demand that every element and its corresponding target span must be consistent with word alignment. Otherwise, we discard the IC-PAS. Afterwards, we can easily extract a rule for PAS transformation, which we call IC-PASTR. As an example, Figure 4(b) is the extracted IC-PASTR from Figure 4(a). Note that we only apply the source-side PAS and word alignment for IC-PASTR extraction. By contrast, Zhai et al. (2012) utilized the result of bilingual SRL (Zhuang and Zong, 2010b). Generally, bilingual SRL could give a better alignment between bilingual elements. However, bilingual SRL usually achieves a really low recall on PASs, about 226,968 entries in our training set while it is 882,702 by using monolingual SRL system. Thus to get a high recall for PASs, we only utilize word alignment instead of capturing the relation between bilingual elements. In addition, to guarantee the accuracy of ICPASTRs, we only retain rules with more than 5 occurrences. 4 Maximum Entropy PAS Disambiguation (MEPD) Model In order to handle the role ambiguities, in this section, we concen"
P13-1111,D10-1030,1,0.906277,"the PAS in Figure 3; (b) The extracted IC-PASTR from (a). Using the IC-PASs, we look for the aligned target span for each element of the IC-PAS. We demand that every element and its corresponding target span must be consistent with word alignment. Otherwise, we discard the IC-PAS. Afterwards, we can easily extract a rule for PAS transformation, which we call IC-PASTR. As an example, Figure 4(b) is the extracted IC-PASTR from Figure 4(a). Note that we only apply the source-side PAS and word alignment for IC-PASTR extraction. By contrast, Zhai et al. (2012) utilized the result of bilingual SRL (Zhuang and Zong, 2010b). Generally, bilingual SRL could give a better alignment between bilingual elements. However, bilingual SRL usually achieves a really low recall on PASs, about 226,968 entries in our training set while it is 882,702 by using monolingual SRL system. Thus to get a high recall for PASs, we only utilize word alignment instead of capturing the relation between bilingual elements. In addition, to guarantee the accuracy of ICPASTRs, we only retain rules with more than 5 occurrences. 4 Maximum Entropy PAS Disambiguation (MEPD) Model In order to handle the role ambiguities, in this section, we concen"
P13-1111,D10-1043,0,\N,Missing
P13-1111,C08-1041,0,\N,Missing
P13-1111,P10-2002,0,\N,Missing
P13-1111,W11-2136,0,\N,Missing
P13-1140,W09-0432,0,0.208778,"2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Bertoldi and Federico, 2009). For the target-side monolingual data, they just use it to train language model, and for the source-side monolingual data, they employ a baseline (word-based SMT or phrasebased SMT trained with small-scale bitext) to first translate the source sentences, combining the source sentence and its target translation as a bilingual sentence pair, and then train a new phrase-base SMT with these pseudo sentence pairs. This method cannot learn idiom translations and unknown word translations. The third is to estimate the translation parameters and reordering parameters using monolingual data given the"
P13-1140,2010.iwslt-papers.3,0,0.0609028,"ce sentence with 的 商业 信息, then we compare the position relationship between s and 的 商业 信息. We increment the swap count if s is just before 的 商业 信息. After counting, we finally use maximum likelihood estimation method to compute the reordering probabilities. 6 Related Work As far as we know, few researchers study phrase pair induction from only monolingual data. There are three research works that are most related with ours. One is using an in-domain probabilistic bilingual lexicon to extract subsentential parallel fragments from comparable corpora (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Berto"
P13-1140,P10-1146,0,0.0190282,"gual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and"
P13-1140,P11-2071,0,0.263272,"Missing"
P13-1140,D12-1025,0,0.194896,"e usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012) viewed the transla"
P13-1140,J93-1003,0,0.0707835,"8 million sentence pairs 1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the numb"
P13-1140,P98-1069,0,0.0350499,"estimation since the phrase pairs are not extracted from parallel sentences. 1429 In this paper, we borrow and extend the idea of (Klementiev et al., 2012) to calculate the phraselevel translation probability with context information in source and target monolingual corpus. The value is calculated using a vector space model. With source and target vocabularies  s1 , s2 , , sN  and t1 , t2 , , tM  , the source-side phrase s and target-side phrase t can be respectively represented in an N- and M-dimensional vector. The k-th component of s’s contextual vector is computed using the method of (Fung and Yee, 1998) as follows: wk  ns , k   log  nmax / nk   1 (3) where ns , k and nk denotes the number of times sk occurs in the context of s and in the entire source language monolingual data, and nmax is the maximum number of occurrence of any source-side word in the source language monolingual data. The k-th element of t’s vector can be computed with the same method. We finally normalize these vectors with L2-norm. With the s’s and t’s contextual vector representations, we calculate two similarities: 1) project s’s vector into target side t with the lexical mapping p(t|s), and then get the similari"
P13-1140,P06-1121,0,0.0425915,"Missing"
P13-1140,P08-1088,0,0.364694,"these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiri"
P13-1140,W06-3601,0,0.135366,"ata on News. Here, we utilize about 2.08 million sentence pairs 1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-"
P13-1140,E12-1014,0,0.460447,"SMT there are four translation probabilities and the reordering probability for each phrase pair. The translation probabilities in the traditional phrase-based SMT include bidirectional phrase translation probabilities and bidirectional lexical weights. For the lexical weights, we can use the plex  s |t , a  and plex t |s, a  computed in the above section without length normalization. However, for the phrase-level probability, we cannot use maximum likelihood estimation since the phrase pairs are not extracted from parallel sentences. 1429 In this paper, we borrow and extend the idea of (Klementiev et al., 2012) to calculate the phraselevel translation probability with context information in source and target monolingual corpus. The value is calculated using a vector space model. With source and target vocabularies  s1 , s2 , , sN  and t1 , t2 , , tM  , the source-side phrase s and target-side phrase t can be respectively represented in an N- and M-dimensional vector. The k-th component of s’s contextual vector is computed using the method of (Fung and Yee, 1998) as follows: wk  ns , k   log  nmax / nk   1 (3) where ns , k and nk denotes the number of times sk occurs in the context of s an"
P13-1140,W04-3250,0,0.0284093,"ortion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. The in-domain reordering table is created for the induced phrase pairs. An in-domain 5gram English language model is trained with the target 1 million monolingual data. We use BLEU (Papineni et al., 2002) score with shortest length penalty as the evaluation metric and apply the pairwise re-sampling approach (Koehn, 2004) to perform the significance test. 7.2 Experimental Results In this section, we first conduct experiments to figure out how the translation performance degrades when the domain changes. To better illustrate the comparison, we first use News data to evaluate the NIST evaluation tests and then use the same News data to evaluate the electronic test sets. For the NIST evaluation, we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives the results. It is obvious that, it is relatively high when using the News training data to evaluate the same News test"
P13-1140,P07-2045,0,0.00688328,"slation rule induction into two steps: bilingual lexicon induction and phrase pair induction. Since many researchers have studied the bilingual lexicon induction, in this paper, we mainly concentrate ourselves on phrase pair induction given a probabilistic bilingual lexicon and two in-domain large monolingual data (source and target language). In addition, we will further introduce how to refine the induced phrase pairs and estimate the parameters of the induced phrase pairs, such as four standard translation features and phrase reordering feature used in the conventional phrase-based models (Koehn et al., 2007). The induced phrase-based model will be used to help domain adaptation for machine translation. In the rest of this paper, we first explain with examples to show what new translation knowledge can be learned with our proposed phrase pair induction method (Section 2), and then we introduce the approach for probabilistic bilingual lexicon acquisition in Section 3. In Section 4 and 5, we respectively present our method for phrase pair induction and introduce an approach for phrase pair refinement and parameter estimation. Section 6 will show the detailed experiments for the task of domain adapta"
P13-1140,W02-0902,0,0.237777,"rmance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-par"
P13-1140,P06-1077,0,0.0769455,"Missing"
P13-1140,W04-3243,0,0.109029,"News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the number of average translations i"
P13-1140,P06-1011,0,0.383114,"ata on News. Here, we utilize about 2.08 million sentence pairs 1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-"
P13-1140,P12-1017,0,0.0510177,"e to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2"
P13-1140,J03-1002,0,0.00749702,"Missing"
P13-1140,P02-1040,0,0.0958777,"olkit (Stolcke, 2002) to train the 5-gram English language model with the target part of the parallel sentences and the Xinhua portion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. The in-domain reordering table is created for the induced phrase pairs. An in-domain 5gram English language model is trained with the target 1 million monolingual data. We use BLEU (Papineni et al., 2002) score with shortest length penalty as the evaluation metric and apply the pairwise re-sampling approach (Koehn, 2004) to perform the significance test. 7.2 Experimental Results In this section, we first conduct experiments to figure out how the translation performance degrades when the domain changes. To better illustrate the comparison, we first use News data to evaluate the NIST evaluation tests and then use the same News data to evaluate the electronic test sets. For the NIST evaluation, we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives t"
P13-1140,P95-1050,0,0.708182,"translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs betw"
P13-1140,P99-1067,0,0.199578,"Missing"
P13-1140,P11-1002,0,0.0896217,"rpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012"
P13-1140,2008.iwslt-papers.6,0,0.168941,"k et al., 2007; Cettolo et al., 2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Bertoldi and Federico, 2009). For the target-side monolingual data, they just use it to train language model, and for the source-side monolingual data, they employ a baseline (word-based SMT or phrasebased SMT trained with small-scale bitext) to first translate the source sentences, combining the source sentence and its target translation as a bilingual sentence pair, and then train a new phrase-base SMT with these pseudo sentence pairs. This method cannot learn idiom translations and unknown word translations. The third is to estimate the translation parameters and reorderi"
P13-1140,C08-1125,1,0.219596,"we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives the results. It is obvious that, it is relatively high when using the News training data to evaluate the same News test set. However, when the test domain is changed, the translation performance decreases to a large extent. Given the in-domain bilingual lexicon and two monolingual data, previous works also proposed some good methods to explore the potential of the given data to improve the translation quality. Here, we implement their approaches and use them as our strong baseline. Wu et al. (2008) regards the in-domain lexicon with corpus translation probability as another phrase table and further use the in-domain language model besides the out-of-domain language model. Table 4 gives the results. We can see from the table that the domain lexicon is much helpful and significantly outperforms the baseline with more than 4.0 BLEU points. When it is enhanced with the in-domain language model, it can further improve the translation performance by more than 2.5 BLEU points. This method has made good use of in-domain lexicon and the target-side indomain monolingual data, but it does not take"
P13-1140,2011.mtsummit-papers.29,1,0.831128,"y-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods"
P13-1140,C12-1186,1,0.886692,"Missing"
P13-1140,P08-1064,0,0.0114137,"el from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in b"
P13-1140,C98-1066,0,\N,Missing
P13-1140,P07-1004,0,\N,Missing
P13-1140,D11-1019,1,\N,Missing
P13-1140,J00-2004,0,\N,Missing
P13-1140,J07-2003,0,\N,Missing
P13-1140,2007.mtsummit-papers.50,0,\N,Missing
P13-1140,P04-1066,0,\N,Missing
P14-1011,D13-1106,0,0.00699973,"ks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic mea"
P14-1011,W04-3250,0,0.0534067,"sh translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). In addition, we pre-train the word embedding with toolkit Word2Vec on large-scale monolingual data including the aforementioned data for SMT. The monolingual data contains 1.06B words for Chinese and 1.12B words for English. To obtain high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on trans"
P14-1011,D13-1054,0,0.266933,"ccessfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the"
P14-1011,D12-1088,0,0.0249865,"Missing"
P14-1011,P13-1078,0,0.0249237,"ts translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase emb"
P14-1011,P13-2119,0,0.0122932,"resentation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase e"
P14-1011,2007.mtsummit-papers.22,0,0.123108,"Missing"
P14-1011,D11-1014,0,0.55051,"sing word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or"
P14-1011,D07-1103,0,0.030171,"Missing"
P14-1011,P13-1045,0,0.574395,"syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully"
P14-1011,D13-1176,0,0.788727,"ecoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase. Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that"
P14-1011,2009.mtsummit-papers.17,0,0.0492238,"Missing"
P14-1011,D13-1140,0,0.00759691,"(multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, n"
P14-1011,J97-3002,0,0.185251,"Missing"
P14-1011,P06-1066,0,0.0963857,"Missing"
P14-1011,P13-1017,1,0.245382,"ties) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilingu"
P14-1011,D12-1089,0,0.0382796,"Missing"
P14-1011,D13-1141,0,0.206555,"didates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core i"
P14-1011,D13-1170,0,\N,Missing
P14-2126,J07-2003,0,0.0480851,"ction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is m"
P14-2126,D11-1014,0,0.509728,"s a better translation. However, (b) wrongly translates phrase “† â9” to “and Sharon” and combines it with [ÙŸ;Bush] incorrectly, leading to a bad translation. To explore the derivation structure’s potential on yielding good translations, in this paper, we propose a novel derivation structure prediction (DSP) model for SMT decoding. • We propose a novel RNN-based model to do derivation structure prediction for SMT decoding. To our best knowledge, this is the first work on this issue in SMT community; • In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al., 2011a; Socher et al., 2013). We go a step further, and design a bilingual RNN to represent the derivation structure; • To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model. 779 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779–784, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 The DSP Model The basic idea of DSP model is to represent the deriva"
P14-2126,P06-1121,0,0.0266993,"ms to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translatio"
P14-2126,P13-1045,0,0.563181,". However, (b) wrongly translates phrase “† â9” to “and Sharon” and combines it with [ÙŸ;Bush] incorrectly, leading to a bad translation. To explore the derivation structure’s potential on yielding good translations, in this paper, we propose a novel derivation structure prediction (DSP) model for SMT decoding. • We propose a novel RNN-based model to do derivation structure prediction for SMT decoding. To our best knowledge, this is the first work on this issue in SMT community; • In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al., 2011a; Socher et al., 2013). We go a step further, and design a bilingual RNN to represent the derivation structure; • To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model. 779 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779–784, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 The DSP Model The basic idea of DSP model is to represent the derivation structure by RNN ("
P14-2126,J97-3002,0,0.772466,"representations for phrase pairs; (2) derivation structure prediction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chine"
P14-2126,W06-3601,0,0.0284407,"Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “† â9” to “and S"
P14-2126,P06-1066,0,0.265425,"tions for phrase pairs; (2) derivation structure prediction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9"
P14-2126,D13-1054,0,0.016505,"the DSP score for sentence ui ’s derivation d. It is computed by summing LNN score (Equation (2)) and DSN score (Equation (5)): s(θ, u, d) = LN Nθ (d) + DSNθ (d) p (7) ˆ G(ui )) is the structure loss margin, which ∆(d, penalizes derivation dˆ more if it deviates more from gold derivations. It is formulated as:  ˆ G(ui ) ∆ d, X ˆ ref ) (8) = αs δ{π 6∈ G(ui )} + αt Dist(y(d), where d denotes the derivation structure and p is the non-leaf node in d. Obviously, by this score, we can easily assess different derivations. Good derivations will get higher scores while bad ones will get lower scores. Li et al. (2013) presented a network to predict how to merge translation candidates, in monotone or inverted order. Our DSN differs from Li’s work in two points. For one thing, DSN can not only predict how to merge candidates, but also evaluate whether two candidates should be merged. For another, DSN focuses on the entire derivation structure, rather than only the two candidates for merging. Therefore, the translation decoder will pursue good derivation structures via DSN. Actually, Li’s work can be easily integrated into our work. We leave it as our future work. 3 Max-Margin Framework π∈dˆ The margin includ"
P14-2126,D13-1112,0,0.0317023,"Missing"
P14-2126,P06-1077,0,0.0375188,"res from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phr"
P14-2126,W06-1606,0,0.0399372,"d derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wron"
P14-2126,Y09-2016,1,0.832998,"language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: ∂J 1 X ∂s(θ, ui , dˆm ) ∂s(θ, ui , dm ) = − +λθ ∂θ N ∂θ ∂θ 3.3 Experiments (1) Using the BTG system to perform force decoding on FBIS part of the bilingual training data5 , and collect the sentences succeeded in force decoding (86,902 sentences in total)6 . We then collect the corresponding force de"
P14-2126,P02-1040,0,0.0905293,"del, we perform experiments on Chinese-to-English translation. The training data contains about 2.1M sentence pairs with about 27.7M Chinese words and 31.9M English words2 . We train a 5-gram language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: ∂J 1 X ∂s(θ, ui , dˆm ) ∂s(θ, ui , dm ) = − +λθ ∂θ N ∂θ ∂θ 3.3 Experiments (1) Using the BTG system to perfor"
P14-2126,P08-1064,0,0.0219994,"ts show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “† â9” to “and Sharon” and combines"
P14-2126,D11-1019,1,0.886265,"Missing"
P14-2126,Q13-1020,1,\N,Missing
P17-2060,C08-1005,0,0.0702694,"Missing"
P17-2060,D10-1092,0,0.00709113,"h are often able to translate rare words in training data. As shown in Table 2, the number of unknown words of our proposed model is 137 fewer than original NMT model. Table 4 shows an example of system combination. The Chinese word zˇuzh¯ıwˇang is an out-ofvocabulary(OOV) for NMT and the baseline NMT cannot correctly translate this word. Although PBMT and HPMT translate this word well, they does not conform to the grammar. By combining the merits of NMT and SMT, our model gets the correct translation. Word Order of Translation We evaluate word order by the automatic evaluation metrics RIBES (Isozaki et al., 2010), whose score is a metric based on rank correlation coefficients with word precision. RIBES is known to have stronger correlation with human judgements than BLEU for English as discussed in Isozaki et al. (2010). 4.6 Effect of Ensemble Decoding The performance of candidate systems is very important to the result of system combination, and we use ensemble strategy with four NMT models to improve the performance of original NMT system. As shown in Table 3, the E-NMT with 2 We use four neural combination models in ensemble model. 381 Source Pinyin Reference PBMT HPMT NMT Jane Multi °é • † ™Ý |„ ï"
P17-2060,W16-2316,0,0.157174,"ion framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods. 1 Introduction Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a). Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words. Additionally, it suffers from over-translation and under-translation to some extent (Tu et al., 2016). Compared to NMT, SMT, such as phrase-based machine translation (PBMT, (Koehn et al., 2003)) and hierarchical phrase-based machine translation (HPMT, ∗ • We propose a neural system combination method, which is adapted from multi-source Corresponding author. 378 Proc"
P17-2060,P09-1106,0,0.0248177,"eijing, China ‡ CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China {long.zhou,wenpeng.hu,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural s"
P17-2060,D13-1176,0,0.0485095,"antages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods. 1 Introduction Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a). Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words. Additionally, it suffers from over-translation and under-translation to some extent (Tu et al., 2016). Compared to NMT, SMT, such as phrase-based machine translation (PBMT, (Koehn et al., 2003)) and hierarchical phrase-based machine translation (HPMT, ∗ • We propose a neural system combinat"
P17-2060,P16-1185,0,0.0565554,"ovement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et"
P17-2060,P05-1033,0,0.119106,"ents on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods. 1 Introduction Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a). Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words. Additionally, it suffers from over-translation and under-translation to some extent (Tu et al., 2016). Compared to NMT, SMT, such as phrase-based machine translation (PBMT, (Koehn et al., 2003)) and hierarchical phrase-based machine translation (HPMT, ∗ • We propose a neural system combination method, which is adapted from multi-source Corresponding author. 378 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 378–384 c Vancouver, Canada, Ju"
P17-2060,N04-1022,0,0.0888887,"aboratory of Pattern Recognition, CASIA, Beijing, China ‡ CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China {long.zhou,wenpeng.hu,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation result"
P17-2060,2009.iwslt-evaluation.13,1,0.80314,", 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method cannot only address the fluency of NMT an"
P17-2060,D09-1115,0,0.0263568,"cognition, CASIA, Beijing, China ‡ CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China {long.zhou,wenpeng.hu,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, w"
P17-2060,N16-1101,0,0.0204172,"ns and terrorist group . hussein also and terrorist group established relations . hussein also established relations with UNK . hussein also has established relations with . hussein also has established relations with the terrorist group . Table 4: Translation examples of single system, Jane and our proposed model. based SMT to pre-translate the inputs into target translations. Then a NMT system generates the final hypothesis using the pre-translation. Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016). Unlike previous works, we adapt multi-source NMT for system combination and design a good strategy to simulate the real training data for our neural system combination. ensemble strategy outperforms the original NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is"
P17-2060,D15-1166,0,0.0931622,"Missing"
P17-2060,D16-1026,0,0.0325604,"SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method cannot only address the fluency of NMT and the adequacy of SMT, but also can accommodate the source sentences as input. Furthermore, our approach can further use ensemble decoding to boost the performance compared to traditional system combination methods. Experiments on Chinese-English datasets show that our approaches obtain signific"
P17-2060,P15-1002,0,0.0491635,"l NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past s"
P17-2060,E14-2008,0,0.477432,"005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model (Zoph and Knight, 2016). Different encoders are employed to model the semantics of the source langua"
P17-2060,D15-1122,0,0.0181203,"gual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method cannot only address the fluency of NMT and the adequacy of SMT, but also can accommodate the source sentences a"
P17-2060,C16-1133,0,0.0349222,". hussein also and terrorist group established relations . hussein also established relations with UNK . hussein also has established relations with . hussein also has established relations with the terrorist group . Table 4: Translation examples of single system, Jane and our proposed model. based SMT to pre-translate the inputs into target translations. Then a NMT system generates the final hypothesis using the pre-translation. Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016). Unlike previous works, we adapt multi-source NMT for system combination and design a good strategy to simulate the real training data for our neural system combination. ensemble strategy outperforms the original NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for sys"
P17-2060,D07-1105,0,0.0863732,"Missing"
P17-2060,C16-1205,0,0.0257155,"est sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level"
P17-2060,D16-1096,0,0.0780792,"BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word"
P17-2060,N16-1004,0,0.0729918,"r decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model (Zoph and Knight, 2016). Different encoders are employed to model the semantics of the source language input and each best translation produced by different NMT and SMT systems. The encoders produce multiple context vector representations, from which the decoder generates the final output word by word. Since the same training data is used for NMT, SMT and neural system combination, we further design a smart strategy to simulate the real training data for neural system combination. Specifically, we make the following contributions in this paper: Neural machine translation (NMT) becomes a new approach to machine trans"
P17-2060,D16-1249,0,0.0332343,"BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word"
P17-2060,C16-1172,0,0.121712,"Missing"
P17-2060,2001.mtsummit-papers.46,0,0.2154,"rks . hussein also has established relations and terrorist group . hussein also and terrorist group established relations . hussein also established relations with UNK . hussein also has established relations with . hussein also has established relations with the terrorist group . Table 4: Translation examples of single system, Jane and our proposed model. based SMT to pre-translate the inputs into target translations. Then a NMT system generates the final hypothesis using the pre-translation. Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016). Unlike previous works, we adapt multi-source NMT for system combination and design a good strategy to simulate the real training data for our neural system combination. ensemble strategy outperforms the original NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments furt"
P17-2060,P02-1040,0,0.12722,"other half into target translations. The MT translations and the gold target reference can be available. (3) where sj−1 is previous hidden state, s˜j−1 is an intermediate state. And cj is the context vector of system combination obtained by attention mechanism, which is computed as weighted sum of the context vectors of three MT systems, just as illustrated in the middle part of Figure 1. cj = K X βjk cjk 4 We perform our experiments on the ChineseEnglish translation task. The MT systems participating in system combination are PBMT, HPMT and NMT. The evaluation metric is caseinsensitive BLEU (Papineni et al., 2002). (4) k=1 where K is the number of MT systems, and βjk is a normalized item calculated as follows: βjk exp(˜ sj−1 · cjk ) =P sj−1 · cjk0 ) k0 exp(˜ 4.1 m X k αji hi Data preparation Our training data consists of 2.08M sentence pairs extracted from LDC corpus. We use NIST 2003 Chinese-English dataset as the validation set, NIST 2004-2006 datasets as test sets. We list all the translation methods as follows: (5) Here, we calculate kth MT system context cjk as a weighted sum of the source annotations: cjk = Experiments • PBMT: It is the start-of-the-art phrase-based SMT system. We use its default"
P17-2060,P07-1040,0,0.0412155,"zong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model (Zoph and Knight, 2016). Different encoders are employed t"
P17-2060,W08-0329,0,0.0259165,"2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method canno"
P17-2060,P16-1009,0,0.0351505,"ms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and"
P17-2060,P16-1162,0,0.052455,"ms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and"
P17-2060,P16-1159,0,0.0515303,"ffective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neu"
P17-2060,P16-1008,0,0.0223345,"has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level"
P17-2060,D16-1160,1,0.248013,"g ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et"
P17-2060,N03-1017,0,\N,Missing
P19-1117,N18-1008,0,0.0128204,"on does not support zero-shot translation because there is no explicit training set for this specific translation task. Therefore, we employ hybrid-attention mechanism in our zero-shot experiments. 3) Language-Sensitive Discriminator: In our method, the representor which shares encoder and decoder makes full use of language commonality, but it weakens the model ability to distinguish different languages. Hence we introduce a new language-sensitive discriminator to strengthen model representation. In NMT framework, the hidden states on the top layer can be viewed as a fine-grained abstraction (Anastasopoulos and Chiang, 2018). For this language-sensitive module, we first employ a neurep ral model fdis on the top layer of reprensentor htop , and the output of this model is a language judgment score Plang . rep hdis = fdis (htop ) Plang (d) = softmax(Wdis ∗ hdis d + bdis ) (8) where Plang (d) is language judgment score for sentence pair d, Wdis , bdis are parameters, which are denoted as θ dis . We test two different types of neural models for fdis , including convolutional network with max pooling layer and two-layer feedforward network. And then, we obtain an discriminant objective function as follows: Ldis (θ dis"
P19-1117,C18-1263,0,0.0486432,"→It translation task. 6 Related Work Our work is related to two lines of research, and we describe each of them as follows: Model Compactness and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmentin"
P19-1117,P17-1176,0,0.0268642,"2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) propose to make use of multilinguality in Multi-NMT to address the zero-shot problem. In this work, we propose a method for Multi-NMT to boost the accuracy of the multilingua"
P19-1117,P15-1166,0,0.637577,"ngual transr to en res Rep lation from M source languages to N target languages. ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decode"
P19-1117,N16-1101,0,0.210155,"M source languages to N target languages. ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usual"
P19-1117,D15-1166,0,0.0692789,"14; Cho et al., 2014) framework and self-attentionbased Transformer (Vaswani et al., 2017). Encoder-Decoder Framework Transformer Network Transformer is a stacked network with several layers containing two or three basic blocks in each layer. For a single layer in the encoder, it consists of a multi-head self-attention and a position-wise 1214 lang-1 feed-forward network. For the decoder model, besides the above two basic blocks, a multi-head cross-attention follows multi-head self-attention. In this block, the calculation method of similarity score et in Equation 3 is a little different from Luong et al. (2015) and Bahdanau et al. (2015): ei,t = √ 1 Wk hienc ∗ Wq htdec dm Residual&Norm Position Embdding Embeddings src lang-1 tgt lang-n + In Multi-NMT model, the encoder and decoder are two key components, which play analogous ... + A Compact Representor lang-2 Language-Sensitive Embedding L |D M Õ Õl |Õ l log P(ytl |x l, y<t ; θ rep, θ attn ) (7) This representor (θ rep ) coordinates the semantic presentation of multiple languages in a closely related universal level, which also increases the utilization of commonality for different languages. 3.2 3.1 lang-1 l=1 d=1 t=1 (6) In this section, we introd"
P19-1117,D18-1103,0,0.0244931,"any researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) propose to make use of mu"
P19-1117,N18-1032,0,0.0872909,"ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared"
P19-1117,D13-1176,0,0.0505833,"lation scenarios. 1 N Encoder M Src Decoder Lang-Sensi Attention N Tgt Lang-Sensi Embedding Ds n im rcr o at Figure 1: Our proposed compact representor, replacing N encoder and decoder, can perform multilingual transr to en res Rep lation from M source languages to N target languages. ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 201"
P19-1117,D16-1139,0,0.0144293,"on Nl-Ro and It-Nl with all sentence pairs in IWSLT-17 (about 200k), which is similar to other training pairs in our balanced corpus. As shown in part II, Multi-NMT Baselines underperform the NMT Baselines on all cases. However, our method performs better than NMT Baselines, and it achieves the improvement up to 1.76 BLEU points on Nl→It translation task. 6 Related Work Our work is related to two lines of research, and we describe each of them as follows: Model Compactness and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target language"
P19-1117,C18-1054,0,0.0650449,"Related Work Our work is related to two lines of research, and we describe each of them as follows: Model Compactness and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2"
P19-1117,W18-6309,0,0.053982,"ponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association f"
P19-1117,P02-1040,0,0.111001,"batch contains roughly 3,072 source and 3,072 target tokens, which belongs to one translation direction. We use Adam optimizer (Kingma and Ba, 2014) with β1 =0.9, β2 =0.98, and =10−9 . For evaluation, we use beam search with a beam size of k = 4 and length penalty α = 0.6. All our methods are trained and tested on a single Nvidia P40 GPU. 5 Results and Analysis In this section, we discuss the results of our experiments about our compact and language-sensitive method on Multi-NMT. The translation performance is evaluated by character-level BLEU5 for En→Zh translation and case-sensitive BLEU4 (Papineni et al., 2002) for other translation tasks. In our experiments, the models trained on individual language pair are denoted by NMT Baselines, and the baseline Multi-NMT models are denoted by Multi-NMT Baselines. 5.1 We implement our compact and languagesensitive method for Multi-NMT based on the tensor2tensor7 library. We use wordpiece method (Wu et al., 2016; Schuster and Nakajima, 2012) to 1 http://www.statmt.org/wmt14/translation-task.html WMT Language pair En-De En-Lv En-Fi En-Zh En-It En-Ro En-Nl Ro-It En-Vi 5.1.1 One-to-Many Translation Main Results The main results on the one-to-many translation scena"
P19-1117,D18-1039,0,0.176192,"d the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguis"
P19-1117,E17-2025,0,0.0333905,"normalization (Ba et al., 2016). Since the Transformer network contains no recurrence, positional embeddings are used in the model to make use of sequence order. More details regarding the architecture can be found in Vaswani et al. (2017). 2.3 lang-n lang-2 ... Language-Sensitive Modules The compact representor maximizes the sharing of parameters and makes full use of language commonality. However, it lacks the ability to discriminate different languages. In our method, we introduce three language-sensitive modules to enhance our model as follows: 1) Language-Sensitive Embedding: Previously, Press and Wolf (2017) conduct the weight tying of input and output embedding in NMT model. Generally, a shared vocabulary is built 1215 upon subword units like BPE (Sennrich et al., 2016b) and wordpiece (Wu et al., 2016; Schuster and Nakajima, 2012). However, it remains under-exploited which kind of embedding sharing is best for Multi-NMT. We divide the sharing manners into four categories including languagebased manner (LB, different languages have separate input embeddings), direction-based manner (DB, languages in source side and target side have different input embeddings), representorbased manner (RB, shared"
P19-1117,K16-1029,0,0.0437771,"Missing"
P19-1117,P16-1009,0,0.259203,"e details regarding the architecture can be found in Vaswani et al. (2017). 2.3 lang-n lang-2 ... Language-Sensitive Modules The compact representor maximizes the sharing of parameters and makes full use of language commonality. However, it lacks the ability to discriminate different languages. In our method, we introduce three language-sensitive modules to enhance our model as follows: 1) Language-Sensitive Embedding: Previously, Press and Wolf (2017) conduct the weight tying of input and output embedding in NMT model. Generally, a shared vocabulary is built 1215 upon subword units like BPE (Sennrich et al., 2016b) and wordpiece (Wu et al., 2016; Schuster and Nakajima, 2012). However, it remains under-exploited which kind of embedding sharing is best for Multi-NMT. We divide the sharing manners into four categories including languagebased manner (LB, different languages have separate input embeddings), direction-based manner (DB, languages in source side and target side have different input embeddings), representorbased manner (RB, shared input embeddings for all languages) and three-way weight tying manner (TWWT) proposed in Press and Wolf (2017), in which the output embedding of the target side is a"
P19-1117,P16-1162,0,0.491605,"e details regarding the architecture can be found in Vaswani et al. (2017). 2.3 lang-n lang-2 ... Language-Sensitive Modules The compact representor maximizes the sharing of parameters and makes full use of language commonality. However, it lacks the ability to discriminate different languages. In our method, we introduce three language-sensitive modules to enhance our model as follows: 1) Language-Sensitive Embedding: Previously, Press and Wolf (2017) conduct the weight tying of input and output embedding in NMT model. Generally, a shared vocabulary is built 1215 upon subword units like BPE (Sennrich et al., 2016b) and wordpiece (Wu et al., 2016; Schuster and Nakajima, 2012). However, it remains under-exploited which kind of embedding sharing is best for Multi-NMT. We divide the sharing manners into four categories including languagebased manner (LB, different languages have separate input embeddings), direction-based manner (DB, languages in source side and target side have different input embeddings), representorbased manner (RB, shared input embeddings for all languages) and three-way weight tying manner (TWWT) proposed in Press and Wolf (2017), in which the output embedding of the target side is a"
P19-1117,D18-1326,1,0.836692,"Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics structures but"
P19-1117,1983.tc-1.13,0,0.308962,"Missing"
P19-1117,D16-1160,1,0.881723,"conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) propose to make use of multilinguality in Multi-NMT to address the zero-shot problem. In thi"
P19-1117,J82-2005,0,0.627478,"Missing"
P19-1117,Q19-1006,1,0.813644,"and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utiliz"
P19-1117,N16-1004,0,0.0607207,", 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics structures but use different parameters, the commonality of languages cannot be fully exploited in this paradigm. A natural question arises that why not share the parameters between encoder and decoder on multilingual translation scenario? T"
P19-1117,D16-1163,0,0.0216677,"and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) pro"
P19-1361,D17-1236,0,0.0306029,"on methods (Tong and Koller, 2001; Culotta and McCallum, 2005) in active learning (Balcan et al., 2009; Dasgupta et al., 2005) can be adopted. When learning new concepts, the cumulative learning system should avoid retraining the whole system and catastrophic forgetting (French, 1999; Kirkpatrick et al., 2017). But the catastrophic forgetting does not happen if the dialogue system is trained with all possible user needs alternatively from scratch. The uncertainty estimation and online learn6 The term “incremental” refers to systems able to operate on a word by word basis in the previous work (Eshghi et al., 2017; Schlangen and Skantze, 2009). In our work, it refers to the system which can adapt to new dialogue scenarios after deployment. 3717 ing methods in our work are inspired by variational inference approach (Rezende et al., 2014; Kingma and Welling, 2014). In the existing work, this approach was used to generate diverse machine responses in both open domain dialogue systems (Zhao et al., 2017; Serban et al., 2016) and task-oriented dialogue systems (Wen et al., 2017). In contrast, our work makes use of the Bayesian nature of variational inference to estimate the uncertainty and learn from humans"
P19-1361,I17-1074,0,0.250225,"ed to define user needs in advance and avoid collecting biased training data laboriously. (2) To achieve this goal, we introduce IDS which is robust to new user actions and can extend itself online to accommodate new user needs. (3) We propose a new benchmark dataset to study the inconsistency of training and testing in task-oriented dialogue systems. 2 Background and Problem Definition Existing work on data-driven task-oriented dialogue systems includes generation based methods (Wen et al., 2016; Eric and Manning, 2017) and retrieval based methods (Bordes et al., 2016; Williams et al., 2017; Li et al., 2017). In this paper, we focus on the retrieval based methods, because they always return fluent responses. In a typical retrieval based system, a user gives an utterance xt to the system at the t-th turn. Let (xt,1 , ..., xt,N ) denote the tokens of xt . Then, the system chooses an answer yt = (yt,1 , ..., yt,M ) from the candidate response set R based on the conditional distribution p(yt |Ct ), where Ct = (x1 , y1 , ..., xt−1 , yt−1 , xt ) is the dialogue context consisting of all user utterances and responses up to the current turn. By convention, the dialogue system is designed to handle predef"
P19-1361,W15-4640,0,0.0286325,"of RSP SubD1 SubD2 SubD3 SubD4 SubD5 41 41 66 72 137 Table 1: The number of normalized response candidates in each sub-dataset after entity replacement, both training and test data included. 5.2 Baselines We compare IDS with several baselines: 3 We use special tokens to anonymize all private information in our corpus. 3714 • IR: the basic tf-idf match model used in (Bordes et al., 2016; Dodge et al., 2015). • Supervised Embedding Model (SEM): the supervised word embedding model used in (Bordes et al., 2016; Dodge et al., 2015). • Dual LSTM (DLSTM): the retrieval-based dialogue model used in (Lowe et al., 2015). • Memory Networks (MemN2N): the scoring model which is used in QA (Sukhbaatar et al., 2015) and dialogue systems (Bordes et al., 2016; Dodge et al., 2015). • IDS− : IDS without updating model parameters during testing. That is, IDS− is trained only with human intervention data on the training set and then we freeze parameters. 5.3 Implementation Details Our word embeddings are randomly initialized. The dimensions of word embeddings and GRU hidden units are both 32. The size of the latent variable z is 20. In uncertainty estimation, the repetition time K is 50. In all experiments, the average"
P19-1361,E09-1081,0,0.0418212,"Koller, 2001; Culotta and McCallum, 2005) in active learning (Balcan et al., 2009; Dasgupta et al., 2005) can be adopted. When learning new concepts, the cumulative learning system should avoid retraining the whole system and catastrophic forgetting (French, 1999; Kirkpatrick et al., 2017). But the catastrophic forgetting does not happen if the dialogue system is trained with all possible user needs alternatively from scratch. The uncertainty estimation and online learn6 The term “incremental” refers to systems able to operate on a word by word basis in the previous work (Eshghi et al., 2017; Schlangen and Skantze, 2009). In our work, it refers to the system which can adapt to new dialogue scenarios after deployment. 3717 ing methods in our work are inspired by variational inference approach (Rezende et al., 2014; Kingma and Welling, 2014). In the existing work, this approach was used to generate diverse machine responses in both open domain dialogue systems (Zhao et al., 2017; Serban et al., 2016) and task-oriented dialogue systems (Wen et al., 2017). In contrast, our work makes use of the Bayesian nature of variational inference to estimate the uncertainty and learn from humans. Specifically, we sample vari"
P19-1361,D18-1415,1,0.816486,"encountering unconsidered user needs such as ”how to update the operating system”, the system will give unreasonable responses. Introduction Data-driven task-oriented dialogue systems have been a focal point in both academic and industry research recently. Generally, the first step of building a dialogue system is to clarify what users are allowed to do. Then developers can collect data to train dialogue models to support the defined capabilities. Such systems work well if all possible combinations of user inputs and conditions are considered in the training stage (Paek and Pieraccini, 2008; Wang et al., 2018). However, as shown 1 https://github.com/Leechikara/ Incremental-Dialogue-System in Fig. 1, if users have unanticipated needs, the system will give unreasonable responses. This phenomenon is mainly caused by a biased understanding of real users. In fact, before system deployment, we do not know what the customers will request of the system. In general, this problem can be alleviated by more detailed user studies. But we can never guarantee that all user needs are considered in the system design. Besides, the user inputs are often diverse due to the complexity of natural language. Thus, it is i"
P19-1361,P17-1062,0,0.568476,"m, developers do not need to define user needs in advance and avoid collecting biased training data laboriously. (2) To achieve this goal, we introduce IDS which is robust to new user actions and can extend itself online to accommodate new user needs. (3) We propose a new benchmark dataset to study the inconsistency of training and testing in task-oriented dialogue systems. 2 Background and Problem Definition Existing work on data-driven task-oriented dialogue systems includes generation based methods (Wen et al., 2016; Eric and Manning, 2017) and retrieval based methods (Bordes et al., 2016; Williams et al., 2017; Li et al., 2017). In this paper, we focus on the retrieval based methods, because they always return fluent responses. In a typical retrieval based system, a user gives an utterance xt to the system at the t-th turn. Let (xt,1 , ..., xt,N ) denote the tokens of xt . Then, the system chooses an answer yt = (yt,1 , ..., yt,M ) from the candidate response set R based on the conditional distribution p(yt |Ct ), where Ct = (x1 , y1 , ..., xt−1 , yt−1 , xt ) is the dialogue context consisting of all user utterances and responses up to the current turn. By convention, the dialogue system is designe"
P19-1361,W16-3601,0,0.0314926,"cases are located in the confidence boundary. In addition, there are multiple clusters in each class. It is due to the fact the same system response can appear in different dialogue scenes. For example, “the system requesting user’s phone number” appears in scenes of both exchange and return goods. Although these contexts have the same response, their representations should be different if they belong to different dialogue scenes. 7 Related Work Task-oriented dialogue systems have attracted numerous research efforts. Data-driven methods, such as reinforcement learning (Williams et al., 2017; Zhao and Eskenazi, 2016; Li et al., 2017) and supervised learning (Wen et al., 2016; Eric and Manning, 2017; Bordes et al., 2016), have been applied to optimize dialogue systems automatically. These advances in task-oriented dialogue systems have resulted in impressive gains in performance. However, prior work has mainly focused on building task-oriented dialogue systems in a closed environment. Due to the biased assumptions of real users, such systems will break down when encountering unconsidered situations. Several approaches have been adopted to address this problem. Gaˇsic et al. (2014) explicitly defined kerne"
P19-1361,P17-1061,0,0.0206155,"with all possible user needs alternatively from scratch. The uncertainty estimation and online learn6 The term “incremental” refers to systems able to operate on a word by word basis in the previous work (Eshghi et al., 2017; Schlangen and Skantze, 2009). In our work, it refers to the system which can adapt to new dialogue scenarios after deployment. 3717 ing methods in our work are inspired by variational inference approach (Rezende et al., 2014; Kingma and Welling, 2014). In the existing work, this approach was used to generate diverse machine responses in both open domain dialogue systems (Zhao et al., 2017; Serban et al., 2016) and task-oriented dialogue systems (Wen et al., 2017). In contrast, our work makes use of the Bayesian nature of variational inference to estimate the uncertainty and learn from humans. Specifically, we sample variables from the prior network as the random perturbation to estimate the model uncertainty following the idea of QueryBy-Committee (Seung et al., 1992) and optimize model parameters by maximizing the ELBO. 8 Conclusion This paper presents a novel incremental learning framework to design dialogue systems, which we call IDS. In this paradigm, users are not expecte"
P19-1541,C18-1305,1,0.834495,"zon Alexa, and Microsoft Cortana. A typical pipeline of SLU includes domain classification, intent detection, and slot filling(Tur and De Mori, 2011), to parse user utterances into semantic frames. Example semantic frames (Chen et al., 2018) are shown in Figure 1 for a restaurant reservation. Traditionally, domain classification and intent detection are treated as classification tasks with popular classifiers such as support vector machine and deep neural network (Haffner et al., 2003; Sarikaya et al., 2011). They can also be combined into one task if there are not many intents of each domain(Bai et al., 2018). Slot filling task is usually treated as a sequence labeling task. Popular approaches for slot filling include conditional random fields (CRF) and recurrent neural network (RNN) (Raymond and Riccardi, 2007; Yao et al., 2014). Considering that pipeline approaches usually suffer from error propagation, the joint model for slot filling and intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (Xu and Sarikaya, 2013; Hakkani-T¨ur et al., 2016; Zhang and Wang, 2016; Goo et al., 2018), which is a direction we follow. To create a more effecti"
P19-1541,W17-5514,0,0.0892639,"intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (Xu and Sarikaya, 2013; Hakkani-T¨ur et al., 2016; Zhang and Wang, 2016; Goo et al., 2018), which is a direction we follow. To create a more effective SLU system, the contextual information has been shown useful (Bhargava et al., 2013; Xu and Sarikaya, 2014), as natural language utterances are often ambiguous. For example, the number 6 of utterance u2 in Figure 1 may refer to either B-time or B-people without considering the context. Popular contextual SLU models (Chen et al., 2016; Bapna et al., 2017) exploit the dialogue history with the memory network (Weston et al., 2014), which covers all three main stages of memory process: encoding (write), storage (save) and retrieval (read) (Baddeley, 1976). With such a memory mechanism, SLU model can retrieve context knowledge to reduce the ambiguity of the current utterance, contributing to a stronger SLU model. However, the memory consolidation, a well-recognized operation for maintaining and updating memory in cognitive psy5448 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5448–5453 c Florence, I"
P19-1541,C18-3006,0,0.0305638,"t O B-people O B-date s Which restaurant would you like to book a table for? u1 Cascal for 6 S2 B-rest O B-time B-people Figure 1: Example semantic frames of utterances u1 and u2 with domain (D), intent (I) and semantic slots in IOB format (S1 , S2 ). Introduction Spoken language understanding (SLU) is a key technique in today’s conversational systems such as Apple Siri, Amazon Alexa, and Microsoft Cortana. A typical pipeline of SLU includes domain classification, intent detection, and slot filling(Tur and De Mori, 2011), to parse user utterances into semantic frames. Example semantic frames (Chen et al., 2018) are shown in Figure 1 for a restaurant reservation. Traditionally, domain classification and intent detection are treated as classification tasks with popular classifiers such as support vector machine and deep neural network (Haffner et al., 2003; Sarikaya et al., 2011). They can also be combined into one task if there are not many intents of each domain(Bai et al., 2018). Slot filling task is usually treated as a sequence labeling task. Popular approaches for slot filling include conditional random fields (CRF) and recurrent neural network (RNN) (Raymond and Riccardi, 2007; Yao et al., 2014"
P19-1541,W17-5506,0,0.151755,"ogue logistic inference (DLI), defined as sorting a shuffled dialogue session into its original logical order. DLI can be trained with contextual SLU jointly if utterances are sorted one by one: selecting the right utterance from remaining candidates based on previously sorted context. In other words, given a response and its context, the DLI task requires our model to infer whether the response is the right one that matches the dialogue context, similar to the next sentence prediction task (Logeswaran and Lee, 2018). We conduct our experiments on the public multi-turn dialogue dataset KVRET (Eric and Manning, 2017), with two popular memory based contextual SLU models. According to our experimental results, noticeable improvements are observed, especially on slot filling. 2 Model Architecture This section first explains the memory mechanism for contextual SLU, including memory encoding and memory retrieval. Then we introduce the SLU tagger with context knowledge, the definition of DLI and how to optimize the SLU and DLI jointly. The overall model architecture is illustrated in Figure 2. memory embedding M = {m1 , m2 , ...mk } with a BiGRU (Chung et al., 2014) layer and then encode the current utterance x"
P19-1541,N18-2118,0,0.0268652,"nto one task if there are not many intents of each domain(Bai et al., 2018). Slot filling task is usually treated as a sequence labeling task. Popular approaches for slot filling include conditional random fields (CRF) and recurrent neural network (RNN) (Raymond and Riccardi, 2007; Yao et al., 2014). Considering that pipeline approaches usually suffer from error propagation, the joint model for slot filling and intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (Xu and Sarikaya, 2013; Hakkani-T¨ur et al., 2016; Zhang and Wang, 2016; Goo et al., 2018), which is a direction we follow. To create a more effective SLU system, the contextual information has been shown useful (Bhargava et al., 2013; Xu and Sarikaya, 2014), as natural language utterances are often ambiguous. For example, the number 6 of utterance u2 in Figure 1 may refer to either B-time or B-people without considering the context. Popular contextual SLU models (Chen et al., 2016; Bapna et al., 2017) exploit the dialogue history with the memory network (Weston et al., 2014), which covers all three main stages of memory process: encoding (write), storage (save) and retrieval (read"
P19-1541,E17-1042,0,0.0497988,"Missing"
Q13-1020,P09-1088,0,0.0892939,"power on tree construction than SCFG. In a STSG-based U-tree or a STSG rule, although not linguistically informed, the nodes labeled by POS tags are also effective on distinguishing different ones. However, with SCFG, we have to discard all the internal nodes (i.e., flattening the Utrees or rules) to express the same sequence, leading to a poor ability of distinguishing different U-trees and production rules. Thus, using STSG, we can build more specific U-trees for translation. In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar (Blunsom et al. 2009) 5. This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models. 5 4 p (ri |N i ) i 1 In (Blunsom et al., 2009), for Chinese-to-English translation, the Bayesian SCFG grammar only outperform the heuristic SCFG grammar by 0.1 BLEU points on NIST MT 2004 and 0.6 BLEU points on NIST MT 2005 in the NEWS domain. 4 Bayesian Model In this section, we present a Bayesian model to learn STSG"
Q13-1020,N10-1028,0,0.04127,"Missing"
Q13-1020,D08-1092,0,0.0168634,"model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on gene"
Q13-1020,N10-1015,0,0.0209123,"ively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervi"
Q13-1020,D12-1079,0,0.015431,"244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG tr"
Q13-1020,J07-2003,0,0.0843774,"ees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the"
Q13-1020,P96-1021,0,0.0379502,"Missing"
Q13-1020,J97-3002,0,0.614637,"Missing"
Q13-1020,D09-1037,0,0.155905,"s further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended"
Q13-1020,P07-1003,0,0.0294755,"variables into the string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word “ (wo-men) ”; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously 247 obtain large rules with enough context information b"
Q13-1020,D11-1018,0,0.020342,"Bayesian model efficiently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used"
Q13-1020,N10-1033,0,0.0421939,"Missing"
Q13-1020,P03-2041,0,0.0527913,"Missing"
Q13-1020,N04-1035,0,0.298489,"ow that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models 1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, ra"
Q13-1020,P06-1121,0,0.0523496,"set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse"
Q13-1020,W11-2160,0,0.0309381,"Missing"
Q13-1020,W06-3601,0,0.690714,"Missing"
Q13-1020,N03-1017,0,0.0509465,"Missing"
Q13-1020,W04-3250,0,0.0422056,"utilize the U-trees from random 1 for further analysis hereafter. 1.060E+07 Total Number of Frontier Nodes the training data. For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up t"
Q13-1020,P07-2045,0,0.00436685,"he string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word “ (wo-men) ”; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously 247 obtain large rules with enough context information b"
Q13-1020,D12-1021,0,0.0293399,"Missing"
Q13-1020,W09-0424,0,0.0370304,"Missing"
Q13-1020,D07-1078,0,0.0219737,"he string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word “ (wo-men) ”; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously 247 obtain large rules with enough context information b"
Q13-1020,J10-2004,0,0.014262,"y built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training tree-based translation models (Wang et al., 2010). Based on the above analysis, we can conclude that the tree structure that is independent from Tree-bank resources and simultaneously considers the bilingual mapping inside the bilingual sentence pairs would be a good choice for building treebased translation models. Therefore, complying with the above conditions, we propose an unsupervised tree structure for treebased translation models in this study. In the structures, tree nodes are labeled by combining the word classes of their boundary words rather than by syntactic labels, such as NP, VP. Furthermore, using these node labels, we design"
Q13-1020,C12-1186,1,0.400255,"on process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SC"
Q13-1020,N06-1033,0,0.0579748,"Missing"
Q13-1020,D12-1078,0,0.0123537,"ian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word clas"
Q13-1020,C08-1136,0,0.055176,"uild a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint p"
Q13-1020,P06-1077,0,0.103477,"s on the tree nodes. Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models 1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to"
Q13-1020,P11-1084,0,0.0251479,"Missing"
Q13-1020,P09-1020,0,0.0385606,"Missing"
Q13-1020,W06-1606,0,0.634183,"ranslation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models 1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual ma"
Q13-1020,P03-1021,0,0.0190538,"Missing"
Q13-1020,P02-1040,0,0.0866083,"Missing"
Q13-1020,P06-1055,0,0.0582151,"testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the transl"
Q13-1020,P05-1034,0,0.118254,"Missing"
Q13-1020,D11-1019,1,0.925641,"ently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used"
Q13-1020,2007.mtsummit-papers.71,0,0.0664862,"Missing"
Q13-1020,W06-3119,0,0.214328,"the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution gram"
Q13-1020,P11-1001,0,0.0524772,"hod to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution grammar (STSG) between source strings and target tree fragments. We take STSG as the generation grammar to match the translation m"
Q13-1020,P08-1064,0,\N,Missing
Q13-1020,P09-1063,0,\N,Missing
Q13-1020,W06-1628,0,\N,Missing
Q13-1020,P08-1066,0,\N,Missing
Q19-1006,W18-6408,0,0.0195983,"xplored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backward decoder. Additionally, both Niehues et al. (2016) and Zhou et al. (2017a) combined the strengths of NMT and SMT, which can also be used to combine the advantages of bidirectional translation texts (Zhang et al., 2018). Compared with previous methods, our method has the following advantages: (1) We use a single model to achieve the goal of synchronous left-to-right and right-to-left decoding. (2) Our model can lever"
Q19-1006,D09-1117,0,0.21497,"Missing"
Q19-1006,P18-1008,0,0.0309062,"an Translation We further demonstrate the effectiveness of our model in WMT14 English-German translation tasks, and we also display the performances of some competitive models including GNMT (Wu et al., 2016), Conv (Gehring et al., 2017), and AttIsAll (Vaswani et al., 2017). As shown in Table 4, our model also significantly outperforms others and gets an improvement of 1.49 more 4.7 Analysis We conduct analyses on Chinese-English translation to better understand our model from different perspectives. 11 The BLEU scores for Transformer model are our reproduced results. Similar to footnote 7 in Chen et al. (2018), our performance is slightly lower than those reported in Vaswani et al. (2017). Additionally, we only use 3 GPUs for English-German, whereas most papers employ 8 GPUs for model training. Parameters and Speeds In contrast to the standard Transformer, our model does not increase any parameters except for a hyper-parameter λ, as 99 Model Transformer Transformer (R2L) Rerank-NMT ABD-NMT Our Model Param 207.8M 207.8M 415.6M 333.8M 207.8M Speed Train Test 2.07 19.97 2.07 19.81 1.03 6.51 1.18 7.20 1.26 17.87 Table 6: Statistics of parameters, training, and testing speeds. Train denotes the number o"
Q19-1006,C16-1172,0,0.0398605,"Missing"
Q19-1006,W17-3204,0,0.0225535,"significantly improved the quality of machine translation in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Zhang and Zong, 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Recent approaches to sequence-to-sequence learning typically leverage recurrence (Sutskever et al., 2014), convolution (Gehring et al., 2017), or attention (Vaswani et al., 2017) as basic building blocks. Typically, NMT adopts the encoder-decoder architecture and generates the target translation from left to right. Despite their remarkable success, NMT models suffer from several weaknesses (Koehn and Knowles, 2017). One of the most prominent issues is the problem of unbalanced outputs in which the translation prefixes are better predicted than the suffixes (Liu et al., 2016). We analyze translation accuracy of the first and last 4 tokens for left-to-right (L2R) and right-toleft (R2L) directions, respectively. As shown in Table 1, the statistical results show that L2R performs better in the first 4 tokens, whereas R2L translates better in terms of the last 4 tokens. This problem is mainly caused by the left-toright unidirectional decoding, which conditions each output word on previously generated outputs"
Q19-1006,P02-1040,0,0.104028,"ataset as the validation set and NIST 2003-2006 (MT03-06) as our test sets. We use BPE (Sennrich et al., 2016b) to encode Chinese and English, respectively. We learn 30K merge operations and limit the source and target vocabularies to the most frequent 30K tokens. For English-German translation, the training set consists of about 4.5 million bilingual sentence pairs from WMT 2014.5 We use newstest2013 as the validation set and newstest2014 as the test set. Sentences are encoded using BPE, which has a shared vocabulary of about 37,000 tokens. To evaluate the models, we compute the BLEU metric (Papineni et al., 2002) on tokenized, truecase output.6 For Russian-English translation, we use the following resources from the WMT parallel data7 : ParaCrawl corpus, Common Crawl corpus, News Commentary v13, and Yandex Corpus. We do not use Wiki Headlines and UN Parallel Corpus V1.0. The training corpus consists of 14M sentence pairs. We employ the Moses Tokenizer8 for preprocessing. For subword segmentation, we use 50,000 joint BPE operations and choose the most frequent 52,000 tokens as vocabularies. We use newstest2017 as the development set and the newtest2018 as the test set. Training We design a simple yet e"
Q19-1006,N18-1125,0,0.0196439,"bidirectional decoding. We discuss these topics in the following. Future Modeling Standard neural sequence decoders generate target sentences from left to right, and it has been proven to be important to establish the direct information flow between currently predicted word and previously generated words (Zhou et al., 2017b; Vaswani et al., 2017). However, current methods still fail to estimate some desired information in the future. To address this problem, reinforcement learning methods have been applied to predict future properties (Li et al., 2017; Bahdanau et al., 2017; He et al., 2017). Li et al. (2018) presented a target foresight based attention which uses the POS tag as the partial information of a target foresight word to improve alignment and translation. Inspired by human cognitive behaviors, Xia et al. (2017) proposed a deliberation network, which leverages global information by observing both back and forward information in sequence decoding through a deliberation process. Zheng et al. (2018) introduced two additional recurrent layers to model translated past contents and untranslated future contents. The most relevant models in future modeling are twin networks (Serdyuk et al., 2018"
Q19-1006,W17-4739,0,0.115793,"( y g ). More specifically, we first train an − L2R model using (x, → y g ) and an R2L model ← − using (x, y g ). Then we use the two models to translate source sentences x into pseudo target − − sentences → y p and ← y p , respectively. Finally, we − − − − get two triples (x, → y p, ← y g ) and (x, → y g, ← y p ) as our training data. Once the proposed model is trained, we employ the bidirectional beam search algorithm to predict the target sequence, as illustrated in Figure 4. Compared with previous work that usually adopts a two-phase scheme to translate input sentences (Liu et al., 2016; Sennrich et al., 2017; Zhang et al., 2018), our decoding approach is more compact and effective. 4 The corpora includes LDC2000T50, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, and LDC2004T07. Following previous work, we also use case-insensitive tokenized BLEU to evaluate ChineseEnglish which have been segmented by Stanford word segmentation and Moses Tokenizer, respectively. 5 http://www.statmt.org/wmt14/translation-task.html. All preprocessed datasets and vocab can be directly download in tensor2tensor website https://drive.google.com/ open?id=0B_bZck-ksdkpM25jRUN2X2UxMm8. 6 This procedure is use"
Q19-1006,N16-1046,0,0.475567,"t al., 2017; Vaswani et al., 2017). Recent approaches to sequence-to-sequence learning typically leverage recurrence (Sutskever et al., 2014), convolution (Gehring et al., 2017), or attention (Vaswani et al., 2017) as basic building blocks. Typically, NMT adopts the encoder-decoder architecture and generates the target translation from left to right. Despite their remarkable success, NMT models suffer from several weaknesses (Koehn and Knowles, 2017). One of the most prominent issues is the problem of unbalanced outputs in which the translation prefixes are better predicted than the suffixes (Liu et al., 2016). We analyze translation accuracy of the first and last 4 tokens for left-to-right (L2R) and right-toleft (R2L) directions, respectively. As shown in Table 1, the statistical results show that L2R performs better in the first 4 tokens, whereas R2L translates better in terms of the last 4 tokens. This problem is mainly caused by the left-toright unidirectional decoding, which conditions each output word on previously generated outputs only, but leaving the future information from target-side contexts unexploited during translation. The future context is commonly used in reading and writing in h"
Q19-1006,W16-2323,0,0.380916,"n of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than the conventional NMT framework because they require two NMT models or decoders. Furthermore, the L2R and R2L decoders are independent from each other (Liu et al"
Q19-1006,P16-1162,0,0.883079,"n of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than the conventional NMT framework because they require two NMT models or decoders. Furthermore, the L2R and R2L decoders are independent from each other (Liu et al"
Q19-1006,D15-1166,0,0.080125,"ta. Results on Chinese-English Translation Effect of Fusion Mechanism We first investigate the impact of different fusion mechanisms with different λs on the development set. As shown in Table 2, we find that linear interpolation is sensitive to parameters λ. Nonlinear interpolation, which is more robust than linear interpolation, achieves the best performance when we use tanh with λ = 0.1. Compared with gate mechanism, nonlinear interpolation is much simpler and needs less parameters. Therefore, we will use nonlinear interpolation with tanh and λ = 0.1 for all experiments thereafter. • RNMT (Luong et al., 2015): it is a state-ofthe-art RNN-based NMT system with default setting. • Transformer: it has obtained the state-ofthe-art performance on machine translation, which predicts target sentence from left to right relying on self-attention (Vaswani et al., 2017). Translation Quality Table 3 shows translation performance for Chinese-English. Specifically, the proposed model significantly outperforms Moses, RNMT, Transformer, Transformer (R2L), Rerank-NMT, and ABD-NMT by 13.23, 8.54, 3.92, 4.90, 2.91, and 2.82 BLEU points, respectively. Compared with Transformer and Transformer (R2L), our model exhibits"
Q19-1006,P16-1159,0,0.0248224,"eling and optimizing with left-to-right and right-to-left decoding behaves better in leveraging bidirectional decoding. 4.5 DEV 35.28 35.22 36.38 Results on Russian-English Translation Table 5 shows the results of large-scale WMT18 Russian-English translation, and our approach still significantly outperforms the state-of-the-art Transformer model in development and test sets by 1.10 and 1.04 BLEU points, respectively. Note that the BLEU score gains of English-German and Russian-English are not as significant as that on Chinese-English. The underlying reasons, which have also been mentioned in Shen et al. (2016) and Zhang et al. (2018), are that (1) the Chinese-English datasets contain four reference translations for each source sentence while the English-German and Russian-English datasets only have a single reference; (2) English is more distantly related to Chinese than German and Russian, leading to the predominant improvements for Chinese-English translation when leveraging bidirectional decoding. Results on English-German Translation We further demonstrate the effectiveness of our model in WMT14 English-German translation tasks, and we also display the performances of some competitive models in"
Q19-1006,D16-1096,0,0.0356452,"sh translation tasks. L2R denotes left-to-right decoding and R2L means right-to-left decoding for conventional NMT. Figure 1: Illustration of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than the conventional N"
Q19-1006,D18-1342,0,0.0362405,"Missing"
Q19-1006,W17-4740,0,0.0201058,"e. ::::::: Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backward decoder. Additionally, both Niehues et al. (2016) and Zhou et al. (2017a) combined the strengths of NMT and SMT, which can also be used to combine the advantages of bidirectional translation texts (Zhang et al., 2018). Compared with previous methods, our method has the following advantages: (1) We use a single model to achieve the goal of synchronous left"
Q19-1006,P16-1008,0,0.0277988,"IST Chinese-English translation tasks. L2R denotes left-to-right decoding and R2L means right-to-left decoding for conventional NMT. Figure 1: Illustration of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than"
Q19-1006,N13-1002,0,0.0189311,"::::: bomb :. they are developing a super-large scale , called the mother of the bomb . ::::::::::::::::::::::::: Table 8: Chinese-English translation examples of Transformer decoding in left-to-right and right-to-left way, and our proposed models. L2R performs well in the first half sentence, whereas R2L translates well in the second half :::::::::::: sentence. ::::::: Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backwar"
Q19-1006,C02-1050,0,0.382588,"r mother ::::: called::: the:::::: mother:: of:: a ::::: bomb :. they are developing a super-large scale , called the mother of the bomb . ::::::::::::::::::::::::: Table 8: Chinese-English translation examples of Transformer decoding in left-to-right and right-to-left way, and our proposed models. L2R performs well in the first half sentence, whereas R2L translates well in the second half :::::::::::: sentence. ::::::: Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encod"
Q19-1006,P17-2060,1,0.820297,"(2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backward decoder. Additionally, both Niehues et al. (2016) and Zhou et al. (2017a) combined the strengths of NMT and SMT, which can also be used to combine the advantages of bidirectional translation texts (Zhang et al., 2018). Compared with previous methods, our method has the following advantages: (1) We use a single model to achieve the goal of synchronous left-to-right and right-to-left decoding. (2) Our model can leverage and combine the two decoding directions in every layer of the Transformer decoder, which can run in parallel. (3) By using synchronous bidirectional attention, our model is an end-to-end joint framework and can optimize L2R and R2L decoding simultan"
Q19-1006,D17-1014,0,\N,Missing
Q19-1006,Q18-1011,0,\N,Missing
W17-6005,2007.mtsummit-papers.9,0,0.142563,"Missing"
W17-6005,C96-2141,0,0.0623041,"e term “interprocess communication”, c1 = “所以”, c2 = “只能”, c3 = “使用”, c4 = “进程”, c5 = “间”, c6 = “通讯”, e1 = “interprocess”, e2 = “communication”. And the left boundary is incorrectly recognized by our baseline system as c5 , namely, the target term is c5 c6 =“间 通 讯”. In order to correct the detection error, we enlarge or shrink the anchor from the left boundary to re-generate target terms, including the correct target term c4 c5 c6 =“进程 间 通讯”. Then, we select a best regenerated term which maximizes the joint probability according to Equation 8. In this work, the HMM-based word alignment model (Vogel et al., 1996) is employed to align words. 3.1 Experimental Setup All the experiments are conducted on our in-house developed MT toolkit which has a typical phrasebased decoder (Xiong et al., 2006) and a series of tools, including word alignment and phrase table extraction. We test our method on English-to-Chinese translation in the field of software localization. The training data (1,199,589 sentences) and annotated test data (1,100 sentences) are taken from Microsoft Translation Memory, which is a domain-specific dataset. And additional data employed by this paper includes: the seed dictionary (102,308 so"
W17-6005,W04-3250,0,0.0151133,"nspermia Determinism Target 拂多诞 威卡尔 弗朗西斯达希武德 宗教学 宗教科学引论 宗教史学 宗教现象学 无法则一元论 感质 泛种论 决定论 Table 1: Extracted English-Chinese term translation candidates crosoft Terminology Collection), Chinese Pinyin table (7,809 Chinese characters3 ). The gold standard of term translation of test data are human annotated. All the MT systems are tuned by the development set (1,000 sentences) using ZMERT (Zaidan, 2009) with the objective to optimize BLEU (Papineni et al., 2002). The higher the BLEU score, the better the translation is. And the statistical significance test is performed by the re-sampling approach (Koehn, 2004). 3.2 In contrast to the baseline approach, the figures in Table 2 show that the precision of Chinese terms has been increased by 2.9 points, and the precision of term pairs has been increased by 4.1 points. Thus, according to the bold figures in Table 2, we can draw a conclusion that term extraction can be substantially increased by the proposed framework. (2) The SMT Translation Tests Secondly, we test whether the proposed framework can further improve the performance of term and sentence translation, compared with the baseline system. The strong baseline system, e.g., well tuned Moses, is d"
W17-6005,P07-2045,0,0.00735632,"Missing"
W17-6005,P06-1066,0,0.0181661,"ecognized by our baseline system as c5 , namely, the target term is c5 c6 =“间 通 讯”. In order to correct the detection error, we enlarge or shrink the anchor from the left boundary to re-generate target terms, including the correct target term c4 c5 c6 =“进程 间 通讯”. Then, we select a best regenerated term which maximizes the joint probability according to Equation 8. In this work, the HMM-based word alignment model (Vogel et al., 1996) is employed to align words. 3.1 Experimental Setup All the experiments are conducted on our in-house developed MT toolkit which has a typical phrasebased decoder (Xiong et al., 2006) and a series of tools, including word alignment and phrase table extraction. We test our method on English-to-Chinese translation in the field of software localization. The training data (1,199,589 sentences) and annotated test data (1,100 sentences) are taken from Microsoft Translation Memory, which is a domain-specific dataset. And additional data employed by this paper includes: the seed dictionary (102,308 source words2 , 24,094 terms from Mi2 http://www.mdbg.net/chindict/chindict.php?page=cccedict 42 Source Mihr-Ohrmazd Wicca Francis Dashwood Religious Studies Introduction to the Science"
W17-6005,zhang-etal-2008-comparative,0,0.0550332,"Missing"
W17-6005,E09-1057,0,0.0159903,"the important boundary information of terms and can be adopted as training data of term recognizers. s ∈page where |page |refers to the number of sentences in this page. In this paper, the default values of λ are set to the following weights: λ1 = λ2 = 0.2, λ3 = λ4 = 0.3. 2.3 Term Translation Knowledge Extractor In order to extract bilingual term translation candidates, the key task is to identify the left boundary of a target term. However, traditional term recognition methods employing statistical measures to rand the candidates terms (n-gram sequences), such as log likelihood (Cohen, 1995; Lefever et al., 2009), TF-IDF (Evans and Lefferts, 1995; Medelyan and Witten, 2006), C-value/NCvalue (Frantzi et al., 2000) and many others (Ahmad et al., 2000; Park et al., 2002; Kozakov et al., 2004; Sclano and Velardi, 2007; Zhou et al., 2008; Zhang et al., 2008; Kostoff et al., 2009), leads to very low recall for some domains. What’s worse, some approaches apply frequency threshold to reduce the algorithm’s search space by filtering out low frequency term candidates. Such methods have not taken into account Zipf’s law, again leading to the reduced recall. In this paper, to achieve a higher recall, we adopt nat"
W17-6005,P02-1040,0,0.0983223,"Ohrmazd Wicca Francis Dashwood Religious Studies Introduction to the Science of Religion History of Religions Phenomenology of Religion anomalous monism qualia Panspermia Determinism Target 拂多诞 威卡尔 弗朗西斯达希武德 宗教学 宗教科学引论 宗教史学 宗教现象学 无法则一元论 感质 泛种论 决定论 Table 1: Extracted English-Chinese term translation candidates crosoft Terminology Collection), Chinese Pinyin table (7,809 Chinese characters3 ). The gold standard of term translation of test data are human annotated. All the MT systems are tuned by the development set (1,000 sentences) using ZMERT (Zaidan, 2009) with the objective to optimize BLEU (Papineni et al., 2002). The higher the BLEU score, the better the translation is. And the statistical significance test is performed by the re-sampling approach (Koehn, 2004). 3.2 In contrast to the baseline approach, the figures in Table 2 show that the precision of Chinese terms has been increased by 2.9 points, and the precision of term pairs has been increased by 4.1 points. Thus, according to the bold figures in Table 2, we can draw a conclusion that term extraction can be substantially increased by the proposed framework. (2) The SMT Translation Tests Secondly, we test whether the proposed framework can furth"
W17-6005,C02-1142,0,0.0123359,"is page. In this paper, the default values of λ are set to the following weights: λ1 = λ2 = 0.2, λ3 = λ4 = 0.3. 2.3 Term Translation Knowledge Extractor In order to extract bilingual term translation candidates, the key task is to identify the left boundary of a target term. However, traditional term recognition methods employing statistical measures to rand the candidates terms (n-gram sequences), such as log likelihood (Cohen, 1995; Lefever et al., 2009), TF-IDF (Evans and Lefferts, 1995; Medelyan and Witten, 2006), C-value/NCvalue (Frantzi et al., 2000) and many others (Ahmad et al., 2000; Park et al., 2002; Kozakov et al., 2004; Sclano and Velardi, 2007; Zhou et al., 2008; Zhang et al., 2008; Kostoff et al., 2009), leads to very low recall for some domains. What’s worse, some approaches apply frequency threshold to reduce the algorithm’s search space by filtering out low frequency term candidates. Such methods have not taken into account Zipf’s law, again leading to the reduced recall. In this paper, to achieve a higher recall, we adopt naturally annotated resources for term In this paper, we design following features for the term recognizer: the four words immediately to the left of the term,"
Y09-2016,E09-1011,0,0.0561839,"e rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the source sentences with syntactic reordering rules so as to m"
Y09-2016,P08-1009,0,0.019079,"eordering the source sentence directly, the rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the source sentences"
Y09-2016,P05-1066,0,0.349976,"ior to translation, and then instead of reordering the source sentence directly, the rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is"
Y09-2016,2007.mtsummit-papers.29,0,0.027998,"sentence list for decoding. The former method depends much on the author’s professional knowledge in linguistics and the performance in parsing technology. The latter approach is more robust to the errors in parsing stage but increases the burden of decoding as it has to translate an n-best sentences, and furthermore, it might still produce pre-reordering errors prior to translation because the n-best list includes only part of but not all of the reordering hypotheses. It should be noted that both the two methods are implemented directly in parse trees, and it is pointed out in previous work (Habash, 2007) that ∗ We would like to thank Yu Zhou for her suggestions to revise the earlier draft and thank anonymous reviewers for their helpful comments. The research work has been partially funded by the Natural Science Foundation of China under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under grant No. 2006AA010108-4, and also supported by the China-Singapore Institute of Digital Media as well. Copyright 2009 by Jiajun Zhang and Chengqing Zong 23rd Pacific Asia Conf"
Y09-2016,P03-1054,0,0.00948754,"Experimental Settings We carried out the experiments on Chinese-to-English translation using NIST05 test set. The development set including 571 Chinese sentences is chosen from the test set of NIST06 and NIST08. The training set consists of 297K parallel sentences which are filtered from LDC. Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The target fourgram language model was built with the English part of training data using the SRI Language Modeling Toolkit (Stolcke, 2002). In order to acquire syntactic rules, we parse the Chinese sentence using the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar. We built the maximum entropy model with a MaxEnt Toolkit developed by (Zhang, 2004). All the models were optimized and tested using the case-sensitive BLEU-4 with “shortest” reference length. Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results Before giving the experimental results, some notations of our new systems are first introduced here. The system INcorporating the Hard Rules into the Modified Baseline Decoder is named 585 IN-HR-MBDecoder. Likewise, IN-SR-MBDecoder"
Y09-2016,W04-3250,0,0.03522,"Ney, 2000). The target fourgram language model was built with the English part of training data using the SRI Language Modeling Toolkit (Stolcke, 2002). In order to acquire syntactic rules, we parse the Chinese sentence using the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar. We built the maximum entropy model with a MaxEnt Toolkit developed by (Zhang, 2004). All the models were optimized and tested using the case-sensitive BLEU-4 with “shortest” reference length. Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results Before giving the experimental results, some notations of our new systems are first introduced here. The system INcorporating the Hard Rules into the Modified Baseline Decoder is named 585 IN-HR-MBDecoder. Likewise, IN-SR-MBDecoder is used to denote the system incorporating the soft rules into modified baseline decoder. In Table 2, we present our results. Like (Wang et al., 2007) and (Zhang et al., 2007), we find that reordering the source sentences whether with hard rules or with soft rules can both obtain a significant improvement over the baseline MEBTG by 0.58 an"
Y09-2016,P07-1091,0,0.258903,"on, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the source sentences with syntactic reordering rules so as to make the input much closer to the target language in word order. (Collins et al., 2005; Wang et al, 2007 and Badr et al., 2009) used hard syntactic rules (namely manually created) obtained from source parse trees to directly reorder the input sentences. (Li et al., 2007) employed soft syntactic rules (namely probabilistic) to get an n-best reordered sentence list for decoding. The former method depends much on the author’s professional knowledge in linguistics and the performance in parsing technology. The latter approach is more robust to the errors in parsing stage but increases the burden of decoding as it has to translate an n-best sentences, and furthermore, it might still produce pre-reordering errors prior to translation because the n-best list includes only part of but not all of the reordering hypotheses. It should be noted that both the two methods"
Y09-2016,P00-1056,0,0.0361074,"Rules Pre-Reordering. Likewise, the system using soft rules is called MEBTG+SRP indicating MEBTG system with Soft Rules Pre-reordering (only 1-best reordered source sentence used for source-side of training data and 10-best for test data). 5.2 Corpora and Experimental Settings We carried out the experiments on Chinese-to-English translation using NIST05 test set. The development set including 571 Chinese sentences is chosen from the test set of NIST06 and NIST08. The training set consists of 297K parallel sentences which are filtered from LDC. Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The target fourgram language model was built with the English part of training data using the SRI Language Modeling Toolkit (Stolcke, 2002). In order to acquire syntactic rules, we parse the Chinese sentence using the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar. We built the maximum entropy model with a MaxEnt Toolkit developed by (Zhang, 2004). All the models were optimized and tested using the case-sensitive BLEU-4 with “shortest” reference length. Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 200"
Y09-2016,P03-1021,0,0.0354385,"O is employed. To emphasize the importance of syntactic phrase reordering, we further create another feature to enhance syntactic reordering (because weights tuning cannot promise the weight of syntactic reordering model bigger and more importance than that of non-syntactic reordering model). The final score of merging rules are calculated as follows: ; ; 78 L / & MO ;N ·PQ R · MS ;T ·PU R · WS <X · 7GHI 2 (5) In which WS is a binary feature in order to reward syntactic reordering and it equals to 1 if ΩS is active. All the ten feature weights J ~J in our new model are tuned with MERT (Och, 2003). 4.3 Algorithm of Integrating Syntactic Rules After knowing the translation model and the decoding algorithm we have used, the most important thing we care about is how to integrate the syntactic rules during decoding. The ultimate format of syntactic rule we adopt is designed as (spanN  , spanN  , Pi *, and the merging rules used in decoding always handle two continuous phrases, so if spanN  and spanN  are successive, then Pi will be used to replace the syntactic reordering score MS which is predicted with lexical boundary words in baseline. However, spanN  and spanN  584 will"
Y09-2016,D07-1077,0,0.408572,"d then instead of reordering the source sentence directly, the rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the sour"
Y09-2016,J97-3002,0,0.010852,"nslation is a source phrase but not a parse tree node, we have to make a conversion from tree nodes to source phrases in order to incorporate the syntactic rules. Since each tree node can be projected to be a span on the source sentence, we can just use spans to denote the tree nodes. Finally, any syntactic rule can be represented as a triple (spanN , spanN , Pi *. 4 Integrating Syntactic Rules We integrate the syntactic rules into a phrase-based SMT to help the decoder performs more linguistically. In this paper, we choose the decoder with Bracket Transduction Grammar (BTG) style model (Wu, 1997; Xiong et al., 2006) as our baseline. 4.1 BTG-based Model The BTG-based translation can be viewed as a monolingual parsing process, in which only lexical rules / 0 1, 2 and two binary merging rules / 0 3/4 , /5 6 and / 0 (/4 , /5 * are allowed. During decoding, the source sentence is first divided into phrase sequence, then the lexical rule / 0 1, 2 translates the source phrase 1 into target phrase 2 and forms a block /. The 583 straight rule / 0 3/4 , /5 6 and the inverted rule / 0 (/4 , /5 * merge the two neighboring blocks into a bigger one until the whole source sentence is covered. It"
Y09-2016,P06-1066,0,0.729605,"s a source phrase but not a parse tree node, we have to make a conversion from tree nodes to source phrases in order to incorporate the syntactic rules. Since each tree node can be projected to be a span on the source sentence, we can just use spans to denote the tree nodes. Finally, any syntactic rule can be represented as a triple (spanN , spanN , Pi *. 4 Integrating Syntactic Rules We integrate the syntactic rules into a phrase-based SMT to help the decoder performs more linguistically. In this paper, we choose the decoder with Bracket Transduction Grammar (BTG) style model (Wu, 1997; Xiong et al., 2006) as our baseline. 4.1 BTG-based Model The BTG-based translation can be viewed as a monolingual parsing process, in which only lexical rules / 0 1, 2 and two binary merging rules / 0 3/4 , /5 6 and / 0 (/4 , /5 * are allowed. During decoding, the source sentence is first divided into phrase sequence, then the lexical rule / 0 1, 2 translates the source phrase 1 into target phrase 2 and forms a block /. The 583 straight rule / 0 3/4 , /5 6 and the inverted rule / 0 (/4 , /5 * merge the two neighboring blocks into a bigger one until the whole source sentence is covered. It is natural to adopt a"
Y09-2016,C08-1127,0,0.0117942,"t list. However, all these methods are separated from decoder and reorder the source sentences arbitrarily prior to translation. Once a pre-reordering error happens, it is very difficult to make up for the mistake in later translation steps. In our approach, we just retain the syntactic rules rather than use them to reorder the source sentences directly. During decoding, the syntactic rules will serve as a strong feature to guide and enhance the phrase reordering. Zhang et al., (2007) only allowed reordering between syntactic phrases and enforced the non-syntactic phrases translated in order. Xiong et al. (2008) proposed a linguistically annotated BTG for SMT. The method used some heuristic rules to linguistically annotate every source phrase with the source-side parse tree in decoding and built a linguistical reordering model. The two approaches both acquired and applied the syntactic rules in the decoding stage but meanwhile increased the decoding time to a large extent. Our work differs from theirs in three ways. First, when translating a test sentence, we obtain the corresponding syntactic rules prior to translation rather than in decoding stage and thus alleviate the decoding complexity. Second,"
Y09-2016,D07-1056,0,0.0874122,"English translation to determine whether the children of a node should be reordered or not, and finally to obtain a reordered n-best list. However, all these methods are separated from decoder and reorder the source sentences arbitrarily prior to translation. Once a pre-reordering error happens, it is very difficult to make up for the mistake in later translation steps. In our approach, we just retain the syntactic rules rather than use them to reorder the source sentences directly. During decoding, the syntactic rules will serve as a strong feature to guide and enhance the phrase reordering. Zhang et al., (2007) only allowed reordering between syntactic phrases and enforced the non-syntactic phrases translated in order. Xiong et al. (2008) proposed a linguistically annotated BTG for SMT. The method used some heuristic rules to linguistically annotate every source phrase with the source-side parse tree in decoding and built a linguistical reordering model. The two approaches both acquired and applied the syntactic rules in the decoding stage but meanwhile increased the decoding time to a large extent. Our work differs from theirs in three ways. First, when translating a test sentence, we obtain the co"
