2020.acl-main.257,Q17-1002,0,0.0183207,"t activate selectively or more strongly for a particular function such as modalityspecific or category-specific semantics (such as objects/actions, abstract/concrete, animate/inanimate, animals, fruits/vegetables, colours, body parts, countries, flowers, etc.) (Warrington, 1975; Warrington and McCarthy, 1987; McCarthy and Warrington, 1988). This indicates a function-specific 2873 division of lower-level semantic processing. Singlespace distributional word models have been found to partially correlate to these distributed brain activity patterns (Mitchell et al., 2008; Huth et al., 2012, 2016; Anderson et al., 2017), but fail to explain the full spectrum of fine-grained word associations humans are able to make. Our work has been partly inspired by this literature. Compositional Distributional Semantics. Partially motivated by similar observations, prior work frequently employs tensor-based methods for composing separate tensor spaces (Coecke et al., 2010): there, syntactic categories are often represented by tensors of different orders based on assumptions on their relations. One fundamental difference is made between atomic types (e.g., nouns) versus compositional types (e.g., verbs). Atomic types are"
2020.acl-main.257,J10-4006,0,0.0840012,"th neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed da"
2020.acl-main.257,D08-1007,0,0.0581951,"Missing"
2020.acl-main.257,P09-1068,0,0.0571221,"additionally evaluate our models on a number of other established datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this t"
2020.acl-main.257,P10-1046,0,0.060836,"usibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc ,"
2020.acl-main.257,D14-1082,0,0.00762763,"igh similarity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,W09-0211,0,0.0610713,"Missing"
2020.acl-main.257,D14-1004,0,0.0502357,"Missing"
2020.acl-main.257,W16-1605,0,0.0209594,"ile effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enab"
2020.acl-main.257,Q17-1010,0,0.285232,"V), I(O)). The space is optimised such that vectors for plausible SVO compositions will be close. Note that one word can have several vectors, for example chicken can occur both as S and O. Introduction Word representations are in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The"
2020.acl-main.257,J10-4007,0,0.031176,"Missing"
2020.acl-main.257,P19-1318,0,0.0205895,"actorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vectors encode knowledge that is often complementary to what is coded in word vectors. Rei et al. (2018) and Vuli´c and Mrkˇsi´c (2018) described related task-dependent neural nets for mapping word embeddings into relation-specific spaces for scoring lexical entailment. In this work, we propose a task-independent approach and extend it to work with a variable number of relations. Neuroscience. Theories from cognitive linguistics and neuroscience reveal that single-space repre"
2020.acl-main.257,W15-1106,0,0.122987,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,N15-1003,0,0.119856,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,D11-1129,0,0.0855066,"Missing"
2020.acl-main.257,W11-2507,0,0.311832,"SVO study (V) researcher (S), scientist (S), subject (O), art (O) eat (V) food (O), cat (S), dog (S) need (V) help (O), implementation (S), support (O) Table 1: Nearest neighbours in a function-specific space trained for the SVO structure. In the Joint SVO space (bottom) we show nearest neighbors for verbs (V) from the two other subspaces (O and S). Mann and Ruhlen, 2011). In language, this event understanding information is typically captured by the SVO structures and, according to the cognitive science literature, is well aligned with how humans process sentences (McRae et al., 1997, 1998; Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014); it reflects the likely distinct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation"
2020.acl-main.257,P14-2050,0,0.194288,"and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vec"
2020.acl-main.257,P16-1020,0,0.018132,"nal solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enable a better model of associations between concepts and consequently improved event representations by encoding the relevant information directly into the parameters for each word during training. Word vectors offer several advantages over tensors: a large reduction in parameters and fixed dimensionality across concepts. This facilitates their reuse and transfer across different tasks. For this reason, we find our multidirection"
2020.acl-main.257,N16-1118,0,0.0466753,"Missing"
2020.acl-main.257,J15-4004,1,0.911699,"in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as i"
2020.acl-main.257,C12-2054,0,0.0226717,"tandalone: their meaning is independent from other types. On the other hand, verbs are compositional as they rely on their subjects and objects for their exact meaning. Due to this added complexity, the compositional types are often represented with more parameters than the atomic types, e.g., with a matrix instead of a vector. The goal is then to compose constituents into a semantic representation which is independent of the underlying grammatical structure. Therefore, a large body of prior work is concerned with finding appropriate composition functions (Grefenstette and Sadrzadeh, 2011a,b; Kartsaklis et al., 2012; Milajevs et al., 2014) to be applied on top of word representations. Since this approach represents different syntactic structures with tensors of varying dimensions, comparing syntactic constructs is not straightforward. This compositional approach thus struggles with transferring the learned knowledge to downstream tasks. State-of-the-art compositional models (Tilk et al., 2016; Weber et al., 2018) combine similar tensor-based approaches with neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of"
2020.acl-main.257,P08-1028,0,0.227251,"Missing"
2020.acl-main.257,K16-1008,0,0.0310574,"tablished datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this task function-specific representations a"
2020.acl-main.257,Q17-1022,1,0.909677,"Missing"
2020.acl-main.257,L16-1262,0,0.0649244,"Missing"
2020.acl-main.257,D14-1162,0,0.108435,"arity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,K15-1026,1,0.838922,"across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as illustrated by Figure 1."
2020.acl-main.257,D16-1017,0,0.246401,"nct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram"
2020.acl-main.257,N18-1103,1,0.908435,"Missing"
2020.acl-main.257,P18-2101,1,0.90757,"Missing"
2020.acl-main.257,P99-1014,0,0.453354,"Missing"
2020.acl-main.257,W16-2518,0,0.142282,"itional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspe"
2020.acl-main.259,E06-1042,0,0.226567,"ontain a set of sentences in which a single verb token is labelled as metaphorical or not. There is also an index provided that specifies the location of the target token in the sentence. MOH-X. MOH-X is based on earlier work by Mohammad et al. (2016). It consists of short ‘example’ sentences from WordNet (Fellbaum, 1998)2 with labels for metaphorical verbs along with associated confidence scores. Shutova et al. (2016) created a subset of this dataset, referred to as MOH-X, and added annotations for each verb and its argument. This dataset has 214 unique verbs. TroFi. Similar to MOH-X, TroFi (Birke and Sarkar, 2006) has annotations for target verbs in each sentence. It has a comparatively longer average sentence length with 28.3 words per sentence compared to MOH-X’s 8.0. The sentences in TroFi are constructed from the Wall Street Journal Corpus (Charniak et al., 2000). There are only 50 unique target verbs in this dataset. 4.2 MWE Identification We extract MWEs using the GCN-based system proposed by Rohanian et al. (2019). Since we are focusing on verbal metaphors in this study, we train the system on the PARSEME English dataset verbal metaphor MWE Examples are sentences after the gloss that show incont"
2020.acl-main.259,D16-1053,0,0.0357131,"Missing"
2020.acl-main.259,D19-1227,1,0.538799,"omputational Linguistics annotators to tell them apart (Gross, 1982). 1 Most state-of-the-art MWE identification models are based on neural architectures (Ramisch et al., 2018; Taslimipoor and Rohanian, 2018) with some employing graph-based methods to make use of structured information such as dependency parse trees (Waszczuk et al., 2019; Rohanian et al., 2019). Top-performing metaphor detection models also use neural methods (Rei et al., 2017; Gao et al., 2018), with some utilising additional data such as sentiment and linguistic information to further improve performance (Mao et al., 2019; Dankers et al., 2019). Key, and Value which are represented with Q, K, and V respectively. The output of self-attention is computed with: 3 3.2 Graph Convolutional Networks Graph Convolutional Networks (GCNs) (Kipf and Welling, 2016) are a variation of the classic CNNs that perform the convolution operation on nodes of a graph, making them suitable for capturing nonsequential inter-dependencies in the input. Using the per-sentence formalism (Marcheggiani and Titov, 2017; Rohanian et al., 2019), GCN can be defined as: GCN = f (W X T A + b) (1) where W , X, A, b, and GCN refer to the weight matrix, representation of"
2020.acl-main.259,N19-1423,0,0.0658678,"Missing"
2020.acl-main.259,D18-1060,0,0.600296,"0 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2890–2895 c July 5 - 10, 2020. 2020 Association for Computational Linguistics annotators to tell them apart (Gross, 1982). 1 Most state-of-the-art MWE identification models are based on neural architectures (Ramisch et al., 2018; Taslimipoor and Rohanian, 2018) with some employing graph-based methods to make use of structured information such as dependency parse trees (Waszczuk et al., 2019; Rohanian et al., 2019). Top-performing metaphor detection models also use neural methods (Rei et al., 2017; Gao et al., 2018), with some utilising additional data such as sentiment and linguistic information to further improve performance (Mao et al., 2019; Dankers et al., 2019). Key, and Value which are represented with Q, K, and V respectively. The output of self-attention is computed with: 3 3.2 Graph Convolutional Networks Graph Convolutional Networks (GCNs) (Kipf and Welling, 2016) are a variation of the classic CNNs that perform the convolution operation on nodes of a graph, making them suitable for capturing nonsequential inter-dependencies in the input. Using the per-sentence formalism (Marcheggiani and Tito"
2020.acl-main.259,P19-1024,0,0.0118548,"by Vaswani et al. (2017) in which the weighting is determined by scaled dot product. Given the input representation X, three smaller sized vectors are created. These are Query, 1 See PARSEME annotation guidelines https://parsemefr.lis-lab.fr/parseme-st-guidelines/1.1/ at QK T Att(Q, K, V ) = sof tmax( √ )V (2) d N different self-attention mechanisms are activated in parallel. This approach is known as N -headed self-attention, where each head Hi = Att(QWiQ , KWiK , V ) and the projections WiQ and WiK are parameter matrices. The outputs from these individual heads are later used in GCN layers (Guo et al., 2019). Attention Guided Adjacency Central to GCN is the adjacency matrix where the relations between nodes are defined. Converting the graph of relations to an adjacency matrix involves a rule-based hard pruning strategy and potentially results in discarding valuable information due to the sparsity of the matrix. Influenced by Guo et al. (2019), in this work we consider dependency parse information as an undirected graph ˜ we combine mawith adjacency A. To obtain A, trix A with matrices H0 , H1 ,..., HN −1 induced by the N -headed self-attention mechanism defined in Section 3.1. Given an N -headed"
2020.acl-main.259,P18-5005,0,0.0219953,"hor classification model can be bolstered by knowledge of VMWEs. In this work we focus on how identification of verbal metaphors can be helped by verbal MWEs. We devise a deep learning model based on attention-guided graph convolutional neural networks (GCNs) that encode syntactic dependencies alongside information about the existence of VMWEs and we test the model on two established metaphor datasets. 2 Related Works The tasks of MWE and metaphor identification share some similarities. Many idiomatic MWEs can be considered as lexicalised metaphors. Idioms are where the overlap becomes clear (Kordoni, 2018). It is important to note, however, that not all verbal metaphors are VMWEs. Metaphors that are less conventionalised and appear in creative context (e.g. within a poem or a literary piece) and are not established enough to make it as entries into dictionaries are examples of such cases. However, the distinction between these categories is not always clear, and few precise tests exist for the 2890 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2890–2895 c July 5 - 10, 2020. 2020 Association for Computational Linguistics annotators to tell them ap"
2020.acl-main.259,P19-1378,0,0.567971,"Association for Computational Linguistics annotators to tell them apart (Gross, 1982). 1 Most state-of-the-art MWE identification models are based on neural architectures (Ramisch et al., 2018; Taslimipoor and Rohanian, 2018) with some employing graph-based methods to make use of structured information such as dependency parse trees (Waszczuk et al., 2019; Rohanian et al., 2019). Top-performing metaphor detection models also use neural methods (Rei et al., 2017; Gao et al., 2018), with some utilising additional data such as sentiment and linguistic information to further improve performance (Mao et al., 2019; Dankers et al., 2019). Key, and Value which are represented with Q, K, and V respectively. The output of self-attention is computed with: 3 3.2 Graph Convolutional Networks Graph Convolutional Networks (GCNs) (Kipf and Welling, 2016) are a variation of the classic CNNs that perform the convolution operation on nodes of a graph, making them suitable for capturing nonsequential inter-dependencies in the input. Using the per-sentence formalism (Marcheggiani and Titov, 2017; Rohanian et al., 2019), GCN can be defined as: GCN = f (W X T A + b) (1) where W , X, A, b, and GCN refer to the weight ma"
2020.acl-main.259,D17-1159,0,0.0323806,"Missing"
2020.acl-main.259,S16-2003,0,0.128478,"t is a concatenation of the outputs from both components: GCN = concat[GCNsM W E ; GCNsDEP ] (4) 4 Experiments We describe the datasets used in the experiments and then provide details of the overall system. 4.1 Datasets We apply the systems on two different metaphor datasets: MOH-X, and TroFi, which contain annotations for verb classification. Both of these datasets contain a set of sentences in which a single verb token is labelled as metaphorical or not. There is also an index provided that specifies the location of the target token in the sentence. MOH-X. MOH-X is based on earlier work by Mohammad et al. (2016). It consists of short ‘example’ sentences from WordNet (Fellbaum, 1998)2 with labels for metaphorical verbs along with associated confidence scores. Shutova et al. (2016) created a subset of this dataset, referred to as MOH-X, and added annotations for each verb and its argument. This dataset has 214 unique verbs. TroFi. Similar to MOH-X, TroFi (Birke and Sarkar, 2006) has annotations for target verbs in each sentence. It has a comparatively longer average sentence length with 28.3 words per sentence compared to MOH-X’s 8.0. The sentences in TroFi are constructed from the Wall Street Journal"
2020.acl-main.259,D17-1162,1,0.878046,"exist for the 2890 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2890–2895 c July 5 - 10, 2020. 2020 Association for Computational Linguistics annotators to tell them apart (Gross, 1982). 1 Most state-of-the-art MWE identification models are based on neural architectures (Ramisch et al., 2018; Taslimipoor and Rohanian, 2018) with some employing graph-based methods to make use of structured information such as dependency parse trees (Waszczuk et al., 2019; Rohanian et al., 2019). Top-performing metaphor detection models also use neural methods (Rei et al., 2017; Gao et al., 2018), with some utilising additional data such as sentiment and linguistic information to further improve performance (Mao et al., 2019; Dankers et al., 2019). Key, and Value which are represented with Q, K, and V respectively. The output of self-attention is computed with: 3 3.2 Graph Convolutional Networks Graph Convolutional Networks (GCNs) (Kipf and Welling, 2016) are a variation of the classic CNNs that perform the convolution operation on nodes of a graph, making them suitable for capturing nonsequential inter-dependencies in the input. Using the per-sentence formalism (Ma"
2020.acl-main.259,W19-5113,0,0.0202343,"mples of such cases. However, the distinction between these categories is not always clear, and few precise tests exist for the 2890 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2890–2895 c July 5 - 10, 2020. 2020 Association for Computational Linguistics annotators to tell them apart (Gross, 1982). 1 Most state-of-the-art MWE identification models are based on neural architectures (Ramisch et al., 2018; Taslimipoor and Rohanian, 2018) with some employing graph-based methods to make use of structured information such as dependency parse trees (Waszczuk et al., 2019; Rohanian et al., 2019). Top-performing metaphor detection models also use neural methods (Rei et al., 2017; Gao et al., 2018), with some utilising additional data such as sentiment and linguistic information to further improve performance (Mao et al., 2019; Dankers et al., 2019). Key, and Value which are represented with Q, K, and V respectively. The output of self-attention is computed with: 3 3.2 Graph Convolutional Networks Graph Convolutional Networks (GCNs) (Kipf and Welling, 2016) are a variation of the classic CNNs that perform the convolution operation on nodes of a graph, making the"
2020.acl-main.259,P10-1071,0,0.0353583,"paving the way for further experiments on the complex interactions of these phenomena. The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets. 1 Introduction Human language is rife with a wide range of techniques that facilitate communication and expand the capacities of thinking and argumentation. One phenomenon of such kind is metaphor. Metaphor is defined as a figure of speech in which the speaker makes an implicit comparison between seemingly unrelated things which nonetheless have certain common characteristics (Shutova, 2010). This is done to convey an idea which is otherwise difficult to express succinctly or simply for rhetorical effect. As an example, in the sentence she devoured his novels, the verb devour is used in a metaphorical sense that implies reading quickly and eagerly. The literal and metaphorical senses share the element of intense desire which in turn helps to decode the meaning of the word in its context. It is clear that a mere literal understanding of semantics would not result in proper understanding of a metaphorical expression and a non-compositional approach would be required (Shutova et al."
2020.acl-main.259,N16-1020,0,0.145639,"vide details of the overall system. 4.1 Datasets We apply the systems on two different metaphor datasets: MOH-X, and TroFi, which contain annotations for verb classification. Both of these datasets contain a set of sentences in which a single verb token is labelled as metaphorical or not. There is also an index provided that specifies the location of the target token in the sentence. MOH-X. MOH-X is based on earlier work by Mohammad et al. (2016). It consists of short ‘example’ sentences from WordNet (Fellbaum, 1998)2 with labels for metaphorical verbs along with associated confidence scores. Shutova et al. (2016) created a subset of this dataset, referred to as MOH-X, and added annotations for each verb and its argument. This dataset has 214 unique verbs. TroFi. Similar to MOH-X, TroFi (Birke and Sarkar, 2006) has annotations for target verbs in each sentence. It has a comparatively longer average sentence length with 28.3 words per sentence compared to MOH-X’s 8.0. The sentences in TroFi are constructed from the Wall Street Journal Corpus (Charniak et al., 2000). There are only 50 unique target verbs in this dataset. 4.2 MWE Identification We extract MWEs using the GCN-based system proposed by Rohani"
2020.acl-main.259,J13-2003,0,0.0323259,"Shutova, 2010). This is done to convey an idea which is otherwise difficult to express succinctly or simply for rhetorical effect. As an example, in the sentence she devoured his novels, the verb devour is used in a metaphorical sense that implies reading quickly and eagerly. The literal and metaphorical senses share the element of intense desire which in turn helps to decode the meaning of the word in its context. It is clear that a mere literal understanding of semantics would not result in proper understanding of a metaphorical expression and a non-compositional approach would be required (Shutova et al., 2013; Vulchanova et al., 2019). The human brain is equipped with the necessary machinery to decode the intended message behind a metaphorical utterance. This involves mentally linking the seemingly unrelated concepts based on their similarities (Rapp et al., 2004). Verbal MWEs (VMWEs) are another example of non-literal language in which multiple words form a single unit of meaning. These two phenomena share some common ground. Expressions like take the bull by the horns, go places, kick the bucket, or break someone’s heart can be categorised as metaphorical VMWEs. Based on this observation we hypo"
2020.coling-main.195,P17-1074,0,0.0280055,"h word token in the C ROWD ED Corpus we added a binary complexity label obtained from a model pre-trained on separate data (Gooding and Kochmar, 2019). Words in the complexity training data were labelled as complex or not by twenty crowdworkers (Yimam et al., 2017), and the model is currently state-of-the-art for the complex word identification task. We expect that complex words are more likely to be involved in or around grammatical errors. In total 4485 word tokens were identified as complex – for instance, ‘guarantee’, ‘presentation’, and ‘suppliers’. Error types were obtained from ERRANT (Bryant et al., 2017) based on the alignment of the Prolific transcription and its error-corrected version. Predicting error types, such as R:NOUN (replace noun), M:DET (missing determiner), and so on, as an auxiliary task improved GED performance in previous work (Rei and Yannakoudakis, 2017). Finally, we extracted a number of prosodic values associated with each word token from the audio recordings. First we force aligned the transcriptions with the audio using the SPPAS toolkit (Bigi, 2015). Based on the resulting token start and end timestamps we could then calculate token durations, durations of any pauses pr"
2020.coling-main.195,W19-4406,0,0.0527833,"Missing"
2020.coling-main.195,L16-1340,1,0.853334,"various configurations including pretrained and contextual word representations as input, additional features and auxiliary objectives, and extra training data from written error-annotated corpora. We find that a model concatenating pre-trained and contextual word representations as input performs best, and that additional information does not lead to further performance gains. 1 Introduction We introduce a new resource for speech-centric natural language processing (speech NLP) – more than a thousand transcriptions and error annotations for 383 distinct recordings from the C ROWD ED Corpus (Caines et al., 2016). C ROWD ED is a crowdsourced English corpus of short monologues on business topics, recorded by both native and non-native speakers. It was created in response to the lack of speech corpora freely available for research use, and the lack of appropriate native speaker reference corpora with which language learners’ exam monologues can be compared. In this new project, crowdworkers were asked to first correct existing speech transcriptions and then to edit the resulting transcriptions to make them more fluent. These new annotations enable both post-editing of noisy speech transcriptions – such"
2020.coling-main.195,W17-5010,1,0.798898,"apply minimal grammatical error corrections to their updated transcriptions, in order to make them “sound like something you would expect to hear or produce yourself in English”. With this statement we intended to convey the error correction task to crowdworkers in a straightforward way without the use of jargon: to alter the text into something linguistically acceptable in that worker’s judgement, without referring to notions of grammar or ‘correctness’ which may have particularly strong connotations for some. We developed the annotation web-app in R Shiny (Chang et al., 2020), adapted from Caines et al. (2017a) with a simple user-interface and text instructions kept to a minimum so as not to overload the workers with information. The relevant audio recordings were provided for unlimited playback, and workers were presented with the ‘machine-made’ transcription which was formed from the original two CrowdFlower transcriptions, along with the prompt for context. There were two text boxes, firstly for the worker to edit the existing transcription so that it better matched the recording, and the second being a copy of that updated transcription ready for the second task of error correction. A screensh"
2020.coling-main.335,C18-1139,0,0.0174284,"kqh k2 · kqi k2 (16) h=1 i>h Lastly, we include an auxiliary objective for language modeling (LM) operating over characters and words, following the settings proposed by Rei (2017). The hidden representations from the forward and backward LSTMs are mapped to a new, non-linear space and used to predict the next word in the sequence, from a fixed smaller vocabulary. Recently, many NLP systems using multi-task learning include LM objectives along the core task to inject corpus-specific information into the model, as well as syntactic and semantic patterns (Dai and Le, 2015; Peters et al., 2017; Akbik et al., 2018; Marvin and Linzen, 2018). In our case, we include an LM loss to help the model learn general language features. While performing well on language modelling itself is not an objective, we expect it to provide improved biases and language-specific knowledge that would benefit performance. The final loss function L is a weighted sum of all the objectives described above. Setting particular coefficients λ allows us to investigate the effect of the different components as well as controlling the flow of the supervision signal and the importance of each auxiliary task: L = λsent Lsent + λtok Ltok"
2020.coling-main.335,K18-1030,1,0.850015,"e task on multiple levels. By carefully designing the network and including specific auxiliary objectives, these levels are able to provide mutually beneficial information to each other. Other hierarchical multi-task systems, such as the models proposed by Hashimoto et al. (2017) and Sanh et al. (2018), solve each task at a different DNN layer, but their formulation does not follow a compositional linguistic motivation. Our work is most similar to Rei and Søgaard (2019) and Rei and Søgaard (2018), who described an architecture for supervising attention in a binary text classification setting. Barrett et al. (2018) also used a related model to guide the network to focus on similar areas as humans, based on human gaze recordings. We build on these ideas and describe a more general framework, extending it to both multiclass text classification and multiclass sequence labeling. An important part of our new architecture is based on attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015), and, in particular, on the properties of multi-head attention (Vaswani et al., 2017; Li et al., 2019). Other regularization techniques that explicitly encourage the learning of more diverse attention functions have"
2020.coling-main.335,E17-2026,0,0.026072,"r zero-shot experiments (see Appendix B for details). 3768 4 Related work Most methods for text classification (hierarchical or not) treat sentence classification and sequence labeling as completely separate tasks (Lample et al., 2016; Huang et al., 2015; Lei et al., 2018; Cui and Zhang, 2019). More recently, training a model end-to-end for more than one language task has become increasingly popular (Yang et al., 2017; Devlin et al., 2018), as well as using auxiliary objectives to inject useful inductive biases (Mart´ınez Alonso and Plank, 2017; Søgaard and Goldberg, 2016; Plank et al., 2016; Bingel and Søgaard, 2017). Our work is similar in terms of motivation for the auxiliary objectives and multi-task training procedure. However, instead of learning to perform multiple tasks on the same level, we focus on performing the same task on multiple levels. By carefully designing the network and including specific auxiliary objectives, these levels are able to provide mutually beneficial information to each other. Other hierarchical multi-task systems, such as the models proposed by Hashimoto et al. (2017) and Sanh et al. (2018), solve each task at a different DNN layer, but their formulation does not follow a"
2020.coling-main.335,C18-1251,0,0.0170423,"erformance on the development set (see Table 6 in Appendix A). We perform each experiment with five different random seeds and report the average results. Following Vaswani et al. (2017), we also applied label smoothing (Szegedy et al., 2016) with  = 0.15 to increase the robustness to noise and regularize the label predictions during training. As evaluation metrics, we report (based on the task) the precision (P), accuracy (Acc), and micro-averaged F1 score of all the labels and of all the non-default labels (denoted by a superscript ∗), as it is common in the multi-task learning literature (Changpinyo et al., 2018; Mart´ınez Alonso and Plank, 2017). For CoNLL03, we use the dedicated CoNLL evaluation script, which calculates F1 on the entity level. 3766 3.3 Model variants We can optimize different variations of the architecture by changing the λ weights in the loss and thereby choosing which components are active. We experiment with the following variations of the model: • MHAL-joint: Corresponds to the fully supervised experiment, and is optimized both as a sentence classifier and a sequence labeler by setting λsent = λtok = 1.0, while all the other λ values are 0.0. • MHAL-joint+: In addition to train"
2020.coling-main.335,D19-1223,0,0.0253609,"on these ideas and describe a more general framework, extending it to both multiclass text classification and multiclass sequence labeling. An important part of our new architecture is based on attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015), and, in particular, on the properties of multi-head attention (Vaswani et al., 2017; Li et al., 2019). Other regularization techniques that explicitly encourage the learning of more diverse attention functions have been proposed by Li et al. (2018), who introduced a disagreement regularization term, and by Niculae and Blondel (2017) and Correia et al. (2019), who proposed sparse attention for increased interpretability. 5 Conclusion We investigated a novel neural architecture for natural language representations, which explicitly ties together predictions on multiple levels of granularity. The dynamically calculated weights in a multi-head attention component for composing sentence representations are also connected to token-level predictions, with each attention head focusing on detecting one particular label. This model can then be optimized as either a sentence classifier or a token labeler, or jointly for both tasks, with information being sh"
2020.coling-main.335,D19-1422,0,0.0222524,"ce level to the tokens. We visualized the decisions computed inside the attention heads for different example sentences and provide them in Appendix A (Figures 3 and 4, respectively). We also observed that the choice of the metric based on which the stopping criterion is selected plays an important and interesting role in our zero-shot experiments (see Appendix B for details). 3768 4 Related work Most methods for text classification (hierarchical or not) treat sentence classification and sequence labeling as completely separate tasks (Lample et al., 2016; Huang et al., 2015; Lei et al., 2018; Cui and Zhang, 2019). More recently, training a model end-to-end for more than one language task has become increasingly popular (Yang et al., 2017; Devlin et al., 2018), as well as using auxiliary objectives to inject useful inductive biases (Mart´ınez Alonso and Plank, 2017; Søgaard and Goldberg, 2016; Plank et al., 2016; Bingel and Søgaard, 2017). Our work is similar in terms of motivation for the auxiliary objectives and multi-task training procedure. However, instead of learning to perform multiple tasks on the same level, we focus on performing the same task on multiple levels. By carefully designing the ne"
2020.coling-main.335,Y18-1061,0,0.025823,"not connect the different hierarchical levels and specialize only on sentence classification or sequence labeling. • BiLSTM-sent: Following the description and implementation of Yang et al. (2016), we built one of the strongest neural sentence classifiers based on BiLSTMs and soft attention; we tuned the hyper-parameters based on the development set to achieve the best performance on our tasks. • BiLSTM-tok: Widely-used bidirectional LSTM architecture for sequence labeling, which has been applied to many tasks including part-of-speech tagging (Plank et al., 2016) and named entity recognition (Panchendrarajan and Amaresan, 2018). We also tuned the hyperparameters based on the development set in order to achieve the best results on each of the evaluation datasets. 3.4 Results Fully-supervised: In this setting, we investigate whether training a joint model to solve the task on multiple levels provides a performance improvement over focusing only on one level. Table 2 compares the MHAL joint text classification performances to a BiLSTM attention-based sentence classifier and a BiLSTM sequence labeler. The results show that the multi-task models systematically outperform the single-task models across all tasks and datase"
2020.coling-main.335,D14-1162,0,0.0858097,"hen passed through a multi-head attention mechanism, which predicts label distributions for both individual words and the whole sentence. Each attention head is incentivized to be predictive of a particular label, allowing the system to also assign labels to individual words while composing a sentence-level representation for sentence classification. The network takes as input a tokenized sentence of length N and maps it to a sequence of vectors [x1 , x2 , ..., xN ]. Each vector xi , corresponding to the ith token in a sentence, is the concatenation of its pre-trained GloVe word embedding wi (Pennington et al., 2014) with its character-level representation ci , similar to Lample et al. (2016). Passing each vector xi to a BiLSTM (Graves and Schmidhuber, 2005), we obtain compact token representations zi by concatenating the hidden states from each direction at every time step and projecting these onto a joint feature space using a tanh activation (Equations 1-3). This is followed by a multi-head attention mechanism with H heads (Vaswani et al., 2017). By setting H equal to the size of our token-level tagset, we can create a direct one-to-one correspondence between attention heads and possible token labels –"
2020.coling-main.335,P17-1161,0,0.0247758,"qh · qi Rq = H(H − 1) kqh k2 · kqi k2 (16) h=1 i>h Lastly, we include an auxiliary objective for language modeling (LM) operating over characters and words, following the settings proposed by Rei (2017). The hidden representations from the forward and backward LSTMs are mapped to a new, non-linear space and used to predict the next word in the sequence, from a fixed smaller vocabulary. Recently, many NLP systems using multi-task learning include LM objectives along the core task to inject corpus-specific information into the model, as well as syntactic and semantic patterns (Dai and Le, 2015; Peters et al., 2017; Akbik et al., 2018; Marvin and Linzen, 2018). In our case, we include an LM loss to help the model learn general language features. While performing well on language modelling itself is not an objective, we expect it to provide improved biases and language-specific knowledge that would benefit performance. The final loss function L is a weighted sum of all the objectives described above. Setting particular coefficients λ allows us to investigate the effect of the different components as well as controlling the flow of the supervision signal and the importance of each auxiliary task: L = λsen"
2020.coling-main.335,P16-2067,0,0.170442,"son, we also evaluate two baseline models that do not connect the different hierarchical levels and specialize only on sentence classification or sequence labeling. • BiLSTM-sent: Following the description and implementation of Yang et al. (2016), we built one of the strongest neural sentence classifiers based on BiLSTMs and soft attention; we tuned the hyper-parameters based on the development set to achieve the best performance on our tasks. • BiLSTM-tok: Widely-used bidirectional LSTM architecture for sequence labeling, which has been applied to many tasks including part-of-speech tagging (Plank et al., 2016) and named entity recognition (Panchendrarajan and Amaresan, 2018). We also tuned the hyperparameters based on the development set in order to achieve the best results on each of the evaluation datasets. 3.4 Results Fully-supervised: In this setting, we investigate whether training a joint model to solve the task on multiple levels provides a performance improvement over focusing only on one level. Table 2 compares the MHAL joint text classification performances to a BiLSTM attention-based sentence classifier and a BiLSTM sequence labeler. The results show that the multi-task models systematic"
2020.coling-main.335,N18-1027,1,0.85089,"ining procedure. However, instead of learning to perform multiple tasks on the same level, we focus on performing the same task on multiple levels. By carefully designing the network and including specific auxiliary objectives, these levels are able to provide mutually beneficial information to each other. Other hierarchical multi-task systems, such as the models proposed by Hashimoto et al. (2017) and Sanh et al. (2018), solve each task at a different DNN layer, but their formulation does not follow a compositional linguistic motivation. Our work is most similar to Rei and Søgaard (2019) and Rei and Søgaard (2018), who described an architecture for supervising attention in a binary text classification setting. Barrett et al. (2018) also used a related model to guide the network to focus on similar areas as humans, based on human gaze recordings. We build on these ideas and describe a more general framework, extending it to both multiclass text classification and multiclass sequence labeling. An important part of our new architecture is based on attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015), and, in particular, on the properties of multi-head attention (Vaswani et al., 2017; Li et al."
2020.coling-main.335,P17-1194,1,0.853126,"he term Rq and calculate it as the average cosine similarity between every pair of queries qh and qi , with h 6= i (equation 16). Rq penalizes high similarity between any two query vectors and motivates the model to push them apart. Thus, this technique imposes a wider angle between the queries, encouraging the model to learn unique, diverse, and meaningful vector representations for the tags. H−1 H XX 2 qh · qi Rq = H(H − 1) kqh k2 · kqi k2 (16) h=1 i>h Lastly, we include an auxiliary objective for language modeling (LM) operating over characters and words, following the settings proposed by Rei (2017). The hidden representations from the forward and backward LSTMs are mapped to a new, non-linear space and used to predict the next word in the sequence, from a fixed smaller vocabulary. Recently, many NLP systems using multi-task learning include LM objectives along the core task to inject corpus-specific information into the model, as well as syntactic and semantic patterns (Dai and Le, 2015; Peters et al., 2017; Akbik et al., 2018; Marvin and Linzen, 2018). In our case, we include an LM loss to help the model learn general language features. While performing well on language modelling itsel"
2020.emnlp-main.680,W19-4412,0,0.0106932,"te an inability to rely on language modelling in low errordensity domains. 5.2 Perplexity ratio Score differences for the R:SPELL error type seem to be driven by a different propensity of spelling errors being of a typographical vs. phonetical nature in the two datasets. versions of an input sentence and then deciding if any of the alternatives are preferable to the original version, based on language model probabilities. The authors use an n-gram language model, which we replace with GPT-2 (Radford et al., 2019) to see how a strong neural language model performs – this approach is similar to Alikaniotis and Raheja (2019). Hyperparameters are tuned for each dataset (see Appendix C for details). Table 7 displays the results on the different datasets. Recall and, in particular, precision is substantially lower on CWEB and AESW compared to other datasets. In general, scores are higher in domains with a higher proportion of errors and those containing edits which result in high perplexity improvements. In these cases systems can rely on a rough heuristic of replacing low probability sequences with high probability ones. However, in CWEB, where errors are fewer and more subtle, this leads to low precision, as perpl"
2020.emnlp-main.680,D19-1435,0,0.0222983,"Missing"
2020.emnlp-main.680,W18-0529,0,0.014767,"OTHER, R:SPELL and R:VERB. These are open class errors, where the error and correction can be quite different. It is therefore reasonable that differences in edits’ degree of semantic change and perplexity improvement across domains are particularly observed in these cases.22 Language Model Importance We also investigate the degree to which systems can rely on a strong internal language model representation when evaluated against different domains. We examine this by looking at the performance of a purely language model based GEC system over the different datasets. We build on the approach of Bryant and Briscoe (2018), using confusion sets to generate alternative 22 0.34 0.51 0.69 Table 8: Examples of false positives on the CWEB dataset that improve perplexity substantially – even more than the average gold edit in CWEB (0.86 perplexity ratio). Table 7: Scores of a language model based GEC system. The lower scores on CWEB and AESW indicate an inability to rely on language modelling in low errordensity domains. 5.2 Perplexity ratio Score differences for the R:SPELL error type seem to be driven by a different propensity of spelling errors being of a typographical vs. phonetical nature in the two datasets. ve"
2020.emnlp-main.680,W19-4406,0,0.288356,"e. We use the jusText5 tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English6 and incomplete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that"
2020.emnlp-main.680,P17-1074,0,0.113216,"ng of error corrections, as as there are often many different ways to correct a sentence (Bryant and Ng, 2015). Kappa is 0.39 and 0.44 for sponsored (CWEB-S) and generic website (CWEB-G) data respectively, and Table 3 presents how our agreement results compare to those of existing GEC datasets. The table also includes a number of other statistics, and the different datasets are further analyzed, compared and contrasted in Section 5. 7 8 The texts are tokenized using SpaCy9 and automatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017). Release For each dataset, we release a development and a test set: we propose a roughly equal division of the data into the two splits, which presents a fair amount of errors to evaluate on (see Table 2). To avoid copyright restrictions, we split the collected paragraphs into sentences and shuffle all sentences in order to break the original and coherent structure that would be needed to reproduce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Sch¨afer, 2015; Biemann et al., 2007). The data is available at https: //github.c"
2020.emnlp-main.680,P15-1068,0,0.133747,"minimum number of edits to make the text grammatical. During error annotation, the annotators have access to the entire paragraph in which a sentence belongs, therefore using the context of a sentence to help them in the correction. Examples of erroneous sentences from our data are shown in Table 1. Annotator agreement is calculated at the sentence level using Cohen’s Kappa, i.e. we calculate whether annotators agree on which sentences are erroneous. This approach is preferable to relying on exact matching of error corrections, as as there are often many different ways to correct a sentence (Bryant and Ng, 2015). Kappa is 0.39 and 0.44 for sponsored (CWEB-S) and generic website (CWEB-G) data respectively, and Table 3 presents how our agreement results compare to those of existing GEC datasets. The table also includes a number of other statistics, and the different datasets are further analyzed, compared and contrasted in Section 5. 7 8 The texts are tokenized using SpaCy9 and automatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017). Release For each dataset, we release a development and a test set: we propose a roughly"
2020.emnlp-main.680,D16-1195,0,0.0124626,"s gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhibit low precision due to a tendency to over-predict errors. Recent work tackled the domain adaptation problem, and released GEC benchmarks from Wikipedia data and online comments [GMEG Wiki+Yahoo (Napoles et al., 2019)]. However, these datasets p"
2020.emnlp-main.680,W13-1703,0,0.0688895,".77 41.89 37.57 32.26 36.00 14.05 21.34 13.24 23.00 13.88 21.58 17.27 19.97 15.75 20.28 16.91 19.98 33.08 26.97 31.29 8.78 14.29 9.67 18.91 8.94 14.98 5.73 10.80 8.78 15.11 6.15 11.43 PIE system 32.77 44.71 23.11 19.66 30.24 35.58 Table 5: Scores of two SOTA GEC systems on each domain. For both systems performance is substantially lower on CWEB than ESL domains. Scores are calculated against each individual annotator and averaged pre-trained on synthetic errors and fine-tuned on learner data from the train section of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011), and NUCLE (Dahlmeier et al., 2013) and for GEC-PSEUDODATA additionally on the W&I train split (Bryant et al., 2019). Performance is evaluated using the F0.5 metric calculated by ERRANT (Bryant et al., 2017).18 However, the more annotators a dataset has, the higher score a system will get on this data (Bryant and Ng, 2015). In order to perform a fair comparison of systems across datasets with a different number of annotators, we calculate the ERRANT score against each individual annotator and then take the average to get the final score. Evaluation results are presented in Table 5. Across all datasets, we observe lower scores w"
2020.emnlp-main.680,W16-0506,0,0.109906,"ements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English6 and incomplete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhi"
2020.emnlp-main.680,N19-1423,0,0.00755079,"mprovements than datasets from more ad∆S ∆P 0.3 4 3 0.2 2 0.1 1 0 :P R: U N C OT T H E M R :D U :P ET U N R: CT SP EL R: L PR R: EP O RT R: H V ER B 0 ∆ Semantic Similarity (∆S) ∆ Perplexity Ratio (∆P ) ·10−2 M We limit our analysis to sentences containing exactly one edit, as we are interested in how individual edits change a sentence, regardless of how domains differ in amounts of erroneous sentences and in the number of edits per sentence (Table 3). Regarding 1), to measure the semantic change of a sentence after an edit is introduced, we use sentence embeddings generated by Sentence-BERT (Devlin et al., 2019) and calculate the cosine similarity between the original sentence and its corrected counterpart. Regarding 2), the degree of sentence improvement is calculated as the ratio of the perplexity of GPT-2 (Radford et al., 2019) on a sentence after and before it has been edited. Figure 4: Difference in semantic similarity and perplexity ratio between CWEB-S and FCE for the most frequent error types (M: missing; R: replace; U: unnecessary). vanced speakers. CWEB and AESW in particular stand out, with edits that largely retain the semantics of a sentence and that result in more subtle improvements. E"
2020.emnlp-main.680,D19-1119,0,0.063297,"sets). In particular, precision is improved (+20.8/+18.6 on CWEB-G/S) at the expense of recall (−6.4/−2.8 on CWEB-G/S). However, performance is still low compared to the language learning domain (F0.5 of at least 41), further indicating that there is scope for developing more robust and general-purpose, open-domain GEC systems. For the purpose of future benchmarking, Appendix B lists the system's ERRANT scores based on both annotators – as opposed to the average of individual annotator scores reported in Table 6. 19 www.github.com/chrisjbryant/errant 8471 We use the fine-tuning parameters of Kiyono et al. (2019). 5 Analysis FCE (PSEUDO) Wiki (PSEUDO) CWEB-G (PSEUDO) In order to assess the impact our new dataset can have on the GEC field, we carry out analyses to show 1) to what degree the domain of our data is different from existing GEC corpora, and how existing GEC systems are affected by the domain shift; and 2) that a factor behind the performance drop on CWEB data is the inability of systems to rely on a strong internal language model in low error density domains. 5.1 FCE (PIE) Wiki (PIE) CWEB-G (PIE) precision 60 40 20 Domain Shift Moving from error correction in learner texts to error correcti"
2020.emnlp-main.680,I11-1017,0,0.0219533,"13 35.94 47.09 52.81 34.13 23.02 43.77 41.89 37.57 32.26 36.00 14.05 21.34 13.24 23.00 13.88 21.58 17.27 19.97 15.75 20.28 16.91 19.98 33.08 26.97 31.29 8.78 14.29 9.67 18.91 8.94 14.98 5.73 10.80 8.78 15.11 6.15 11.43 PIE system 32.77 44.71 23.11 19.66 30.24 35.58 Table 5: Scores of two SOTA GEC systems on each domain. For both systems performance is substantially lower on CWEB than ESL domains. Scores are calculated against each individual annotator and averaged pre-trained on synthetic errors and fine-tuned on learner data from the train section of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011), and NUCLE (Dahlmeier et al., 2013) and for GEC-PSEUDODATA additionally on the W&I train split (Bryant et al., 2019). Performance is evaluated using the F0.5 metric calculated by ERRANT (Bryant et al., 2017).18 However, the more annotators a dataset has, the higher score a system will get on this data (Bryant and Ng, 2015). In order to perform a fair comparison of systems across datasets with a different number of annotators, we calculate the ERRANT score against each individual annotator and then take the average to get the final score. Evaluation results are presented in Table 5. Across all"
2020.emnlp-main.680,D19-5504,0,0.0150608,"graphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhibit low precision due to a tendency to over-predict errors. Recent work tackled the domain adaptation problem, and released GEC benchmarks from Wikipedia data and online comments [GMEG Wiki+Yahoo (Napoles et al., 2019)]. However, these datasets present a high density of error"
2020.emnlp-main.680,Q19-1032,0,0.0408577,"Missing"
2020.emnlp-main.680,E17-2037,0,0.017618,"into sentences and shuffle all sentences in order to break the original and coherent structure that would be needed to reproduce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Sch¨afer, 2015; Biemann et al., 2007). The data is available at https: //github.com/SimonHFL/CWEB. 3 GEC Corpora We compare our data with existing GEC corpora which cover a range of domains and proficiency levels. Table 3 presents a number of different statistics and Table 4 their error-type frequencies.10 3.1 English as a second language (ESL) JFLEG (Napoles et al., 2017) The JHU FluencyExtended GUG corpus consists of sentences written by English language learners (with different proficiency levels and L1s) for the TOEFL® exam, 9 top-level domains: .gov, .edu, .mil, .int, and .museum. top-level domains: .com, .info, .net, .org. 10 8469 https://spacy.io/ See links to downloadable versions in Appendix A W&I JFLEG FCE 2.1 CoNLL14 A P UNCT V ERB OTHER D ET N OUN P REP S PELL A LL 147.7 233.5 295.6 180.7 167.7 107.1 242.5 112.3 176.7 138.3 149.1 105.4 113.8 107.8 1675.6 1084.9 65.5 200.5 158.1 134.9 116.8 92.7 26.0 244.8 300.0 237.3 159.1 139.8 137.2 79.3 LOCNESS B"
2020.emnlp-main.680,W14-1701,0,0.0302274,"1050.7 504.1 400.6 732.3 635.3 239.2 G S 48.9 23.4 31.6 20.9 19.6 15.6 3.8 48.7 13.1 21.0 19.7 12.8 9.8 2.4 208.9 147.2 Table 4: Number of error occurrences for the most frequent error types (per 10, 000 token). covering a range of topics. Texts have been corrected for grammatical errors and fluency. FCE (Yannakoudakis et al., 2011) consists of 1, 244 error corrected texts produced by learners taking the First Certificate in English exam, which assesses English at an upper-intermediate level. We use the data split made available for the BEA GEC shared task 2019 (Bryant et al., 2019). CoNLL14 (Ng et al., 2014) consists of (mostly argumentative) essays written by ESL learners from the National University of Singapore, which are annotated for grammatical errors by two native speakers of English. Write&Improve (W&I) (Bryant et al., 2019) Cambridge English Write & Improve (Yannakoudakis et al., 2018) is an online web platform that automatically provides diagnostic feedback to non-native English-language learners, including an overall language proficiency score based on the Common European Framework of Reference for Languages (CEFR).11 The W&I corpus contains 3, 600 texts across 3 different CEFR levels"
2020.emnlp-main.680,W17-5019,0,0.0334002,"Missing"
2020.emnlp-main.680,P11-1019,1,0.767761,"rse range of writing and constitute a major part of what people read and write on an everyday basis. This work highlights two major prevailing challenges of current approaches to GEC: domain adaptation and low precision in texts with low error density. Previous work has primarily targeted essaystyle text with high error density (see Figure 1); however, this lack of diversity means that it is not clear how systems perform on other domains and under different error distributions (Sakaguchi et al., 2017).2 Current publicly available datasets are restricted to non-native English essays [e.g. FCE (Yannakoudakis et al., 2011); CoNLL14 (Ng et al., 2 Leacock et al. (2010) highlighted the variations in the distribution of errors in non-native and native English writings. 8467 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8467–8478, c November 16–20, 2020. 2020 Association for Computational Linguistics Error type Example sentence V ERB :S VA They develop positive relationships with swimmers and members, and promotes promote programs in order to generate more participation. In a small agriculture agricultural town on the east side of Washington state State called Yakima."
2021.findings-emnlp.200,I05-5002,0,0.0113363,"NLP and involves modelling the semantic relationship between two sentences in a binary classification setup. We work with the following five widely used English language datasets which cover a range of sizes and tasks (including paraphrase detection, duplicate question identification and answer sentence selection, see Appendix A for details). MSRP The Microsoft Research Paraphrase dataset (MSRP) contains 5K pairs of sentences from news websites which were collected based on heuristics and an SVM classifier. Gold labels are based on human binary annotations for sentential paraphrase detection (Dolan and Brockett, 2005). SemEval The SemEval 2017 CQA dataset (Nakov et al., 2017) consists of three subtasks involving posts from the online forum Qatar Living3 . Each subtask provides an initial post as well as 10 posts which were retrieved by a search engine and annotated with binary labels by humans. The task requires the distinction between relevant and non-relevant posts. The original problem is a ranking setting, but since the gold labels are binary, we focus on a classification setup. In subtask A, the posts are questions and comments from the same thread, in an answer sentence selection setup (26K instances"
2021.findings-emnlp.200,N15-1184,0,0.056371,"Missing"
2021.findings-emnlp.200,S17-2053,0,0.0838483,"variety of application areas and technical duplicate question identification and answer senapproaches.We broadly categorise such approaches tence selection which require detecting the semaninto input-related, external and internal. Input modifications (Zhao et al., 2020; Singh et al., tic similarity between text pairs (Peinelt et al., 2020). Early semantic similarity methods used 2020; Lai et al., 2020; Ruan et al., 2020) adapt the feature-engineering techniques, exploring various information that is fed to BERT – e.g. feeding text triples separated by [SEP] tokens instead of sen- syntactic (Filice et al., 2017), semantic (Balchev et al., 2016) and lexical features (Tran et al., 2015; tence pairs as in Lai et al. (2020) – while leaving Almarwani and Diab, 2017). Subsequent work the architecture unchanged. Output modifications tried to model text pair relationships either based on (Xuan et al., 2020; Zhang et al., 2020) build on increasingly complex neural architectures (Deriu BERT’s pre-trained representation by adding external information after the encoding step – e.g. and Cieliebak, 2017; Wang et al., 2017; Tan et al., 2018) or by combining both approaches through combining it with additional seman"
2021.findings-emnlp.200,J15-4004,0,0.0725811,"Missing"
2021.findings-emnlp.200,P14-2050,0,0.310452,"tative analysis shows that counter-fitted embedding injection is particularly beneficial, with notable improvements on examples that require synonym resolution. 1 Introduction Before the rise of contextualised models, transfer of pre-trained information between datasets and tasks in NLP was based on word embeddings. Over many years, substantial effort was placed into the creation of such embeddings. While originally capturing mainly collocation patterns (Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for injecting pretrained linguistically-enriched embeddings into any layer of BERT. The model maps any word embeddings into"
2021.findings-emnlp.200,2020.emnlp-main.333,0,0.0165923,"(Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for injecting pretrained linguistically-enriched embeddings into any layer of BERT. The model maps any word embeddings into the same space as BERT’s hidden representations, then combines them using learned gating parameters. Evaluation of this method on five semantic similarity tasks shows that injecting pre-trained dependency-based and counter-fitted embeddings can further enhance BERT’s performance. More specifically, we make the following contributions: Detecting the semantic similarity between a given text pair is at the core of many NLP tasks. It is a challenging"
2021.findings-emnlp.200,W13-3512,0,0.0592034,"ding injection is particularly beneficial, with notable improvements on examples that require synonym resolution. 1 Introduction Before the rise of contextualised models, transfer of pre-trained information between datasets and tasks in NLP was based on word embeddings. Over many years, substantial effort was placed into the creation of such embeddings. While originally capturing mainly collocation patterns (Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for injecting pretrained linguistically-enriched embeddings into any layer of BERT. The model maps any word embeddings into the same space as BERT’s hidden represen"
2021.findings-emnlp.200,S17-2003,0,0.0292533,"sentences in a binary classification setup. We work with the following five widely used English language datasets which cover a range of sizes and tasks (including paraphrase detection, duplicate question identification and answer sentence selection, see Appendix A for details). MSRP The Microsoft Research Paraphrase dataset (MSRP) contains 5K pairs of sentences from news websites which were collected based on heuristics and an SVM classifier. Gold labels are based on human binary annotations for sentential paraphrase detection (Dolan and Brockett, 2005). SemEval The SemEval 2017 CQA dataset (Nakov et al., 2017) consists of three subtasks involving posts from the online forum Qatar Living3 . Each subtask provides an initial post as well as 10 posts which were retrieved by a search engine and annotated with binary labels by humans. The task requires the distinction between relevant and non-relevant posts. The original problem is a ranking setting, but since the gold labels are binary, we focus on a classification setup. In subtask A, the posts are questions and comments from the same thread, in an answer sentence selection setup (26K instances). Subtask B is question paraphrase detection (4K instances"
2021.findings-emnlp.200,P19-1268,1,0.853642,"TBASE . Metrics SemBERT Additionally we compare with the semantics-aware BERT model (SemBERT, Zhang et al. 2020) which uses a semantic role labeler. As the original paper reports results on different dataset versions, we ran the official code on our datasets. The longer sentences in SemEval could not fit on a single GPU due to the larger model size. Our main evaluation metric is the F1 score as this is more meaningful than accuracy for datasets with imbalanced label distributions (such as SemEval C, see Appendix A). We also report performance on difficult cases using the non-obvious F1 score (Peinelt et al., 2019). This metric distinguishes obvious from non-obvious instances in a dataset based on lexical overlap and gold labels, and calculates a separate F1 score for challenging cases. This value therefore tends to be lower than the regular F1 score. Dodge et al. (2020) recently showed that early stopping and random seeds can have considerable impact on the performance of finetuned BERT models, therefore we finetune all models for 3 epochs with early stopping (based on dev F1) and report average model performance across two different seeds. Hyperparameter settings of all BERT-based models are identical"
2021.findings-emnlp.200,2020.acl-main.630,1,0.903015,"pread success in NLP, recent studies Semantic similarity detection Semantic simihave focused on further improving BERT by larity detection is a framework for binary text pair introducing external information. Such work classification tasks such as paraphrase detection, covers a variety of application areas and technical duplicate question identification and answer senapproaches.We broadly categorise such approaches tence selection which require detecting the semaninto input-related, external and internal. Input modifications (Zhao et al., 2020; Singh et al., tic similarity between text pairs (Peinelt et al., 2020). Early semantic similarity methods used 2020; Lai et al., 2020; Ruan et al., 2020) adapt the feature-engineering techniques, exploring various information that is fed to BERT – e.g. feeding text triples separated by [SEP] tokens instead of sen- syntactic (Filice et al., 2017), semantic (Balchev et al., 2016) and lexical features (Tran et al., 2015; tence pairs as in Lai et al. (2020) – while leaving Almarwani and Diab, 2017). Subsequent work the architecture unchanged. Output modifications tried to model text pair relationships either based on (Xuan et al., 2020; Zhang et al., 2020) build on"
2021.findings-emnlp.200,D14-1162,0,0.0990297,"milarity datasets indicate that such information is beneficial and currently missing from the original model. Our qualitative analysis shows that counter-fitted embedding injection is particularly beneficial, with notable improvements on examples that require synonym resolution. 1 Introduction Before the rise of contextualised models, transfer of pre-trained information between datasets and tasks in NLP was based on word embeddings. Over many years, substantial effort was placed into the creation of such embeddings. While originally capturing mainly collocation patterns (Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for"
2021.findings-emnlp.200,N18-1202,0,0.337499,"the same space as BERT’s hidden representations, then combines them using learned gating parameters. Evaluation of this method on five semantic similarity tasks shows that injecting pre-trained dependency-based and counter-fitted embeddings can further enhance BERT’s performance. More specifically, we make the following contributions: Detecting the semantic similarity between a given text pair is at the core of many NLP tasks. It is a challenging problem due to the inherent variability of language and the limitations of surface form similarity. Recent pre-trained language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have led to noticeable improvements in semantic similarity detection and subsequent work has explored how these architectures can be fur1. We propose GiBERT - a lightweight gated ther improved. One line of work aims at model method for injecting externally pre-trained compression, making BERT smaller and accessible embeddings into BERT (section 3.1).1 while mostly preserving its performance (Xu et al., 2. We provide an ablation study and a detailed 2020; Goyal et al., 2020; Sanh et al., 2019; Aguilar analysis of the components in the injection et al., 2020; Lan"
2021.findings-emnlp.200,D19-1005,0,0.287391,"One line of work aims at model method for injecting externally pre-trained compression, making BERT smaller and accessible embeddings into BERT (section 3.1).1 while mostly preserving its performance (Xu et al., 2. We provide an ablation study and a detailed 2020; Goyal et al., 2020; Sanh et al., 2019; Aguilar analysis of the components in the injection et al., 2020; Lan et al., 2020; Chen et al., 2020). architecture (section 5). Other studies seek to further improve model performance by enhancing BERT with external infor3. We demonstrate that the proposed model immation from knowledge bases (Peters et al., 2019; proves BERT’s performance on multiple seWang et al., 2020) or additional modalities (Lu 1 et al., 2019; Lin et al., 2020). Code available at https://github.com/wuningxi/GiBERT. 2322 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2322–2336 November 7–11, 2021. ©2021 Association for Computational Linguistics mantic similarity detection datasets. In comparison to multi-head attention injection, our gated injection method uses fewer parameters while achieving comparable performance for dependency embeddings and improved results for counter-fitted embeddings (section"
2021.findings-emnlp.200,2020.tacl-1.54,0,0.115823,"resentations that are beneficial for the semantic similarity detection tasks and also contain information complementary to BERT. Embeddings such as word2vec and Glove leverage cooccurrence patterns which have been shown to be also captured by BERT (Gan et al., 2020). Recent contextualised embeddings risk redundancy with BERT due to the similarity of used approaches. We reason that linguistically-enriched embeddings are most likely to be complementary to BERT, as the model has not been explicitly trained on semantic or syntactic resources and has only partial knowledge of syntax and semantics (Rogers et al., 2020). We hence experiment with injecting dependencybased (Levy and Goldberg, 2014) and counter-fitted embeddings (Mrkši´c et al., 2016) into BERT, which have been found useful for semantic similarity modelling and other related tasks (Filice et al., 2017; Feng et al., 2017; Alzantot et al., 2018; Jin et al., 2020). The 300-dim dependency-based embeddings by Levy and Goldberg (2014) extend the SkipGram embedding algorithm proposed by Mikolov et al. (2013) by replacing linear bag-of-word contexts with dependency-based contexts which are extracted from parsed English Wikipedia sentences. As BERT has"
2021.findings-emnlp.200,S15-2038,0,0.0186345,"n and answer senapproaches.We broadly categorise such approaches tence selection which require detecting the semaninto input-related, external and internal. Input modifications (Zhao et al., 2020; Singh et al., tic similarity between text pairs (Peinelt et al., 2020). Early semantic similarity methods used 2020; Lai et al., 2020; Ruan et al., 2020) adapt the feature-engineering techniques, exploring various information that is fed to BERT – e.g. feeding text triples separated by [SEP] tokens instead of sen- syntactic (Filice et al., 2017), semantic (Balchev et al., 2016) and lexical features (Tran et al., 2015; tence pairs as in Lai et al. (2020) – while leaving Almarwani and Diab, 2017). Subsequent work the architecture unchanged. Output modifications tried to model text pair relationships either based on (Xuan et al., 2020; Zhang et al., 2020) build on increasingly complex neural architectures (Deriu BERT’s pre-trained representation by adding external information after the encoding step – e.g. and Cieliebak, 2017; Wang et al., 2017; Tan et al., 2018) or by combining both approaches through combining it with additional semantic information hybrid techniques (Wu et al., 2017a; Feng et al., as in Z"
2021.findings-emnlp.271,P17-1074,0,0.0259986,"rds – noisy or not – with respect to the training. different levels of noise (number of noisy words n ∈ {1, 2, 4, 6, 10}) during hyperparameter tuning. See Appendix B for more details. Training and Evaluation We use ADAM (Kingma and Ba, 2015) as the optimizer and adopt the noam learning rate scheduler (Vaswani et al., 2017) with a warm-up of 8000 steps. The training batch size is 64. Models are evaluated using the METEOR score (Denkowski and Lavie, 2014), which is the main metric for multimodal machine translation (Barrault et al., 2018). For the evaluation of error correction, we use ERRANT (Bryant et al., 2017) to compute the F0.5 score. During evaluation, we select the checkpoint with the best performance on the development set and generate the translation and correction using beam search of size 12. All models are implemented using nmtpytorch4 and pysimt5 . Each model is run with three random seeds and the average results are reported. Each run takes approximately 2 hours to train on an RTX 2080 Ti GPU. 5 Results 5.1 Testing for Robustness to Noise We first evaluate the robustness of standard NMT and MMT models trained on clean data by testing on the noise-injected data. This setting represents re"
2021.findings-emnlp.271,2020.emnlp-main.184,1,0.9024,"texts. 1 Introduction jeune chanson jeune enfant young [v] Abstract young song Zhenhao Li, Marek Rei, Lucia Specia Language and Multimodal AI (LAMA) Lab, Imperial College London {zhenhao.li18, marek.rei, l.specia}@imperial.ac.uk Figure 1: As showed by Caglayan et al. (2019), multimodality can help translate unknown words, but fail when there is noise in the input. The misspelled word “song” is correctly translated as “enfant” (child) when it is replaced with an unknown token, but translated literally as “chanson” (song) otherwise. is incomplete (Caglayan et al., 2019; Imankulova et al., 2020; Caglayan et al., 2020). However, as exemplified by Caglayan et al. (2019) (Figure 1), an MMT model trained on clean data was not able to handle noise. When the word “son” was misspelled as “song”, the model disregarded the visual information and used the literal translation “chanson”. The MMT model attended to the relevant region in the image and generated the intended translation “enfant” only when the noise was masked by a placeholder in the input, imitating an out-ofvocabulary (OOV) example. Neural Machine Translation (NMT) has been shown to be very sensitive to noise (Belinkov and Bisk, 2018; Michel and Neubig,"
2021.findings-emnlp.271,N19-1422,1,0.790202,"on, we describe a novel error correction training regime that can be used as an auxiliary task to further improve translation robustness. Experiments on EnglishFrench and English-German translation show that both multimodal and error correction components improve model robustness to noisy texts, while still retaining translation quality on clean texts. 1 Introduction jeune chanson jeune enfant young [v] Abstract young song Zhenhao Li, Marek Rei, Lucia Specia Language and Multimodal AI (LAMA) Lab, Imperial College London {zhenhao.li18, marek.rei, l.specia}@imperial.ac.uk Figure 1: As showed by Caglayan et al. (2019), multimodality can help translate unknown words, but fail when there is noise in the input. The misspelled word “song” is correctly translated as “enfant” (child) when it is replaced with an unknown token, but translated literally as “chanson” (song) otherwise. is incomplete (Caglayan et al., 2019; Imankulova et al., 2020; Caglayan et al., 2020). However, as exemplified by Caglayan et al. (2019) (Figure 1), an MMT model trained on clean data was not able to handle noise. When the word “son” was misspelled as “song”, the model disregarded the visual information and used the literal translation"
2021.findings-emnlp.271,W16-2359,0,0.0243152,"l., 2020). In our case, the extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at"
2021.findings-emnlp.271,D17-1105,0,0.0186055,"for the translation task. MMT Multimodal machine translation extends the framework of NMT by incorporating extra modalities, e.g. image (Specia et al., 2016a) or audio (Sulubacak et al., 2020). In our case, the extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulo"
2021.findings-emnlp.271,P17-1175,0,0.0359492,"Missing"
2021.findings-emnlp.271,P19-1642,0,0.0184778,"context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common approach to improve transla- sibility of multimodality helping NMT in dealing tion robustness is"
2021.findings-emnlp.271,P19-1425,0,0.0277514,"Missing"
2021.findings-emnlp.271,2020.acl-main.529,0,0.0670326,"Missing"
2021.findings-emnlp.271,P18-1163,0,0.0278474,"duce the types of noise injected and the error correction training method. In Section 4, we describe our experiment settings, with experiment results in Section 5, and further analysis in Section 6. data is often injected with different types of artificial noise, e.g. random word perturbations like character insertion/deletion/substitution (Belinkov and Bisk, 2018; Karpukhin et al., 2019; Passban et al., 2020; Xu et al., 2021), noise generated via back-translation (Berard et al., 2019; Vaibhav et al., 2019; Li and Specia, 2019), and adversarial examples generated by white-box generator model (Cheng et al., 2018, 2019, 2020). Even though this method has been shown to improve NMT performance on noisy data, the types of noise used thus far are not common in real data. For example, it would be highly unlikely for human authors to misspell the word “robust” as “zobust”, but such random transformations are used when synthesizing noisy training data for MT. In addition, back-translation paraphrases the texts to introduce noise, however such noise is less realistic as human-generated errors, which include mispellings and grammatical errors. In adversarial approaches for other NLP tasks, Ribeiro et al. (2020"
2021.findings-emnlp.271,W17-4718,1,0.93263,"cor ) = 1 |D| 1 |D| X − log P (y|x0 ; θmt ) (x0 ,y)∈D X − log P (x|x0 ; θcor ) (x0 ,x)∈D L(θ) = Lmt (θmt ) + λLcor (θcor ) (2) where λ ≥ 0 is the factor that controls the weight of the error correction loss, and D represents the noise-injected data consisting of triples in the form of (x, x0 , y). 4 4.1 Experiments Datasets We experiment with the Multi30K dataset (Elliott et al., 2016), using both the En-Fr and En-De language pairs. This is the standard dataset for MMT N Y and has been used in all open challenges on the P (y|x0 ; θmt ) = P (yt |y1:t−1 , x0 ; θmt ) topic (Specia et al., 2016b; Elliott et al., 2017a; t=1 Barrault et al., 2018). Following Caglayan et al. (1) M Y (2019), we use both the train and valid splits as our P (x|x0 ; θcor ) = P (xt |x1:t−1 , x0 ; θcor ) training set. The test2016-flickr set is used as our t=1 development set for checkpoint selection. For evalThe θmt represents parameters for the translation uation, we test the models on both test2017-flickr component and the θcor represents parameters and test2017-mscoco sets (Elliott et al., 2017b). We for the error correction component, with θmt = use a word-level vocabulary and build vocabular{θenc , θmt_dec }, θcor = {θenc ,"
2021.findings-emnlp.271,W19-5303,0,0.0386482,"Missing"
2021.findings-emnlp.271,D19-5543,1,0.891793,"e inputs often leading to mistranslations. To improve the robustness of NMT models, current research mostly focuses on adapting the model to noisy texts via methods such as fine-tuning (Michel and Neubig, 2018; Alam and Anastasopoulos, 2020), noiseGiven that the visual modality has been shown injection (Belinkov and Bisk, 2018; Cheng et al., to help predict unknown words, we investi2018; Karpukhin et al., 2019), and data augmenta- gate whether adding multimodal information to tion through back-translation (Berard et al., 2019; adaption-based methods would further improve Vaibhav et al., 2019; Li and Specia, 2019), etc. In translation robustness. To answer this question, these approaches, the translation model is trained we build MMT models in conjunction with noise or fine-tuned on the noisy data so that it can learn injection techniques and investigate their behaviour from the noise. However, methods using extra during training and inference on both noisy and context to help translate noisy texts have not been clean data. To further improve robustness, we exinvestigated. tend the current adversarial training method (i.e., Studies in Multimodal Machine Translation training NMT models on noisy texts) a"
2021.findings-emnlp.271,W16-3210,1,0.895537,"Missing"
2021.findings-emnlp.271,P17-2031,0,0.0192223,"ng extra modalities, e.g. image (Specia et al., 2016a) or audio (Sulubacak et al., 2020). In our case, the extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al"
2021.findings-emnlp.271,2020.wmt-1.70,0,0.0193185,"slation quality on clean texts. 1 Introduction jeune chanson jeune enfant young [v] Abstract young song Zhenhao Li, Marek Rei, Lucia Specia Language and Multimodal AI (LAMA) Lab, Imperial College London {zhenhao.li18, marek.rei, l.specia}@imperial.ac.uk Figure 1: As showed by Caglayan et al. (2019), multimodality can help translate unknown words, but fail when there is noise in the input. The misspelled word “song” is correctly translated as “enfant” (child) when it is replaced with an unknown token, but translated literally as “chanson” (song) otherwise. is incomplete (Caglayan et al., 2019; Imankulova et al., 2020; Caglayan et al., 2020). However, as exemplified by Caglayan et al. (2019) (Figure 1), an MMT model trained on clean data was not able to handle noise. When the word “son” was misspelled as “song”, the model disregarded the visual information and used the literal translation “chanson”. The MMT model attended to the relevant region in the image and generated the intended translation “enfant” only when the noise was masked by a placeholder in the input, imitating an out-ofvocabulary (OOV) example. Neural Machine Translation (NMT) has been shown to be very sensitive to noise (Belinkov and Bisk,"
2021.findings-emnlp.271,W18-6326,0,0.0563372,"Missing"
2021.findings-emnlp.271,P19-1653,1,0.830617,"(Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common approach to improve transla- sibility of multimodality helping NMT in dealing tion robustness is to train the model on noisy data, with natural noise stemming from the speech recogwhich is referred to as adversarial training. Since nition system used as a first step in their pipeline parallel data with noisy source sentences and clean approach to speech translations from videos. Their translations is difficult to obtain, the clean training results, however, were inconclusive. 1 Salesky e"
2021.findings-emnlp.271,D19-5506,0,0.248603,"ulary (OOV) example. Neural Machine Translation (NMT) has been shown to be very sensitive to noise (Belinkov and Bisk, 2018; Michel and Neubig, 2018; Ebrahimi et al., 2018), with even small perturbations in the inputs often leading to mistranslations. To improve the robustness of NMT models, current research mostly focuses on adapting the model to noisy texts via methods such as fine-tuning (Michel and Neubig, 2018; Alam and Anastasopoulos, 2020), noiseGiven that the visual modality has been shown injection (Belinkov and Bisk, 2018; Cheng et al., to help predict unknown words, we investi2018; Karpukhin et al., 2019), and data augmenta- gate whether adding multimodal information to tion through back-translation (Berard et al., 2019; adaption-based methods would further improve Vaibhav et al., 2019; Li and Specia, 2019), etc. In translation robustness. To answer this question, these approaches, the translation model is trained we build MMT models in conjunction with noise or fine-tuned on the noisy data so that it can learn injection techniques and investigate their behaviour from the noise. However, methods using extra during training and inference on both noisy and context to help translate noisy texts h"
2021.findings-emnlp.271,2021.emnlp-main.576,0,0.0415779,"al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common approach to improve transla- sibility of multimodality helping NMT in dealing tion robustness is to train the model on noisy data, with natural noise stemming from the speech recogwhich is referred to as adversarial training. Since nition system used as a first step in their pipeline parallel data with noisy source sentences and clean approach to speech translations from videos. Their translations is difficult to obtain, the clean training results, however, were inconclusive. 1 Salesky et al. (2021) investigate the robustness of Codes are available at https://github.com/ Nickeilf/Visual-Cues-Error-Correction open-vocabulary translation by representing texts 3154 clean edit-distance homophone keyboard a pink flower is starting to bloom . a pink flower is staring to loom . a pink flour is starting to bloom . a pink flower is starring to bloom . Table 1: An example of noise injected to the clean text. The noisy substitutes are marked in red. as images followed by optical character recognition to cover some cases of noise such as misspellings. This is an interesting but orthogonal area of re"
2021.findings-emnlp.271,W16-2346,1,0.834894,"Missing"
2021.findings-emnlp.271,2020.acl-main.400,0,0.029889,"extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common ap"
2021.findings-emnlp.271,N16-1042,0,0.0304574,"oder ResNet-101 noisy: a man and too girls playing on they shore of the beach noisy: a man and too girls playing on they shore of the beach en: a man and two girls playing on the shore of the beach en: a man and two girls playing on the shore of the beach Figure 2: Illustration of the joint training of machine translation and error correction for NMT and MMT models. Solid lines: translation flow. Dotted lines: error correction flow. Left: NMT with error correction training. Right: MMT with error correction training. 3.2 Error Correction Training We introduce error correction (Ng et al., 2014; Yuan and Briscoe, 2016) as an auxiliary task to help improve the robustness against noisy inputs. For that, we add a second decoder to the MT architecture, which is only used for the error correction task. During training, the noisy sentence x0 is encoded by the encoder, which is shared between the translation and correction tasks, into hidden states h0 . The hidden state representation is then fed to both decoders. The translation decoder aims to generate a correct translation y while the correction decoder aims to recover the original source sentence x. This method is also compatible with the MMT model, where the"
2021.findings-emnlp.271,N19-1190,0,0.104658,"ll perturbations in the inputs often leading to mistranslations. To improve the robustness of NMT models, current research mostly focuses on adapting the model to noisy texts via methods such as fine-tuning (Michel and Neubig, 2018; Alam and Anastasopoulos, 2020), noiseGiven that the visual modality has been shown injection (Belinkov and Bisk, 2018; Cheng et al., to help predict unknown words, we investi2018; Karpukhin et al., 2019), and data augmenta- gate whether adding multimodal information to tion through back-translation (Berard et al., 2019; adaption-based methods would further improve Vaibhav et al., 2019; Li and Specia, 2019), etc. In translation robustness. To answer this question, these approaches, the translation model is trained we build MMT models in conjunction with noise or fine-tuned on the noisy data so that it can learn injection techniques and investigate their behaviour from the noise. However, methods using extra during training and inference on both noisy and context to help translate noisy texts have not been clean data. To further improve robustness, we exinvestigated. tend the current adversarial training method (i.e., Studies in Multimodal Machine Translation training NMT mo"
2021.repl4nlp-1.20,2020.acl-main.386,0,0.0227411,"Missing"
2021.repl4nlp-1.20,W19-4828,0,0.0183357,"entences separately. The generated scores were used to perform binary classification of tokens, with the threshold based on F1 performance on the development set. The token-level predictions were evaluated against human explanations of the entailment relation using the e-SNLI dataset (Camburu et al., 2018). LIME was found to outperform other methods, however, it was also 1000× slower than attention-based methods at generating these explanations. 2.2 Attention heads The attention heads in a trained transformer model are designed to identify and combine useful information for a particular task. Clark et al. (2019) 1 https://github.com/bujol12/ bert-seq-interpretability 2.3 Soft attention Rei and Søgaard (2018) described a method for predicting token-level labels based on a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) architecture supervised at the sentence-level only. A dedicated attention module was integrated for building sentence representations, with its attention weights also acting as token-level importance scores. The architecture was found to outperform a gradient-based approach on the tasks of zero-shot sequence labeling for error detection, uncertainty detection, and sentiment analys"
2021.repl4nlp-1.20,N19-1357,0,0.0611159,"Missing"
2021.repl4nlp-1.20,C18-1328,0,0.0184561,". We apply LIME to a RoBERTa model supervised as a sentence classifier and investigate whether its scores can be used for sequence labeling. We use RoBERTa’s MASK token to mask out individual words and allow LIME to generate 5000 masked samples per sentence. The resulting explanation weights are then used as classification scores for each word, with the decision threshold fine-tuned based on the development set performance. Thorne et al. (2019) found LIME to outperform attention-based approaches on the task of explaining NLI models. LIME was used to probe a LSTMbased sentence-pair classifier (Lan and Xu, 2018) by removing tokens from the premise and hypothesis sentences separately. The generated scores were used to perform binary classification of tokens, with the threshold based on F1 performance on the development set. The token-level predictions were evaluated against human explanations of the entailment relation using the e-SNLI dataset (Camburu et al., 2018). LIME was found to outperform other methods, however, it was also 1000× slower than attention-based methods at generating these explanations. 2.2 Attention heads The attention heads in a trained transformer model are designed to identify a"
2021.repl4nlp-1.20,2021.ccl-1.108,0,0.0582218,"Missing"
2021.repl4nlp-1.20,N18-1027,1,0.916053,"with the threshold based on F1 performance on the development set. The token-level predictions were evaluated against human explanations of the entailment relation using the e-SNLI dataset (Camburu et al., 2018). LIME was found to outperform other methods, however, it was also 1000× slower than attention-based methods at generating these explanations. 2.2 Attention heads The attention heads in a trained transformer model are designed to identify and combine useful information for a particular task. Clark et al. (2019) 1 https://github.com/bujol12/ bert-seq-interpretability 2.3 Soft attention Rei and Søgaard (2018) described a method for predicting token-level labels based on a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) architecture supervised at the sentence-level only. A dedicated attention module was integrated for building sentence representations, with its attention weights also acting as token-level importance scores. The architecture was found to outperform a gradient-based approach on the tasks of zero-shot sequence labeling for error detection, uncertainty detection, and sentiment analysis. In order to obtain a single raw attention value eei for each token, biLSTM output vectors were"
2021.repl4nlp-1.20,P16-1112,1,0.888283,"Missing"
2021.repl4nlp-1.20,N16-3020,0,0.0554086,"tance for the overall task. Given a particular head, we can obtain an importance score for each token by averaging the attention scores from all the tokens that attend to it. In order to investigate the best possible setting, we report results for the attention head that achieves the highest token-level Mean Average Precision score on the development set. • We make our source code and models publicly available to facilitate further research in the field.1 2 Methods We evaluate four different methods for turning sentence-level transformer models into zero-shot sequence labelers. 2.1 LIME LIME (Ribeiro et al., 2016) generates local wordlevel importance scores through a meta-model that is trained on perturbed data generated by randomly masking out words in the input sentence. It was originally investigated in the context of Support Vector Machine (Hearst et al., 1998) text classifiers with unigram features. We apply LIME to a RoBERTa model supervised as a sentence classifier and investigate whether its scores can be used for sequence labeling. We use RoBERTa’s MASK token to mask out individual words and allow LIME to generate 5000 masked samples per sentence. The resulting explanation weights are then use"
C16-1030,D15-1041,0,0.0206421,"ch into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim et al. (2016) implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level. Cao and Rei (2016) proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters. There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, wh"
C16-1030,W16-1603,1,0.0607575,"atasets. Character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim et al. (2016) implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level. Cao and Rei (2016) proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters. There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from"
C16-1030,D14-1080,0,0.085782,"by useful for a wide range of downstream applications. Work in this area has traditionally involved task-specific feature engineering – for example, integrating gazetteers for named entity recognition, or using features from a morphological analyser in POS-tagging. Recent developments in neural architectures and representation learning have opened the door to models that can discover useful features automatically from the data. Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results (Collobert et al., 2011; Irsoy and Cardie, 2014). Current neural models generally make use of word embeddings, which allow them to learn similar representations for semantically or functionally similar words. While this is an important improvement over count-based models, they still have weaknesses that should be addressed. The most obvious problem arises when dealing with out-of-vocabulary (OOV) words – if a token has never been seen before, then it does not have an embedding and the model needs to back-off to a generic OOV representation. Words that have been seen very infrequently have embeddings, but they will likely have low quality du"
C16-1030,W04-1213,0,0.127232,".54 87.98 84.21 87.75 87.99 78.63 82.80 83.75 79.74 83.56 84.53 75.46 76.82 77.38 70.75 72.24 72.70 97.55 98.59 98.67 97.39 98.49 98.60 Table 2: Comparison of word-based and character-based sequence labeling architectures on 8 datasets. The evaluation measure used for each dataset is specified in Section 6. • CHEMDNER: The BioCreative IV Chemical and Drug (Krallinger et al., 2015) NER corpus consists of 10,000 abstracts annotated for mentions of chemical and drug names using a single class. We make use of the official splits provided by the shared task organizers. • JNLPBA: The JNLPBA corpus (Kim et al., 2004) consists of 2,404 biomedical abstracts and is annotated for mentions of five entity types: CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN. The corpus was derived from GENIA corpus entity annotations for use in the shared task organized in conjuction with the BioNLP 2004 workshop. • GENIA-POS: The GENIA corpus (Ohta et al., 2002) is one of the most widely used resources for biomedical NLP and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events. Here, we make use of the GENIA POS annotations, which cover 2,000 PubMed abstracts (approx. 20,0"
C16-1030,N16-1030,0,0.275407,"ber 11-17 2016. We evaluate the neural models on 8 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that including a character-based component in the sequence labeling model provides substantial performance improvements on all the benchmarks. In addition, the attention-based architecture achieves the best results on all evaluations, while requiring a smaller number of parameters. 2 Bidirectional LSTM for sequence labeling We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information. Figure 1 shows the general architecture of the sequence labeling network. The model receives a sequence of tokens (w1 , ..., wT ) as input, and predicts a label corresponding to each of the input tokens. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings (x1 , ..., xT ). Next, the embeddings are given as input to two LSTM (Hochreiter and Schmidhuber, 1997) components moving in opposite directions through the text, creating"
C16-1030,D15-1176,0,0.0909934,"Missing"
C16-1030,J93-2004,0,0.0531267,": The CoNLL-2000 dataset (Tjong Kim Sang and Buchholz, 2000) is a frequently used benchmark for the task of chunking. Wall Street Journal Sections 15-18 from the Penn Treebank are used for training, and Section 20 as the test data. As there is no official development set, we separated some of the training set for this purpose. • CoNLL03: The CoNLL-2003 corpus (Tjong Kim Sang and De Meulder, 2003) was created for the shared task on language-independent NER. We use the English section of the dataset, containing news stories from the Reuters Corpus1 . • PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal, annotated for part-of-speech tags. The PTB label set includes 36 main tags and an additional 12 tags covering items such as punctuation. • FCEPUBLIC: The publicly released subset of the First Certificate in English (FCE) dataset contains short essays written by language learners and manual corrections by examiners (Yannakoudakis et al., 2011). We use a version of this corpus converted into a binary error detection task, where each token is labeled as being correct or incorrect in the given context. • BC2GM: The BioCreative II Gene Mention corpus (S"
C16-1030,D16-1209,0,0.0423358,"laces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, where the representations are combined using an attention mechanism, and evaluated both approaches on a range of tasks and datasets. Recently, Miyamoto and Cho (2016) have also described a related method for the task of language modelling, combining characters and word embeddings using gating. 9 Conclusion Developments in neural network research allow for model architectures that work well on a wide range of sequence labeling datasets without requiring hand-crafted data. While word-level representation learning is a powerful tool for automatically discovering useful features, these models still come with certain weaknesses – rare words have low-quality representations, previously unseen words cannot be modeled at all, and morpheme-level information is not"
C16-1030,P16-1112,1,0.425,"te the neural models on 8 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that including a character-based component in the sequence labeling model provides substantial performance improvements on all the benchmarks. In addition, the attention-based architecture achieves the best results on all evaluations, while requiring a smaller number of parameters. 2 Bidirectional LSTM for sequence labeling We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information. Figure 1 shows the general architecture of the sequence labeling network. The model receives a sequence of tokens (w1 , ..., wT ) as input, and predicts a label corresponding to each of the input tokens. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings (x1 , ..., xT ). Next, the embeddings are given as input to two LSTM (Hochreiter and Schmidhuber, 1997) components moving in opposite directions through the text, creating context-specific representations"
C16-1030,W00-0726,0,0.201026,"ter-based model to shift towards predicting high-quality word embeddings, it is not desireable to optimise the word embeddings towards the character-level representations. This can be achieved by making sure that the optimisation is performed only in one direction; in Theano (Bergstra et al., 2010), the disconnected grad function gives the desired effect. 5 Datasets We evaluate the sequence labeling models and character architectures on 8 different datasets. Table 1 contains information about the number of labels and dataset sizes for each of them. • CoNLL00: The CoNLL-2000 dataset (Tjong Kim Sang and Buchholz, 2000) is a frequently used benchmark for the task of chunking. Wall Street Journal Sections 15-18 from the Penn Treebank are used for training, and Section 20 as the test data. As there is no official development set, we separated some of the training set for this purpose. • CoNLL03: The CoNLL-2003 corpus (Tjong Kim Sang and De Meulder, 2003) was created for the shared task on language-independent NER. We use the English section of the dataset, containing news stories from the Reuters Corpus1 . • PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Jou"
C16-1030,P11-1019,0,0.0312929,"e Meulder, 2003) was created for the shared task on language-independent NER. We use the English section of the dataset, containing news stories from the Reuters Corpus1 . • PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal, annotated for part-of-speech tags. The PTB label set includes 36 main tags and an additional 12 tags covering items such as punctuation. • FCEPUBLIC: The publicly released subset of the First Certificate in English (FCE) dataset contains short essays written by language learners and manual corrections by examiners (Yannakoudakis et al., 2011). We use a version of this corpus converted into a binary error detection task, where each token is labeled as being correct or incorrect in the given context. • BC2GM: The BioCreative II Gene Mention corpus (Smith et al., 2008) consists of 20,000 sentences from biomedical publication abstracts and is annotated for mentions of the names of genes, proteins and related entities using a single NE class. 1 http://about.reuters.com/researchandstandards/corpus/ 313 CoNLL00 Word-based Char concat Char attention PTB-POS FCEPUBLIC TEST DEV TEST DEV TEST DEV TEST 91.48 92.57 92.92 91.23 92.35 92.67 86.8"
C16-1030,W04-1219,0,0.0523839,"Missing"
D15-1026,D13-1176,0,0.0513528,"odel keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found t"
D15-1026,P14-1062,0,0.0162822,"aptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found that a recurrent neural network language model (RNNLM) combined with a 9-gram MaxEnt model"
D17-1162,P16-2017,1,0.746585,"l associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014"
D17-1162,E17-2084,1,0.556468,"sentations for the metaphor identification task via supervised training; (3) quantifies metaphoricity via a weighted similarity function that automatically selects the relevant dimensions of similarity. We experimented with two types of word representations 1537 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1537–1546 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics as inputs to the network: the standard skip-gram word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof. We evaluate our method in the metaphor identification task, focusing on adjective–noun, verb– subject and verb–direct object constructions where the verbs and adjectives can be used metaphorically. Our results show that our architecture outperforms both a metaphor agnostic deep learning baseline (a basic feed forward network) and the previous corpus-based approaches to metaphor identification. We also investigate the effects of training data on this task, and demonstrate that with a sufficiently large training set our method also outperforms the best existin"
D17-1162,W16-1104,0,0.224064,"cepts from corpora. For example, the feature vector for politics would contain GAME or MECHA NISM terms among the frequent features. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain. Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples. Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way. Do Dinh and Gurevych (2016) investigated metaphors through the task of sequence labelling, detecting metaphor related words in context. Guti´errez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework. Their method learns metaphors as linear transformations in a vector space and they demonstrated that it produces superior phrase representations for both metaphorical and literal language, as compared to the traditional ”single-sense” compositional distributional model. They then used these representations in the metaphor identification task, achieving promising result"
D17-1162,W15-0107,0,0.0198019,"misclassified examples. The diagram of the complete network can be seen in Figure 1. 4 Word Representations Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations. The word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair. We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015). These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space. 5 Datasets We evaluate our method using two datasets of phrases manually annotated for metaphoricity. Literal bloody nose cold weather dry skin empty can frosty morning hot chocolate gold coin soft leather sour cherry steep hill Table 2: Annotated adjective–noun pairs from TSV- TEST . Since these datasets include examples for different senses (both metaphorical and literal) of"
D17-1162,W06-3506,0,0.539391,"ge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-"
D17-1162,W13-0907,0,0.669226,"concept). Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word em"
D17-1162,D14-1005,1,0.818881,"to the traditional ”single-sense” compositional distributional model. They then used these representations in the metaphor identification task, achieving promising results. The more recent approaches of Shutova et al. (2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features. Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features. They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively. They then measured the difference between the phrase representation and those of its component words in terms of their cosine similarity, which served as a predictor of metaphoricity. They found basic cosine similarity between the component words in the phrase to be a powerful measure – the neural embeddings of the words were compared with cosine similar1538 Figure 1: The network architecture for supervised metaphorical phrase classification. The symbol is used to indicate element-wise multiplication. ity and a threshold was tuned on the development set to distinguish between li"
D17-1162,S16-2003,1,0.553565,"Missing"
D17-1162,N16-1020,1,0.690494,"nd higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to impro"
D17-1162,N13-1118,1,0.956289,"al and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a s"
D17-1162,C10-1113,1,0.957575,"tures, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which sug"
D17-1162,shutova-teufel-2010-metaphor,1,0.874656,"ns between two distinct concepts or domains. For instance, when we talk about “curing juvenile delinquency” or “corruption transmitting through the government ranks”, we view the general concept of crime (the target concept) in terms of the properties of a disease (the source concept). Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-eng"
D17-1162,W13-0909,0,0.0517548,"s. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patt"
D17-1162,P14-1024,0,0.734219,"n important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We"
D17-1162,D11-1063,0,0.371488,"guage is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about"
D17-1162,W13-0904,0,0.0694063,"nd event status ( PROCESS , STATE , OBJECT ). Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features. They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of sentence trees. Mohler et al. (2013) aimed at modelling conceptual information. They derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets. The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that mapped new metaphors to the semantic signatures of the known ones. With the aim of reducing the dependence on manually-annotated lexical resources, other research focused on modelling metaphor using corpus-driven information alone. Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large po"
D17-1162,W14-1608,1,0.811713,"using annotated metaphor examples, resulting in word representations that are more suitable for this task. Furthermore, the adjectives and nouns use separate mapping weights, which allows the model to better distinguish between the different functionalities of these words. In contrast, the original cosine similarity is not position-specific and would give the same result regardless of the word order. 3.3 Metaphorical absorb cost attack problem attack cancer breathe life design excuse deflate economy leak news swallow anger Table 1: Annotated verb-direct object and verbsubject pairs from MOH. Rei and Briscoe (2014) used a fixed formula to calculate weights for different dimensions of cosine similarity and showed that it helped in recovering hyponym relations. We extend this even further and allow the network to use multiple different weighting strategies which are all optimised during training. This is done by first creating a vector m, which is an element-wise multiplication of the two word representations: mi = z1,i z2,i d = γ(Wd m) If the vectors x1 and x2 are normalised to unit length, the cosine similarity between them is equal to their dot product, which in turn is equal to their elementwise multi"
D17-1297,W07-1604,0,0.0155587,"res that require no linguistic information, in contrast to previous work that has utilised a large set of features in a supervised setting (Hoang et al., 2016; Yuan et al., 2016). 2 Previous work The first approaches to GEC primarily treat the task as a classification problem over vectors of contextual lexical and syntactic features extracted from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron cla"
D17-1297,P06-1032,0,0.480342,"L 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-f"
D17-1297,D16-1195,0,0.0250888,"Missing"
D17-1297,D11-1010,0,0.0316973,"iors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number"
D17-1297,D12-1052,0,0.302328,"b agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number, verb form, and subject–verb agreement errors. The SMT approach has better capacity to correct complex errors, and it onl"
D17-1297,N12-1067,0,0.427367,"b agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number, verb form, and subject–verb agreement errors. The SMT approach has better capacity to correct complex errors, and it onl"
D17-1297,W13-1703,0,0.314071,"eural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of each token in a sentence as being correct or incorrect; using the error detection model, we propose a small set of features that require no linguistic processing to re-rank the N best hypotheses. We evaluate our approach on three different GEC datasets and achieve stateof-the-art results, outperforming all previous approaches to GEC. 3 Datasets We use the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) that was used in the CoNLL GEC shared tasks. Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly ava"
D17-1297,W11-2838,0,0.0383093,"error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and"
D17-1297,C08-1022,0,0.384912,"Missing"
D17-1297,W14-1702,1,0.236149,"different datasets, and it has the additional advantage of only using a small set of easily computed features that require no linguistic input. 1 Introduction Grammatical Error Correction (GEC) in nonnative text attempts to automatically detect and correct errors that are typical of those found in learner writing. High precision and good coverage of learner errors is important in the development of GEC systems. Phrase-based Statistical Machine Translation (SMT) approaches to GEC have attracted considerable attention in recent years as they have been shown to achieve state-of-the-art results (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016). Given an ungrammatical input sentence, the task is formulated as “translating” it to its grammatical counterpart. Using a parallel dataset of input sentences and their corrected counterparts, SMT systems are typically trained to correct all error types in text without requiring any further linguistic input. To further adapt SMT approaches to the task of GEC and tackle the paucity of error-annotated learner data, previous work has investigated a number of extensions, ranging from the addition of further features into the decoding process (Felice et al."
D17-1297,N10-1019,0,0.0251462,"ow around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examp"
D17-1297,I08-1059,0,0.0352658,"ed from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to"
D17-1297,D16-1161,0,0.654357,"and it has the additional advantage of only using a small set of easily computed features that require no linguistic input. 1 Introduction Grammatical Error Correction (GEC) in nonnative text attempts to automatically detect and correct errors that are typical of those found in learner writing. High precision and good coverage of learner errors is important in the development of GEC systems. Phrase-based Statistical Machine Translation (SMT) approaches to GEC have attracted considerable attention in recent years as they have been shown to achieve state-of-the-art results (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016). Given an ungrammatical input sentence, the task is formulated as “translating” it to its grammatical counterpart. Using a parallel dataset of input sentences and their corrected counterparts, SMT systems are typically trained to correct all error types in text without requiring any further linguistic input. To further adapt SMT approaches to the task of GEC and tackle the paucity of error-annotated learner data, previous work has investigated a number of extensions, ranging from the addition of further features into the decoding process (Felice et al., 2014) via reranking the SMT decoder’s o"
D17-1297,N03-1017,0,0.0389189,".69 26.53 CoNLL test annotation 2 P R F0.5 23.60 25.10 23.90 27.62 21.18 25.88 69.60 7.91 27.18 Table 1: Token-level error detection performance of our detection models (LSTMFCE and LSTM) on FCE and the two CoNLL 2014 test set annotations. Baseline LSTMFCE and LSTMFCE are trained only on the public FCE training set. A Language Model (LM) is used to estimate the correction hypothesis probability pLM (c) from a corpus of correct English, and a translation model to estimate the conditional p(s|c) from a parallel corpus of corrected learner sentences. Stateof-the-art SMT systems are phrase-based (Koehn et al., 2003) in that they use phrases as “translation” units and therefore allow many-to-many “translation” mappings. The translation model is decomposed into a phrase-translation probability model and a phrase re-ordering probability model, and the 1-best correction hypothesis is of the following log-linear form (Och and Ney, 2002): ∗ c = arg max exp c K ∑ λi hi (c, s) (14) i=1 where h represents a feature function (e.g., phrasetranslation probability) and λ the feature weight. In this work, we employ two SMT systems: Yuan et al. (2016)2 and Junczys-Dowmunt and Grundkiewicz (2016). We apply our re-rankin"
D17-1297,C12-2084,0,0.0335458,"CoNLL GEC shared tasks. Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly available Lang-8 corpus (Mizumoto et al., 2012; Tajiri et al., 2012) and the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). Lang-8 contains learner English from lang-8.com, a languagelearning social networking service, which has been corrected by native speakers. JFLEG is a newly released corpus for GEC evaluation that contains fluency edits to make the text more native-like in addition to correcting grammatical errors, and contains learner data from a range of proficiency levels. We use Lang-8 and the FCE and CoNLL training sets to train our neural sequence-labelling model, and test correction performance on JFLEG, and t"
D17-1297,N16-1133,0,0.030573,"substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather than discrete ones, and learn non-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation (NMT) model and propose an approach that tackles the rare-word problem in NMT. Yuan et al. (2016) and Mizumoto and Matsumoto (2016) employ supervised discriminative methods to re-rank the SMT decoder’s N -best list output based on language model and syntactic features respectively. Hoang et al. (2016) also exploit syntactic features in a supervised framework, but further extend their approach to generate new hypotheses. Our approach is similar in spirit, but differs in the following aspects: inspired by the work of Rei and Yannakoudakis (2016) who tackle error detection rather than correction within a neural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of"
D17-1297,P15-2097,0,0.0456016,"ate hypothesis c according to feature i; λ is a parameter that controls the effect feature i has on the final ranking; and K = 4 as we have four different features (three features presented in this section, plus the original score output by the SMT system). λs are tuned on the FCE development set and are set to 1, except for the sentence probability feature which has λ = 1.5.3 6 Evaluation We evaluate the effectiveness of our re-ranking approach on three different datasets: FCE, CoNLL 2014 and JFLEG. We report F0.5 using the shared task’s M 2 scorer (Dahlmeier and Ng, 2012b), and GLEU scores (Napoles et al., 2015). The latter is based on a variant of BLEU (Papineni et al., 2002) that is designed to reward correct edits and penalise ungrammatical ones. As mentioned in Section 5, we re-rank the 10-best lists of two SMT systems: Yuan et al. (2016) (CAMB16SMT ) and Junczys-Dowmunt and Grundkiewicz (2016) (AMU16SMT ). The results are presented in Table 2. We replicate the AMU16SMT system to obtain the 10-best output, and report results using this 3 We experimented with a small set of values (from 0 to 2 with increments of .1), though not exhaustively. version (AMU16SMT (replicated) ). Compared to the origin"
D17-1297,E17-2037,0,0.172934,"ggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly available Lang-8 corpus (Mizumoto et al., 2012; Tajiri et al., 2012) and the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). Lang-8 contains learner English from lang-8.com, a languagelearning social networking service, which has been corrected by native speakers. JFLEG is a newly released corpus for GEC evaluation that contains fluency edits to make the text more native-like in addition to correcting grammatical errors, and contains learner data from a range of proficiency levels. We use Lang-8 and the FCE and CoNLL training sets to train our neural sequence-labelling model, and test correction performance on JFLEG, and the FCE and CoNLL test sets. For JFLEG, we use the 754 sentences on which Napoles et al. (2017"
D17-1297,W14-1701,0,0.362538,"o correct complex errors, and it only requires parallel corrected sentences as input. Two state-of-the-art systems in the 2014 CoNLL shared task on correction of all errors regardless of type use SMT systems: Felice et al. (2014) use a hybrid approach that includes a rule-based and an SMT system augmented by a large web-based language model and combined with correction-type estimation to filter out error types with zero precision. Junczys-Dowmunt and Grundkiewicz (2016) investigate parameter tuning based on the MaxMatch (M 2 ) scorer, the sharedtask evaluation metric (Dahlmeier and Ng, 2012b; Ng et al., 2014), and experiment with different optimisers and interactions of dense and sparse features. Susanto et al. (2014) and Rozovskaya and Roth (2016) explore combinations of SMT systems and classifiers, the latter showing substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather"
D17-1297,W13-3601,0,0.19139,"-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data"
D17-1297,P16-1208,0,0.384563,"lgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise"
D17-1297,W11-2843,0,0.0380695,"Missing"
D17-1297,P02-1038,0,0.0436227,"guage Model (LM) is used to estimate the correction hypothesis probability pLM (c) from a corpus of correct English, and a translation model to estimate the conditional p(s|c) from a parallel corpus of corrected learner sentences. Stateof-the-art SMT systems are phrase-based (Koehn et al., 2003) in that they use phrases as “translation” units and therefore allow many-to-many “translation” mappings. The translation model is decomposed into a phrase-translation probability model and a phrase re-ordering probability model, and the 1-best correction hypothesis is of the following log-linear form (Och and Ney, 2002): ∗ c = arg max exp c K ∑ λi hi (c, s) (14) i=1 where h represents a feature function (e.g., phrasetranslation probability) and λ the feature weight. In this work, we employ two SMT systems: Yuan et al. (2016)2 and Junczys-Dowmunt and Grundkiewicz (2016). We apply our re-ranking approach to each SMT system’s N -best list using features derived from the neural sequencelabelling model for error detection described in the previous section, improve each of the SMT systems, and achieve state-of-the-art results on all three GEC datasets: FCE, CoNLL and JFLEG. 5.1 N -best list re-ranking For each SMT"
D17-1297,W12-2032,0,0.0440396,"of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated l"
D17-1297,P02-1040,0,0.115851,"ntrols the effect feature i has on the final ranking; and K = 4 as we have four different features (three features presented in this section, plus the original score output by the SMT system). λs are tuned on the FCE development set and are set to 1, except for the sentence probability feature which has λ = 1.5.3 6 Evaluation We evaluate the effectiveness of our re-ranking approach on three different datasets: FCE, CoNLL 2014 and JFLEG. We report F0.5 using the shared task’s M 2 scorer (Dahlmeier and Ng, 2012b), and GLEU scores (Napoles et al., 2015). The latter is based on a variant of BLEU (Papineni et al., 2002) that is designed to reward correct edits and penalise ungrammatical ones. As mentioned in Section 5, we re-rank the 10-best lists of two SMT systems: Yuan et al. (2016) (CAMB16SMT ) and Junczys-Dowmunt and Grundkiewicz (2016) (AMU16SMT ). The results are presented in Table 2. We replicate the AMU16SMT system to obtain the 10-best output, and report results using this 3 We experimented with a small set of values (from 0 to 2 with increments of .1), though not exhaustively. version (AMU16SMT (replicated) ). Compared to the original results on CoNLL reported in their paper (AMU16SMT (reported) )"
D17-1297,D14-1102,0,0.0791023,"systems in the 2014 CoNLL shared task on correction of all errors regardless of type use SMT systems: Felice et al. (2014) use a hybrid approach that includes a rule-based and an SMT system augmented by a large web-based language model and combined with correction-type estimation to filter out error types with zero precision. Junczys-Dowmunt and Grundkiewicz (2016) investigate parameter tuning based on the MaxMatch (M 2 ) scorer, the sharedtask evaluation metric (Dahlmeier and Ng, 2012b; Ng et al., 2014), and experiment with different optimisers and interactions of dense and sparse features. Susanto et al. (2014) and Rozovskaya and Roth (2016) explore combinations of SMT systems and classifiers, the latter showing substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather than discrete ones, and learn non-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation"
D17-1297,P17-1194,1,0.776657,"m (7) where Wz1 , Wz2 and Wz3 are weight matrices, z is a dynamically calculated gating vector, and x et is the resulting token representation at position t. We optimise the model by minimising crossentropy between the predicted label distributions and the annotated labels. In addition to training the error detection objective, we make use of a multi-task loss function and train specific parts of the architecture as language models. This provides the model with a more informative loss function, while also encouraging it to learn more general compositional features and acting as a regulariser (Rei, 2017). First, two extra hidden layers are constructed: 2797 → → − − → = tanh − m Wm ht t (8) 4.1 Experimental settings Figure 1: Error detection network architecture that is repeated for all the words in a sentence (illustration for the word “cat”). − ← − ← ← m−t = tanh Wm ht (9) − → ← − where Wm and Wm are direction-specific weight matrices, used for connecting a forward or backward LSTM hidden state to a separate layer. The surrounding tokens are then predicted based on each hidden state using a softmax output layer: − → → P (wt+1 |w1 ...wt ) = softmax Wq − mt (10) ← − − P (wt−1 |wt ...wT ) = sof"
D17-1297,P12-2039,0,0.136333,"Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly available Lang-8 corpus (Mizumoto et al., 2012; Tajiri et al., 2012) and the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). Lang-8 contains learner English from lang-8.com, a languagelearning social networking service, which has been corrected by native speakers. JFLEG is a newly released corpus for GEC evaluation that contains fluency edits to make the text more native-like in addition to correcting grammatical errors, and contains learner data from a range of proficiency levels. We use Lang-8 and the FCE and CoNLL training sets to train our neural sequence-labelling model, and test correction performance on JFLEG, and the FCE and CoNLL test"
D17-1297,C16-1030,1,0.0986294,"STM, ht is the hidden state of the backwardmoving LSTM, and ht is the concatenation of both hidden states. A feedforward hidden layer with tanh activation is then used to map the representations from both directions into a more suitable combined space, and allow the model to learn higher-level features: dt = tanh Wd ht (4) where Wd is a weight matrix. Finally, a softmax output layer predicts the label distribution for each token, given the input sequence: P (yt |w1 ...wT ) = softmax Wo dt (5) where Wo is an output weight matrix. We also make use of the character-level architecture proposed by Rei et al. (2016), allowing the model to learn morphological patterns and capture out-of-vocabulary words. Each individual character is mapped to a character embedding and a bidirectional LSTM is used to combine them together into a character-based token representation. This vector m, constructed only from individual characters, is then combined with the regular token embedding xt using an adaptive gating mechanism: ( ) z = σ Wz1 · tanh(Wz2 xt + Wz3 m) (6) x et = z · xt + (1 − z) · m (7) where Wz1 , Wz2 and Wz3 are weight matrices, z is a dynamically calculated gating vector, and x et is the resulting token re"
D17-1297,P10-2065,0,0.0766109,"roblem over vectors of contextual lexical and syntactic features extracted from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the"
D17-1297,P16-1112,1,0.853892,"-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation (NMT) model and propose an approach that tackles the rare-word problem in NMT. Yuan et al. (2016) and Mizumoto and Matsumoto (2016) employ supervised discriminative methods to re-rank the SMT decoder’s N -best list output based on language model and syntactic features respectively. Hoang et al. (2016) also exploit syntactic features in a supervised framework, but further extend their approach to generate new hypotheses. Our approach is similar in spirit, but differs in the following aspects: inspired by the work of Rei and Yannakoudakis (2016) who tackle error detection rather than correction within a neural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of each token in a sentence as being correct or incorrect; using the error detection model, we propose a small set of features that require no linguistic processing to re-rank the N best hypotheses. We evaluate our approach on three different GEC datasets and achieve stateof-the-art results, outperforming all previous approaches to GEC. 3 Datasets We use the First Certificate in English (FCE) dataset (Yannakoudakis e"
D17-1297,C08-1109,0,0.0161468,"and syntactic features extracted from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability t"
D17-1297,W14-1704,0,0.0477036,"Missing"
D17-1297,D10-1094,0,0.0212298,"target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily re"
D17-1297,P11-1093,0,0.0170905,"008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained bea"
D17-1297,P11-1019,1,0.86773,"koudakis (2016) who tackle error detection rather than correction within a neural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of each token in a sentence as being correct or incorrect; using the error detection model, we propose a small set of features that require no linguistic processing to re-rank the N best hypotheses. We evaluate our approach on three different GEC datasets and achieve stateof-the-art results, outperforming all previous approaches to GEC. 3 Datasets We use the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) that was used in the CoNLL GEC shared tasks. Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has b"
D17-1297,N16-1042,1,0.785741,"ractions of dense and sparse features. Susanto et al. (2014) and Rozovskaya and Roth (2016) explore combinations of SMT systems and classifiers, the latter showing substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather than discrete ones, and learn non-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation (NMT) model and propose an approach that tackles the rare-word problem in NMT. Yuan et al. (2016) and Mizumoto and Matsumoto (2016) employ supervised discriminative methods to re-rank the SMT decoder’s N -best list output based on language model and syntactic features respectively. Hoang et al. (2016) also exploit syntactic features in a supervised framework, but further extend their approach to generate new hypotheses. Our approach is similar in spirit, but differs in the following aspects: inspired by the work of Rei and Yannakoudakis (2016) who tackle e"
D17-1297,W16-0530,1,0.489914,"an ungrammatical input sentence, the task is formulated as “translating” it to its grammatical counterpart. Using a parallel dataset of input sentences and their corrected counterparts, SMT systems are typically trained to correct all error types in text without requiring any further linguistic input. To further adapt SMT approaches to the task of GEC and tackle the paucity of error-annotated learner data, previous work has investigated a number of extensions, ranging from the addition of further features into the decoding process (Felice et al., 2014) via reranking the SMT decoder’s output (Yuan et al., 2016) to neural-network adaptation components to SMT (Chollampatt et al., 2016a). In this paper, we propose an approach to N -best list re-ranking using neural sequence-labelling models. N -best list re-ranking allows for fast experimentation since the decoding process remains unchanged and only needs to be performed once. Crucially, it can be applied to any GEC system that can produce multiple alternative hypotheses. More specifically, we train a neural compositional model for error detection that calculates the probability of each token in a sentence being correct or incorrect, utilising the full"
D17-1297,W13-3607,1,0.685519,"he one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number, verb form, and subject–verb agreement errors. The SMT approach has better capacity to correct complex errors, and it only requires parallel corrected sentences as input. Two state-of-the-art systems in the 2014 CoNLL shared task on correction of all errors regardless of type use SMT systems: Felice et al. (2014) use a hybrid approach that includes a rule-based and an SMT system augmented by a large web-based language model and combined with correc"
D17-1297,P17-1074,0,\N,Missing
D19-1125,W17-5506,0,0.0356549,"process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to learn Dialogue State Tracking (DST) jointly with the text generation component (Eric et al., 2017; Wu et al., 2019), the most effective models use it as an intermediate signal (Wen et al., 2017; Lei et al., 2018). The difficulty of state tracking has made this task a driving force behind most of the Dialog System Technology Challenges in recent years (Henderson et al., 2014; Kim et al., 2017). In this paper, we reduce the reliance of taskoriented dialogue systems on data collection by leveraging semi-supervised training (Chapelle et al., 2009). Two approaches are investigated and evaluated for providing an improved training signal to the dialogue state tracking component in an end-to-end"
D19-1125,W10-4334,0,0.0889143,"Missing"
D19-1125,P18-1133,0,0.0505559,"en et al., 2015). Recent progress in sequence-to-sequence (seq2seq) modelling has enabled the development of fully neural end-to-end architectures, allowing for different components to be optimized jointly in order to share information (Wen et al., 2017; Zhao et al., 2017; Budzianowski and Vuli´c, 2019). Dialogue systems benefit greatly from optimizing on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to le"
D19-1125,W15-4640,0,0.030933,"on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to learn Dialogue State Tracking (DST) jointly with the text generation component (Eric et al., 2017; Wu et al., 2019), the most effective models use it as an intermediate signal (Wen et al., 2017; Lei et al., 2018). The difficulty of state tracking has made this task a driving force behind most of the Dialog System Technology Challenges in recent years (Hen"
D19-1125,D15-1166,0,0.00765738,"rrectly recognize the mentioned slot-value pairs in the user utterance and to maintain the updated dialogue (belief) state. Let i, j and k denote the index of domain, slot and value. As depicted at the top of Figure 1, the user utterance w1 :wL at turn t is first encoded by the BiLSTM to obtain the hidden states ht1:L . The encoding of the slot-value pair svkij is the output of the affine layer that takes the concatenation of the embeddings of domain i, slot j and value k as the input. The context vector aij k is then computed by the attention mechanism, denoted as attn in Figure 1, following Luong et al. (2015): el = sim(hl , svkij ) aij k = L ∑ el hl , (1) (2) l=1 where l is the word index of the user utterance and sim denotes any function that calculates the similarity of two vectors. We adopt here the dot product function, following Mrkši´c et al. (2017); Zhong et al. (2018); Ramadan et al. (2018). The ij ij similarity score sij k between ak and svk is then computed to see whether the slot-value pair svkij is mentioned in the utterance. The mentioned pair should have higher similarity score to its context vector than those which are not mentioned. The softmax layer is then applied to form the pro"
D19-1125,P17-1163,0,0.0997314,"Missing"
D19-1125,P18-2069,1,0.913302,"Missing"
D19-1125,P17-1061,0,0.0312897,"ts or providing information to visitors in a new city (Raux et al., 2005). Most current industry-oriented systems rely on modular, domain-focused frameworks (Young et al., 2013; Sarikaya et al., 2016), with separate components for user understanding (Henderson et al., 2014), decision making (Gaši´c et al., 2010) and system answer generation (Wen et al., 2015). Recent progress in sequence-to-sequence (seq2seq) modelling has enabled the development of fully neural end-to-end architectures, allowing for different components to be optimized jointly in order to share information (Wen et al., 2017; Zhao et al., 2017; Budzianowski and Vuli´c, 2019). Dialogue systems benefit greatly from optimizing on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are sev"
D19-1125,P18-1135,0,0.0479066,"by the BiLSTM to obtain the hidden states ht1:L . The encoding of the slot-value pair svkij is the output of the affine layer that takes the concatenation of the embeddings of domain i, slot j and value k as the input. The context vector aij k is then computed by the attention mechanism, denoted as attn in Figure 1, following Luong et al. (2015): el = sim(hl , svkij ) aij k = L ∑ el hl , (1) (2) l=1 where l is the word index of the user utterance and sim denotes any function that calculates the similarity of two vectors. We adopt here the dot product function, following Mrkši´c et al. (2017); Zhong et al. (2018); Ramadan et al. (2018). The ij ij similarity score sij k between ak and svk is then computed to see whether the slot-value pair svkij is mentioned in the utterance. The mentioned pair should have higher similarity score to its context vector than those which are not mentioned. The softmax layer is then applied to form the probability distribution pinf ij for each informable slot sinf ij , where the predicted value is the value with 1274 the highest probability. The same attention mechanism is used for each requestable slot reqr to decide whether the user has asked for the slot in the current"
D19-1125,D15-1199,0,0.0220238,"Missing"
D19-1125,E17-1042,0,0.0922133,"Missing"
D19-6104,E17-1088,0,0.0293678,"to evaluate few-shot learning methods: Definitional Nonce (Herbelot and Baroni, 2017), Chimera (Lazaridou et al., 2017), and Contextual Rare Words (Khodak et al., 2018), which we describe Introduction Word embeddings have impacted almost every aspect of NLP, proving effective in a wide range of use cases. Often used in the form of a pre-trained model, these vectors provide easy to use representations of semantic meaning. However, distributional models are known to struggle with words for which training data is sparse, often resulting in low-quality vector representations (Huang et al., 2012; Adams et al., 2017). The default approach in this case has historically been to ignore these rare words, preferring an incomplete view over an incorrect one (Mikolov et al., 2013). Another option is to use the surface form of a word to obtain a vector, leveraging morphological characteristics (Luong et al., 2013) or subword embeddings (Bojanowski et al., 2017). As neither of these approaches fully resolves the problem, more techniques have been proposed for few-shot learning 31 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 31–39 c Hong Kong, China, November 3, 2"
D19-6104,Q15-1016,0,0.300512,"wever, each of these tasks was designed for context-based few-shot learning, without considering hybrid methods, which also have access to word-form information. We show that the existing tasks do not suffice to fully assess the performance of hybrid models, with relatively simple, purely form-based methods dominating two out of three tasks. To provide a better overview and performance comparison, in Section 3 we introduce three new tasks based on these three datasets. In Sections 4–6, we show that, just as hyperparameters are essential to good performance with standard distributional models (Levy et al., 2015), the same is true for few-shot distributional models. With three straightforward modifications, we substantially improve the baseline scores, outperforming several advanced methods from previous work, as well as achieving a new state of the art on 4 out of 6 evaluation tasks. 2 2.1 Contextual Rare Words The Contextual Rare Words (CRW) dataset (Khodak et al., 2018; Luong et al., 2013) consists of 255 context sentences selected randomly from Wikipedia for each of 455 existing words. Vectors are inferred for each word using 1, 2, 4, ..., 128 sentences. In similar fashion to the Chimera task, hum"
D19-6104,W13-3512,0,0.60781,"f use cases. Often used in the form of a pre-trained model, these vectors provide easy to use representations of semantic meaning. However, distributional models are known to struggle with words for which training data is sparse, often resulting in low-quality vector representations (Huang et al., 2012; Adams et al., 2017). The default approach in this case has historically been to ignore these rare words, preferring an incomplete view over an incorrect one (Mikolov et al., 2013). Another option is to use the surface form of a word to obtain a vector, leveraging morphological characteristics (Luong et al., 2013) or subword embeddings (Bojanowski et al., 2017). As neither of these approaches fully resolves the problem, more techniques have been proposed for few-shot learning 31 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 31–39 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chimera The Chimera dataset (Lazaridou et al., 2017) consists of a series of novel words that are built as hybrids between two existing words. For each hybrid word, trials with 2, 4 and 6 context sentences are p"
D19-6104,Q17-1010,0,0.0839249,"e-trained model, these vectors provide easy to use representations of semantic meaning. However, distributional models are known to struggle with words for which training data is sparse, often resulting in low-quality vector representations (Huang et al., 2012; Adams et al., 2017). The default approach in this case has historically been to ignore these rare words, preferring an incomplete view over an incorrect one (Mikolov et al., 2013). Another option is to use the surface form of a word to obtain a vector, leveraging morphological characteristics (Luong et al., 2013) or subword embeddings (Bojanowski et al., 2017). As neither of these approaches fully resolves the problem, more techniques have been proposed for few-shot learning 31 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 31–39 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chimera The Chimera dataset (Lazaridou et al., 2017) consists of a series of novel words that are built as hybrids between two existing words. For each hybrid word, trials with 2, 4 and 6 context sentences are provided, with half of the sentences coming from"
D19-6104,W19-0408,1,0.839551,"even outperformed across the board by these new baselines. However, simple methods typically struggle to generalise to multiple sub-tasks. The main benefit of more complex methods is that they are more flexible, at the price of overhead and a risk of overfitting. For both context-based and hybrid few-shot learning, we have achieved a new state of the art on 4 out of the 6 evaluation tasks used, showing that a careful, optimised approach can be the key to success in few-shot learning. Future work could explore other distributional models, such as dependency embeddings (Levy and Goldberg, 2014; Czarnowska et al., 2019), but it is clear from our results that careful optimisation will be required to adapt other models to the few-shot setting. Conclusion Different situations and goals in few-shot learning have different optimal solutions. The difference between learning from natural language usage and definitions is especially apparent: only the original A La Carte method performs well for both types, while other models that do very well on the latter typically trail on the former. The principles behind Word2Vec work well in other models when using unfiltered, natural usage examples, but are less consistent wh"
D19-6104,D17-1030,0,0.597962,"ubstantially improve systems working with technical language or dialects. However, few-shot learning is also interesting from a human language learning perspective: unlike current-day distributional models, humans excel at learning meaning from sparse data through a process called ‘fast mapping’ (Trueswell et al., 2013; Lake et al., 2017). Lessons learned from psychology might prove effective in machines, and novel few-shot learning techniques might provide insight into fast mapping in humans. Three evaluation tasks have been proposed to evaluate few-shot learning methods: Definitional Nonce (Herbelot and Baroni, 2017), Chimera (Lazaridou et al., 2017), and Contextual Rare Words (Khodak et al., 2018), which we describe Introduction Word embeddings have impacted almost every aspect of NLP, proving effective in a wide range of use cases. Often used in the form of a pre-trained model, these vectors provide easy to use representations of semantic meaning. However, distributional models are known to struggle with words for which training data is sparse, often resulting in low-quality vector representations (Huang et al., 2012; Adams et al., 2017). The default approach in this case has historically been to ignore"
D19-6104,D18-1173,0,0.0200065,"lowing the model to adapt to differand v(w,C) ent scenarios. for the test word. Stopwords2 are dropped from this sum, as this has been found to consistently improve performance (Khodak et al., 2018). Nonce2Vec The Nonce2Vec algorithm heavily modifies several aspects of the standard SkipGram Word2Vec algorithm. This allows for a higher-risk initial learning approach, followed by a more cautious strategy as more data is presented (Herbelot and Baroni, 2017). Mem2Vec The Mem2Vec algorithm uses a long-range memory over the whole corpus to find a vector corresponding to a small number of contexts (Sun et al., 2018). 3 Several issues can be observed in the evaluation setup used in previous work. First of all, results on the Chimera task are inconsistent, showing almost no trends between different models. This can largely be attributed to the the size of the test set: only 110 chimera words are used. By using the CRW development set to optimise for both the CRW and Chimera task, we can include the training set as well, resulting in the ‘Full Chimera Task’ with a total of 330 words. For the CRW and DN tasks, the issues are not in the consistency of results, but rather in how to interpret the results. Schic"
D19-6104,P12-1092,0,0.0515993,"s have been proposed to evaluate few-shot learning methods: Definitional Nonce (Herbelot and Baroni, 2017), Chimera (Lazaridou et al., 2017), and Contextual Rare Words (Khodak et al., 2018), which we describe Introduction Word embeddings have impacted almost every aspect of NLP, proving effective in a wide range of use cases. Often used in the form of a pre-trained model, these vectors provide easy to use representations of semantic meaning. However, distributional models are known to struggle with words for which training data is sparse, often resulting in low-quality vector representations (Huang et al., 2012; Adams et al., 2017). The default approach in this case has historically been to ignore these rare words, preferring an incomplete view over an incorrect one (Mikolov et al., 2013). Another option is to use the surface form of a word to obtain a vector, leveraging morphological characteristics (Luong et al., 2013) or subword embeddings (Bojanowski et al., 2017). As neither of these approaches fully resolves the problem, more techniques have been proposed for few-shot learning 31 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 31–39 c Hong Kong,"
D19-6104,P18-1002,0,0.236107,"ot learning is also interesting from a human language learning perspective: unlike current-day distributional models, humans excel at learning meaning from sparse data through a process called ‘fast mapping’ (Trueswell et al., 2013; Lake et al., 2017). Lessons learned from psychology might prove effective in machines, and novel few-shot learning techniques might provide insight into fast mapping in humans. Three evaluation tasks have been proposed to evaluate few-shot learning methods: Definitional Nonce (Herbelot and Baroni, 2017), Chimera (Lazaridou et al., 2017), and Contextual Rare Words (Khodak et al., 2018), which we describe Introduction Word embeddings have impacted almost every aspect of NLP, proving effective in a wide range of use cases. Often used in the form of a pre-trained model, these vectors provide easy to use representations of semantic meaning. However, distributional models are known to struggle with words for which training data is sparse, often resulting in low-quality vector representations (Huang et al., 2012; Adams et al., 2017). The default approach in this case has historically been to ignore these rare words, preferring an incomplete view over an incorrect one (Mikolov et"
K18-1030,W16-2206,0,0.0213051,"to all its words (Carpenter and Just, 1983; Rayner and Duffy, 1988). For example, humans are likely to omit many function words and other words that are predictable in context and focus on less predictable content words. Moreover, when they fixate on a word, the duration of that fixation depends on a number of linguistic factors (Clifton et al., 2007; Demberg and Keller, 2008). Since learning good attention functions for recurrent neural networks requires large volumes of data (Zoph et al., 2016; Britz et al., 2017), and errors in attention are known to propagate to classification decisions (Alkhouli et al., 2016), we explore the idea of using human attention, as estimated from eye-tracking corpora, as an inductive bias on such attention functions. Penalizing attention functions for departing from human attention may enable us to learn better attention functions when data is limited. Eye-trackers provide millisecond-accurate records on where humans look when they are reading, and they are becoming cheaper and more easily available by the day (San Agustin et al., 2009). In this paper, we use publicly available 2 Method We present a recurrent neural architecture that jointly learns the recurrent paramete"
K18-1030,P16-2094,1,0.583842,"ai aei = P k ak (9) Our model thus combines two distinct objectives: one at the sentence level and one at the token level. The sentence-level objective is to minimize the squared error between output activations and true sentence labels yb. Lsent = X (y (j) − yb(j) )2 j 303 (10) The token-level objective, similarly, is to minimize the squared error for the attention not aligning with our human attention metric. Ltok = XX j b(j)(t) )2 (a(j)(t) − a same text. For every token, we compute the mean duration of all fixations to this token as our measure of human attention, following previous work (Barrett et al., 2016a; Gonzalez-Garduno and Søgaard, 2018). (11) t These are finally combined to a weighted sum, using λ (between 0 and 1) to trade off loss functions at the sentence and token levels. L = Lsent + λLtok Dundee The English part of the Dundee corpus (Kennedy et al., 2003) comprises 2,368 sentences and more than 50,000 tokens. The texts were read by ten skilled, adult, native speakers. The texts are 20 newspaper articles from The Independent. The reading was self-paced and as close to natural, contextualized reading as possible for a laboratory data collection. The apparatus was a Dr Bouis Oculometer"
K18-1030,C16-1126,1,0.941516,"ai aei = P k ak (9) Our model thus combines two distinct objectives: one at the sentence level and one at the token level. The sentence-level objective is to minimize the squared error between output activations and true sentence labels yb. Lsent = X (y (j) − yb(j) )2 j 303 (10) The token-level objective, similarly, is to minimize the squared error for the attention not aligning with our human attention metric. Ltok = XX j b(j)(t) )2 (a(j)(t) − a same text. For every token, we compute the mean duration of all fixations to this token as our measure of human attention, following previous work (Barrett et al., 2016a; Gonzalez-Garduno and Søgaard, 2018). (11) t These are finally combined to a weighted sum, using λ (between 0 and 1) to trade off loss functions at the sentence and token levels. L = Lsent + λLtok Dundee The English part of the Dundee corpus (Kennedy et al., 2003) comprises 2,368 sentences and more than 50,000 tokens. The texts were read by ten skilled, adult, native speakers. The texts are 20 newspaper articles from The Independent. The reading was self-paced and as close to natural, contextualized reading as possible for a laboratory data collection. The apparatus was a Dr Bouis Oculometer"
K18-1030,N06-1038,0,0.128832,"Missing"
K18-1030,W15-1814,1,0.840513,"aard (2018) use auxiliary data to regularize attention functions in recurrent neural networks; not from psycholinguistics data, but using small amounts of task-specific, token-level annotations. While their motivation is very different from ours, technically our models are very related. In a different context, Das et al. (2017) investigated whether humans attend to the same regions as neural networks solving visual question answering problems. Lindsey (2017) also used human-inspired, unsupervised attention in a computer vision context. Gaze has also been used in the context of grammaticality (Klerke et al., 2015a,b), as well as in readability assessment (Gonzalez-Garduno and Søgaard, 2018). Other work on multi-purpose attention functions While our work is the first to use gaze data to guide attention in a recurrent architectures, there has recently been some work on sharing attention functions across tasks. Firat et al. (2016), for example, share attention functions between languages in the context of multi-way neural machine translation. Gaze has either been used as features (Barrett and Søgaard, 2015a; Barrett et al., 2016b) or as a direct supervision signal in multi-task learning scenarios (Klerke"
K18-1030,P15-1166,0,0.0306673,"we explore this idea, proposing a particular architecture and training method that, in effect, uses human attention to regularize machine attention. hi = tanh(Wh hei + bh ) (1) (2) (3) (4) The final (reduced) hidden state is sometimes used as a sentence representation s, but we instead use attention to compute s by multiplying dynamically predicted attention weights with the hidden states for each time step. The final sentence predictions y are then computed by passing s through two more hidden layers: Our training method is similar to a standard approach to training multi-task architectures (Dong et al., 2015; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017), sometimes referred to as the alternating training approach (Luong et al., 2016): We randomly select a data point from our training data or the eye-tracking corpus with some (potentially equal) probability. If the data point is sampled from our training data, we predict a discrete category and use the computed loss to update our parameters. If the data point is sampled from the eye-tracking corpus, we still run the recurrent network to produce a category, but this time we only monitor the attention weights assigned to the input tokens. We"
K18-1030,W15-2402,1,0.776564,"aard (2018) use auxiliary data to regularize attention functions in recurrent neural networks; not from psycholinguistics data, but using small amounts of task-specific, token-level annotations. While their motivation is very different from ours, technically our models are very related. In a different context, Das et al. (2017) investigated whether humans attend to the same regions as neural networks solving visual question answering problems. Lindsey (2017) also used human-inspired, unsupervised attention in a computer vision context. Gaze has also been used in the context of grammaticality (Klerke et al., 2015a,b), as well as in readability assessment (Gonzalez-Garduno and Søgaard, 2018). Other work on multi-purpose attention functions While our work is the first to use gaze data to guide attention in a recurrent architectures, there has recently been some work on sharing attention functions across tasks. Firat et al. (2016), for example, share attention functions between languages in the context of multi-way neural machine translation. Gaze has either been used as features (Barrett and Søgaard, 2015a; Barrett et al., 2016b) or as a direct supervision signal in multi-task learning scenarios (Klerke"
K18-1030,D17-1169,1,0.853198,"Missing"
K18-1030,N16-1179,1,0.886802,"to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feature that carries an enormous amount of information about the text and the language understanding process. Carpenter and Just (1983) show that readers are more likely to fixate on open-class words that are not predictable f"
K18-1030,N16-1101,0,0.0198253,". (2017) investigated whether humans attend to the same regions as neural networks solving visual question answering problems. Lindsey (2017) also used human-inspired, unsupervised attention in a computer vision context. Gaze has also been used in the context of grammaticality (Klerke et al., 2015a,b), as well as in readability assessment (Gonzalez-Garduno and Søgaard, 2018). Other work on multi-purpose attention functions While our work is the first to use gaze data to guide attention in a recurrent architectures, there has recently been some work on sharing attention functions across tasks. Firat et al. (2016), for example, share attention functions between languages in the context of multi-way neural machine translation. Gaze has either been used as features (Barrett and Søgaard, 2015a; Barrett et al., 2016b) or as a direct supervision signal in multi-task learning scenarios (Klerke et al., 2016; Gonzalez-Garduno and Søgaard, 2018). We are, to the best of our knowledge, the first to use gaze to inform attention functions in recurrent neural networks. Sentiment analysis While sentiment analysis is most often considered a supervised learning problem, several authors have leveraged other signals 308"
K18-1030,D16-1012,0,0.0340398,"Missing"
K18-1030,P17-1194,1,0.888294,"Missing"
K18-1030,P17-1035,0,0.287094,"double standards and certain rules. The gaze-informed sentiment classifier, on the other hand, focuses more on sorry I am not sexist which, in isolation, reads like an apologetic disclaimer. This model also gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feat"
K18-1030,N18-1027,1,0.920247,"hierarchical model whose word representations are concatenations of the output of character-level LSTMs and word embeddings, following Plank et al. (2016), but we ignore the character-level part of our architecture in the equations below: Behind our approach lies the simple observation that we can correlate the token-level attention devoted by a recurrent neural network, even if trained on sentence-level signals, with any measure defined at the token level. In other words, we can compare the attention devoted by a recurrent neural network to various measures, including token-level annotation (Rei and Søgaard, 2018) and eye-tracking measures. The latter is particularly interesting as it is typically considered a measurement of human attention. → − −−→ hi = LST M (xi , hi−1 ) ← − ←−− hi = LST M (xi , hi+1 ) → − ← − hei = [ hi ; hi ] We go beyond this: Not only can we compare machine attention with human attention, we can also constrain or inform machine attention by human attention in various ways. In this paper, we explore this idea, proposing a particular architecture and training method that, in effect, uses human attention to regularize machine attention. hi = tanh(Wh hei + bh ) (1) (2) (3) (4) The fi"
K18-1030,P16-1112,1,0.835869,"three-way classification task with positive, negative and neutral classes. We reduce the task to binary tasks detecting negative sentences vs. non-negative and vice versa for the positive class. Therefore the dataset size is the same for POS and NEG experiments. 3.3 Grammatical error detection Our second task is grammatical error detection. We use the First Certificate in English error detection dataset (FCE) (Yannakoudakis et al., 2011). This dataset contains essays written by English learners during language examinations, where any grammatical errors have been manually annotated by experts. Rei and Yannakoudakis (2016) converted the dataset for a sequence labeling task and we use their splits for training, development and testing. Similarly to Rei and Søgaard (2018), we perform sentence-level binary classification of sentences that need some editing vs. grammatically correct sentences. We do not use the tokenlevel labels for training our model. 3.4 Hate speech detection Our third and final task is detection of abusive language; or more specifically, hate speech detection. We use the datasets of Waseem (2016) and Waseem and Hovy (2016). The former contains 6,909 tweets; the latter 14,031 tweets. They are man"
K18-1030,W17-5004,1,0.881134,"Missing"
K18-1030,rohanian-etal-2017-using,0,0.207971,"hate speech indicator. It also gives weight to double standards and certain rules. The gaze-informed sentiment classifier, on the other hand, focuses more on sorry I am not sexist which, in isolation, reads like an apologetic disclaimer. This model also gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fix"
K18-1030,W11-0603,0,0.0233109,"tic disclaimer. This model also gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feature that carries an enormous amount of information about the text and the language understanding process. Carpenter and Just (1983) show that readers are more likely to fixate on op"
K18-1030,S15-2078,0,0.0589074,"Missing"
K18-1030,Q13-1006,0,0.0309346,"Missing"
K18-1030,D14-1162,0,0.0811882,"n and using the same regularization scheme as in our human attention model. sampling from the eye-tracking corpus initially is 1 0.5, but drops linearly for each epoch ( E+1 ; see 2.1. We apply the models with the best average F1 scores over three random seeds on the validation data, to our test sets. Hyperparameters Basic hyper-parameters such as number of hidden layers, layer size, and activation functions were following the settings of Rei and Søgaard (2018). The dimensionality of our word embedding layer was set to size 300, and we use publicly available pre-trained Glove word embeddings (Pennington et al., 2014) that we finetune during training. The dimensionality of the character embedding layer was set to 100. The recurrent layers in the character-level component have dimensionality 100; the word-level recurrent layers dimensionality 300. The dimensionality of our feed-forward layer, leading to reduced combined representations hi , is 200, and the attention layer has dimensionality 100. Three hyper-parameters, however, we tune for each architecture and for each task, by measuring sentence-level F1 -scores on the development sets. These are: (a) learning rate, (b) λ in Equation (12), i.e., controlli"
K18-1030,C16-1059,0,0.0281674,"gives weight to double standards and certain rules The gaze-informed grammatical error detection 7 Discussion and related work Gaze in NLP It has previously been shown that several NLP tasks benefit from gaze information, including part-of-speech tagging (Barrett and Søgaard, 2015b; Barrett et al., 2016a), prediction of multiword expressions (Rohanian et al., 2017) and sentiment analysis (Mishra et al., 2017b). Gaze information and other measures from psycholinguistics have been used in different ways in NLP. Some authors have used discretized, single features (Pate and Goldwater, 2011, 2013; Plank, 2016; Klerke et al., 2016), whereas others have used multidimensional, continuous values (Barrett et al., 2016a; Bingel et al., 2016). We follow Gonzalez-Garduno and Søgaard (2018) in using a single, continuous feature. We did not experiment with other representations, however. Specifically, we only considered the signal from token-level, normalized mean fixation durations. Fixation duration is a feature that carries an enormous amount of information about the text and the language understanding process. Carpenter and Just (1983) show that readers are more likely to fixate on open-class words that"
K18-1030,P16-2067,1,0.755477,"eriments) overlap in any way. Our experimental protocol, in other words, does not require in-task eye-tracking recordings, but simply leverages information from existing, available corpora. Model Our architecture is a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) that encodes word representations xi into forward and backward representations, and into combined hidden states hi (of slightly lower dimensionality) at every timestep. In fact, our model is a hierarchical model whose word representations are concatenations of the output of character-level LSTMs and word embeddings, following Plank et al. (2016), but we ignore the character-level part of our architecture in the equations below: Behind our approach lies the simple observation that we can correlate the token-level attention devoted by a recurrent neural network, even if trained on sentence-level signals, with any measure defined at the token level. In other words, we can compare the attention devoted by a recurrent neural network to various measures, including token-level annotation (Rei and Søgaard, 2018) and eye-tracking measures. The latter is particularly interesting as it is typically considered a measurement of human attention. →"
K18-1030,W17-1101,0,0.0207736,"Missing"
K18-1030,D13-1170,0,0.0267271,"Missing"
K18-1030,P16-2038,1,0.742011,"a, proposing a particular architecture and training method that, in effect, uses human attention to regularize machine attention. hi = tanh(Wh hei + bh ) (1) (2) (3) (4) The final (reduced) hidden state is sometimes used as a sentence representation s, but we instead use attention to compute s by multiplying dynamically predicted attention weights with the hidden states for each time step. The final sentence predictions y are then computed by passing s through two more hidden layers: Our training method is similar to a standard approach to training multi-task architectures (Dong et al., 2015; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017), sometimes referred to as the alternating training approach (Luong et al., 2016): We randomly select a data point from our training data or the eye-tracking corpus with some (potentially equal) probability. If the data point is sampled from our training data, we predict a discrete category and use the computed loss to update our parameters. If the data point is sampled from the eye-tracking corpus, we still run the recurrent network to produce a category, but this time we only monitor the attention weights assigned to the input tokens. We then compute the minimum sq"
K18-1030,W16-5618,0,0.100077,"Missing"
K18-1030,N16-2013,0,0.18631,"models and the model with gaze-informed attention by the attention weights of an example sentence. Though it is a single, cherry-picked example, it is representative of the general trends we observe in the data, when manually inspecting attention patterns. Table 3 presents a coarse visualization of the attention weights of six different models, namely our baseline architecture and the architecture with gaze-informed attention, trained on three different tasks: hate speech detection, negative sentiment classification, and error detection. The sentence is a positive hate speech example from the Waseem and Hovy (2016) development set. The words with more attention than the sentence average are bold-faced. First note that the baseline models only attend to one or two coherent text parts. This pattern was very consistent across all the sentences we examined. This pattern was not observed with gazeinformed attention. Our second observation is that the baseline models are more likely to attend to stop words than gaze-informed attention. This suggests that gazeinformed attention has learned to simulate human attention to some degree. We also see many differences between the jointly learned task-specific, gaze-i"
K18-1030,P11-1019,0,0.0266868,"orpus better. 3.2 set (S EMEVAL T WITTER POS |NEG), and an out-of-domain test set, SemEval-2013 SMS test set (S EMEVAL SMS POS |NEG). The SemEval2013 sentiment classification task was a three-way classification task with positive, negative and neutral classes. We reduce the task to binary tasks detecting negative sentences vs. non-negative and vice versa for the positive class. Therefore the dataset size is the same for POS and NEG experiments. 3.3 Grammatical error detection Our second task is grammatical error detection. We use the First Certificate in English error detection dataset (FCE) (Yannakoudakis et al., 2011). This dataset contains essays written by English learners during language examinations, where any grammatical errors have been manually annotated by experts. Rei and Yannakoudakis (2016) converted the dataset for a sequence labeling task and we use their splits for training, development and testing. Similarly to Rei and Søgaard (2018), we perform sentence-level binary classification of sentences that need some editing vs. grammatically correct sentences. We do not use the tokenlevel labels for training our model. 3.4 Hate speech detection Our third and final task is detection of abusive langu"
K18-1030,D16-1163,0,0.0187587,"/coastalcph/ Sequence_classification_with_ human_attention. Introduction When humans read a text, they do not attend to all its words (Carpenter and Just, 1983; Rayner and Duffy, 1988). For example, humans are likely to omit many function words and other words that are predictable in context and focus on less predictable content words. Moreover, when they fixate on a word, the duration of that fixation depends on a number of linguistic factors (Clifton et al., 2007; Demberg and Keller, 2008). Since learning good attention functions for recurrent neural networks requires large volumes of data (Zoph et al., 2016; Britz et al., 2017), and errors in attention are known to propagate to classification decisions (Alkhouli et al., 2016), we explore the idea of using human attention, as estimated from eye-tracking corpora, as an inductive bias on such attention functions. Penalizing attention functions for departing from human attention may enable us to learn better attention functions when data is limited. Eye-trackers provide millisecond-accurate records on where humans look when they are reading, and they are becoming cheaper and more easily available by the day (San Agustin et al., 2009). In this paper,"
N13-1040,W10-0216,0,0.0604882,"Missing"
N13-1040,P11-1070,0,0.0491511,"Missing"
N13-1040,P06-2006,1,0.830229,"node in graph g, e ∈ Eg is an edge in graph g, isDep(e, n) is a function returning 1.0 if n is the dependent in edge e, and 0.0 otherwise. NScore(n) is set to 0 if the node does not appear as a dependent in any edges. We found this metric performs well, as it prefers graphs that connect together many nodes without simply rewarding a larger number of edges. While the score calculation is done using the modified graph gr0 , the resulting score is directly assigned to the corresponding original graph gr , and 396 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2 Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3 ilexir.co.uk/2012/open-source-rasp-3-1/ C&C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marc"
N13-1040,P06-4020,1,0.81925,"Missing"
N13-1040,D07-1101,0,0.0180584,"dence scores for bilexical relations. Finally, we describe methods for combining together these scores and calculating an overall score for a dependency graph. We make publically available all the code developed for performing these steps in the parse reranking system.1 3.1 Graph modifications For every dependency graph gr the graph expansion procedure creates a modified representation gr0 which contains a wider range of bilexical relations. The motivation for this graph expansion step is similar to that motivating the move from first-order to higher-order dependency path feature types (e.g., Carreras (2007)). However, compared to using all nth-order paths, these rules are chosen to maximise the utility and minimise the sparsity of the resulting bilexical features. In addition, the cascading nature of the expansion steps means in some cases the expansion captures useful 3rd and 4th order dependencies. Similar approaches to graph modifications have been successfully used for several NLP tasks (van Noord, 2007; Arora et al., 2010). For any edge e we also use notation (rel, w1 , w2 ), referring to an edge from w1 to w2 with the label rel. We perform the following modifications on every dependency gr"
N13-1040,P05-1022,0,0.278708,"Missing"
N13-1040,P07-1032,0,0.0457012,"Missing"
N13-1040,W01-0521,0,0.169205,"Missing"
N13-1040,W03-2401,0,0.040173,"s a function returning 1.0 if n is the dependent in edge e, and 0.0 otherwise. NScore(n) is set to 0 if the node does not appear as a dependent in any edges. We found this metric performs well, as it prefers graphs that connect together many nodes without simply rewarding a larger number of edges. While the score calculation is done using the modified graph gr0 , the resulting score is directly assigned to the corresponding original graph gr , and 396 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2 Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3 ilexir.co.uk/2012/open-source-rasp-3-1/ C&C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marcus et al., 1993), divided into development (140 sent"
N13-1040,P03-1054,0,0.0187076,"ic analysis in ambiguous contexts. However, utilising such features leads the parser to learn information that is often specific to the domain and/or genre of the training data. Several experiments have demonstrated that many lexical features learnt in In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In this work, instead of trying to adapt a lexicalised parser to new domains, we explore how bilexical features can be integrated effectively with any unlexicalised parser. As our novel self-learning framework requires only a large unannotated corpus, lexical features can be easily tuned to a specific domain or genre by selecting a suitable dataset. In addition, we describe a graph expansion process that captures selected bilexical relations which improve performance but would otherwise require sparse higherorder dependency path feature types in most approaches to depen"
N13-1040,J93-2004,0,0.0429583,"Missing"
N13-1040,N06-1020,0,0.190011,"Missing"
N13-1040,U10-1014,0,0.0399222,"Missing"
N13-1040,P06-1055,0,0.0450946,"contexts. However, utilising such features leads the parser to learn information that is often specific to the domain and/or genre of the training data. Several experiments have demonstrated that many lexical features learnt in In contrast, unlexicalised parsers avoid using lexical information and select a syntactic analysis using only more general features, such as POS tags. While they cannot be expected to achieve optimal performance when trained and tested in a single domain, unlexicalised parsers can be surprisingly competitive with their lexicalised counterparts (Klein and Manning, 2003; Petrov et al., 2006). In this work, instead of trying to adapt a lexicalised parser to new domains, we explore how bilexical features can be integrated effectively with any unlexicalised parser. As our novel self-learning framework requires only a large unannotated corpus, lexical features can be easily tuned to a specific domain or genre by selecting a suitable dataset. In addition, we describe a graph expansion process that captures selected bilexical relations which improve performance but would otherwise require sparse higherorder dependency path feature types in most approaches to dependency parsing. As many"
N13-1040,W08-1302,0,0.0420778,"Missing"
N13-1040,A97-1015,0,0.225961,"Missing"
N13-1040,tateisi-etal-2008-genia,0,0.0368155,"Missing"
N13-1040,W07-2201,0,0.0575934,"Missing"
N13-1040,W07-2203,1,0.830065,"performs well, as it prefers graphs that connect together many nodes without simply rewarding a larger number of edges. While the score calculation is done using the modified graph gr0 , the resulting score is directly assigned to the corresponding original graph gr , and 396 DepBank We evaluated our self-learning framework using the DepBank/GR reannotation (Briscoe and Carroll, 2006) of the PARC 700 Dependency Bank (King et al., 2003). The dataset is provided with the open-source RASP distribution3 and has been used for evaluating different parsers, including RASP (Briscoe and Carroll, 2006; Watson et al., 2007) and 2 Slight changes in the performance of the baseline parser compared to previous publications are due to using a more recent version of the parser and minor corrections to the gold standard annotation. 3 ilexir.co.uk/2012/open-source-rasp-3-1/ C&C (Clark and Curran, 2007). It contains 700 sentences, randomly chosen from section 23 of the WSJ Penn Treebank (Marcus et al., 1993), divided into development (140 sentences) and test data (560 sentences). We made use of the development data to experiment with a wider selection of edge and graph scoring methods, and report the final results on the"
N13-1040,P11-1156,0,0.0283641,"Missing"
N18-1027,W16-0506,0,0.0134864,"he tweet is annotated as negative: grammatically correct sentence. The task has numerous applications for writing improvement and assessment, and recent work has focused on error detection as a supervised sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error-annotated short essays written by language learners. While the original corpus is focused on aligned corrections, Rei and Yannakoudakis (2016) converted the dataset to a sequence labeling format, which we make use of here. An example from the dataset, with bold font indicating tokens that have been annotated as incorrect given the context: They may have a SuperBowl in"
N18-1027,D14-1080,0,0.0715659,"ce systems. Our results indicate that attention-based methods are able to predict token-level labels more accurately, compared to gradient-based methods, sometimes even rivaling the supervised oracle network. 1 Introduction Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence. Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detection, are often formulated as variations of sequence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Irsoy and Cardie, 2014), character-based representations (Lample et al., 2016), and additional external features (Peters et al., 2017). Optimization of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language. In this paper, we investigate the task of performing sequence labeling without having access to any training data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture T"
N18-1027,P16-1068,1,0.8665,"ing token-level labels, based on visualization methods using gradient analysis. Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014). Denil et al. (2014) extended this approach to natural language processing, in order to find and visualize the most important sentences in a text. Recent work has also used the gradient-based approach for visualizing the decisions of text classification models on the token level (Li et al., 2016; Alikaniotis et al., 2016). In this section we propose an adaptation that can be used for sequence labeling tasks. We first perform a forward pass through the network and calculate the predicted sentence-level score y. Next, we define a pseudo-label y ∗ = 0, regardless of the true label of the sentence. We then calculate the gradient of the word representation wi with respect to the loss function using this pseudo-label: gi = ∂L1 ∂wi (y∗ ,y) 3.2 Relative Frequency Baseline The system for producing token-level predictions based on sentence-level training data does not necessarily need to be a neural network. As the init"
N18-1027,S14-2009,0,0.135007,"(14) where γ is used to control the importance of the auxiliary objectives. 3 Alternative Methods We compare the attention-based system for inferring sequence labeling with 3 alternative methods. 3.1 Labeling Through Backpropagation We experiment with an alternative method for inducing token-level labels, based on visualization methods using gradient analysis. Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014). Denil et al. (2014) extended this approach to natural language processing, in order to find and visualize the most important sentences in a text. Recent work has also used the gradient-based approach for visualizing the decisions of text classification models on the token level (Li et al., 2016; Alikaniotis et al., 2016). In this section we propose an adaptation that can be used for sequence labeling tasks. We first perform a forward pass through the network and calculate the predicted sentence-level score y. Next, we define a pseudo-label y ∗ = 0, regardless of the true label of the sentence. We then calculate"
N18-1027,I17-1005,0,0.0124097,"s a whole. A single tweet could contain both positive and negative phrases, regardless of its overall polarity, and was therefore separately annotated on the tweet level. In the following example from the dataset, negative phrases are indicated with a bold font and positive phrases are marked with italics, whereas the overall sentiment of the tweet is annotated as negative: grammatically correct sentence. The task has numerous applications for writing improvement and assessment, and recent work has focused on error detection as a supervised sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error-annotated short essa"
N18-1027,D15-1044,0,0.0509415,"raining data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture The main system takes as input a sentence, separated into tokens, and outputs a binary prediction as the label of the sentence. We use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) architecture for sentence classification, with dynamic attention over words for constructing the sentence representations. Related architectures have been successful for machine translation (Bahdanau et al., 2015), sentence summarization (Rush and Weston, 2015), entailment detection (Rockt¨aschel et al., 2016), and error correction (Ji et al., 2017). In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for 293 Proceedings of NAACL-HLT 2018, pages 293–302 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics also inferring binary token labels, while still performing well as a sentence classifier. Figure 1 contains a diagram of the network architecture. The tokens are first mapped to a sequence of word representations [w1 , w2 , w3 , ..., wN ], which"
N18-1027,N16-1030,0,0.306041,"ods are able to predict token-level labels more accurately, compared to gradient-based methods, sometimes even rivaling the supervised oracle network. 1 Introduction Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence. Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detection, are often formulated as variations of sequence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Irsoy and Cardie, 2014), character-based representations (Lample et al., 2016), and additional external features (Peters et al., 2017). Optimization of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language. In this paper, we investigate the task of performing sequence labeling without having access to any training data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture The main system takes as input a sentence, separated int"
N18-1027,N16-1082,0,0.0244825,"method for inducing token-level labels, based on visualization methods using gradient analysis. Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014). Denil et al. (2014) extended this approach to natural language processing, in order to find and visualize the most important sentences in a text. Recent work has also used the gradient-based approach for visualizing the decisions of text classification models on the token level (Li et al., 2016; Alikaniotis et al., 2016). In this section we propose an adaptation that can be used for sequence labeling tasks. We first perform a forward pass through the network and calculate the predicted sentence-level score y. Next, we define a pseudo-label y ∗ = 0, regardless of the true label of the sentence. We then calculate the gradient of the word representation wi with respect to the loss function using this pseudo-label: gi = ∂L1 ∂wi (y∗ ,y) 3.2 Relative Frequency Baseline The system for producing token-level predictions based on sentence-level training data does not necessarily need to be a"
N18-1027,D14-1162,0,0.0796777,"Missing"
N18-1027,P17-1161,0,0.0267294,"ly, compared to gradient-based methods, sometimes even rivaling the supervised oracle network. 1 Introduction Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence. Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detection, are often formulated as variations of sequence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Irsoy and Cardie, 2014), character-based representations (Lample et al., 2016), and additional external features (Peters et al., 2017). Optimization of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language. In this paper, we investigate the task of performing sequence labeling without having access to any training data with token-level annotation. Instead of training the model directly to predict the label for each token, the model is optimized using 2 Network Architecture The main system takes as input a sentence, separated into tokens, and outputs a binary prediction as the label o"
N18-1027,P11-1019,0,0.0605862,"ed sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error-annotated short essays written by language learners. While the original corpus is focused on aligned corrections, Rei and Yannakoudakis (2016) converted the dataset to a sequence labeling format, which we make use of here. An example from the dataset, with bold font indicating tokens that have been annotated as incorrect given the context: They may have a SuperBowl in Dallas, but Dallas ain’t winning a SuperBowl. Not with that quarterback and owner. @S4NYC @RasmussenPoll Sentiment analysis is a three-way task, as the system needs to differentiate between positive, n"
N18-1027,P17-1194,1,0.943039,"..”). An example sentence from the dataset, with bold font indicating the hedge cue and curly brackets marking the scope of uncertainty: Supervised Sequence Labeling Finally, we also report the performance of a supervised sequence labeling model on the same tasks. This serves as an indicator of an upper bound for a given dataset – how well the system is able to detect relevant tokens when directly optimized for sequence labeling and provided with token-level annotation. We construct a bidirectional LSTM tagger, following the architectures from Irsoy and Cardie (2014), Lample et al. (2016) and Rei (2017). Character-based representations are concatenated with word embeddings, passed through a bidirectional LSTM, and the hidden states from both direction are concatenated. Based on this, a probability distribution over the possible labels is predicted and the most probable label is chosen for each word. While Lample et al. (2016) used a CRF on top of the network, we exclude it here as the token-level scores coming from that network do not necessarily reflect the individual labels, since the best label sequence is chosen globally based on the combined sentence-level score. The supervised model is"
N18-1027,P16-1112,1,0.875977,"ment detection of the tweet as a whole. A single tweet could contain both positive and negative phrases, regardless of its overall polarity, and was therefore separately annotated on the tweet level. In the following example from the dataset, negative phrases are indicated with a bold font and positive phrases are marked with italics, whereas the overall sentiment of the tweet is annotated as negative: grammatically correct sentence. The task has numerous applications for writing improvement and assessment, and recent work has focused on error detection as a supervised sequence labeling task (Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Rei, 2017). Error detection can also be performed on the sentence level – detecting whether the sentence needs to be edited or not. Andersen et al. (2013) described a practical tutoring system that provides sentence-level feedback to language learners. The 2016 shared task on Automated Evaluation of Scientific Writing (Daudaravicius et al., 2016) also required participants to return binary predictions on whether the input sentence needs to be corrected. We evaluate our system on the First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing error"
N18-1027,S15-2078,0,0.0659446,"Missing"
N18-1027,S13-2052,0,\N,Missing
N18-1028,P16-1112,1,0.821307,"res. For each edge e, let a be the position of its left-most component (variable or type) and b the position of its rightmost component (variable or symbol). the embedding vector of its supertype or “NONE” otherwise. These features are mapped to a separate embedding space and then concatenated with the word embedding to form a single task-specific word representation. This allows us to capture useful information about each word, and also designate which words to focus on when processing the sentence. We use a neural sequence labeling architecture, based on the work of Lample et al. (2016) and Rei and Yannakoudakis (2016). The constructed word representations are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and a context-specific representation of each word is created by concatenating the hidden representations from both directions. A hidden layer is added on top to combine the features from both directions. Finally, we use a softmax output layer that predicts a probability distribution over positive or negative assignment for a given edge. We also make use of an extension of neural sequence labeling that combines character-based word representations with word embeddings using a p"
N18-1028,P15-2055,1,0.847572,"ll return instances of the typed SLT in figure 1(b) where the variables are vectors, as opposed to integers. Therefore, a typed index can reduce the number of false positives and increase precision. Four MIR retrieval models are introduced in Section 7.3 designed to control for text indexing/retrieval so that the effects of type-aware vs type-agnostic formula indexing and scoring can be isolated. These models make use of the Tangent formula indexing and scoring functions (Pattaniyil and Zanibbi, 2014), which we have implemented. We use the Cambridge University Math IR Test Collection (CUMTC) (Stathopoulos and Teufel, 2015) which is composed of 120 research-level mathematical information needs and 160 queries. The CUMTC is ideal for our evaluation for two reasons. First, topics in the CUMTC are expressed in natural language and are rich in mathematical types. This allows us to directly apply our best performing variable typing model (BiLSTM) in our retrieval experiment in order to extract variable typings for documents and queries. Second, the CUMTC uses the MREC as its underlying document collection, which enables downstream evaluation in an optimal setting for variable typing. 7.1 m(d, e1 , . . . , en ) = n X"
N18-1028,C16-1221,1,0.862635,"performing model is evaluated on an extrinsic task: MIR, by producing a typed formula index. Our results show that the best performing MIR models make use of our typed index, compared to a formula index only containing raw symbols, thereby demonstrating the usefulness of variable typing. 1 the variables P and N in the symbolic context are assigned the meaning “parabolic subgroup” and “unipotent radical” by the textual context surrounding them respectively. We will refer to the task of assigning one mathematical type to each variable in a sentence as variable typing. We use mathematical types (Stathopoulos and Teufel, 2016) as variable denotation labels. Types are multi-word phrases drawn from the technical terminology of the mathematical discourse that label mathematical objects (e.g., “set”), algebraic structures (e.g., “monoid”) and instantiable notions (e.g., “cardinality of a set”). In the sentence presented earlier, the phrases “parabolic subgroup”, “Levi decomposition” and “unipotent radical” are examples of types. Typing variables may be beneficial to other natural language processing (NLP) tasks, such as topic modeling, to group documents that assign meaning to variables consistently (e.g., “E” is “ener"
N18-1028,N16-1030,0,0.0107615,"tence Table 4: SVM+ features. For each edge e, let a be the position of its left-most component (variable or type) and b the position of its rightmost component (variable or symbol). the embedding vector of its supertype or “NONE” otherwise. These features are mapped to a separate embedding space and then concatenated with the word embedding to form a single task-specific word representation. This allows us to capture useful information about each word, and also designate which words to focus on when processing the sentence. We use a neural sequence labeling architecture, based on the work of Lample et al. (2016) and Rei and Yannakoudakis (2016). The constructed word representations are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and a context-specific representation of each word is created by concatenating the hidden representations from both directions. A hidden layer is added on top to combine the features from both directions. Finally, we use a softmax output layer that predicts a probability distribution over positive or negative assignment for a given edge. We also make use of an extension of neural sequence labeling that combines character-based word representatio"
N18-1028,C16-1030,1,0.836001,"sentations are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), and a context-specific representation of each word is created by concatenating the hidden representations from both directions. A hidden layer is added on top to combine the features from both directions. Finally, we use a softmax output layer that predicts a probability distribution over positive or negative assignment for a given edge. We also make use of an extension of neural sequence labeling that combines character-based word representations with word embeddings using a predictive gating operation (Rei et al., 2016). This allows our model to capture character-level patterns and estimate representations for previously unseen words. In this framework, an alternative word reprethe model). The filters are applied to the input text (i.e. convolutions), and then max-pooled, flattened, concatenated, and a dropout layer (p = 0.5) is then applied before being fed into a multilayer perceptron (MLP), with the number of hidden layers and their hidden units as hyperparameters. Finally, a softmax layer is used to output a binary decision. The model is implemented using the Keras library using binary cross-entropy as l"
N19-1251,W13-1704,1,0.827158,"uding GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules. On the other hand, language is continuously evolving, and there are exceptions to most grammar rules we know. Additionally, rule-based matching typically relies on syntactic pre-processing, which is error-prone, leading to compounding errors that hurt the downstream GED performance. Our contri"
N19-1251,P06-1032,0,0.0728054,"er the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we th"
N19-1251,W18-0529,0,0.0173854,"focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement detection Following recent work on GED (Rei and Yannakoudakis, 2016), we define SVA error detection as a sequence labeling task, where each token is simply labeled as correct or incorrect. For a given SVA error, only the verb is labeled as incorrect. Error types other than SVA are ignored, i.e., we do not corr"
N19-1251,P17-1074,0,0.0690779,"Missing"
N19-1251,Y09-1008,0,0.0357055,") who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun e"
N19-1251,D14-1082,0,0.0353693,"Missing"
N19-1251,W13-1703,0,0.0312982,"5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write texts and native speakers of that language can provide feedback via error correction (Mizumoto et al., 2011). It contains 1, 047, 393 sentences. • NUCLE comprises around 1, 400 essays written by students from the National University of Singapore. It is annotated for error tags and corrections by professional English instructors (Dahlmeier et al., 2013). It contains 57, 151 sentences. • FCE train set. We use the publicly available FCE training set, containing 25, 748 sentences. A subset of 5, 000 sentences was separated and used for development experiments. 4 Artificial errors. We generate artificial subject– verb agreement errors from large amounts of data. Specifically, we use the British National Corpus (BNC, BNC-Consortium et al., 2007), a collection of British English sentences that includes samples from different media such as newspapers, journals, letters or essays. Subject–verb agreement in English merely consists of inflecting 3rd p"
N19-1251,W16-0506,0,0.0147561,"there. 2 2420 • FCE. The Cambridge Learner Corpus of First Certificate in English (FCE) exam scripts consists of texts produced by ESL learners taking the FCE exam, which assesses English at the upper-intermediate proficiency level (Yannakoudakis et al., 2011). We use the publicly available test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different proficiency levels and L1s (Napoles et al., 2017). We evaluate our models on the public test set. • CoNLL14. The test dataset from the CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al.,"
N19-1251,E14-3013,0,0.0221815,"g et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical error"
N19-1251,N18-1108,0,0.022276,"s, and on average achieves a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four"
N19-1251,D18-1541,0,0.0142505,"imilar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al"
N19-1251,L16-1498,0,0.0153074,"ers or essays. Subject–verb agreement in English merely consists of inflecting 3rd person singular verbs in the present tense (and be in the past), which makes any text in English fairly easy to corrupt with SVA errors. We assume that the BNC data is written in correct British English. Using predicted PoS tags provided by the Stanford Log-linear PoS Tagger, we identify verbs in present tense, as well as was and were for the past tense, and flip them to their respective opposite version using the list of inflected English words (annotated with morphological features) from the Unimorph project (Kirov et al., 2016). The final artificial training set includes the sentences with injected errors (265, 742 sentences), their original counterpart, and sentences where SVA errors could not be injected due to not containing candidate verbs that could be flipped (241, 295 sentences). Sentences containing special placeholders for mathematical equations, dates, etc. are filtered out. 5 http://lang-8.com/ 6 Experiments The models. We compare our neural model trained on both artificially generated errors and ESL data (LSTMESL+art ) to three baselines: a neural model trained only on ESL data (LSTMESL ) (i.e., reflecti"
N19-1251,P18-1132,0,0.019321,"es a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence typ"
N19-1251,N16-1030,0,0.0592706,"rd Neural Network Dependency Parser (Chen and Manning, 2014) respectively. 4.2 Neural system We use the state-of-the-art neural sequence labeling architecture for error detection (Rei and Yannakoudakis, 2016). The model receives a sequence of tokens (w1 , ..., wT ) as input and outputs a sequence of labels (l1 , ..., lT ), i.e., one for each token, indicating whether a token is grammatically correct (in agreement) or not, in the given context. All tokens are first mapped to distributed word representations, pre-trained using word2vec (Mikolov et al., 2013) on the Google News corpus. Following Lample et al. (2016), character-based representations are also built for every word using a bi-LSTM (Hochreiter and Schmidhuber, 1997) and then concatenated onto the word embedding. The combined embeddings are then given as input to a word-level bi-LSTM, creating representations that are conditioned on the context from both sides of the target word. These representations are then passed through an additional feedforward layer, in order to combine the extracted features and map them to a more suitable space. A softmax output layer returns the probability distribution over the two possible labels (correct or incorr"
N19-1251,Q16-1037,0,0.479233,"ches. Sequence labeling problems in NLP, including GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules. On the other hand, language is continuously evolving, and there are exceptions to most grammar rules we know. Additionally, rule-based matching typically relies on syntactic pre-processing, which is error-prone, leading to compounding errors that"
N19-1251,J93-2004,0,0.0653251,"cts and verbs are far apart, i.e., when the agreement relation is defined over a long-distance dependency. In order to see how our systems are affected by the distance between the subject and verb, we split the test sets based on different subject–verb distances. Note, however, that our benchmarks are not annotated with PoS tags and dependency relations. If we binned our test data based on predicted dependencies, the inductive bias of our syntactic parser and the errors it made would bias our evaluation. Instead, we perform our analyses on section 22 and 23 of the Penn Treebank (PTB) dataset (Marcus et al., 1993). The PTB however is not annotated with grammatical errors. We therefore corrupt the sentences by injecting SVA errors, in the same 2423 way we corrupted the BNC (§5.3) to create additional training data. For each sentence in the PTB, we identify a subject–verb pair, and group the sentences by the subject–verb distance. We then run our models on two versions of each sentence: an unaltered version and a corrupted one, where we have generated an SVA error by corrupting the verb, using the method described earlier (§5.3). This way we can compute the performance of our models as F0.5 scores over t"
N19-1251,I11-1017,0,0.023707,"CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al., 2014). 5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write texts and native speakers of that language can provide feedback via error correction (Mizumoto et al., 2011). It contains 1, 047, 393 sentences. • NUCLE comprises around 1, 400 essays written by students from the National University of Singapore. It is annotated for error tags and corrections by professional English instructors (Dahlmeier et al., 2013). It contains 57, 151 sentences. • FCE train set. We use the publicly available FCE training set, containing 25, 748 sentences. A subset of 5, 000 sentences was separated and used for development experiments. 4 Artificial errors. We generate artificial subject– verb agreement errors from large amounts of data. Specifically, we use the British National"
N19-1251,C10-2103,0,0.0340272,"00, while character representations have size 100. The word-level LSTM hidden layers have size 300 for each direction, and the character-level LSTM hidden layers have size 100 for each direction. Evaluation. Existing approaches are typically optimised for high precision at the cost of recall, as a system’s utility depends strongly on the ratio of true to false positives, which has been found to be more important in terms of learning effect. A high number of false positives would mean that the system often flags correct language as incorrect, and may therefore end up doing more harm than good (Nagata and Nakatani, 2010). Because of this, F0.5 is preferred to F1 in the GED domain as it puts more weight on precision than recall. For each experiment, we report the token-level precision (P), the recall (R), and the F0.5 scores. 7 Results The main results are summarized in Table 1. Looking at the performance of the LSTMESL+art system, we see that on 3 out of 4 benchmarks, our neural model trained on artificially generated errors outperforms the LSTMESL system with respect to F0.5 . On average, over the four benchmarks, its F0.5 score is 2.43 points higher than the best performing baseline. Both neural models obta"
N19-1251,E17-2037,0,0.0178762,"vailable test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different proficiency levels and L1s (Napoles et al., 2017). We evaluate our models on the public test set. • CoNLL14. The test dataset from the CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al., 2014). 5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write"
N19-1251,P17-1194,1,0.856235,"(Hochreiter and Schmidhuber, 1997) and then concatenated onto the word embedding. The combined embeddings are then given as input to a word-level bi-LSTM, creating representations that are conditioned on the context from both sides of the target word. These representations are then passed through an additional feedforward layer, in order to combine the extracted features and map them to a more suitable space. A softmax output layer returns the probability distribution over the two possible labels (correct or incorrect) for each word. We also include the language modeling objective proposed by Rei (2017), which encourages the model to learn better representations via multi-tasking and predicting surrounding words in the sentence. Dropout (Srivastava et al., 2014) with probability 0.5 is applied to word representations and to the output from the word-level bi-LSTM. The model is optimised using categorical cross-entropy with AdaDelta (Zeiler, 2012). 5 Data 5.1 Data preprocessing As the public datasets either have their own taxonomy or they are not annotated with error types at all, we apply the error type extraction tool of Bryant, Felice, and Briscoe (2017) to automatically get error types map"
N19-1251,W17-5032,1,0.856999,"tterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correcti"
N19-1251,P16-1112,1,0.831037,"ine translation systems, guiding automatically generated output towards grammatically correct sequences. The problem of detecting subject–verb agreement (SVA) errors is an important subtask of GED. In this work, we focus on detecting subject– verb agreement errors in the English as a Second Language (ESL) domain. Most SVA errors occur at the third-person present tense when determining Approaches. Sequence labeling problems in NLP, including GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-ba"
N19-1251,W17-5004,1,0.852217,"ural sequence labeling models. We demonstrate that a system trained on a combination of available labeled data and large volumes of silver standard data outperforms both neural and rule-based baselines by a margin on three out of four standard benchmarks, and on average achieves a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified,"
N19-1251,P11-1093,0,0.0378914,"ch type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task o"
N19-1251,N03-1033,0,0.0125469,"the system. However, our rulebased system is not limited to the detection of simple cases of SVA errors. It relies on PoS tags and dependency relations to identify all types of SVA errors. Specifically, our rule-based system operates as follows: (i) it identifies the candidate verbs based on PoS tags;1 (ii) for a given verb, it uses the dependency relations to find its subject;2 (iii) the PoS tag of the verb and its subject are used to check whether they agree in number and person. We use predicted Penn Treebank PoS tags and dependency relations provided by the Stanford Loglinear PoS Tagger (Toutanova et al., 2003) and the Stanford Neural Network Dependency Parser (Chen and Manning, 2014) respectively. 4.2 Neural system We use the state-of-the-art neural sequence labeling architecture for error detection (Rei and Yannakoudakis, 2016). The model receives a sequence of tokens (w1 , ..., wT ) as input and outputs a sequence of labels (l1 , ..., lT ), i.e., one for each token, indicating whether a token is grammatically correct (in agreement) or not, in the given context. All tokens are first mapped to distributed word representations, pre-trained using word2vec (Mikolov et al., 2013) on the Google News cor"
N19-1251,Y15-2040,0,0.0259707,"ech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan"
N19-1251,N18-1057,0,0.0158796,"cause of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement d"
N19-1251,P11-1019,1,0.706811,"s as our target class. 5.2 Test data We compare the rule-based and neural approaches for the task of SVA error detection on four benchmarks in the ESL domain. 1 Present tense verbs + “was” and “were”. The subject can be direct – attached with a nsubj relation – or indirect, such as when the syntactic subject is a relative pronoun, e.g., who, or an expletive, e.g., there. 2 2420 • FCE. The Cambridge Learner Corpus of First Certificate in English (FCE) exam scripts consists of texts produced by ESL learners taking the FCE exam, which assesses English at the upper-intermediate proficiency level (Yannakoudakis et al., 2011). We use the publicly available test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different profi"
N19-1251,D17-1297,1,0.826452,"asewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement detection Following recent work on GED (Rei and Yannakoudakis, 2016), we define SVA error detection as a sequence labeling task, where each token is simply labeled as correct or incorrect. For a given SVA error, only the verb is labeled as incorrect. Error types other than SVA are ign"
P16-1068,P06-4020,0,0.0160396,"the LSTM in a uni-directional manner (i.e., from left to right) might leave out important information about the sentence. For example, our 3 The maximum time for jointly training a particular SSWE + LSTM combination took about 55–60 hours on an Amazon EC2 g2.2xlarge instance (average time was 27–30 hours). 719 3.4 hidden layer back to the embedding matrix (i.e., we do not provide any pre-trained word embeddings).4 Other Baselines We train a Support Vector Regression model (see Section 4), which is one of the most widely used approaches in text scoring. We parse the data using the RASP parser (Briscoe et al., 2006) and extract a number of different features for assessing the quality of the essays. More specifically, we use character and part-of-speech unigrams, bigrams and trigrams; word unigrams, bigrams and trigrams where we replace open-class words with their POS; and the distribution of common nouns, prepositions, and coordinators. Additionally, we extract and use as features the rules from the phrase-structure tree based on the top parse for each sentence, as well as an estimate of the error rate based on manually-derived error rules. N grams are weighted using tf–idf, while the rest are count-base"
P16-1068,P13-1113,0,0.193059,"Missing"
P16-1068,W03-0209,0,0.0370904,"depends largely on the ability to examine their characteristics, whether they measure what is intended to be measured, and whether their internal marking criteria can be interpreted in a meaningful and useful way. The deep architecture of neural network models, however, makes it rather difficult to identify and extract those properties of text that the network has identified as discriminative. Therefore, we also describe a preliminary method for visualizing the information the model is exploiting when assigning a specific score to an input text. 2 the training set to which it is most similar. Lonsdale and Strong-Krause (2003) use the Link Grammar parser (Sleator and Templerley, 1995) to analyse and score texts based on the average sentencelevel scores calculated from the parser’s cost vector. The Bayesian Essay Test Scoring sYstem (Rudner and Liang, 2002) investigates multinomial and Bernoulli Naive Bayes models to classify texts based on shallow content and style features. eRater (Attali and Burstein, 2006), developed by the Educational Testing Service, was one of the first systems to be deployed for operational scoring in high-stakes assessments. The model uses a number of different features, including aspects o"
P16-1068,W15-0608,0,0.141078,"ssments. The model uses a number of different features, including aspects of grammar, vocabulary and style (among others), whose weights are fitted to a marking scheme by regression. Chen et al. (2010) use a voting algorithm and address text scoring within a weakly supervised bag-of-words framework. Yannakoudakis et al. (2011) extract deep linguistic features and employ a discriminative learning-to-rank model that outperforms regression. Recently, McNamara et al. (2015) used a hierachical classification approach to scoring, utilizing linguistic, semantic and rhetorical features, among others. Farra et al. (2015) utilize variants of logistic and linear regression and develop models that score persuasive essays based on features extracted from opinion expressions and topical elements. There have also been attempts to incorporate more diverse features to text scoring models. Klebanov and Flor (2013) demonstrate that essay scoring performance is improved by adding to the model information about percentages of highly associated, mildly associated and dis-associated pairs of words that co-exist in a given text. Somasundaran et al. (2014) exploit lexical chains and their interaction with discourse elements"
P16-1068,W15-0625,1,0.834457,"ns about where to next evaluate the function. The hyperparameters for our baselines were also determined using the same methodology. All models are trained on our training set (see Section 4), except the one prefixed ‘word2vecpre-trained ’ which uses pre-trained embeddings on the Google News Corpus. We report the Spearman’s rank correlation coefficient ρ, Pearson’s product-moment correlation coefficient r, and the root mean square error (RMSE) between the predicted scores and the gold standard on our test set, which are considered more appropriate metrics for evaluating essay scoring systems (Yannakoudakis and Cummins, 2015). However, we also report Cohen’s κ with quadratic weights, which was the evaluation metric used in the Kaggle competition. Performance of the models is shown in Table 1. In terms of correlation, SVMs produce competitive results (ρ = 0.78 and r = 0.77), outperforming doc2vec, LSTM and BLSTM, as well as their deep counterparts. As described 721 fair comparison as these are trained on a much larger corpus than our training set (which we use to train our models). Nevertheless, when we use our SSWEs models we are able to outperform ‘word2vecpre-trained + Two-layer BLSTM’, even though our embedding"
P16-1068,P11-1019,1,0.949718,"g, 2002) investigates multinomial and Bernoulli Naive Bayes models to classify texts based on shallow content and style features. eRater (Attali and Burstein, 2006), developed by the Educational Testing Service, was one of the first systems to be deployed for operational scoring in high-stakes assessments. The model uses a number of different features, including aspects of grammar, vocabulary and style (among others), whose weights are fitted to a marking scheme by regression. Chen et al. (2010) use a voting algorithm and address text scoring within a weakly supervised bag-of-words framework. Yannakoudakis et al. (2011) extract deep linguistic features and employ a discriminative learning-to-rank model that outperforms regression. Recently, McNamara et al. (2015) used a hierachical classification approach to scoring, utilizing linguistic, semantic and rhetorical features, among others. Farra et al. (2015) utilize variants of logistic and linear regression and develop models that score persuasive essays based on features extracted from opinion expressions and topical elements. There have also been attempts to incorporate more diverse features to text scoring models. Klebanov and Flor (2013) demonstrate that e"
P16-1068,N15-1111,0,0.0368613,"Missing"
P16-1068,C14-1090,0,0.0332949,"oring, utilizing linguistic, semantic and rhetorical features, among others. Farra et al. (2015) utilize variants of logistic and linear regression and develop models that score persuasive essays based on features extracted from opinion expressions and topical elements. There have also been attempts to incorporate more diverse features to text scoring models. Klebanov and Flor (2013) demonstrate that essay scoring performance is improved by adding to the model information about percentages of highly associated, mildly associated and dis-associated pairs of words that co-exist in a given text. Somasundaran et al. (2014) exploit lexical chains and their interaction with discourse elements for evaluating the quality of persuasive essays with respect to discourse coherence. Crossley et al. (2015) identify student attributes, such as standardized test scores, as predictive of writing success and use them in conjunction with textual features to develop essay scoring models. In 2012, Kaggle,2 sponsored by the Hewlett Foundation, hosted the Automated Student Assessment Prize (ASAP) contest, aiming to demonRelated Work In this section, we describe a number of the more influential and/or recent approaches in automate"
P16-1068,P15-1150,0,0.0120681,"Missing"
P16-1112,W13-1704,1,0.557473,"UI consists of different classifiers for each individual error type; and P1+P2+S1+S2 is a combination of four different error correction systems. In contrast, the Bi-LSTM is a single model for detecting all error types, and therefore represents a more scalable data-driven approach. 8 Essay Scoring In this section, we perform an extrinsic evaluation of the efficacy of the error detection system and examine the extent to which it generalises at higher levels of granularity on the task of automated essay scoring. More specifically, we replicate experiments using the text-level model described by Andersen et al. (2013), which is currently deployed in a self-assessment and tutoring system (SAT), an online automated writing feedback tool actively used by language learners.2 The SAT system predicts an overall score for a given text, which provides a holistic assessment of linguistic competence and language proficiency. The authors trained a supervised ranking perceptron model on the FCE-public dataset, using features such as error-rate estimates from a language model and various lexical and grammatical properties of text (e.g., word n-grams, part-of-speech n-grams and phrase-structure rules). We replicate this"
P16-1112,W14-4012,0,0.00617698,"Missing"
P16-1112,A00-2019,0,0.0998518,"nguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition"
P16-1112,W07-1604,0,0.065746,"ally, we integrate the error detection framework with a publicly deployed self-assessment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outp"
P16-1112,C12-1038,0,0.0324911,"ly after the incorrect gap – this is motivated by the intuition that while this token is correct when considered in isolation, it is incorrect in the current context, as another token should have preceeded it. As the main evaluation measure for error detection we use F0.5 , which was also the measure adopted in the CoNLL-14 shared task on error correction (Ng et al., 2014). It combines both precision and recall, while assigning twice as much weight to precision, since accurate feedback is often more important than coverage in error detection applications (Nagata and Nakatani, 2010). Following Chodorow et al. (2012), we also report raw counts for predicted and correct tokens. Related evaluation measures, such as the M 2 -scorer (Ng et al., 2014) and the I-measure (Felice and where xt is the current input, ht−1 is the previous hidden state, bi and bf are biases, ct−1 is the previous internal state (referred to as the cell), and σ is the logistic function. The new internal state is calculated based on the current input and the previous hidden state, and then interpolated with the previous internal state using ft and it as weights: cet = tanh(Wc xt + Uc ht−1 + bc ) ct = ft ct−1 + it cet (10) (11) where is e"
P16-1112,W13-1703,0,0.186665,"Missing"
P16-1112,W11-2838,0,0.025034,"neff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011). We extend this line of research, working towards general error detection systems, and investigate the use of neural compositional models on this task. The related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al"
P16-1112,W12-2006,0,0.022702,"iff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011). We extend this line of research, working towards general error detection systems, and investigate the use of neural compositional models on this task. The related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). The current state-of-the-art approaches can broadly be separated into two categories: 1. Phrase-based statistical machine translation techniques, essentially translating the incorrect source text into the corrected version (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) 2. Averaged Perceptrons and Naive Bayes classifiers making use of native-language error correction priors (Rozovskaya et al., 2014; Rozovskaya et al., 2013). Error correction systems require very specialised models, as they need to generate an improved version of the input text"
P16-1112,P15-1068,0,0.0366487,", this assumes that systems are able to propose a correction for every detected error, and accurate systems for correction might not be optimal for detection. While closed-class errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks (Leacock et al., 2014; Kochmar and Briscoe, 2014). Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold-standard corrections (Bryant and Ng, 2015). Therefore, we treat error detection in learner writing as an independent task and propose a system for labeling each token as being correct or incorrect in context. Common approaches to similar sequence labeling tasks involve learning weights or probabilities for context n-grams of varying sizes, or relying on previously extracted high-confidence context patterns. Both of these methods can suffer from data sparsity, as they treat words as independent units and miss out on potentially related patterns. In addition, they need to specify a fixed context size and are therefore often limited to u"
P16-1112,N15-1060,0,0.0556486,"Missing"
P16-1112,W14-1702,1,0.788251,"orking towards general error detection systems, and investigate the use of neural compositional models on this task. The related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). The current state-of-the-art approaches can broadly be separated into two categories: 1. Phrase-based statistical machine translation techniques, essentially translating the incorrect source text into the corrected version (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) 2. Averaged Perceptrons and Naive Bayes classifiers making use of native-language error correction priors (Rozovskaya et al., 2014; Rozovskaya et al., 2013). Error correction systems require very specialised models, as they need to generate an improved version of the input text, whereas a wider range of tagging and classification models can be deployed on error detection. In addition, automated writing feedback systems that indicate the presence and location of errors may be better from a pedagogic point of view, rather than providing a panacea and cor"
P16-1112,W11-1422,0,0.0194899,"ecific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011). We extend this line of research, working towards general error detection systems, and investigate the use of neural compositional models on this task. The related a"
P16-1112,han-etal-2004-detecting,0,0.0633033,"tion framework with a publicly deployed self-assessment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency pars"
P16-1112,D14-1080,0,0.0428244,"guage modeling (Mikolov et al., 2011; Chelba et al., 2013), where they learn an incremental composition function for predicting the next token in the sequence. However, while language models can estimate the probability of each token, they are unable to differentiate between infrequent and incorrect token sequences. For error detection, the composition function needs to learn to identify semantic anomalies or ungrammatical combinations, independent of their frequency. The bidirectional model provides extra information, as it allows the network to use context on both sides of the target token. Irsoy and Cardie (2014) created an extension of this architecture by connecting together multiple layers of bidirectional Elman-type recurrent network modules. This deep bidirectional RNN (Figure 1d) calculates a context-dependent representation for each token using a bidirectional RNN, and then uses this as input to another bidirectional RNN. The multi-layer structure allows the model to learn more complex higher-level features and effectively perform multiple recurrent passes through the sentence. The long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is an advanced alternative to the Elman-type netw"
P16-1112,D13-1176,0,0.0109538,"s can suffer from data sparsity, as they treat words as independent units and miss out on potentially related patterns. In addition, they need to specify a fixed context size and are therefore often limited to using a small window near the target. Neural network models aim to address these weaknesses and have achieved success in various NLP tasks such as language modeling (Bengio et al., 2003) and speech recognition (Dahl et al., 2012). Recent developments in machine translation have also shown that text of varying length can be represented as a fixed-size vector using convolutional networks (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a) or recurrent neural networks (Cho et al., 2014b; Bahdanau et al., 2015). In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional structures for constructing informative context representations. Based on the findings, we propose a novel framework for performing error detection in learner writing, which achieves state-of-the-art results on two datasets of errorannotated learner essays. The sequence labeling model creates a single variable-si"
P16-1112,C14-1164,0,0.179908,"rs has focussed on error correction, with error detection performance measured as a byproduct of the correction output (Ng et al., 2013; Ng et al., 2014). However, this assumes that systems are able to propose a correction for every detected error, and accurate systems for correction might not be optimal for detection. While closed-class errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks (Leacock et al., 2014; Kochmar and Briscoe, 2014). Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold-standard corrections (Bryant and Ng, 2015). Therefore, we treat error detection in learner writing as an independent task and propose a system for labeling each token as being correct or incorrect in context. Common approaches to similar sequence labeling tasks involve learning weights or probabilities for context n-grams of varying sizes, or relying on previously extracted high-confidence context patterns. Both of these methods can suffer from data sparsity, as they treat words as"
P16-1112,P08-1021,0,0.0342,"ssment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Ki"
P16-1112,D15-1166,0,0.0241515,"Missing"
P16-1112,D14-1102,0,0.0446707,"Missing"
P16-1112,C08-1109,0,0.022685,"raining data to the model. Finally, we integrate the error detection framework with a publicly deployed self-assessment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, s"
P16-1112,D15-1199,0,0.00494131,"ex higher-level features and effectively perform multiple recurrent passes through the sentence. The long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is an advanced alternative to the Elman-type networks that has recently become increasingly popular. It uses 1183 two separate hidden vectors to pass information between different time steps, and includes gating mechanisms for modulating its own output. LSTMs have been successfully applied to various tasks, such as speech recognition (Graves et al., 2013), machine translation (Luong et al., 2015), and natural language generation (Wen et al., 2015). Two sets of gating values (referred to as the input and forget gates) are first calculated based on the previous states of the network: more complex features and performing multiple passes through the sentence. For comparison with non-neural models, we also report results using CRFs (Lafferty et al., 2001), which are a popular choice for sequence labeling tasks. We trained the CRF++ 1 implementation on the same dataset, using as features unigrams, bigrams and trigrams in a 7-word window surrouding the target word (3 words before and after). The predicted label is also conditioned on the prev"
P16-1112,P11-1019,1,0.813156,"ral models, we also report results using CRFs (Lafferty et al., 2001), which are a popular choice for sequence labeling tasks. We trained the CRF++ 1 implementation on the same dataset, using as features unigrams, bigrams and trigrams in a 7-word window surrouding the target word (3 words before and after). The predicted label is also conditioned on the previous label in the sequence. it = σ(Wi xt + Ui ht−1 + Vf ct−1 + bi ) (8) 4 ft = σ(Wf xt + Uf ht−1 + Vf ct−1 + bf ) (9) We evaluate the alternative network structures on the publicly released First Certificate in English dataset (FCE-public, Yannakoudakis et al. (2011)). The dataset contains short texts, written by learners of English as an additional language in response to exam prompts eliciting freetext answers and assessing mastery of the upperintermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. We use the released test set for evaluation, containing 2,720 sentences, leaving 30,953 sentences for training. We further separate 2,222 sentences from the training set for development and hyper-parameter tuning. The dataset contains manually annotated error spans of various types of errors, together w"
P16-1112,N10-1018,0,\N,Missing
P16-1112,W14-1701,0,\N,Missing
P16-1112,W13-3602,0,\N,Missing
P16-1112,W14-1703,0,\N,Missing
P16-1112,W13-3601,0,\N,Missing
P16-1112,W11-2843,0,\N,Missing
P16-1112,C10-2103,0,\N,Missing
P17-1194,D14-1080,0,0.0148042,"Missing"
P17-1194,P17-2054,0,0.0494822,"network, making it applicable to existing state-of-the-art sequence labeling frameworks. Plank et al. (2016) described a related architecture for POS-tagging, predicting the frequency of each word together with the part-of-speech, and showed that this can improve tagging accuracy on low-frequency words. While predicting word frequency can be useful for POS-tagging, language modeling provides a more general training signal, allowing us to apply the model to many different Figure 4: Token-level accuracy on the PTB-POS development set after each training epoch. sequence labeling tasks. Recently, Augenstein and Søgaard (2017) explored multi-task learning for classifying keyphrase boundaries, by incorporating tasks such as semantic super-sense tagging and identification of multi-word expressions. Bingel and Søgaard (2017) also performed a systematic comparison of task relationships by combining different datasets through multi-task learning. Both of these approaches involve switching to auxiliary datasets, whereas our proposed language modeling objective requires no additional data. 9 Conclusion We proposed a novel sequence labeling framework with a secondary objective – learning to predict surrounding words for ea"
P17-1194,E17-2026,0,0.0506633,"er with the part-of-speech, and showed that this can improve tagging accuracy on low-frequency words. While predicting word frequency can be useful for POS-tagging, language modeling provides a more general training signal, allowing us to apply the model to many different Figure 4: Token-level accuracy on the PTB-POS development set after each training epoch. sequence labeling tasks. Recently, Augenstein and Søgaard (2017) explored multi-task learning for classifying keyphrase boundaries, by incorporating tasks such as semantic super-sense tagging and identification of multi-word expressions. Bingel and Søgaard (2017) also performed a systematic comparison of task relationships by combining different datasets through multi-task learning. Both of these approaches involve switching to auxiliary datasets, whereas our proposed language modeling objective requires no additional data. 9 Conclusion We proposed a novel sequence labeling framework with a secondary objective – learning to predict surrounding words for each word in the dataset. One half of a bidirectional LSTM is trained as a forward-moving language model, whereas the other half is trained as a backward-moving language model. At the same time, both o"
P17-1194,W04-1213,0,0.292077,"Missing"
P17-1194,N16-1030,0,0.167554,"f the correct labels: E=− T X t=1 log(P (yt |dt )) (6) While this architecture returns predictions based on all words in the input, the labels are still predicted independently. For some tasks, such as named entity recognition with a BIO1 scheme, there are strong dependencies between subsequent labels and it can be beneficial to explicitly model these connections. The output of the architecture can be modified to include a Conditional Random Field (CRF, Lafferty et al. (2001)), which allows the network to look for the most optimal path through all possible label sequences (Huang et al., 2015; Lample et al., 2016). The model is then optimised by maximising the score for the correct label sequence, while minimising the scores for all other sequences: E = −s(y) + log (1) X es(˜y) (7) y˜∈Ye Next, the concatenated representation is passed through a feedforward layer, mapping the components into a joint space and allowing the model to learn features based on both context directions: where s(y) is the score for a given sequence y and Y is the set of all possible label sequences. We also make use of the character-level component described by Rei et al. (2016), which builds an alternative representation for ea"
P17-1194,J93-2004,0,0.06094,"Missing"
P17-1194,W00-0726,0,0.833477,"Missing"
P17-1194,P11-1019,0,0.12796,"nd that a small static value performed comparably well or even better. These experiments indicate that the language modeling objective helps the network learn general-purpose features that are useful for sequence labeling even in the later stages of training. 5 Error Detection We first evaluate the sequence labeling architectures on the task of error detection – given a sentence written by a language learner, the system needs to detect which tokens have been manually tagged by annotators as being an error. As the first benchmark, we use the publicly released First Certificate in English (FCE, Yannakoudakis et al. (2011)) dataset, containing 33,673 manually annotated sentences. The texts were written by learners during language examinations in response to prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. In addition, we evaluate on the CoNLL 2014 shared task dataset (Ng et al., 2014), which has been converted to an error detection task. This contains 1,312 sentences, written by higher-proficiency learners on more technical topics. They have been manually corrected by two separate annotators, and we report results on each of these annotations. For both datas"
P17-1194,W04-1219,0,0.0302124,"Missing"
P17-1194,P16-2067,0,0.0416629,"98) and has since been extended to many language processing tasks using neural networks. For example, Collobert and Weston (2008) proposed a multitask framework using weight-sharing between networks that are optimised for different supervised tasks. Cheng et al. (2015) described a system for detecting out-of-vocabulary names by also predicting the next word in the sequence. While they use a regular recurrent architecture, we propose a language modeling objective that can be integrated with a bidirectional network, making it applicable to existing state-of-the-art sequence labeling frameworks. Plank et al. (2016) described a related architecture for POS-tagging, predicting the frequency of each word together with the part-of-speech, and showed that this can improve tagging accuracy on low-frequency words. While predicting word frequency can be useful for POS-tagging, language modeling provides a more general training signal, allowing us to apply the model to many different Figure 4: Token-level accuracy on the PTB-POS development set after each training epoch. sequence labeling tasks. Recently, Augenstein and Søgaard (2017) explored multi-task learning for classifying keyphrase boundaries, by incorpor"
P17-1194,W03-0419,0,\N,Missing
P17-1194,P16-1112,1,\N,Missing
P18-2101,andersen-etal-2008-bnc,0,0.0144548,"the features of the broader term. We include this weighted cosine in both directions. • The proportion of shared unique contexts, compared to the number of contexts for one word. This measure is able to capture whether one of the words appears in a subset of the contexts, compared to the other word. This feature is also directional and is therefore included in both directions. We build the sparse distributional word vectors from two versions of the British National Corpus (Leech, 1992). The first counts contexts simply based on a window of size 3. The second uses a parsed version of the BNC (Andersen et al., 2008) and extracts contexts based on dependency relations. In both cases, the features are weighted using pointwise mutual information. Each of the five features is calculated separately for the two vector spaces, resulting in 10 corpus-based features. We integrate them into the network by conditioning the hidden layer h on this vector: h = tanh(Wh d + Wx x + bh ) (11) where x is the feature vector of length 10 and Wx is the corresponding weight matrix. Additional Supervision (AS). Methods such as retrofitting (Faruqui et al., 2015), ATTRACT- REPEL (Mrkˇsi´c et al., 2017) and Poincar´e embeddings ("
P18-2101,Q17-1010,0,0.0805041,"Missing"
P18-2101,N15-1184,0,0.0455295,"Missing"
P18-2101,D17-1185,0,0.658099,"Missing"
P18-2101,P15-2020,1,0.924375,"Missing"
P18-2101,E14-4008,0,0.153031,"al is to make fine-grained assertions regarding the directional hierarchical semantic relationships between concepts (Vuli´c et al., 2017). The task is grounded in theories of concept (proto)typicality and category vagueness from cognitive science (Rosch, 1975; Kamp and Partee, 1995), and aims at answering the following question: “To what degree is X a type of Y ?”. It quantifies the degree of lexical entailment instead of providing only a binary yes/no decision on the relationship between the concepts X and Y , as done in hypernymy detection tasks (Kotlerman et al., 2010; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2017). Graded lexical entailment provides finer-grained 638 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 638–643 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics y embeddings come from a standard distributional vector space, pre-trained on a large unannotated corpus, and are not fine-tuned during training. An element-wise gating operation is then applied to each word, conditioned on the other word: h x d m1 m2 ~ w1 ~ w g1 g2 w1 2 w2 Figure 1: Supervis"
P18-2101,P16-1226,0,0.291171,"Missing"
P18-2101,P14-2050,0,0.0887418,"e pairs. However, only binary labels are attached to all word pairs, whereas the task 640 requires predicting a graded score. Initial experiments with optimising the network to predict the minimal and maximal possible score for these cases did not lead to improved performance. Therefore, we instead make use of a hinge loss function that optimises the network to only push these examples to the correct side of the decision boundary: L= X max((y − yˆ)2 − ( i S − R)2 , 0) 2 (12) Evaluation SDSN Training Setup. As input to the SDSN network we use 300-dimensional dependency-based word embeddings by Levy and Goldberg (2014). Layers m1 and m2 also have size 300 and layer h has size 100. For regularisation, we apply dropout to the embeddings with p = 0.5. The margin R is set to 1 for the supervised pre-training stage. The model is optimised using AdaDelta (Zeiler, 2012) with learning rate 1.0. In order to control for random noise, we run each experiment with 10 different random seeds and average the results. Our code and detailed configuration files will be made available online.1 Evaluation Data. We evaluate graded lexical entailment on the HyperLex dataset (Vuli´c et al., 2017) which contains 2,616 word pairs in"
P18-2101,N15-1098,0,0.0609825,"amples that are not yet on the correct side of the boundary, including a margin. This prevents us from penalising the model for predicting a score with slight variations, as the extracted examples are not annotated with sufficient granularity. When optimising the model, we first perform one pretraining pass over these additional word pairs before proceeding with the regular training process. 4 Random DEV TEST Table 1: Graded lexical entailment detection results on the random and lexical splits of the HyperLex dataset. We report Spearman’s ρ on both validation and test sets. split, proposed by Levy et al. (2015), there is no lexical overlap between training and test subsets. This prevents the effect of lexical memorisation, as supervised models tend to learn an independent property of a single concept in the pair instead of learning a relation between the two concepts. In this setup training, validation, and test sets contain 1133, 85, and 269 word pairs, respectively.2 Since plenty of related research on lexical entailment is still focused on the simpler binary detection of asymmetric relations, we also run experiments on the large binary detection HypeNet dataset (Shwartz et al., 2016), where the S"
P18-2101,Q17-1022,1,0.890193,"Missing"
P18-2101,P15-2070,0,0.0457219,"Missing"
P18-2101,D14-1162,0,0.0835971,"Missing"
P18-2101,W14-1608,1,0.878449,"e trouble encoding features such as word frequency, or the number of unique contexts the word has appeared in. This information becomes important when deciding whether one word entails another, as the system needs to determine when a concept is more general and subsumes the other. We construct classical sparse distributional word vectors and use them to extract 5 unique features for every word pair, to complement the features extracted from neural embeddings: • Regular cosine similarity between the sparse distributional vectors of both words. • The sparse weighted cosine measure, described by Rei and Briscoe (2014), comparing the weighted ranks of different distributional contexts. The measure is directional and assigns more importance to the features of the broader term. We include this weighted cosine in both directions. • The proportion of shared unique contexts, compared to the number of contexts for one word. This measure is able to capture whether one of the words appears in a subset of the contexts, compared to the other word. This feature is also directional and is therefore included in both directions. We build the sparse distributional word vectors from two versions of the British National Cor"
P18-2101,D17-1162,1,0.67903,"(Mikolov et al., 2013), making the resulting vector space reflect (a broad relation of) semantic relatedness but unsuitable for lexical entailment (Vuli´c et al., 2017). The mapping stage allows the network to learn a transformation function from the general skip-gram embeddings to a task-specific space for lexical entailment. In addition, the two weight matrices enable asymmetric reasoning, allowing the network to learn separate mappings for hyponyms and hypernyms. We then use a supervised composition function for combining the two representations and returning a confidence score as output. Rei et al. (2017) described a generalised version of cosine similarity for metaphor detection, constructing a supervised operation and learning individual weights for each 639 feature. We apply a similar approach here and modify it to predict a relation score: d = m1 m2 (7) h = tanh(Wh d + bh ) (8) y = S · σ(a(Wy h + by )) (9) where Wh , bh , a, Wy and by are trainable parameters. The annotated labels of lexical relations are generally in a fixed range, therefore we base the output function on logistic regression, which also restricts the range of the predicted scores. by allows for the function to be shifted"
P18-2101,D16-1234,0,0.0941054,"Missing"
P18-2101,E17-1007,0,0.451918,"arding the directional hierarchical semantic relationships between concepts (Vuli´c et al., 2017). The task is grounded in theories of concept (proto)typicality and category vagueness from cognitive science (Rosch, 1975; Kamp and Partee, 1995), and aims at answering the following question: “To what degree is X a type of Y ?”. It quantifies the degree of lexical entailment instead of providing only a binary yes/no decision on the relationship between the concepts X and Y , as done in hypernymy detection tasks (Kotlerman et al., 2010; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2017). Graded lexical entailment provides finer-grained 638 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 638–643 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics y embeddings come from a standard distributional vector space, pre-trained on a large unannotated corpus, and are not fine-tuned during training. An element-wise gating operation is then applied to each word, conditioned on the other word: h x d m1 m2 ~ w1 ~ w g1 g2 w1 2 w2 Figure 1: Supervised directional similarity network (SDSN) fo"
P18-2101,J17-4004,1,0.795247,"Missing"
P18-2101,C14-1212,0,0.199844,"l entailment, the goal is to make fine-grained assertions regarding the directional hierarchical semantic relationships between concepts (Vuli´c et al., 2017). The task is grounded in theories of concept (proto)typicality and category vagueness from cognitive science (Rosch, 1975; Kamp and Partee, 1995), and aims at answering the following question: “To what degree is X a type of Y ?”. It quantifies the degree of lexical entailment instead of providing only a binary yes/no decision on the relationship between the concepts X and Y , as done in hypernymy detection tasks (Kotlerman et al., 2010; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2017). Graded lexical entailment provides finer-grained 638 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 638–643 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics y embeddings come from a standard distributional vector space, pre-trained on a large unannotated corpus, and are not fine-tuned during training. An element-wise gating operation is then applied to each word, conditioned on the other word: h x d m1 m2 ~ w1 ~ w g1 g2 w1 2"
P18-2101,Q15-1025,0,0.116782,"Missing"
S19-2100,W18-5109,1,0.709828,"Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional document classification methods for spam detection and sentiment analysis, and tend to use lexical and syntactic features (Nobata et al., 2016; Li et al., 2017; Bourgonje et al., 2018). Machine learning"
S19-2100,D14-1179,0,0.0468349,"Missing"
S19-2100,L18-1008,0,0.0298282,"to the GBDT classifier. We provide details of each extension in the following sections. For each subtask, we experiment with combinations of the above and additionally tune the RNN type (between LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014)), dimension, and batch size, whether to use character ngrams (n ∈ [1, 4]), and, when used, the size of self-attention layers. We also run experiments using the unmodified model to find which pretrained embeddings give the best performance. We compare publicly available embeddings trained using Word2Vec (Mikolov et al., 2013), FastText (Mikolov et al., 2018), and GLoVe (Pennington et al., 2014). 4.1 and a further 1-dimensional dense layer. The final dense layer has either sigmoid or exponential activation, corresponding to soft or sharp attention respectively. The weights are normalised to sum to 1, yielding final attention values aei , which are used P to obtain the final sentential representation s = i aei hi . The RNN is then trained using categorical cross-entropy on s passed through a final tanh layer. 4.3 This modification includes the post-softmax output of the RNN as an additional input feature to the decision tree. 4.4 ELMo Self-attentio"
S19-2100,W17-3007,0,0.0223019,"Missing"
S19-2100,C18-1093,1,0.299803,"nstitute for Logic, Language and Computation, University of Amsterdam, Netherlands e.shutova@uva.nl Abstract one, but one with real world impact: if measures can be taken to identify and curtail trolling, the toxicity of the internet can to some extent be reduced. There is evidence that online harassment is connected with oppression, violence and suicide (Dinakar et al., 2011; Sood et al., 2012; Wulczyn et al., 2017), and there may moreover be reasons for concern about the perpetrator’s wellbeing along with that of the victims (Cheng et al., 2017). Our approach to the task extends the work of Mishra et al. (2018b), who extract features from tweets using an RNN for subsequent use in a gradient-boosted decision tree (GBDT) (Ke et al., 2017). Firstly, we experiment with changes to the RNN, including the use of self-attention (Rei and Søgaard, 2019) and ELMo embeddings (Peters et al., 2018). Secondly, we add additional features to the GBDT, including globally-optimised hashtag embeddings learned from a graph of tweet contents using node2vec (Grover and Leskovec, 2016). We show that this method of learning distributional information about hashtags improves performance over just learning their embeddings w"
S19-2100,W17-3013,0,0.0443658,"Missing"
S19-2100,W18-5101,1,0.388541,"nstitute for Logic, Language and Computation, University of Amsterdam, Netherlands e.shutova@uva.nl Abstract one, but one with real world impact: if measures can be taken to identify and curtail trolling, the toxicity of the internet can to some extent be reduced. There is evidence that online harassment is connected with oppression, violence and suicide (Dinakar et al., 2011; Sood et al., 2012; Wulczyn et al., 2017), and there may moreover be reasons for concern about the perpetrator’s wellbeing along with that of the victims (Cheng et al., 2017). Our approach to the task extends the work of Mishra et al. (2018b), who extract features from tweets using an RNN for subsequent use in a gradient-boosted decision tree (GBDT) (Ke et al., 2017). Firstly, we experiment with changes to the RNN, including the use of self-attention (Rei and Søgaard, 2019) and ELMo embeddings (Peters et al., 2018). Secondly, we add additional features to the GBDT, including globally-optimised hashtag embeddings learned from a graph of tweet contents using node2vec (Grover and Leskovec, 2016). We show that this method of learning distributional information about hashtags improves performance over just learning their embeddings w"
S19-2100,W17-3008,0,0.0259519,"training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional document classification methods for spam detection and sentiment analysis, and tend to use lexical and syntactic features (Nobata et al., 2016; Li et al., 2017; Bourgonje et al., 2018). Machine learning techniques range from logistic regression (Cheng et al., 2015) to support vector machines (Yin et al., 2009) to neural networks (Gamb¨ack and Sikdar, 2017). We draw on the work by Mishra and colleagues, who used a character-based recurrent neural network to form contextual word"
S19-2100,N13-1082,0,0.0285617,"nline texts, including those posted in discussion forums, news article comment sections, and social networks. Such detection is not straightforwardly a matter of identifying texts containing obscene words (Malmasi and Zampieri, 2018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Associatio"
S19-2100,D14-1162,0,0.0840716,"details of each extension in the following sections. For each subtask, we experiment with combinations of the above and additionally tune the RNN type (between LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014)), dimension, and batch size, whether to use character ngrams (n ∈ [1, 4]), and, when used, the size of self-attention layers. We also run experiments using the unmodified model to find which pretrained embeddings give the best performance. We compare publicly available embeddings trained using Word2Vec (Mikolov et al., 2013), FastText (Mikolov et al., 2018), and GLoVe (Pennington et al., 2014). 4.1 and a further 1-dimensional dense layer. The final dense layer has either sigmoid or exponential activation, corresponding to soft or sharp attention respectively. The weights are normalised to sum to 1, yielding final attention values aei , which are used P to obtain the final sentential representation s = i aei hi . The RNN is then trained using categorical cross-entropy on s passed through a final tanh layer. 4.3 This modification includes the post-softmax output of the RNN as an additional input feature to the decision tree. 4.4 ELMo Self-attention The model proposed by Mishra et al."
S19-2100,N18-1202,0,0.144031,"Missing"
S19-2100,N19-1144,0,0.0368619,"gs in instances where words were deliberately obscured to evade detection. Following common practice in named entity recognition (Sang and De Meulder, 2003), where fine-grained labels are used to improve performance on the sequence labeling task, we take advantage of the hierarchical labels available for each tweet. For subtasks A and B we train a model to predict all cascading labels, and sum the probabilities of labels under the relevant class to make a final prediction. For example, for subtask A the Data The OffensEval shared task uses the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), which hierarchically labels tweets according to whether or not they are offensive, whether any offence is targeted, and if so targeted at whom: an individual, a group or otherwise. The three subtasks in this shared task correspond to predicting labels at each level of granularity. The data is structured to allow this: all tweets presented in subtask B are guaranteed to be offensive, and all of those in subtask C are targeted. Tweets were collected by using the Twitter API to search for terms that are frequently associated with offensive behaviour. These included political keywords, as poli"
S19-2100,S19-2010,0,0.0283946,"gs in instances where words were deliberately obscured to evade detection. Following common practice in named entity recognition (Sang and De Meulder, 2003), where fine-grained labels are used to improve performance on the sequence labeling task, we take advantage of the hierarchical labels available for each tweet. For subtasks A and B we train a model to predict all cascading labels, and sum the probabilities of labels under the relevant class to make a final prediction. For example, for subtask A the Data The OffensEval shared task uses the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), which hierarchically labels tweets according to whether or not they are offensive, whether any offence is targeted, and if so targeted at whom: an individual, a group or otherwise. The three subtasks in this shared task correspond to predicting labels at each level of granularity. The data is structured to allow this: all tweets presented in subtask B are guaranteed to be offensive, and all of those in subtask C are targeted. Tweets were collected by using the Twitter API to search for terms that are frequently associated with offensive behaviour. These included political keywords, as poli"
S19-2100,W03-0419,0,0.229996,"Missing"
S19-2100,W17-3003,0,0.0202596,"n models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional document classification methods for spam detection and sentiment analysis, and tend to use lexical and syntactic features (Nobata et al., 2016; Li et al., 2017; Bourgonje et al., 2018). Machine learning techniques range from logistic regression (Cheng et al., 2015) to support vector machines (Yin et al., 2009) to neural networks (Gamb¨ack and Sikdar, 2017). We draw on the work by Mishra and colleagues, who used a character-based recurrent neural network to form contextual word representations of out-of-v"
S19-2100,W12-2103,0,0.0540799,"tic detection of offensive opinions expressed in online texts, including those posted in discussion forums, news article comment sections, and social networks. Such detection is not straightforwardly a matter of identifying texts containing obscene words (Malmasi and Zampieri, 2018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota,"
S19-2100,W17-3012,0,0.0306069,"uding those posted in discussion forums, news article comment sections, and social networks. Such detection is not straightforwardly a matter of identifying texts containing obscene words (Malmasi and Zampieri, 2018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational L"
S19-2100,N16-2013,0,0.0364426,"018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional docu"
W10-3008,P06-4020,1,0.80229,"Missing"
W10-3008,W10-3001,0,0.460822,"Missing"
W10-3008,W08-0607,0,0.697422,"Missing"
W10-3008,W04-3103,0,0.466187,"Missing"
W10-3008,P07-1125,1,0.85684,"Missing"
W10-3008,W09-1304,0,0.817847,"supervised CRF classifier was used to refine these predictions. As a final step, scopes were constructed from the classifier output using a small set of post-processing rules. Development of the system revealed a number of issues with the annotation scheme adopted by the organisers. 1 1. Detecting the cues using a token-level supervised classifier. 2. Finding the scopes with a combination of manual rules and a second supervised tokenlevel classifier. 3. Applying postprocessing rules to convert the token-level annotation into predictions about scope. Parts of the system are similar to that of Morante and Daelemans (2009) — both make use of machine learning to tag tokens as being in a cue or a scope. The most important differences are the use of manually defined rules and the inclusion of grammatical relations from a parser as critical features. Introduction Speculative or, more generally, “hedged” language is a way of weakening the strength of a statement. It is usually signalled by a word or phrase, called a hedge cue, which weakens some clauses or propositions. These weakened portions of a sentence form the scope of the hedge cues. Hedging is an important tool in scientific language allowing scientists to g"
W10-3008,P08-1033,0,0.466444,"Missing"
W10-3008,W08-0606,0,0.349839,"Missing"
W11-0202,bentivogli-etal-2010-building,0,0.027228,"sis), (larval fat body → larval tissue) and (the synthesis of x in y ↔ x is synthesised in y). Pattern (2) entails pattern (1) and would also return results matching the information need. (2) the overexpression of x in the larval fat body A system for entailment detection can automatically extract a database of entailing fragments from a large corpus and use them to modify any query given by the user. Recent studies have also investigated how complex sentence-level entailment relations can be broken down into smaller consecutive steps involving fragment-level entailment (Sammons et al., 2010; Bentivogli et al., 2010). For example: (3) Text: The mitogenic effects of the B beta chain of fibrinogen are mediated through cell surface calreticulin. Hypothesis: Fibrinogen beta chain interacts with CRP55. To recognise that the hypothesis is entailed by the text, it can be decomposed into five separate steps involving text fragments: 1. B beta chain of fibrinogen → Fibrinogen beta chain In order to cover a wide variety of language phenomena, a fragment is defined in the following way: Definition 1. A fragment is any connected subgraph of a directed dependency graph containing one or more words and the grammatical"
W11-0202,P10-1124,0,0.0199709,"ean and Rus (2009) make use of weighted dependencies and word semantics to detect paraphrases. In addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing. Lin and Pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations. Szpektor et al. (2004) describe the TEASE method for extracting entailing relation templates from the Web. Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al. (2010) apply it to binary relations in focused entailment graphs. Snow et al. (2005) described a basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text. The use of directional distributional similarity measures to find inference relations between single words is explored by Kotlerman et al. (2010). They propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisation. In contrast to current work, each of the approaches described above only focuses on detecting entailment be"
W11-0202,P06-4020,1,0.695974,"Missing"
W11-0202,W09-0215,0,0.0206492,"various schemes for feature weighting and found the best one to be a variation of Dice’s coefficient (Dice, 1945), described by Curran (2003): wA (f ) = 2P (A, f ) P (A, ∗) + P (∗, f ) where wA (f ) is the weight of feature f for fragment A, P (∗, f ) is the probability of the feature appearing in the corpus with any fragment, P (A, ∗) is the probability of the fragment appearing with any feature, and P (A, f ) is the probability of the fragment and the feature appearing together. Different measures of distributional similarity, both symmetrical and directonal, were also tested and ClarkeDE (Clarke, 2009) was used for the final system as it achieved the highest performance on graph fragments: P ClarkeDE(A → B) = min(wA (f ), wB (f )) f ∈FA ∩FB P f ∈FA wA (f ) where FA is the set of features for fragment A and wA (f ) is the weight of feature f for fragment A. It quantifies the weighted coverage of the features of A by the features of B by finding the sum of minimum weights. 2 http://www.biomedcentral.com/info/about/datamining/ The ClarkeDE similarity measure is designed to detect whether the features of A are a proper subset of the features of B. This works well for finding more general versio"
W11-0202,W10-3001,0,0.042392,"Missing"
W11-0202,H05-1049,0,0.0586057,"Missing"
W11-0202,P98-2127,0,0.0602462,"Missing"
W11-0202,C08-1066,0,0.0246429,"rds is explored by Kotlerman et al. (2010). They propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisation. In contrast to current work, each of the approaches described above only focuses on detecting entailment between specific subtypes of fragments (either sentences, relations or words) and optimising the system for a single scenario. This means only limited types of entailment relations are found and they cannot be used for entailment generation or compositional entailment detection as described in Section 2. MacCartney and Manning (2008) approach sentence-level entailment detection by breaking the problem into a sequence of atomic edits linking the premise to the hypothesis. Entailment relations are then predicted for each edit, propagated up through a syntax tree and then used to compose the resulting entailment decision. However, their system focuses more on natural logic and uses a predefined set of compositional rules to capture a subset of valid inferences with high precision but low recall. It also relies on a supervised classifier and information from WordNet to reach the final entailment decision. Androutsopoulos and"
W11-0202,P10-1122,0,0.0138678,"verexpression → synthesis), (larval fat body → larval tissue) and (the synthesis of x in y ↔ x is synthesised in y). Pattern (2) entails pattern (1) and would also return results matching the information need. (2) the overexpression of x in the larval fat body A system for entailment detection can automatically extract a database of entailing fragments from a large corpus and use them to modify any query given by the user. Recent studies have also investigated how complex sentence-level entailment relations can be broken down into smaller consecutive steps involving fragment-level entailment (Sammons et al., 2010; Bentivogli et al., 2010). For example: (3) Text: The mitogenic effects of the B beta chain of fibrinogen are mediated through cell surface calreticulin. Hypothesis: Fibrinogen beta chain interacts with CRP55. To recognise that the hypothesis is entailed by the text, it can be decomposed into five separate steps involving text fragments: 1. B beta chain of fibrinogen → Fibrinogen beta chain In order to cover a wide variety of language phenomena, a fragment is defined in the following way: Definition 1. A fragment is any connected subgraph of a directed dependency graph containing one or more"
W11-0202,C08-1107,0,0.0202487,"(2006) combine lexico-syntactic features and automatically acquired paraphrases to classify entailing sentences. Lintean and Rus (2009) make use of weighted dependencies and word semantics to detect paraphrases. In addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing. Lin and Pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations. Szpektor et al. (2004) describe the TEASE method for extracting entailing relation templates from the Web. Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al. (2010) apply it to binary relations in focused entailment graphs. Snow et al. (2005) described a basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text. The use of directional distributional similarity measures to find inference relations between single words is explored by Kotlerman et al. (2010). They propose new measures based on feature ranks and compare them with existing ones for the tasks of lexical expansion and text categorisat"
W11-0202,W04-3206,0,0.0277677,"ies when calculating similarity, which supports incorporation of extra syntactic information. Hickl et al. (2006) combine lexico-syntactic features and automatically acquired paraphrases to classify entailing sentences. Lintean and Rus (2009) make use of weighted dependencies and word semantics to detect paraphrases. In addition to similarity they look at dissimilarity between two sentences and use their ratio as the confidence score for paraphrasing. Lin and Pantel (2001) were one of the first to extend the distributional hypothesis to dependency paths to detect entailment between relations. Szpektor et al. (2004) describe the TEASE method for extracting entailing relation templates from the Web. Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al. (2010) apply it to binary relations in focused entailment graphs. Snow et al. (2005) described a basic method of syntactic pattern matching to automatically discover word-level hypernym relations from text. The use of directional distributional similarity measures to find inference relations between single words is explored by Kotlerman et al. (2010). They propose new measures base"
W11-0202,W05-1202,0,0.0152433,"d to a truly recursive method where fragments consist of smaller fragments. 3.2 Extrinsic similarity The extrinsic similarity between two fragments or words is modelled using measures of directional distributional similarity. We define a context relation as a tuple (a, d, r, a0 ) where a is the main word, a0 is a word connected to it through a dependency relation, r is the label of that relation and d shows the direction of the relation. The tuple f : (d, r, a0 ) is referred to as a feature of a. To calculate the distributional similarity between two fragments, we adopt an approach similar to Weeds et al. (2005). Using the previous notation, (d, r, a0 ) is a feature of fragment A if (d, r, a0 ) is a feature for a word which is contained in A. The general algorithm for feature collection is as follows: 13 1. Find the next instance of a fragment in the background corpus. 2. For each word in the fragment, find dependency relations which connect to words not contained in the fragment. 3. Count these dependency relations as distributional features for the fragment. For example, in Figure 1 the fragment (* induces * in *) has three features: (1, subj, B61), (1, dobj, autophosphorylation) and (1, dobj, cell"
W11-0202,morante-2010-descriptive,0,\N,Missing
W11-0202,C98-2122,0,\N,Missing
W14-1608,andersen-etal-2008-bnc,1,0.788635,"occur more than once in the dataset, and weight them using pointwise mutual information to construct feature vectors for every term. Features with negative weights were retained, as they proved to be beneficial for some similarity measures. The window-based, dependency-based and word2vec vector sets were all trained on 112M words from the British National Corpus, with preprocessing steps for lowercasing and lemmatising. Any numbers were grouped and substituted by more generic tokens. For constructing the dependency-based vector representations, we used the parsed version of the BNC created by Andersen et al. (2008) with the RASP toolkit (Briscoe et al., 2006). When saved as plain text, the 500dimensional word2vec vectors and dependencybased vectors are comparable in size (602MB and 549MB), whereas the window-based vectors are twice as large (1,004MB). We make these vector 1. Systematic evaluation of different vector space models and similarity measures on the task of hyponym generation. 2. Proposal of new properties for modelling the directional hyponymy relation. 3. Release of three lexical vector datasets, trained using neural network, window-based, and dependency-based features. 2 Vector space models"
W14-1608,W11-2501,0,0.100829,"inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Aut"
W14-1608,C92-2082,0,0.297896,"ions. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. Howe"
W14-1608,E12-1004,0,0.0727088,"to discard WordNet hypernyms that are very rare in practical use, and would not have enough examples for learning informative vector representations. The final dataset contains the remaining terms, together with all of their hyponyms, including the rare/unseen hyponyms. As expected, some general terms, such as group or location, have a large number of inherited hyponyms. On average, each hypernym in the dataset has 233 hyponyms, but the distribution is roughly exponential, and the median is only 36. In order to better facilitate future experiments with supervised methods, such as described by Baroni et al. (2012), we randomly separated the data into training (1230 hypernyms), validation (922), and test (922) sets, and we make these datasets publically available online.4 function decreasing linearly as the rank number increases, but the weights for the shared features always remain higher compared to the non-shared features. Tied feature values are handled by assigning them the average rank value. Adding 1 to the denominator of the relative rank calculation avoids exceptions with empty vectors, and also ensures that the value will always be strictly greater than C. While the basic function is still the"
W14-1608,P98-2127,0,0.0911649,", but this can lead to unreliable results when the number of features is very small. The motivation behind combining these measures is that the symmetric Lin measure will decrease the final score for such word pairs, thereby balancing the results. Similarity measures We compare the performance of a range of similarity measures, both directional and symmetrical, on the task of hyponym generation. Cosine similarity is defined as the angle between two feature vectors and has become a standard measure of similarity between weighted vectors in information retrieval (IR). Lin similarity, created by Lin (1998), uses the ratio of shared feature weights compared to all feature weights. It measures the weighted proportion of features that are shared by both words. DiceGen2 is one possible method for generalising the Dice measure to real-valued weights (Curran, 2003; Grefenstette, 1994). The dot product of the weight vectors is normalised by the total sum of all weights. The same formula can also be considered as a possible generalisation for the Jaccard measure. WeedsPrec and WeedsRec were proposed by Weeds et al. (2004) who suggested using precision and recall as directional measures of word similari"
W14-1608,P06-4020,1,0.745272,"Missing"
W14-1608,N13-1090,0,0.0222916,"rk that takes the concatenated vectors of context words as input, and is trained to predict the vector representation of the next word, which is then transformed into a probability distribution over possible words. To speed up training and testing, they use a hierarchical data structure for filtering down the list of candidates. Both CW and HLBL vectors were trained using 37M words from RCV1. Word2vec: We created word representations using the word2vec2 toolkit. The tool is based on a feedforward neural network language model, with modifications to make representation learning more efficient (Mikolov et al., 2013a). We make use of the skip-gram model, which takes each word in a sequence as an input to a log-linear classifier with a continuous projection layer, and predicts words within a certain range before and after the input word. The window size was set to 5 and vectors were trained with both 100 and 500 dimensions. Dependencies: Finally, we created vector representations for words by using dependency relations from a parser as features. Every incoming and outgoing dependency relation is counted as a feature, together with the connected term. For example, given the dependency relation (play, dobj,"
W14-1608,P99-1016,0,0.0247043,"tionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly"
W14-1608,W09-0215,0,0.0276972,"is in the role of retrieval results. Precision is then calculated by comparing the intersection (items correctly returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity by taking their geometric average. The APInc measure finds the proportion of shared features relative to the features for the narrower term, but this can lead to unreliable results when the number of features is very small. The motivation behind combining these"
W14-1608,N04-1041,0,0.0116134,"words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-"
W14-1608,C08-1107,0,0.00872041,"time, and the result is compared to hyponym candidates using cosine similarity. For sparse high-dimensional vector space models it was not feasible to use the full offset vector during experiments, therefore we retain only the top 1,000 highest-weighted features. 3 term a is in the role of retrieval results. Precision is then calculated by comparing the intersection (items correctly returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity"
W14-1608,P10-1040,0,0.0133961,"co-occurrences in a fixed context window. Every word that occurs within a window of three words before or after is counted as a feature for the target word. Pointwise mutual information is then used for weighting. CW: Collobert and Weston (2008) constructed a neural network language model that is trained to predict the next word in the sequence, and simultaneously learns vector representations for each word. The vectors for context words are concatenated and used as input for the neural network, which uses a sample of possible outputs for gradient calculation to speed up the training process. Turian et al. (2010) recreated their experiments and made the vectors available online.1 HLBL: Mnih and Hinton (2007) created word representations using the hierarchical log-bilinear 1 2 http://metaoptimize.com/projects/wordreprs/ 69 https://code.google.com/p/word2vec/ sets publically available for download.3 Recently, Mikolov et al. (2013b) published interesting results about linguistic regularities in vector space models. They proposed that the relationship between two words can be characterised by their vector offset, for example, we could find the vector for word “queen” by performing the operation “king - ma"
W14-1608,W96-0103,0,0.0249265,"ten make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies through clustering (Ushioda, 1996; Bisson et al., 2000; Wagner, 2000; Paaß et al., 2004; Cimiano and Staab, 2005), and is not directly applicable to hyponym generation. While most work has been done on hyponym detection (and the related task of lexical substitution), barely any evaluation has been done for hyponym generation. We have found that systems for hyponym detection often perform poorly on hyponym generation, as the latter requires returning results from a much less restricted candidate set, The task of detecting and generating hyponyms is at the core of semantic understanding of language, and has numerous practical a"
W14-1608,J05-4002,0,0.0172427,"Missing"
W14-1608,C04-1146,0,0.79769,"returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity by taking their geometric average. The APInc measure finds the proportion of shared features relative to the features for the narrower term, but this can lead to unreliable results when the number of features is very small. The motivation behind combining these measures is that the symmetric Lin measure will decrease the final score for such word pairs, thereby balancing the res"
W14-1608,J09-3004,0,0.0525115,"marek@swiftkey.net Abstract the input text. Entailment and inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of"
W14-1608,C98-2122,0,\N,Missing
W16-0533,W13-1704,0,0.0140127,"upervised SVM classifier to train a binary sentence-based relevance model with 18 sentencelevel features. We extend this line of work and investigate unsupervised methods using neural embeddings for the task of assessing topical relevance of individual sentences. By providing sentence-level feedback, our approach is able to highlight specific areas of the text that require more attention, as opposed to showing a single overall score. Sentencebased relevance scores could also be used for estimating coherence in an essay, or be combined with a more general score for indicating sentence quality (Andersen et al., 2013). 283 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 283–288, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics In the following sections we explore a number of alternative similarity functions for this task. The evaluation of the methods was performed on two different publicly available datasets and revealed that alternative approaches are required, depending on the nature of the prompts. We propose a new method which achieves substantially better performance on one of the datasets, and construct a co"
W16-0533,N04-1024,0,0.0604221,"ogical variations. Ideally, the assessment system should be able to handle the introduction of new prompts, i.e. ones for which no previous data exists. This allows the list of available topics to be edited dynamically, and students or teachers can insert their own unique prompts for every essay. We can achieve this by constructing an unsupervised function that measures similarity between the prompt and the learner writing. While previous work on prompt relevance assessment has mostly focussed on full essays, scoring individual sentences for prompt relevance has been relatively underexplored. Higgins et al. (2004) used a supervised SVM classifier to train a binary sentence-based relevance model with 18 sentencelevel features. We extend this line of work and investigate unsupervised methods using neural embeddings for the task of assessing topical relevance of individual sentences. By providing sentence-level feedback, our approach is able to highlight specific areas of the text that require more attention, as opposed to showing a single overall score. Sentencebased relevance scores could also be used for estimating coherence in an essay, or be combined with a more general score for indicating sentence"
W16-0533,N16-1162,0,0.0296498,"e ICLE dataset. By combining the two methods, in the form of IDF-Embeddings, accuracy is consistently improved on both datasets, confirming the hypothesis that weighting word embeddings can lead to a better sentence representation. The Skip-Thoughts method does not perform well for the task of sentence-level topic detection. This is possibly due to the model being trained to predict individual words in neighbouring sentences, therefore learning various syntactic and paraphrasing patterns, whereas prompt relevance requires more general topic similarity. Our results are consistent with those of Hill et al. (2016), who found that SkipThoughts performed very well when the vectors were used as features in a separate supervised classifier, but gave low results when used for unsupervised similarity tasks. The newly proposed Weighted-Embeddings method substantially outperforms Word2Vec and IDF-Embeddings on both datasets, showing that automatically learning word weights in combination with pretrained embeddings is a beneficial approach. In addition, this method achieves the best overall performance on the ICLE dataset by a large margin. Finally, we experimented with a combination method, creating a weighted"
W16-0533,W10-1013,0,0.372124,"topical relevance scores has been done using supervised methods. Persing and Ng (2014) trained a linear regression model for detecting relevance to each prompt, but this approach requires substantial training data for all the possible prompts. Higgins et al. (2006) addressed off-topic detection by measuring the cosine similarity between tf-idf vector representations of the prompt and the entire essay. However, as this method only captures similarity using exact matching at the word-level, it can miss many topically relevant word occurrences in the essay. In order to overcome this limitation, Louis and Higgins (2010) investigated a number of methods that expand the prompt with related words, such as morphological variations. Ideally, the assessment system should be able to handle the introduction of new prompts, i.e. ones for which no previous data exists. This allows the list of available topics to be edited dynamically, and students or teachers can insert their own unique prompts for every essay. We can achieve this by constructing an unsupervised function that measures similarity between the prompt and the learner writing. While previous work on prompt relevance assessment has mostly focussed on full e"
W16-0533,P14-1144,0,0.163967,"ay attempt to shift the topic of the essay in a more familiar direction, which grammatical error detection systems are not able to capture. In an automated examination framework, this weakness could be further exploited by memorising a grammatically correct essay and presenting it in response to any prompt. Being able to detect topical relevance can help prevent such weaknesses, provide useful feedback to the students, and is also a step towards evaluating more creative aspects of learner writing. Most existing work on assigning topical relevance scores has been done using supervised methods. Persing and Ng (2014) trained a linear regression model for detecting relevance to each prompt, but this approach requires substantial training data for all the possible prompts. Higgins et al. (2006) addressed off-topic detection by measuring the cosine similarity between tf-idf vector representations of the prompt and the entire essay. However, as this method only captures similarity using exact matching at the word-level, it can miss many topically relevant word occurrences in the essay. In order to overcome this limitation, Louis and Higgins (2010) investigated a number of methods that expand the prompt with r"
W16-0533,P11-1019,0,0.0443145,"te each sentence in a learner essay. While 3 https://github.com/ryankiros/skip-thoughts X http://www.marekrei.com/projects/weighted-embeddings not all sentences in an essay are expected to directly convey the prompt, any noise in the dataset equally disadvantages all systems, and the ability to assign a higher score to the correct prompt directly reflects the ability of the model to capture topical relevance. Two separate publicly available corpora of learner essays, written by upper-intermediate level language learners, were used for evaluation. The First Certificate in English dataset (FCE, Yannakoudakis et al. (2011)), consisting of 30,899 sentences written in response to 60 prompts; and the International Copus of Learner English dataset (ICLE, Granger et al. (2009)) containing 20,883 sentences, written in response to 13 prompts.4 There are substantial differences in the types of prompts used in these two datasets. The ICLE prompts are short and general, designed to point the student towards an open discussion around a topic. In contrast, the FCE contains much more detailed prompts, describing a scenario or giving specific instructions on what should be mentioned in the text. An average prompt in ICLE con"
W16-0533,D14-1179,0,\N,Missing
W16-1603,D15-1041,0,0.170798,"nalyzer (§4.1), test how well it learns word semantics, including for unseen words (§4.2), and examine the structure of the embedding space (§4.3). 2 2.2 Another approach to go beyond words is based on on character-level neural network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS tagging have been promising, with reductions in word perplexity for language modelling and state-of-the-art English POS tagging accuracy (Ling et al., 2015; Kim et al., 2016). Ballesteros et al. (2015) train a character-level model for parsing. Zhang et al. (2015) do away with words completely, and train a convolutional neural network to do text classification directly from characters. Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology (Ling et al., 2015; Kim et al., 2016). Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging (Xu et al., 2015; Toutanova e"
W16-1603,Q15-1016,0,0.0914049,"on and suffix information have long been used in tasks such as POS tagging (Xu et al., 2015; Toutanova et al., 2003). By explicitly modelling these features, one might expect good performance gains in many NLP tasks. What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). In addition, the correlation between model word similarity scores and human similarity judgments is typically high (Levy et al., 2015). However, no previous work (to our knowledge) evaluates the similarity judgments Related Work While words are often treated as the fundamental unit of language, they are in fact themselves compositional. The smallest unit of semantics is the morpheme, while the smallest unit of orthography is the grapheme, or character. Both have been used as a method to go beyond word-level models. 2.1 Character-level models Morphemic analysis and semantics As word semantics is compositional, one might ask whether it is possible to learn morpheme representations, and compose them to obtain good word represen"
W16-1603,W13-3512,0,0.355395,"semantics is the morpheme, while the smallest unit of orthography is the grapheme, or character. Both have been used as a method to go beyond word-level models. 2.1 Character-level models Morphemic analysis and semantics As word semantics is compositional, one might ask whether it is possible to learn morpheme representations, and compose them to obtain good word representations. Lazaridou et al. (2013) demonstrated precisely this: one can derive good representations of morphemes distributionally, and apply tools from compositional distributional semantics to obtain good word representations. Luong et al. (2013) also trained a morphological composition model based on recursive neural networks. Botha and Blunsom (2014) built a language model incorporating morphemes, and demonstrated improvements in language modelling and in machine translation. All of these 19 vector for a word w should have high inner product with context vectors for words with which it is typically seen, and low inner products with context vectors for words it is not typically seen with. Figure 1 illustrates this for a particular example. In Mikolov et al. (2013b), the noise distribution P (w) is proportional to the unigram probabil"
W16-1603,Q15-1012,0,0.121664,"Missing"
W16-1603,N15-1186,0,0.0174991,", pages 18–26, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics approaches incorporated external morphological knowledge, either in the form of gold standard morphological analyses such as CELEX (Baayen et al., 1995) or an external morphological analyzer such as Morfessor (Creutz and Lagus, 2007). Unsupervised morphology induction aims to decide whether two words are morphologically related or to generate a morphological analysis for a word (Goldwater et al., 2005; Goldsmith, 2001). While they may use semantic insights to perform the morphological analysis (Soricut and Ochs, 2015), they typically are not concerned with obtaining a semantic representation for morphemes, nor of the resulting word. segment corresponding to the stem will have the most weight. Our model ‘reads’ the word and outputs a sequence of word segments. We weight each segment, and then combine the segments to obtain the final word representation. These representations are trained to predict context words, as this has been shown to give word representations which capture word semantics well (Mikolov et al., 2013b). As the root morpheme has the most context-predictive power, we expect our model to assi"
W16-1603,J01-2001,0,0.116742,"s decline in quality for 18 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 18–26, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics approaches incorporated external morphological knowledge, either in the form of gold standard morphological analyses such as CELEX (Baayen et al., 1995) or an external morphological analyzer such as Morfessor (Creutz and Lagus, 2007). Unsupervised morphology induction aims to decide whether two words are morphologically related or to generate a morphological analysis for a word (Goldwater et al., 2005; Goldsmith, 2001). While they may use semantic insights to perform the morphological analysis (Soricut and Ochs, 2015), they typically are not concerned with obtaining a semantic representation for morphemes, nor of the resulting word. segment corresponding to the stem will have the most weight. Our model ‘reads’ the word and outputs a sequence of word segments. We weight each segment, and then combine the segments to obtain the final word representation. These representations are trained to predict context words, as this has been shown to give word representations which capture word semantics well (Mikolov et"
W16-1603,N03-1033,0,0.0838295,"al. (2015) train a character-level model for parsing. Zhang et al. (2015) do away with words completely, and train a convolutional neural network to do text classification directly from characters. Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology (Ling et al., 2015; Kim et al., 2016). Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging (Xu et al., 2015; Toutanova et al., 2003). By explicitly modelling these features, one might expect good performance gains in many NLP tasks. What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). In addition, the correlation between model word similarity scores and human similarity judgments is typically high (Levy et al., 2015). However, no previous work (to our knowledge) evaluates the similarity judgments Related Work Whi"
W16-1603,P15-2041,0,0.0127986,"). Ballesteros et al. (2015) train a character-level model for parsing. Zhang et al. (2015) do away with words completely, and train a convolutional neural network to do text classification directly from characters. Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology (Ling et al., 2015; Kim et al., 2016). Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging (Xu et al., 2015; Toutanova et al., 2003). By explicitly modelling these features, one might expect good performance gains in many NLP tasks. What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). In addition, the correlation between model word similarity scores and human similarity judgments is typically high (Levy et al., 2015). However, no previous work (to our knowledge) evaluates the similarity j"
W16-1603,P82-1020,0,0.815509,"Missing"
W16-1603,P13-1149,0,0.33087,"g. Finally, we show that incorporating morphology explicitly into character-level models helps them produce embeddings for unseen words which correlate better with human judgments. 1 2. word embedding models handle out-ofvocabulary words badly, typically as a single ‘OOV’ token. 3. the word distribution has a long tail, and many parameters are needed to capture all of the words in a corpus (for an embedding size of 300 with a vocabulary of 10k words, 3 million parameters are needed) One approach to smooth word distributions is to operate on the smallest meaningful semantic unit, the morpheme (Lazaridou et al., 2013; Botha and Blunsom, 2014). However, previous work on the morpheme level has all used external morphological analyzers. These require a separate preprocessing step, and cannot be adapted to suit the problem at hand. Another is to operate on the smallest orthographic unit, the character (Ling et al., 2015; Kim et al., 2016). However, the link between shape and meaning is often complicated (de Saussure, 1916), as alphabetic characters carry no inherent semantic meaning. To account for this, the model has to learn complicated dependencies between strings of characters to accurately capture word m"
W16-1603,D15-1176,0,\N,Missing
W16-1603,N16-1080,0,\N,Missing
W17-5004,D16-1195,0,0.0135021,"e parameters were optimised. contains more fine-grained labels per error. For example, the FCE (Yannakoudakis et al., 2011) training set has 75 different labels for individual error types, such as missing determiners or incorrect verb forms. By giving the model access to these labels, the system can learn more fine-grained error patterns that are based on the individual error types. 4 Evaluation setup and datasets • first language: Previous work has experimentally demonstrated that the distribution of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoudakis (2016) investigate a number of compositional architectures for error detection, and present state-of-the-art results using a bidirectional LSTM. We follow their experimental setup and investigate the impact of auxiliary loss functions on the same datasets: the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) and the CoNLL-14 shared task test set (Ng et al., 2014b). FCE contains texts written by non-native learners of English in response to exam prompts eliciting free-te"
W17-5004,W13-1704,1,0.89379,"ls detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the following year, Additional Training Data The main benefits of multi-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who"
W17-5004,W13-1703,0,0.0458884,"Additional Training Data The main benefits of multi-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who also experimented with augmenting the publicly available datasets with training data from a large proprietary corpus. In total, we train this large model on 17.8M tokens from the Cambridge Learner Corpus (CLC, Nicholls 2003), the NUS Corpus of Learner English (NUCLE, Dahlmeier et al. 2013), and the Lang-8 corpus (Mizumoto et al., 2011). We use the same model architecture as Rei and Yannakoudakis (2016), adding only the auxiliary objective of predicting the automatically generated POS tag, which was the most successful additional objective based on the development experiments. Table 6 contains results for evaluating this model, when trained on the large training set. On the FCE test data, the auxiliary objective does not provide an improvement and the model performance is comparable to the results by Rei and Yannakoudakis (2016) (R&Y). Since most of the 39 by randomly switching"
W17-5004,E17-2026,0,0.119476,"equence labeling datasets that have been manually annotated for different tasks: • The CoNLL 2000 dataset (Tjong Kim Sang and Buchholz, 2000) for chunking, containing sections of the Wall Street Journal and annotated with 22 different labels. • The CoNLL 2003 corpus (Tjong Kim Sang and De Meulder, 2003) contains texts from the Reuters Corpus and has been annotated with 8 labels for named entity recognition (NER). • The Penn Treebank (PTB) POS corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 POS tags. The CoNLL-00 dataset was identified by Bingel and Søgaard (2017) as being the most useful additional training resource in a multi-task setting; The CoNLL-03 NER dataset has a similar label density as the error detection task; and the PTB corpus was chosen as POS tags gave consistently good performance for error detection on both the development and test sets, as demonstrated in the previous section. In the first setting, each of these datasets is used to train a sequence labeling model for their respective tasks, and the resulting model is used to initialise a network for training an error detection system. While it is common to preload word embeddings fro"
W17-5004,W12-2006,0,0.0182062,"(Section 6). There has been some research on using auxiliary training objectives in the context of other tasks. Cheng et al. (2015) described a system for detecting out-of-vocabulary names by also predicting the next word in the sequence. Plank et al. (2016) predicted the frequency of each word together with the POS, and showed that this can improve tagging accuracy on low-frequency words. However, we are the first to explore the auxiliary objectives described in Section 3 in the context of error detection. the HOO 2012 shared task only focused on correcting preposition and determiner errors (Dale et al., 2012). The recent CoNLL shared tasks (Ng et al., 2013, 2014a) focused on error correction rather than detection: CoNLL-13 targeted correcting noun number, verb form and subjectverb agreement errors, in addition to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and ver"
W17-5004,P06-4020,0,0.0414369,"2,222 sentences for development and 2,720 sentences for testing. The development set was randomly sampled from the training data, and the test set contains texts from a different examination year. The CoNLL-14 test set contains 50 texts annotated by two experts. Compared to FCE, the texts are more technical and are written by higherproficiency learners. In order to make our results comparable to Rei and Yannakoudakis (2016), we • part-of-speech: POS tagging is a wellestablished sequence labeling task, requiring the model to disambiguate the word types based on their contexts. We use the RASP (Briscoe et al., 2006) parser to automatically generate POS labels for the training data, and include them as additional objectives. • grammatical relations: We include as an auxiliary objective the type of the Grammatical Relation (GR) in which the current token is a dependent, in order to incentivise the model to learn more about semantic composition. Again we use the RASP parser, which is unlexicalised and therefore more suitable for learner data where spelling and grammatical errors are common. Table 1 presents the labels for each of the auxiliary tasks for an example sentence from the FCE training data. The au"
W17-5004,W11-2838,0,0.0498802,", 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the following year, Additional Training Data The main benefits of multi-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who also experimented with augmenting the publicly available datasets with training data from a large proprietary corpus. In total, we train this large model on 17.8M tokens"
W17-5004,E14-3013,0,0.0755885,"resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance. 1 Introduction Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjective– noun combinations. To tackle the scarcity of errorannotated training data, previous work has investigated the utility of automatically generated ungrammatical data (Foster and Andersen, 2009; Felice and Yuan, 2014), as well as explored learning from native well-formed data (Rozovskaya and Roth, 2016; Gamon, 2010). In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data. We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and de2 Error Detection Model In addition to the scarcity of errors in the training data (i.e., the majority of tokens are correct), recent research has highlighted"
W17-5004,W14-1702,1,0.873966,"to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), as well as phrase-based statistical machine translation, under the hypothesis that incorrect source sentences can be “translated” to correct target sentences (Felice et al., 2014; Grundkiewicz, 2014). 9 Conclusion We have described a method for integrating auxiliary loss functions with a neural sequence labeling framework, in order to improve error detection in learner writing. While predicting binary error labels, the model also learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection. We performed a systematic comparison of possible auxiliary labels, which are either available in existing annotations or can be generated automatically. Our experiments showed that POS t"
W17-5004,D15-1085,0,0.17153,"Missing"
W17-5004,C12-1038,0,0.0146383,"detection results on the CoNLL-14 test set using different auxiliary loss functions. also evaluate our models on the two CoNLL14 test annotations and train our models only on the public FCE dataset. This corresponds to their FCE-public model that treats the CoNLL-14 dataset as an out-of-domain test set corpus. Following the CoNLL-14 shared task, we also report F0.5 as the main evaluation metric. However, while the shared task focused on correction and calculated F0.5 over error spans using multiple annotations, we evaluate token-level error detection performance. Following recommendations by Chodorow et al. (2012), we also report the raw counts for predicted and correct tokens. For pre-processing, all the texts are lowercased and digits are replaced with zeros for the tokenlevel representations, although the character-based component has access to the original version of each token. Tokens that occur only once are mapped to a single OOV token, which is then used to represent previously unseen tokens during testing. The word embeddings have size 300 and are initialised with publicly available word2vec (Mikolov et al., 2013) embeddings trained on Google News. The LSTM hidden layers have size 200 and the"
W17-5004,J93-2004,0,0.0584361,"Missing"
W17-5004,N10-1019,0,0.171794,"ficiently and achieve better performance. 1 Introduction Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjective– noun combinations. To tackle the scarcity of errorannotated training data, previous work has investigated the utility of automatically generated ungrammatical data (Foster and Andersen, 2009; Felice and Yuan, 2014), as well as explored learning from native well-formed data (Rozovskaya and Roth, 2016; Gamon, 2010). In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data. We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and de2 Error Detection Model In addition to the scarcity of errors in the training data (i.e., the majority of tokens are correct), recent research has highlighted the variability in manual correction of writing errors: re-annotation of the CoNLL 2014 shared task"
W17-5004,I08-1059,0,0.0333398,"004). More recent approaches have exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the followin"
W17-5004,I11-1017,0,0.0473082,"ulti-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who also experimented with augmenting the publicly available datasets with training data from a large proprietary corpus. In total, we train this large model on 17.8M tokens from the Cambridge Learner Corpus (CLC, Nicholls 2003), the NUS Corpus of Learner English (NUCLE, Dahlmeier et al. 2013), and the Lang-8 corpus (Mizumoto et al., 2011). We use the same model architecture as Rei and Yannakoudakis (2016), adding only the auxiliary objective of predicting the automatically generated POS tag, which was the most successful additional objective based on the development experiments. Table 6 contains results for evaluating this model, when trained on the large training set. On the FCE test data, the auxiliary objective does not provide an improvement and the model performance is comparable to the results by Rei and Yannakoudakis (2016) (R&Y). Since most of the 39 by randomly switching between different tasks and updating parameters"
W17-5004,W14-1703,0,0.0349582,"terminer errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), as well as phrase-based statistical machine translation, under the hypothesis that incorrect source sentences can be “translated” to correct target sentences (Felice et al., 2014; Grundkiewicz, 2014). 9 Conclusion We have described a method for integrating auxiliary loss functions with a neural sequence labeling framework, in order to improve error detection in learner writing. While predicting binary error labels, the model also learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection. We performed a systematic comparison of possible auxiliary labels, which are either available in existing annotations or can be generated automatically. Our experiments showed that POS tags, grammatical rela"
W17-5004,W14-1701,0,0.0382055,"ion of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoudakis (2016) investigate a number of compositional architectures for error detection, and present state-of-the-art results using a bidirectional LSTM. We follow their experimental setup and investigate the impact of auxiliary loss functions on the same datasets: the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) and the CoNLL-14 shared task test set (Ng et al., 2014b). FCE contains texts written by non-native learners of English in response to exam prompts eliciting free-text answers. The texts have been manually annotated with error types and error spans by professional examiners, which Rei and Yannakoudakis (2016) convert to a binary correct/incorrect token-level labeling for error detection. For missing-word errors, the error label is assigned to the next word in the sequence. The released version contains 28,731 sentences for training, 2,222 sentences for development and 2,720 sentences for testing. The development set was randomly sampled from the t"
W17-5004,N16-1179,0,0.0336032,"nce been applied to many language processing tasks and neural network architectures. For example, Collobert and Weston (2008) constructed a convolutional architecture that shared some weights between tasks such as POS tagging, NER and chunking. Whereas their model only shared word embeddings, our approach focuses on learning better compositional features through a shared bidirectional LSTM. Luong et al. (2016) explored a multi-task architecture for sequence-to-sequence learning where encoders and decoders in different languages are trained jointly using the same semantic representation space. Klerke et al. (2016) used eye tracking measurements as a secondary task in order to improve a model for sentence compression. Bingel and Søgaard (2017) explored beneficial task relationships for training multitask models on different datasets. All of these architectures are trained 40 functions and better word representations. The error detection model, which also learns to predict automatically generated POS tags, achieved improved performance on both CoNLL-14 benchmarks. A useful direction for future work would be to investigate dynamic weighting strategies for auxiliary objectives that allow the network to ini"
W17-5004,W13-3601,0,0.0234403,"auxiliary training objectives in the context of other tasks. Cheng et al. (2015) described a system for detecting out-of-vocabulary names by also predicting the next word in the sequence. Plank et al. (2016) predicted the frequency of each word together with the POS, and showed that this can improve tagging accuracy on low-frequency words. However, we are the first to explore the auxiliary objectives described in Section 3 in the context of error detection. the HOO 2012 shared task only focused on correcting preposition and determiner errors (Dale et al., 2012). The recent CoNLL shared tasks (Ng et al., 2013, 2014a) focused on error correction rather than detection: CoNLL-13 targeted correcting noun number, verb form and subjectverb agreement errors, in addition to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), a"
W17-5004,C14-1164,0,0.0136672,"exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the following year, Additional Training Data The mai"
W17-5004,P16-2067,0,0.408618,"not feasible. In addition, writing errors can be very sparse, leaving the system with very little useful training data for learning error patterns. In order to train models that generalise well with limited training examples, we would want to encourage them to learn more generic patterns of language, grammar, syntax and composition, which can then be exploited for error detection. Multi-task learning allows models to learn from multiple objectives via shared representations, using information from related tasks to boost performance on tasks for which there is limited target data. For example, Plank et al. (2016) explored the option of using word frequency as an auxiliary loss function for part-of-speech (POS) tagging. Rei (2017) describe a semi-supervised framework for multi-task learning, integrating language modeling as an additional objective. Following this work, we adapt auxiliary objectives for the task of error detection, and further experiNext, the network includes a tanh-activated feedforward layer, using the hidden states from both LSTMs as input, allowing the model to learn more complex higher-level features. By combining the hidden states from both directions, we are able to have a vector"
W17-5004,P11-1019,1,0.818124,"468 1461 1458 1584 1803 1654 1781.0 55.7 54.4 54.9 57.0 54.3 57.9 57.7 23.3 23.2 23.1 25.1 28.6 26.2 28.3 43.4 42.7 42.8 45.5 46.0 46.4 47.7 Table 2: Error detection results on the FCE dataset using different auxiliary loss functions. hidden and output layers. However, these components are required only during the training process; at testing time, these can be removed and the resulting model has the same architecture and number of parameters as the baseline, with the only difference being in how the parameters were optimised. contains more fine-grained labels per error. For example, the FCE (Yannakoudakis et al., 2011) training set has 75 different labels for individual error types, such as missing determiners or incorrect verb forms. By giving the model access to these labels, the system can learn more fine-grained error patterns that are based on the individual error types. 4 Evaluation setup and datasets • first language: Previous work has experimentally demonstrated that the distribution of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoud"
W17-5004,P17-1194,1,0.818541,"ing error patterns. In order to train models that generalise well with limited training examples, we would want to encourage them to learn more generic patterns of language, grammar, syntax and composition, which can then be exploited for error detection. Multi-task learning allows models to learn from multiple objectives via shared representations, using information from related tasks to boost performance on tasks for which there is limited target data. For example, Plank et al. (2016) explored the option of using word frequency as an auxiliary loss function for part-of-speech (POS) tagging. Rei (2017) describe a semi-supervised framework for multi-task learning, integrating language modeling as an additional objective. Following this work, we adapt auxiliary objectives for the task of error detection, and further experiNext, the network includes a tanh-activated feedforward layer, using the hidden states from both LSTMs as input, allowing the model to learn more complex higher-level features. By combining the hidden states from both directions, we are able to have a vector that represents a specific token but also takes into account context on both sides: dt = tanh(Wf ht XX (3) where Wf an"
W17-5004,C16-1030,1,0.870248,"as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) moving through the sentence in both directions. At each step, the LSTM calculates a new hidden representation based on the current token embedding and the hidden state from the previous step. (f ) ht (f ) (1) (b) (2) = LSTM(xt , ht−1 ) (b) ht = LSTM(xt , ht+1 ) E=− t (f ) (b) + Wb ht ) yet,k log(yt,k ) (5) k where yt,k is the predicted probability of token t having label k, and yet,k has the value 1 if the correct label for token t is k, and the value 0 otherwise. We also make use of the character-level extension described by Rei et al. (2016). Each token is separated into individual characters and mapped to character embeddings. Using a bidirectional LSTM and a hidden feedforward component, the character vectors are composed into a characterbased token representation. Finally, a dynamic gating function is used to combine this representation with a regular token embedding, taking advantage of both approaches. This component allows the model to capture useful morphological and character-based patterns, in addition to learning individual token-level vectors of common tokens. 3 Auxiliary Loss Functions The model in Section 2 learns to"
W17-5004,W14-1704,0,0.0384609,"ared tasks (Ng et al., 2013, 2014a) focused on error correction rather than detection: CoNLL-13 targeted correcting noun number, verb form and subjectverb agreement errors, in addition to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), as well as phrase-based statistical machine translation, under the hypothesis that incorrect source sentences can be “translated” to correct target sentences (Felice et al., 2014; Grundkiewicz, 2014). 9 Conclusion We have described a method for integrating auxiliary loss functions with a neural sequence labeling framework, in order to improve error detection in learner writing. While predicting binary error labels, the model also learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection. We per"
W17-5004,P11-1093,0,0.0610564,"difference being in how the parameters were optimised. contains more fine-grained labels per error. For example, the FCE (Yannakoudakis et al., 2011) training set has 75 different labels for individual error types, such as missing determiners or incorrect verb forms. By giving the model access to these labels, the system can learn more fine-grained error patterns that are based on the individual error types. 4 Evaluation setup and datasets • first language: Previous work has experimentally demonstrated that the distribution of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoudakis (2016) investigate a number of compositional architectures for error detection, and present state-of-the-art results using a bidirectional LSTM. We follow their experimental setup and investigate the impact of auxiliary loss functions on the same datasets: the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) and the CoNLL-14 shared task test set (Ng et al., 2014b). FCE contains texts written by non-native learners of English in response to exa"
W17-5004,P16-1208,0,0.0676779,"it to be optimised more efficiently and achieve better performance. 1 Introduction Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjective– noun combinations. To tackle the scarcity of errorannotated training data, previous work has investigated the utility of automatically generated ungrammatical data (Foster and Andersen, 2009; Felice and Yuan, 2014), as well as explored learning from native well-formed data (Rozovskaya and Roth, 2016; Gamon, 2010). In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data. We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and de2 Error Detection Model In addition to the scarcity of errors in the training data (i.e., the majority of tokens are correct), recent research has highlighted the variability in manual correction of writing errors: re-annotation of the CoNLL 201"
W17-5004,P10-2065,0,0.0271596,"ually constructed error grammars and mal-rules (e.g., Foster and Vogel 2004). More recent approaches have exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems"
W17-5004,C08-1109,0,0.0623554,"ules (e.g., Foster and Vogel 2004). More recent approaches have exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class err"
W17-5004,W00-0726,0,0.0590858,"over the basic system. The inclusion of POS tags in the auxiliary objective consistently leads to the highest F0.5 . While GRs also improve performance over the main system, their overall contribution is less compared to the FCE test set, which can be explained by the different writing style in the CoNLL data. 6 effectiveness of our approach, we implement two alternative multi-task learning strategies for error detection. For these experiments, we make use of three established sequence labeling datasets that have been manually annotated for different tasks: • The CoNLL 2000 dataset (Tjong Kim Sang and Buchholz, 2000) for chunking, containing sections of the Wall Street Journal and annotated with 22 different labels. • The CoNLL 2003 corpus (Tjong Kim Sang and De Meulder, 2003) contains texts from the Reuters Corpus and has been annotated with 8 labels for named entity recognition (NER). • The Penn Treebank (PTB) POS corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 POS tags. The CoNLL-00 dataset was identified by Bingel and Søgaard (2017) as being the most useful additional training resource in a multi-task setting; The CoNLL-03 NER dataset has a simil"
W17-5004,W09-2112,0,\N,Missing
W17-5004,C08-1022,0,\N,Missing
W17-5004,W03-0419,0,\N,Missing
W17-5004,P15-1068,0,\N,Missing
W17-5004,P16-1112,1,\N,Missing
W17-5016,P16-1068,1,0.395662,"commercial use, including Project Essay Grade (PEG) (Page, 2003), e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003) and Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) among others. They employ statistical approaches that exploit a wide range of textual features. A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writin"
W17-5016,D14-1162,0,0.0764291,"ce not only reduce grader workload, but also bypass grader inconsistencies as only one system would be responsible for the 1 https://www.kaggle.com/c/asap-aes/ 149 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 149–158 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics into neural networks to determine what minimum useful information they can utilize to enhance their predictive power. Initializing neural models with contextually rich word embeddings pre-trained on large corpora (Mikolov et al., 2013; Pennington et al., 2014; Turian et al., 2010) has been used to feed the networks with meaningful embeddings rather than random initialization. Those embeddings are generic and widely employed in Natural Language Processing (NLP) tasks, yet few attempts have been made to learn more task-specific embeddings. For instance, Alikaniotis et al. (2016) developed score-specific word embeddings (SSWE) to address the AA task on the ASAP dataset. Their embeddings are constructed by ranking correct ngrams against their “noisy” counterparts, in addition to capturing words’ informativeness measured by their contribution to the ov"
W17-5016,W13-1704,0,0.0139616,"s. Subsequently, a second filter is applied over sentence representations followed by a pooling operation then a fully-connected layer to predict the final score. Their CNN was applied to the ASAP dataset and its efficacy in in-domain and domain-adaptation essay evaluation was demonstrated in comparison to traditional state-of-the-art baselines. Several AA approaches in the literature have exploited the “quality” or “correctness” of ngrams as a feature to discriminate between good and poor essays. Phandi et al. (2015) defined good essays Figure 1: Error-specific Word Embeddings (ESWE). (SAT) (Andersen et al., 2013) that applied a supervised ranking perceptron to rich linguistic features. Adding their correctness probability feature successfully enhanced the predictive power of the SAT. as the ones with grades above or equal to the average score and the rest as poor ones. They calculated the Fisher scores (Fisher, 1922) of ngrams and selected 201 with the highest scores as “useful ngrams”. Similarly, they generated correct POS ngrams from grammatically correct texts, classified the rest as “bad POS ngrams” and used them along with the useful ngrams and other shallow lexical features as bag-of-words featu"
W17-5016,D15-1049,0,0.249167,"onal systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recourse to handcrafted features. However, other AA datasets pose a challenge to neural models and they still fail to beat state-of-the-art methods when evaluated on these sets. An example of such datasets is the First Certificate in English (FCE) set where applying a rank preference Support Vector Machine (SVM) trained on various lexical and grammatical features achieved the best results (Yannakoudakis et al., 2011). This motivates further investigation We p"
W17-5016,P16-1112,1,0.926663,"other initialization methods and augmenting the model with error corrections helps alleviate the effects of data sparsity. Finally, we further analyse the pre-trained representations and demonstrate that our embeddings are better at detecting errors which is inherent for AA. 2 number of erroneous script words script length This correlation could even be higher if error severity is accounted for as some errors could be more serious than others. Therefore, it seems plausible to exploit writing errors and integrate them into AA systems, as was successfully done by Yannakoudakis et al. (2011) and Rei and Yannakoudakis (2016), but not by capturing this information directly in word embeddings in a neural AA model. Our pre-training model learns to predict a score for each ngram based on the errors it contains and modifies the word vectors accordingly. The idea is to arrange the embedding space in a way that discriminates between “good” and “bad” ngrams based on their contribution to writing errors. Bootstrapping the assessment neural model with those learned embeddings could help detect wrong pat150 Related Work There have been various attempts to employ neural networks to assess the essays in the ASAP dataset. Tagh"
W17-5016,P06-4020,1,0.66906,"to examine the effects of training with extra data, we conduct experiments where we augment the public set with additional FCE scripts and refer to this extended version as FCEext , which contains 9, 822 scripts. We report the results of both datasets on the released test set. The public FCE dataset is divided into 1, 061 scripts for training and 80 for development while for FCEext , 8, 842 scripts are used for training and 980 are held out for development. The only data preprocessing employed is word tokenization which is achieved using the Robust Accurate Statistical Parsing (RASP) system (Briscoe et al., 2006). Evaluation. We replicate the SSWE model, implement our ESWE and ECSWE models, use Google and GloVe embeddings and conduct a comparison between the 5 initilization approaches by feeding their output embeddings to the AA system from Section 3.2. All the models are implemented using the open-source Python library Theano (Al-Rfou et al., 2016). For evaluation, we calculate Spearman’s rank correlation coefficient (ρ), Pearson’s product-moment correlation coefficient (r) and root mean square error (RM SE) between the final predicted script scores and the ground-truth values (Yannakoudakis and Cumm"
W17-5016,D16-1193,0,0.199001,"Project Essay Grade (PEG) (Page, 2003), e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003) and Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) among others. They employ statistical approaches that exploit a wide range of textual features. A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recours"
W17-5016,D16-1115,0,0.294813,"tein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003) and Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) among others. They employ statistical approaches that exploit a wide range of textual features. A recent direction of research has focused on applying deep learning to the AA task in order to circumvent the heavy feature engineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recourse to handcrafted features. However, other AA datasets pose a ch"
W17-5016,P10-1040,0,0.0628994,"workload, but also bypass grader inconsistencies as only one system would be responsible for the 1 https://www.kaggle.com/c/asap-aes/ 149 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 149–158 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics into neural networks to determine what minimum useful information they can utilize to enhance their predictive power. Initializing neural models with contextually rich word embeddings pre-trained on large corpora (Mikolov et al., 2013; Pennington et al., 2014; Turian et al., 2010) has been used to feed the networks with meaningful embeddings rather than random initialization. Those embeddings are generic and widely employed in Natural Language Processing (NLP) tasks, yet few attempts have been made to learn more task-specific embeddings. For instance, Alikaniotis et al. (2016) developed score-specific word embeddings (SSWE) to address the AA task on the ASAP dataset. Their embeddings are constructed by ranking correct ngrams against their “noisy” counterparts, in addition to capturing words’ informativeness measured by their contribution to the overall score of the ess"
W17-5016,P11-1019,1,0.811718,"ineering involved in traditional systems. Several neural architectures have been employed including variants of Long Short-Term Memory (LSTM) (Alikaniotis et al., 2016; Taghipour and Ng, 2016) and Convolutional Neural Networks (CNN) (Dong and Zhang, 2016). They were all applied to the Automated Student Assessment Prize (ASAP) dataset, released in a Kaggle contest1 , which contains essays written by middle-school English speaking students. On this dataset, neural models that only operate on word embeddings outperformed state-of-the-art statistical methods that rely on rich linguistic features (Yannakoudakis et al., 2011; Phandi et al., 2015). The results obtained by neural networks on the ASAP dataset demonstrate their ability to capture properties of writing quality without recourse to handcrafted features. However, other AA datasets pose a challenge to neural models and they still fail to beat state-of-the-art methods when evaluated on these sets. An example of such datasets is the First Certificate in English (FCE) set where applying a rank preference Support Vector Machine (SVM) trained on various lexical and grammatical features achieved the best results (Yannakoudakis et al., 2011). This motivates furt"
W17-5016,W15-0625,0,0.0150669,"(Briscoe et al., 2006). Evaluation. We replicate the SSWE model, implement our ESWE and ECSWE models, use Google and GloVe embeddings and conduct a comparison between the 5 initilization approaches by feeding their output embeddings to the AA system from Section 3.2. All the models are implemented using the open-source Python library Theano (Al-Rfou et al., 2016). For evaluation, we calculate Spearman’s rank correlation coefficient (ρ), Pearson’s product-moment correlation coefficient (r) and root mean square error (RM SE) between the final predicted script scores and the ground-truth values (Yannakoudakis and Cummins, 2015). Dataset. For our experiments, we use the FCE dataset (Yannakoudakis et al., 2011) which consists of exam scripts written by English learners of upper-intermediate proficiency and graded with scores ranging from 1 to 40.8 Each script contains two answers corresponding to two different Training. Hyperparameter tuning is done for each model separately. The SSWE, ESWE and ECSWE models are initialized with GloVe (dwrd = 50) vectors, trained for 20 epochs and the learning rate is set to 0.01. For SSWE, α is set to 0.1, batch size to 128, the number of randomly gen6 https://code.google.com/archive/"
W17-5020,W17-5016,1,0.82151,"rained for image recognition. The visual representation is then conditioned on the input sentence and mapped to a vector representation v. Both u and v are given as input to a function that predicts a confidence score for the answer being relevant to the image. In the next sections we will describe each of these components in more detail. mance improvements for each of the model modifications. 2 Relevance Detection Model Automated methods for scoring essays and short answers have made great progress in recent years (Yannakoudakis et al., 2011; Sakaguchi et al., 2015; Alikaniotis et al., 2016; Hussein et al., 2017), achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the topical relevance of the submitted text. Students with limited language skills may attempt to shift the topic of the response in a more familiar direction, which automated systems would not be able to detect. In a high-stakes examination framework, this weakness could be further exploited by memorising a grammatically correct answer and presenting it in response to any prompt. Being able to detect topical relevance can help prevent such weaknesses, provide u"
W17-5020,P16-1068,1,0.839997,"extracted using a model trained for image recognition. The visual representation is then conditioned on the input sentence and mapped to a vector representation v. Both u and v are given as input to a function that predicts a confidence score for the answer being relevant to the image. In the next sections we will describe each of these components in more detail. mance improvements for each of the model modifications. 2 Relevance Detection Model Automated methods for scoring essays and short answers have made great progress in recent years (Yannakoudakis et al., 2011; Sakaguchi et al., 2015; Alikaniotis et al., 2016; Hussein et al., 2017), achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the topical relevance of the submitted text. Students with limited language skills may attempt to shift the topic of the response in a more familiar direction, which automated systems would not be able to detect. In a high-stakes examination framework, this weakness could be further exploited by memorising a grammatically correct answer and presenting it in response to any prompt. Being able to detect topical relevance can help prevent suc"
W17-5020,P16-1223,0,0.0258774,", preventing the model from excessively relying on the presence of specific features. The process can also be thought of as training a randomly constructed smaller network at each training iteration, resulting in a full combination model. At test time, all the values are retained, but scaled with (1 − p) to compensate for the difference. While dropout is commonly applied to weights inside the network (Tai et al., 2015; Zhang et al., 2015; Kalchbrenner et al., 2015; Kim et al., 2016), there is also some recent work that deploy dropout directly on the word embeddings (Rockt¨aschel et al., 2016; Chen et al., 2016). The relevance scoring model needs to handle texts from different domains, including error-prone sentences from language learners, and dropout on the embeddings allows us to introduce robustness into the training process. We use an LSTM component for processing the word embeddings, building up a sentence representation. It is similar to a traditional recurrent neural network, with specialised gating functions that allow it to dynamically decide which information to carry forward or forget. The LSTM calcu189 scoredot(u,v) v h1 h2 u w1 w2 w3 x&apos; z x Figure 1: The outline of the relevance detecti"
W17-5020,W16-0512,0,0.0195297,"presenting it in response to any prompt. Being able to detect topical relevance can help prevent such weaknesses, provide useful feedback to the students, and is also a step towards evaluating more creative aspects of learner writing. While there is existing work on detecting answer relevance given a textual prompt (Persing and Ng, 2014; Cummins et al., 2015; Rei and Cummins, 2016), only limited previous research has been done to extend this to visual prompts. Some recent work has investigated answer relevance to visual prompts as part of automated scoring systems (Somasundaran et al., 2015; King and Dickinson, 2016), but they reduced the problem to a textual similarity task by relying on hand-written reference descriptions for each image without directly incorporating visual information. Our proposed relevance detection model takes an image and a sentence as input, and assigns a score indicating how relevant the image is to the text. Formulating this as a scoring problem instead of binary classification allows us to treat the model output as a confidence score, and the classification threshold can be selected at a later stage based on the specific application. Kiros et al. (2014) describe a supervised me"
W17-5020,P15-1150,0,0.0836002,"Missing"
W17-5020,D16-1157,0,0.019596,"embedding and the previous hidden representation at time step n − 1: hn = LST M (wn , hn−1 ) (2) The last hidden representation hN is calculated based on all the words in the sequence, thereby allowing the model to iteratively construct a semantic representation of the whole sentence. We use this vector u = hN to represent a given input sentence in the relevance scoring model. Since wordlevel processing is not ideal for handling spelling errors in learner texts, future work could also investigate character-based extensions for text composition, such as those described by Rei et al. (2016) and Wieting et al. (2016). 2.2 z = σ(uWz + bz ) (3) where Wz is a weight matrix, bz is a bias vector, and σ() is the logistic activation function with values between 0 and 1. A new image representation x0 is then calculated by applying these elementwise weights to the visual vector x: Image Processing In order to map images to feature vectors, a pretrained image recognition model is combined with a supervised transformation component. We make use of the BVLC GoogLeNet image recognition model, which is based on an architecture described by Szegedy et al. (2015) and provided by the Caffe toolkit (Jia et al., 2014). The"
W17-5020,P14-1144,0,0.146826,"d on grammaticality (Yannakoudakis et al., 2011; Ng et al., 2014), but are known to be vulnerable to memorised off-topic answers which can be a critical weakness in high-stakes testing. In addition, students who have limited relevant vocabulary may try to shift the topic of their answer in a more familiar direction, which most automated assessment systems are not able to capture. Solutions for detecting topical relevance can help prevent these weaknesses and provide informative feedback to the students. While there is previous work on assessing the relevance of answers given a textual prompt (Persing and Ng, 2014; Cummins et al., 2015; Rei and Cummins, 2016), very little research has been done to incorporate visual writing prompts. In 188 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 188–197 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics relevant and irrelevant textual answers. The outline of our framework can be seen in Figure 1. The input sentence is first passed through a Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber (1997)) component, mapping it to a vector representation u. The visual fea"
W17-5020,P11-1019,0,0.247566,"ng work on detecting answer relevance given a textual prompt, very little previous research has been done to incorporate visual writing prompts. We propose a neural architecture and several extensions for detecting off-topic responses to visual prompts and evaluate it on a dataset of texts written by language learners. 1 Introduction Evaluating the relevance of learner essays with respect to the assigned prompt is an important part of automated writing assessment (Higgins et al., 2006; Briscoe et al., 2010). Existing systems are able to assign high-quality assessments based on grammaticality (Yannakoudakis et al., 2011; Ng et al., 2014), but are known to be vulnerable to memorised off-topic answers which can be a critical weakness in high-stakes testing. In addition, students who have limited relevant vocabulary may try to shift the topic of their answer in a more familiar direction, which most automated assessment systems are not able to capture. Solutions for detecting topical relevance can help prevent these weaknesses and provide informative feedback to the students. While there is previous work on assessing the relevance of answers given a textual prompt (Persing and Ng, 2014; Cummins et al., 2015; Rei"
W17-5020,C16-1030,1,0.846456,"d on the current word embedding and the previous hidden representation at time step n − 1: hn = LST M (wn , hn−1 ) (2) The last hidden representation hN is calculated based on all the words in the sequence, thereby allowing the model to iteratively construct a semantic representation of the whole sentence. We use this vector u = hN to represent a given input sentence in the relevance scoring model. Since wordlevel processing is not ideal for handling spelling errors in learner texts, future work could also investigate character-based extensions for text composition, such as those described by Rei et al. (2016) and Wieting et al. (2016). 2.2 z = σ(uWz + bz ) (3) where Wz is a weight matrix, bz is a bias vector, and σ() is the logistic activation function with values between 0 and 1. A new image representation x0 is then calculated by applying these elementwise weights to the visual vector x: Image Processing In order to map images to feature vectors, a pretrained image recognition model is combined with a supervised transformation component. We make use of the BVLC GoogLeNet image recognition model, which is based on an architecture described by Szegedy et al. (2015) and provided by the Caffe toolki"
W17-5020,Q14-1006,0,0.0263499,"Dropout was applied to both word embeddings and image vectors with p = 0.5. In order to avoid any outlier results due to randomness in the model, which affects both the random initialisation and the sampling of negative image examples, we trained each configuration with 10 different random seeds and present here the averaged results. Ideally, we would like to train the model on examples where pairs of images and sentences are specifically annotated for their semantic relevance. However, since the collected dataset is not large enough for training neural networks, we make use of the Flickr30k (Young et al., 2014) dataset which contains implicitly relevant pairs of images and their corresponding descriptions. Flickr30k is an image captioning dataset, containing 31,014 images and 5 hand-written sentences describing each image. We use the same splits as Karpathy and Li (2015) for training and development; the dataset sizes are shown in Table 1. During training, the model is presented with 32 sentences and their corresponding images in each batch, making sure all the images within a batch are unique. The loss function from Section 2.3 is then minimised to maximise the predicted scores for the 32 relevant"
W17-5020,W16-0533,1,0.918708,"011; Ng et al., 2014), but are known to be vulnerable to memorised off-topic answers which can be a critical weakness in high-stakes testing. In addition, students who have limited relevant vocabulary may try to shift the topic of their answer in a more familiar direction, which most automated assessment systems are not able to capture. Solutions for detecting topical relevance can help prevent these weaknesses and provide informative feedback to the students. While there is previous work on assessing the relevance of answers given a textual prompt (Persing and Ng, 2014; Cummins et al., 2015; Rei and Cummins, 2016), very little research has been done to incorporate visual writing prompts. In 188 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 188–197 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics relevant and irrelevant textual answers. The outline of our framework can be seen in Figure 1. The input sentence is first passed through a Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber (1997)) component, mapping it to a vector representation u. The visual features for the input image are extracted using"
W17-5020,N15-1111,0,0.0281204,"for the input image are extracted using a model trained for image recognition. The visual representation is then conditioned on the input sentence and mapped to a vector representation v. Both u and v are given as input to a function that predicts a confidence score for the answer being relevant to the image. In the next sections we will describe each of these components in more detail. mance improvements for each of the model modifications. 2 Relevance Detection Model Automated methods for scoring essays and short answers have made great progress in recent years (Yannakoudakis et al., 2011; Sakaguchi et al., 2015; Alikaniotis et al., 2016; Hussein et al., 2017), achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the topical relevance of the submitted text. Students with limited language skills may attempt to shift the topic of the response in a more familiar direction, which automated systems would not be able to detect. In a high-stakes examination framework, this weakness could be further exploited by memorising a grammatically correct answer and presenting it in response to any prompt. Being able to detect topical rele"
W17-5020,W15-0605,0,0.0302941,"atically correct answer and presenting it in response to any prompt. Being able to detect topical relevance can help prevent such weaknesses, provide useful feedback to the students, and is also a step towards evaluating more creative aspects of learner writing. While there is existing work on detecting answer relevance given a textual prompt (Persing and Ng, 2014; Cummins et al., 2015; Rei and Cummins, 2016), only limited previous research has been done to extend this to visual prompts. Some recent work has investigated answer relevance to visual prompts as part of automated scoring systems (Somasundaran et al., 2015; King and Dickinson, 2016), but they reduced the problem to a textual similarity task by relying on hand-written reference descriptions for each image without directly incorporating visual information. Our proposed relevance detection model takes an image and a sentence as input, and assigns a score indicating how relevant the image is to the text. Formulating this as a scoring problem instead of binary classification allows us to treat the model output as a confidence score, and the classification threshold can be selected at a later stage based on the specific application. Kiros et al. (201"
W17-5020,W16-0510,0,\N,Missing
W17-5032,P06-4020,1,0.581269,"total of 35,625 patterns from our training data. For each input sentence, we first decide how many errors will be generated (using probabilities from the background corpus) and attempt to cre288 ate them by sampling from the collection of applicable patterns. This process is repeated until all the required errors have been generated or the sentence is exhausted. During generation, we try to balance the distribution of error types as well as keeping the same proportion of incorrect and correct sentences as in the background corpus (Felice, 2016). The required POS tags were generated with RASP (Briscoe et al., 2006), using the CLAWS2 tagset. 3 Wikipedia and news articles), our development experiments showed that keeping the writing style and vocabulary close to the target domain gives better results compared to simply including more data. We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) (Ng et al., 2014). Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for"
W17-5032,P06-1032,0,0.779673,"orrected version ‘We went shopping on Saturday’ would produce the following pattern: Machine Translation We treat AEG as a translation task – given a correct sentence as input, the system would learn to translate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levensh"
W17-5032,N13-1055,0,0.030868,"ore translations from the n-best list of the SMT system. These results reveal that allowing the model to see multiple alternative versions of the same file gives a distinct improvement – showing the model both correct and incorrect variations of the same sentences likely assists in learning a discriminative model. 5 F0.5 0.4 0.3 EVP FCE 0.2 0 2 4 6 8 Number of versions 10 12 Figure 1: F0.5 on FCE development set with increasing amounts of artificial data from SMT. an annotated corpus. However, their method uses a limited number of edit operations and is thus unable to generate complex errors. Cahill et al. (2013) compared different training methodologies and showed that artificial errors helped correct prepositions. Felice and Yuan (2014) learned error type distributions for generating five types of errors, and the system in Section 2.2 is an extension of this model. While previous work focused on generating a specific subset of error types, we explored two holistic approaches to AEG and showed that they are able to significantly improve error detection performance. 6 Related Work Conclusion This paper investigated two AEG methods, in order to create additional training data for error detection. First"
W17-5032,W12-2005,0,0.0604515,"k – given a correct sentence as input, the system would learn to translate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each map(VVD shop VV0 II, VVD shopping VVG II) After collecting statistics from the background corpus, errors can be inse"
W17-5032,P11-1064,0,0.016646,"ike errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each map(VVD shop VV0 II, VVD shopping VVG II) After collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, looking for the correct side of the tuple in the input sentence. We only use patterns with frequency &gt;= 5, which yields a total of 35,625 patterns from our training data. For each input sentence, we first decide how many errors will be generated (using"
W17-5032,C16-1079,1,0.850945,"ey, 1995). We investigate two alternative methods for AEG. The models receive grammatically correct text as input and modify certain tokens to produce incorrect sequences. The alternative versions of each sentence are aligned using Levenshtein distance, allowing us to identify specific words that need to be marked as errors. While these alignments are not always perfect, we found them to be sufficient for practical purposes, since alternative alignments of similar sentences often result in the same binary labeling. Future work could explore more advanced alignment methods, such as proposed by Felice et al. (2016). In Section 4, this automatically labeled data is then used for training error detection models. 2.1 2.2 Pattern Extraction We also describe a method for AEG using patterns over words and part-of-speech (POS) tags, extracting known incorrect sequences from a corpus of annotated corrections. This approach is based on the best method identified by Felice and Yuan (2014), using error type distributions; while they covered only 5 error types, we relax this restriction and learn patterns for generating all types of errors. The original and corrected sentences in the corpus are aligned and used to"
W17-5032,P17-1194,1,0.820995,"s) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) (Ng et al., 2014). Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system. Table 1 contains example sentences from the error generation systems, highlighting each of the edits that are marked as errors. Error Detection Model We construct a neural sequence labeling model for error detection, following the previous work (Rei and Yannakoudakis, 2016; Rei, 2017). The model receives a sequence of tokens as input and outputs a prediction for each position, indicating whether the token is correct or incorrect in the current context. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings. Next, the embeddings are given as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), in order to create context-dependent representations for every token. The hidden states from forward- and backward-LSTMs are concatenated for each word position, resulting in representations that are conditioned on the whole"
W17-5032,D10-1094,0,0.141496,"itional training data for error detection. First, we explored a method using textual patterns learned from an annotated corpus, which are used for inserting errors into correct input text. In addition, we proposed formulating error generation as an MT framework, learning to translate from grammatically correct to incorrect sentences. The addition of artificial data to the training process was evaluated on three error detection annoOur work builds on prior research into AEG. Brockett et al. (2006) constructed regular expressions for transforming correct sentences to contain noun number errors. Rozovskaya and Roth (2010) learned confusion sets from an annotated corpus in order to generate preposition errors. Foster and Andersen (2009) devised a tool for generating errors for different types using patterns provided by the user or collected automatically from 290 tations, using the FCE and CoNLL 2014 datasets. Making use of artificial data provided improvements for all data generation methods. By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice and Yuan (2014). The combination of the pattern-based method with the machine t"
W17-5032,P11-1093,0,0.166943,"Missing"
W17-5032,P11-1019,1,0.842843,"see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the patternbased approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection. Evaluation We trained our error generation models on the public FCE training set (Yannakoudakis et al., 2011) and used them to generate additional artificial training data. Grammatically correct text is needed as the starting point for inserting artificial errors, and we used two different sources: 1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens).1 . While there are other text corpora that could be used (e.g., 1 We used the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) to calculate statistical significance and found that the improvement http://www.englis"
W17-5032,W13-3607,1,0.876962,"ate it to contain likely errors, based on a training corpus of parallel data. Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors. The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks (Brockett et al., 2006; Ng et al., 2014), and roundtrip translation has also been shown to be promising for correcting grammatical errors (Madnani et al., 2012). Following previous work (Brockett et al., 2006; Yuan and Felice, 2013), we build a phrase-based SMT error generation system. During training, error-corrected sentences in the training data are treated as the source, and the original sentences written by language learners as the target. Pialign (Neubig et al., 2011) is used to create a phrase translation table directly from model probabilities. In addition to default features, we add character-level Levenshtein distance to each map(VVD shop VV0 II, VVD shopping VVG II) After collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, lookin"
W17-5032,W09-2112,0,\N,Missing
W17-5032,P07-2045,0,\N,Missing
W17-5032,P16-1112,1,\N,Missing
W19-4410,P17-1161,0,0.0466069,"tributions are fourfold: Grammatical error detection (GED) in nonnative writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors. 1 Introduction Detecting errors in text written by language learners is a key component of pedagogical applications for language learning and assessment. Supervised learning approaches to the task exploit publi"
W19-4410,han-etal-2004-detecting,0,0.0801466,"used to further improve performance on GEC (Yannakoudakis et al., 2017). Early approaches to GED and GEC relied upon handwritten rules and error grammars (e.g. Foster and Vogel (2004)), while later work focused on supervised learning from error-annotated corpora using feature engineering approaches and often utilizing maximum entropy-based classifiers (e.g. Chodorow et al. (2007); De Felice and Pulman (2008)). A large range of work has focused on the development of systems targeting specific error types, such as preposition (Tetreault and Chodorow, 2008; Chodorow et al., 2007), article usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). Among others, errortype agnostic approaches have focused on generating synthetic ungrammatical data to augment the available training sets, or learning from native English datasets; for example, Foster and Andersen (2009) investigate rule-based error generation methods, while Gamon (2010) trains a language model (LM) on a large, general domain corpus, from which features (e.g. word likelihoods) are derived for use in error classification. As a distinct task, GEC has been formulated as a na¨ıve-bayes classification (Rozovskaya et al., 2013,"
W19-4410,N18-1202,0,0.612303,"ed based on the context in which the words appear. These embeddings are typically the output of a set of hidden layers of a large language modelling network, trained on large volumes of unlabeled and general domain data. As such, they are able to capture detailed information regarding language and composition from a wide range of data sources, and can help overcome resource limitations for supervised learning. We evaluate the use of contextual embeddings in the form of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), embeddings from Language Models (ELMo) (Peters et al., 2018) and Flair embeddings (Akbik et al., 2018). To the best of our knowledge, this is the first evaluation of the use of contextual embeddings for the task of GED. Our contributions are fourfold: Grammatical error detection (GED) in nonnative writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional inf"
W19-4410,P17-1194,1,0.608825,"e GED performance, recent work has investigated the use of artificially generated training data (Rei et al., 2017; Kasewa et al., 2018). On the related task of grammatical error correction (GEC), Junczys-Dowmunt et al. (2018) explore transfer learning approaches to tackle the low-resource bottleneck of the task and, among others, find substantially improved performance when incorporating pre-trained word embeddings (Mikolov et al., 2013), and importing network weights from a language model trained on a large unlabeled corpus. Herein, we extend the current state of the art for error detection (Rei, 2017) to effectively incorporate contextual embeddings: word representations that are constructed based on the context in which the words appear. These embeddings are typically the output of a set of hidden layers of a large language modelling network, trained on large volumes of unlabeled and general domain data. As such, they are able to capture detailed information regarding language and composition from a wide range of data sources, and can help overcome resource limitations for supervised learning. We evaluate the use of contextual embeddings in the form of Bidirectional Encoder Representation"
W19-4410,W14-1703,0,0.0274892,"mized to detect errors as well as predict their • We present a systematic comparison of different contextualized word representations for the task of GED; • We describe an approach for effectively integrating contextual representations to error detection models, achieving a new state of the 103 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103–115 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics treated as the source “language” and the corrected text as its target counterpart) (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014; Rozovskaya and Roth, 2016; Yuan and Briscoe, 2016). Recently, Rei and Yannakoudakis (2016) presented the first approach towards neural GED, training a sequence labeling model based on word embeddings processed by a bidirectional LSTM (bi-LSTM), outputting a probability distribution over labels informed by the entire sentence as context. This approach achieves strong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological simil"
W19-4410,C16-1030,1,0.927485,"Missing"
W19-4410,W17-5032,1,0.934163,"bjective allows the network to learn more generic features about language and composition. At the same time, Rei and Yannakoudakis (2017) investigated the effectiveness of a number of auxiliary (morpho-syntactic) training objectives for the task of GED, finding that predicting part-ofspeech tags, grammatical relations or error types as auxiliary tasks yields improvements in performance over the single-task GED objective (though not as high as when utilizing an LM objective). The current state of the art on GED is based on augmenting neural approaches with artificially generated training data. Rei et al. (2017) showed improved GED performance using the bi-LSTM sequence labeler, by generating artificial errors in two different ways: 1) learning frequent error patterns from error-annotated corpora and applying these to error-free text; 2) using a statistical MT approach to “translate” correct text to its incorrect counterpart using parallel corpora. Recently, Kasewa et al. (2018) applied the latter approach using a neural MT system instead, and achieved a new state of the art on GED using the neural model of Rei (2017). art on a number of public GED datasets, and make our code and models publicly avai"
W19-4410,W17-5004,1,0.874647,"trong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological similarities such as word endings. Rei (2017) subsequently added a secondary LM objective to the neural sequence labeling architecture, operating on both word and character-level embeddings. This was found to be particularly useful for GED – introducing an LM objective allows the network to learn more generic features about language and composition. At the same time, Rei and Yannakoudakis (2017) investigated the effectiveness of a number of auxiliary (morpho-syntactic) training objectives for the task of GED, finding that predicting part-ofspeech tags, grammatical relations or error types as auxiliary tasks yields improvements in performance over the single-task GED objective (though not as high as when utilizing an LM objective). The current state of the art on GED is based on augmenting neural approaches with artificially generated training data. Rei et al. (2017) showed improved GED performance using the bi-LSTM sequence labeler, by generating artificial errors in two different wa"
W19-4410,W13-3602,0,0.0276958,"e usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). Among others, errortype agnostic approaches have focused on generating synthetic ungrammatical data to augment the available training sets, or learning from native English datasets; for example, Foster and Andersen (2009) investigate rule-based error generation methods, while Gamon (2010) trains a language model (LM) on a large, general domain corpus, from which features (e.g. word likelihoods) are derived for use in error classification. As a distinct task, GEC has been formulated as a na¨ıve-bayes classification (Rozovskaya et al., 2013, 2014; Rozovskaya and Roth, 2016) or a monolingual (statistical or neural) machine translation (MT) problem (where uncorrected text is 3 Data In this section, we describe the different public datasets we use to train our models. The First Certificate in English (FCE) dataset 104 compared to systems that miss to detect some errors (Ng et al., 2014). We note that performance on the BEA shared task test set is conducted using the official evaluation tool in CodaLab. We also perform detailed analyses in order to evaluate the performance of our models per error type. As the datasets above either h"
W19-4410,W14-1704,0,0.0575574,"Missing"
W19-4410,P16-1208,0,0.111637,"t their • We present a systematic comparison of different contextualized word representations for the task of GED; • We describe an approach for effectively integrating contextual representations to error detection models, achieving a new state of the 103 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103–115 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics treated as the source “language” and the corrected text as its target counterpart) (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014; Rozovskaya and Roth, 2016; Yuan and Briscoe, 2016). Recently, Rei and Yannakoudakis (2016) presented the first approach towards neural GED, training a sequence labeling model based on word embeddings processed by a bidirectional LSTM (bi-LSTM), outputting a probability distribution over labels informed by the entire sentence as context. This approach achieves strong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological similarities such as word ending"
W19-4410,C08-1109,0,0.0514333,"ion models can be complementary to error correction ones, and can be used to further improve performance on GEC (Yannakoudakis et al., 2017). Early approaches to GED and GEC relied upon handwritten rules and error grammars (e.g. Foster and Vogel (2004)), while later work focused on supervised learning from error-annotated corpora using feature engineering approaches and often utilizing maximum entropy-based classifiers (e.g. Chodorow et al. (2007); De Felice and Pulman (2008)). A large range of work has focused on the development of systems targeting specific error types, such as preposition (Tetreault and Chodorow, 2008; Chodorow et al., 2007), article usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). Among others, errortype agnostic approaches have focused on generating synthetic ungrammatical data to augment the available training sets, or learning from native English datasets; for example, Foster and Andersen (2009) investigate rule-based error generation methods, while Gamon (2010) trains a language model (LM) on a large, general domain corpus, from which features (e.g. word likelihoods) are derived for use in error classification. As a distinct task, GEC has been formulated as"
W19-4410,P11-1019,1,0.944736,"k et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors. 1 Introduction Detecting errors in text written by language learners is a key component of pedagogical applications for language learning and assessment. Supervised learning approaches to the task exploit public error-annotated corpora (Yannakoudakis et al., 2011; Ng et al., 2014; Napoles et al., 2017) that are, however, limited in size, in addition to having a biased distribution of labels: the number of correct tokens in a text far outweighs the incorrect (Leacock et al., 2014). As such, Grammatical Error Detection (GED) can be considered a low/mid-resource task. The current state of the art explores error detection within a semi-supervised, multi-task learning framework, using a neural sequence labeler optimized to detect errors as well as predict their • We present a systematic comparison of different contextualized word representations for the ta"
W19-4410,D17-1297,1,0.815123,"ion to overall improvements in performance; • We perform a detailed analysis of the strengths and weaknesses of different contextual representations for the task of GED, presenting detailed results of their impact on different types of errors in order to guide future work. 2 Related work In this section, we describe previous work on GED and on the related task of GEC. While error correction systems can be used for error detection, previous work has shown that standalone error detection models can be complementary to error correction ones, and can be used to further improve performance on GEC (Yannakoudakis et al., 2017). Early approaches to GED and GEC relied upon handwritten rules and error grammars (e.g. Foster and Vogel (2004)), while later work focused on supervised learning from error-annotated corpora using feature engineering approaches and often utilizing maximum entropy-based classifiers (e.g. Chodorow et al. (2007); De Felice and Pulman (2008)). A large range of work has focused on the development of systems targeting specific error types, such as preposition (Tetreault and Chodorow, 2008; Chodorow et al., 2007), article usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). A"
W19-4410,N16-1042,0,0.0130027,"ematic comparison of different contextualized word representations for the task of GED; • We describe an approach for effectively integrating contextual representations to error detection models, achieving a new state of the 103 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103–115 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics treated as the source “language” and the corrected text as its target counterpart) (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014; Rozovskaya and Roth, 2016; Yuan and Briscoe, 2016). Recently, Rei and Yannakoudakis (2016) presented the first approach towards neural GED, training a sequence labeling model based on word embeddings processed by a bidirectional LSTM (bi-LSTM), outputting a probability distribution over labels informed by the entire sentence as context. This approach achieves strong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological similarities such as word endings. Rei (2017) subsequentl"
W19-4410,W09-2112,0,\N,Missing
W19-4410,C08-1022,0,\N,Missing
W19-4410,W14-1701,0,\N,Missing
W19-4410,W07-1604,0,\N,Missing
W19-4410,P08-1021,0,\N,Missing
W19-4410,W14-1702,1,\N,Missing
W19-4410,P16-1112,1,\N,Missing
W19-4410,E17-2037,0,\N,Missing
W19-4410,P17-1074,0,\N,Missing
W19-4410,N18-1055,0,\N,Missing
W19-4410,C18-1139,0,\N,Missing
W19-4410,D18-1541,0,\N,Missing
W19-4410,W19-4406,0,\N,Missing
W19-4424,D14-1082,0,0.0413013,"composition, we re-score the enriched input lattice I with the system described in Section 2.2. The FST-based system combination uses 7 different features: the convolutional system score, the LM and NMT scores from the Transformer• Two binary features indicating whether two publicly available spell-checkers – HunSpell2 and JamSpell3 – identify the target word as a spelling mistake. 1 https://github.com/marekrei/ sequence-labeler 2 http://hunspell.github.io/ 3 https://github.com/bakwc/JamSpell 231 • The POS tag, NER label and dependency relation of the target word based on the Stanford parser (Chen and Manning, 2014). false positives (FP) on token-level error detection by treating the error detection model as the “gold standard”. Specifically, we count how many times the candidate FST hypothesis disagrees with the detection model on the tokens identified as incorrect, and use as a 1.0 feature the following: FP+1.0 • The number of times the unigram, bigram, or trigram context of the target word appears in the BNC (Burnard, 2007) and in ukWaC (Ferraresi et al., 2008). We use a linear combination of the above three features together with the original score given by the FST system for each candidate hypothesi"
W19-4424,D14-1179,0,0.0326186,"Missing"
W19-4424,Q17-1010,0,0.00763353,"2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring et al., 2017), which employs convolutional neural networks (CNNs) to compute intermediate encoder and decoder states. The parameter settings follow Chollampatt and Ng (2018) and Ge et al. (2018). The source and target word embeddings have size 500, and are initialised with fastText embeddings (Bojanowski et al., 2017) trained on the native English Wikipedia corpus (2, 405, 972, 890 tokens). Each of the encoder and decoder is made up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybri"
W19-4424,W18-0529,0,0.105303,"max layer at the output. In addition to neural text representations, we also include several external features into the model, designed to help it learn more accurate error detection patterns from the limited amounts of training data available: Stahlberg et al. (2019) demonstrated the usefulness of FSTs for grammatical error correction. Their method starts with an input lattice I which is generated with a phrase-based statistical machine translation (SMT) system. The lattice I is composed with a number of FSTs that aim to enrich the search space with further possible corrections. Similarly to Bryant and Briscoe (2018), they rely on external knowledge sources like spell checkers and morphological databases to generate additional correction options for the input sentence. The enriched lattice is then mapped to the subword level by composition with a mapping transducer, and re-scored with neural machine translation models and neural LMs. In this work, rather than combining SMT and neural models, we use the framework of Stahlberg et al. (2019) to combine and enrich the outputs of two neural systems. The input lattice I is now the union of two n-best lists – one from the convolutional system (Section 2.1), and"
W19-4424,W19-4406,0,0.0263852,"as a feature the LD between those binary representations. Specifically, we would like to select the candidate FST sentence that has the smallest LD from the binary sequence created by the detection model, and therefore 1.0 use as a feature the following: LD+1.0 • Cambridge English W&I corpus Cambridge English Write & Improve (W&I)5 (Yannakoudakis et al., 2018) is an online web platform that assists non-native English learners with their writing. Learners from around the world submit letters, stories, articles and essays for automated assessment in response to various prompts. The W&I corpus (Bryant et al., 2019) contains 3, 600 annotated submissions across 3 different CEFR6 levels: A (beginner), B (intermediate), and C (advanced). The data has been 4 We note that there are no restrictions on the use of NLP tools (e.g., POS taggers, parsers, spellcheckers, etc.), nor on the amount of unannotated data that can be used, so long as such resources are publicly available. 5 https://writeandimprove.com/ 6 https://www.cambridgeenglish.org/ exams-and-tests/cefr/ 3. False positives: using the binary sequences described above, we count the number of 232 split into training (3, 000 essays), development (200 essa"
W19-4424,W13-1703,0,0.0241772,"atical error detection system was optimized separately as a sequence labeling model. Word embeddings were set to size 300 and initialized with pre-trained Glove embedding (Pennington et al., 2014). The bi-LSTM has 300dimensional hidden layers for each direction. Dropout was applied to word embeddings and LSTM outputs with probability 0.5. The model was optimized with Adam (Kingma and Ba, 2015), using a default learning rate 0.001. Training was stopped when performance on the development set did not improved over 7 epochs. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) contains 1, 400 essays written by undergraduate students at the National University of Singapore who are non-native English speakers. • Lang-8 Corpus of Learner English Lang-88 is an online language learning website which encourages users to correct each other’s grammar. The Lang-8 Corpus of Learner English (Mizumoto et al., 2011; Tajiri et al., 2012) refers to an English subsection of this website (can be quite noisy). Additional resources used in our system include: • English Wikipedia corpus The English Wikipedia corpus (2, 405, 972, 890 tokens in 110, 698, 467 sentences) is used to pre-tr"
W19-4424,P17-1074,0,0.263796,"tion 3 is a threshold used to filter out sentence pairs with unnecessary changes; e.g., [I look forward to hearing from you. → I am looking forward to hearing from you.]. It is an avA binary classification task is also introduced to predict whether the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learne"
W19-4424,W12-2006,0,0.021788,"rrection (GEC) is the task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring e"
W19-4424,W11-2838,0,0.018545,"uction Grammatical error correction (GEC) is the task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decod"
W19-4424,P17-1070,0,0.0839707,"Missing"
W19-4424,W14-1702,1,0.843944,"der and decoder is made up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and pr"
W19-4424,N18-1055,0,0.0833184,"Missing"
W19-4424,D18-1541,0,0.18577,"ed by learners. Some researchers have investigated ways of incorporating task-specific knowledge, either by directly modifying the training objectives (Schmaltz et al., 2017; Sakaguchi et al., 2017; JunczysDowmunt et al., 2018) or by re-ranking machinetranslation-system correction hypotheses (Yannakoudakis et al., 2017; Chollampatt and Ng, 2018). To ameliorate the lack of large amounts of error-annotated learner data, various approaches have proposed to leverage unlabelled native data within a number of frameworks, including artificial error generation with back translation (Rei et al., 2017; Kasewa et al., 2018), fluency boost learning (Ge et al., 2018), and pre-training with denoising autoencoders (Zhao et al., 2019). Previous work has shown that a GEC system In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly available data, along"
W19-4424,I11-1017,0,0.118142,"Missing"
W19-4424,N18-2046,0,0.036794,"l layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the shared task development set; Section 4 pre"
W19-4424,W14-1701,0,0.340725,"Missing"
W19-4424,P16-1154,0,0.0209614,"and target side of the parallel training data respectively. The same BPE operation is applied to the Wikipedia data before being used for training of our word embeddings. Approach We approach the error correction task using a pipeline of systems, as presented in Figure 1. In the following sections, we describe each of these components in detail. 2.1 p(yt |{y1 , ..., yt−1 }, x) t=1 Transformer 2 m Y Copying mechanism is a technique that has led to performance improvement on various monolingual sequence-to-sequence tasks, such as text summarisation, dialogue systems, and paraphrase generation (Gu et al., 2016; Cao et al., 2017). The idea is to allow the decoder to choose between simply copying an original input word and outputting a translation word. Since the source and target sentences are both in the same language (i.e., monolingual translation) and most words in The convolutional neural network (CNN) system We use a neural sequence-to-sequence model and an encoder–decoder architecture (Cho et al., 2014; Sutskever et al., 2014). An encoder first reads and encodes an entire input sequence x = (x1 , x2 , ..., xn ) into hidden state representations. A decoder then generates an output sequence y ="
W19-4424,W13-3601,0,0.0864338,"he task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring et al., 2017), whi"
W19-4424,W11-2123,0,0.0115977,"providing error detection labels. Instead of only generating a corrected sentence, we extend the system to additionally predict whether a token in the source sentence is correct or incorrect. f (y) ≤σ f (yok ) (3) where f (y) is the normalised log probability of y: Pm f (y) = • Sentence-level labelling t=1 log(P (yt |y&lt;t )) m (4) This ensures that the quality of the artificially generated sentence, as estimated by a language model, is lower compared to the original sentence. We use a 5-gram language model (LM) trained on the One Billion Word Benchmark dataset (Chelba et al., 2014) with KenLM (Heafield, 2011) to compute P (yt |y&lt;t ). The σ in Equation 3 is a threshold used to filter out sentence pairs with unnecessary changes; e.g., [I look forward to hearing from you. → I am looking forward to hearing from you.]. It is an avA binary classification task is also introduced to predict whether the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens usin"
W19-4424,D14-1162,0,0.0842984,"sterov’s Accelerated Gradient Descent (NAG) with a simplified formulation for Nesterov’s momentum (Bengio et al., 2013). The initial learning rate is set to 0.25, with a decaying factor of 0.1 and a momentum value of 0.99. We perform validation after every epoch, and select the best model based on the performance on the development set. During beam search, we keep a beam size of 12 and discard all other hypotheses. The grammatical error detection system was optimized separately as a sequence labeling model. Word embeddings were set to size 300 and initialized with pre-trained Glove embedding (Pennington et al., 2014). The bi-LSTM has 300dimensional hidden layers for each direction. Dropout was applied to word embeddings and LSTM outputs with probability 0.5. The model was optimized with Adam (Kingma and Ba, 2015), using a default learning rate 0.001. Training was stopped when performance on the development set did not improved over 7 epochs. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) contains 1, 400 essays written by undergraduate students at the National University of Singapore who are non-native English speakers. • Lang-8 Corpus of Learner English Lan"
W19-4424,N18-1202,0,0.00953368,"grees with the detection model on the tokens identified as incorrect, and use as a 1.0 feature the following: FP+1.0 • The number of times the unigram, bigram, or trigram context of the target word appears in the BNC (Burnard, 2007) and in ukWaC (Ferraresi et al., 2008). We use a linear combination of the above three features together with the original score given by the FST system for each candidate hypothesis to re-rank the FST system’s 8-best list in an unsupervised way. The new 1-best correction hypothesis c∗ is then the one that maximises: • Contextualized word representations from ELMo (Peters et al., 2018). The discrete features are represented as 10dimensional embeddings and, together with the continuous features, concatenated to each word representation in the model. The overall architecture is optimized for error detection using crossentropy. Once trained, the model returns the predicted probabilities of each token in a sentence being correct or incorrect. c∗ = arg max c K X λi hi (c) (6) i=1 where h represents the score assigned to candidate hypothesis c according to feature i; λ is a weighting parameter that controls the effect feature i has on the final ranking; and K = 4 as we use a tota"
W19-4424,P16-1162,0,0.339169,"of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the shared task development set; Section 4 presents our official results on the shared task test set, including a detailed analysis of the performance of our final system; and, finally, Section 5 concludes the paper and provides an overview of our findings. BPE is introduced to alleviate the rare-word problem, and rare and unknown words are split into multiple frequent subword tokens (Sennrich et al., 2016b). NMT systems often limit vocabulary size on both source and target sides due to the computational complexity during training. Therefore, they are unable to translate out-of-vocabulary (OOV) words, which are treated as unknown tokens, resulting in poor translation quality. As noted by Yuan and Briscoe (2016), this problem is more serious for GEC as non-native text contains, not only rare words (e.g., proper nouns), but also misspelled words (i.e., spelling errors). In our model, each of the source and target vocabularies consist of the 30K most frequent BPE tokens from the source and target"
W19-4424,P17-1194,1,0.930878,"ing (NLP) (Collobert and Weston, 2008) and error generation system using the same network speech recognition (Deng et al., 2013) to computer architecture as the one described here, with errorvision (Girshick, 2015). Multi-task learning alcorrected sentences as the source and their correlows systems to use information from related tasks sponding uncorrected counterparts written by lanand learn from multiple objectives, which leads guage learners as the target. The system is then to performance improvement on individual tasks. used to collect the n-best outputs: yo1 , yo2 , ..., yon , Recently, Rei (2017) and Rei and Yannakoudakis for a given error-free native and/or learner sen(2017) investigated the use of different auxiliary tence y. Since there is no guarantee that the error objectives for the task of error detection in learner generation system will inject errors into the input writing. sentence y to make it less grammatically correct, In addition to our primary error correction task, we apply “quality control”. A pair of artificially we propose two related auxiliary objectives to generated sentences (yok , y), for k ∈ {1, 2, ..., n}, boost model performance: will be added to the training"
W19-4424,N19-1406,1,0.911367,"the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previo"
W19-4424,W17-5032,1,0.94333,"andard language used by learners. Some researchers have investigated ways of incorporating task-specific knowledge, either by directly modifying the training objectives (Schmaltz et al., 2017; Sakaguchi et al., 2017; JunczysDowmunt et al., 2018) or by re-ranking machinetranslation-system correction hypotheses (Yannakoudakis et al., 2017; Chollampatt and Ng, 2018). To ameliorate the lack of large amounts of error-annotated learner data, various approaches have proposed to leverage unlabelled native data within a number of frameworks, including artificial error generation with back translation (Rei et al., 2017; Kasewa et al., 2018), fluency boost learning (Ge et al., 2018), and pre-training with denoising autoencoders (Zhao et al., 2019). Previous work has shown that a GEC system In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly"
W19-4424,W19-4417,1,0.757125,"ence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the"
W19-4424,P16-1112,1,0.857116,"n SMT system based on error detection predictions. Following this work, we also deploy a re-ranking component which re-ranks the n-best correction hypotheses of the FST system (Section 2.3) based on error detection predictions output by an error detection system. FST-based system combination Error detection. Our system for grammatical error detection is based on the model described by Rei (2017).1 The task is formulated as a sequence labeling problem – given a sentence, the model assigns a probability to each token, indicating the likelihood of that token being incorrect in the given context (Rei and Yannakoudakis, 2016). The architecture maps words to distributed embeddings, while also constructing character-based representations for each word with a neural component. These are then passed through a bidirectional LSTM, followed by a feed-forward layer and a softmax layer at the output. In addition to neural text representations, we also include several external features into the model, designed to help it learn more accurate error detection patterns from the limited amounts of training data available: Stahlberg et al. (2019) demonstrated the usefulness of FSTs for grammatical error correction. Their method s"
W19-4424,D17-2005,1,0.86093,"ing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT mode"
W19-4424,W17-5004,1,0.919341,"Missing"
W19-4424,W18-1821,1,0.843308,"Missing"
W19-4424,P16-1208,0,0.0167945,"de up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the s"
W19-4424,I17-2062,0,0.0663944,"Missing"
W19-4424,P12-2039,0,0.20414,"Missing"
W19-4424,D17-1298,0,0.0244066,"Missing"
W19-4424,W18-1819,0,0.0256159,"for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT models are trained with backtranslation (Sennrich et al., 2016a; Rei et al., 2017; Kasewa et al., 2018) and fine-tuning through continued training. For a detailed description of this system we refer the reader to Stahlberg and Byrne (2019). 2.3 2.4 Re-ranking FST output Yannakoudakis et al. (2017) found that grammatical error detection systems can be used to improve error correction outputs. Specifically, they re-rank the n-best correction hypotheses of an SMT system based on error detection predictions. Following this work, we also deploy a re-ranking c"
W19-4424,P11-1019,1,0.784266,"section of 100 essays has been manually annotated, and equally partitioned into development and test sets. In order to cover the full range of English levels and abilities, the official development set consists of 300 essays from W&I (A: 130, B:100, and C:70) and 50 essays from LOCNESS (86, 973 tokens in 4, 384 sentences). The ERRANT scorer (Bryant et al., 2017) is used as the official scorer for the shared task. System performance is evaluated in terms of spanlevel correction using F0.5 , which emphasises precision twice as much as recall. • FCE The First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) is a subset of the Cambridge Learner Corpus (CLC) that consists of 1, 244 exam scripts written by learners of English sitting the FCE exam. 3.2 • NUCLE Training details The convolutional NMT model is trained with a hidden layer size of 1, 024 for both the encoder and the decoder. Dropout at a rate of 0.2 is applied to the embedding layers, convolutional layers and decoder output. The model is optimized using Nesterov’s Accelerated Gradient Descent (NAG) with a simplified formulation for Nesterov’s momentum (Bengio et al., 2013). The initial learning rate is set to 0.25, with a decaying factor"
W19-4424,D17-1297,1,0.858113,"ystem Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT models are trained with backtranslation (Sennrich et al., 2016a; Rei et al., 2017; Kasewa et al., 2018) and fine-tuning through continued training. For a detailed description of this system we refer the reader to Stahlberg and Byrne (2019). 2.3 2.4 Re-ranking FST output Yannakoudakis et al. (2017) found that grammatical error detection systems can be used to improve error correction outputs. Specifically, they re-rank the n-best correction hypotheses of an SMT system based on error detection predictions. Following this work, we also deploy a re-ranking component which re-ranks the n-best correction hypotheses of the FST system (Section 2.3) based on error detection predictions output by an error detection system. FST-based system combination Error detection. Our system for grammatical error detection is based on the model described by Rei (2017).1 The task is formulated as a sequence l"
W19-4424,N16-1042,1,0.819082,"set, including a detailed analysis of the performance of our final system; and, finally, Section 5 concludes the paper and provides an overview of our findings. BPE is introduced to alleviate the rare-word problem, and rare and unknown words are split into multiple frequent subword tokens (Sennrich et al., 2016b). NMT systems often limit vocabulary size on both source and target sides due to the computational complexity during training. Therefore, they are unable to translate out-of-vocabulary (OOV) words, which are treated as unknown tokens, resulting in poor translation quality. As noted by Yuan and Briscoe (2016), this problem is more serious for GEC as non-native text contains, not only rare words (e.g., proper nouns), but also misspelled words (i.e., spelling errors). In our model, each of the source and target vocabularies consist of the 30K most frequent BPE tokens from the source and target side of the parallel training data respectively. The same BPE operation is applied to the Wikipedia data before being used for training of our word embeddings. Approach We approach the error correction task using a pipeline of systems, as presented in Figure 1. In the following sections, we describe each of th"
W19-4424,N19-1014,0,0.20593,"Missing"
