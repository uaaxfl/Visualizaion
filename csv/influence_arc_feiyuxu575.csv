2020.coling-main.201,K16-1002,0,0.0410842,"017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style transfer (Huang et al., 2020; Zhao et al., 2018; Bowman et al., 2016; Hjelm et al., 2018). Third, it is difficult to preserve the content of a text when its style is transferred. To obtain good content preservation for text style transfer, various disentanglement approaches (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Sudhakar et al., 2019) are proposed to separate the content and style of a text in the latent space. However, content-style disentanglement is not easily achievable as content and style typically interact with each other in texts in subtle ways (Lample et al., 2019). In order to solve the issues above, we propose a cycle-consistent adver"
2020.coling-main.201,N19-1374,0,0.0126546,"sive experiments and in-depth analyses on two widely-used public datasets consistently validate the effectiveness of proposed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation. 1 Introduction Unsupervised text style transfer is to rewrite a text in one style into a text in another style while the content of the text remains the same as much as possible without using any parallel data. Style transfer can be utilized in many tasks such as personalization in dialogue systems (Oraby et al., 2018; Colombo et al., 2019), sentiment and word decipherment (Shen et al., 2017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style"
2020.coling-main.201,P19-1601,0,0.0153258,"0) presents a new probabilistic graphical model for unsupervised text style transfer. In the second line of works that avoid disentangled representations of style and content, Lample et al. (2019) use back-translation technique on denoising autoencoder model with latent representation pooling to control the content preservation. Their experiments and analyses show that the contentstyle disentanglement is neither necessary nor always satisfied with practical requirements, even with the domain adversarial training that explicitly aims at learning disentangled representations. Style Transformer (Dai et al., 2019) uses Transformer as a basic module to train a style transfer system. DualRL (Luo et al., 2019) employs a dual reinforcement learning framework with two sequence-to-sequence models in two directions, using style classifier and back-transfer reconstruction probability as rewards. We follow the second line and propose a novel method that makes no assumption on the latent representation disentanglement. But differently, we perform style transfer in the latent representational spaces of the source and target style. And inspired by CycleGAN (Zhu et al., 2017; Zhu et al., 2018) which uses a cycle lo"
2020.coling-main.201,E17-2068,0,0.0498826,"Missing"
2020.coling-main.201,N18-1169,0,0.389116,"transferred sentences in another style. Bottom: cycle-consistent constraint enforcing that sentences transferred into a different style can be translated back to its original meaning in its original style. true samples in the other to obtain the shared latent content distribution. Fu et al. (2018) use an adversarial network to separate content representations from style representations. Prabhumoye et al. (2018) fix the machine translation model and the encoder of the back-translation model to obtain content representations, then generate texts with classifier-guided style-specific generators. Li et al. (2018) extract content words by deleting style indicator words, then combine the content words with retrieved style words to construct the final output. Xu et al. (2018) use reinforcement learning to jointly train a neutralization module which removes style words based on a classifier and an emotionalization module. ARAE (Zhao et al., 2018) and DAAE (Shen et al., 2020b) train GAN-regularized latent representations to obtain styleindependent content representations, then decodes the content representations conditioned on style. He et al. (2020) presents a new probabilistic graphical model for unsuper"
2020.coling-main.201,P18-2031,0,0.0201726,"osed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation. 1 Introduction Unsupervised text style transfer is to rewrite a text in one style into a text in another style while the content of the text remains the same as much as possible without using any parallel data. Style transfer can be utilized in many tasks such as personalization in dialogue systems (Oraby et al., 2018; Colombo et al., 2019), sentiment and word decipherment (Shen et al., 2017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style transfer (Huang et al., 2020; Zhao et al., 2018; Bowman et al., 2016; Hjelm et al., 2018). Third, it is difficult to pre"
2020.coling-main.201,W18-5019,0,0.028298,"ed end-to-end. Extensive experiments and in-depth analyses on two widely-used public datasets consistently validate the effectiveness of proposed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation. 1 Introduction Unsupervised text style transfer is to rewrite a text in one style into a text in another style while the content of the text remains the same as much as possible without using any parallel data. Style transfer can be utilized in many tasks such as personalization in dialogue systems (Oraby et al., 2018; Colombo et al., 2019), sentiment and word decipherment (Shen et al., 2017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous rep"
2020.coling-main.201,P02-1040,0,0.109122,"Missing"
2020.coling-main.201,P18-1080,0,0.0192336,"nsferred samples from one style with the creativecommons.org/licenses/by/4.0/. 2214 Figure 2: The visualization of style transfer and cycle-consistent constraint in CAE. Upper: sentences in one style and style-transferred sentences in another style. Bottom: cycle-consistent constraint enforcing that sentences transferred into a different style can be translated back to its original meaning in its original style. true samples in the other to obtain the shared latent content distribution. Fu et al. (2018) use an adversarial network to separate content representations from style representations. Prabhumoye et al. (2018) fix the machine translation model and the encoder of the back-translation model to obtain content representations, then generate texts with classifier-guided style-specific generators. Li et al. (2018) extract content words by deleting style indicator words, then combine the content words with retrieved style words to construct the final output. Xu et al. (2018) use reinforcement learning to jointly train a neutralization module which removes style words based on a classifier and an emotionalization module. ARAE (Zhao et al., 2018) and DAAE (Shen et al., 2020b) train GAN-regularized latent re"
2020.coling-main.201,D19-1499,0,0.0193195,"to enforce the back-translation of a transferred image to be equivalent to the original image, we also impose a cycle-consistent constraint on our style transfer network. However, training style transfer networks with such a cycle constraint on discrete texts is quite different from those on images and non-trivial. In order to enable cycle training on texts, we project texts onto adversarially regularized latent space collectively learned by the LSTM autoencoders and adversarial transfer networks. Different from latent cross project with Euclidean distance for semi-supervised style transfer (Shang et al., 2019), we construct latent CycleGAN to generate high quality sentences for unsupervised style transfer. 2215 3 CAE: Cycle-consistent Adversarial Autoencoders Suppose we have two non-parallel text datasets X1 = {x1i }ni=1 and X2 = {x2j }m j=1 with different styles s1 and s2 . The CAE employs LSTM autoencoder models to encode discrete text sequences x1i , x2j into rep˜ 1→2 ˜ 2→1 resentations zi1 , zj2 , and to generate sentences x ,x based on latent representations z˜i1→2 , z˜j2→1 i j transferred by the adversarial transfer network T 1→2 , T 2→1 from zi1 , zj2 . 3.1 LSTM autoencoders We use an LSTM ("
2020.coling-main.201,D19-1322,0,0.0199081,"ge-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style transfer (Huang et al., 2020; Zhao et al., 2018; Bowman et al., 2016; Hjelm et al., 2018). Third, it is difficult to preserve the content of a text when its style is transferred. To obtain good content preservation for text style transfer, various disentanglement approaches (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Sudhakar et al., 2019) are proposed to separate the content and style of a text in the latent space. However, content-style disentanglement is not easily achievable as content and style typically interact with each other in texts in subtle ways (Lample et al., 2019). In order to solve the issues above, we propose a cycle-consistent adversarial autoencoders (CAE) for unsupervised text style transfer. In CAE, we learn the representation of a text where we embed both content and style in the same space. Such space is constructed for each style from non-parallel data. We then transfer the learned representation from on"
2020.coling-main.201,P19-1482,0,0.018046,"ith dropout 0.2) to build the language models and calculate PPL and RPPL. These four evaluation metrics together form a comprehensive evaluation and comparison between different approaches. We also conducted human evaluation. We randomly chose 200 instances from each style for the human evaluation. Four human annotators can proficiently understand English texts and have sufficient background knowledge about this evaluation task. The annotation is blind to them in random order. They grade all sentences with scores from one to five for style transfer, content preservation and fluency. Following Wu et al. (2019) and Li et al. (2018), we regard a style transfer with scores larger than or equal to four on all three measures (style transfer, content preservation and fluency) as a successful transfer. We calculate the percentage of successful transfers and refer to this percentage as Suc. 4.2 4.2.1 Results Yelp restaurant reviews sentiment transfer The results are shown in Table 2 (left), from which we clearly observe that CAE obtains better performance than the five baseline approaches. Specifically, CAE yields improvements of 5.1, 6.1 and 7.0 points over the best baselines for sentiment transfer in ter"
2020.coling-main.201,P18-1090,0,0.116653,"ts original meaning in its original style. true samples in the other to obtain the shared latent content distribution. Fu et al. (2018) use an adversarial network to separate content representations from style representations. Prabhumoye et al. (2018) fix the machine translation model and the encoder of the back-translation model to obtain content representations, then generate texts with classifier-guided style-specific generators. Li et al. (2018) extract content words by deleting style indicator words, then combine the content words with retrieved style words to construct the final output. Xu et al. (2018) use reinforcement learning to jointly train a neutralization module which removes style words based on a classifier and an emotionalization module. ARAE (Zhao et al., 2018) and DAAE (Shen et al., 2020b) train GAN-regularized latent representations to obtain styleindependent content representations, then decodes the content representations conditioned on style. He et al. (2020) presents a new probabilistic graphical model for unsupervised text style transfer. In the second line of works that avoid disentangled representations of style and content, Lample et al. (2019) use back-translation tech"
adolphs-etal-2010-question,W08-1301,0,\N,Missing
ai-etal-2014-sprinter,copestake-flickinger-2000-open,0,\N,Missing
ai-etal-2014-sprinter,bunt-2006-dimensions,0,\N,Missing
ai-etal-2014-sprinter,C12-2127,0,\N,Missing
ai-etal-2014-sprinter,ai-charfuelan-2014-mat,1,\N,Missing
C10-2065,P96-1009,0,0.0218864,"Missing"
C10-2065,W07-1207,0,0.0198766,"nline product of the Berlin startup company Metaversum1 . The KomParse NPCs provide various services through con1 http://www.metaversum.com/ In contrast to existing systems using mainly lexical features, i.e. words, single markers such as punctuation (Verbree et al., ) or combinations of various features (Stolcke et al., 2000) for the dialogue act classification, the results of the interpretation component presented in this paper are based on syntactic and semantic relations. The system first gathers linguistic information coming from different levels of deep linguistic processing similar to (Allen et al., 2007). The retrieved information is used as input for an information extraction component that delivers the relations embedded in the actual utterance (Xu et al., 2007). These relations combined with additional features (a small dialogue context and mood of the sentence) are then utilized as features for the machine-learning based recognition. The classifier is trained on a corpus originating from a Wizard-of-Oz experiment which was semiautomatically annotated. It contains automatically annotated syntactic relations namely, predicate argument structures, which were checked and corrected manually af"
C10-2065,J05-1004,0,0.033644,"rogative. • The topic of the utterance. The topic value is coreferent with the currently discussed object. Topic can consist of an object class (e.g. sofa) or an special object instance (sofa 1836). The topic of the directly preceding utterance was chosen as a feature too. 4.2 Annotation with Predicate Argument Structure The second annotation step, applied to the utterance level of the input, automatically enriches the annotation with predicate argument structures. Each utterance is parsed with a predicate argument parser and annotated with syntactic relations organized according to PropBank (Palmer et al., 2005) containing the following features: Predicate, Subject, Objects, Negation, Modifiers, Copula Complements. A single relation mainly consists of a predicate and the belonging arguments. Verb modifiers like attached PPs are classified as “argM” together with negation (“argM neg”) and modal verbs (“argM modal”). Arguments are labeled with numbers according to the found information for the actual structure. PropBank is organized in two layers, the first one being an underspecified representation of a sentence with numbered arguments, the second one containing fine-grained information about the sema"
C10-2065,P98-1013,0,0.0288322,", Objects, Negation, Modifiers, Copula Complements. A single relation mainly consists of a predicate and the belonging arguments. Verb modifiers like attached PPs are classified as “argM” together with negation (“argM neg”) and modal verbs (“argM modal”). Arguments are labeled with numbers according to the found information for the actual structure. PropBank is organized in two layers, the first one being an underspecified representation of a sentence with numbered arguments, the second one containing fine-grained information about the semantic frames for the predicate comparable to FrameNet (Baker et al., 1998). While the information in the second layer is stable for each verb, the values of the numbered arguments can change from verb to verb. While for one verb the “arg0” may refer to the subject of the verb, another verb may encapsulate a direct object behind the same notation “arg0”. This is very complicated to handle in a computational setup, which needs continuous labeling for the successive components. Therefore the arguments were in general named as in PropBank but consistently numbered by syntactic structure. This means for example that the subject is always labeled as “arg1”. Consider the e"
C10-2065,W08-1301,0,0.0219048,"Missing"
C10-2065,W98-0319,0,0.194776,"Missing"
C10-2065,W02-0213,0,0.0246234,"Missing"
C10-2065,P10-4007,1,0.807992,"Missing"
C10-2065,N04-1020,0,0.0168352,"Missing"
C10-2065,J00-3003,0,0.221944,"ng from the user input. The work presented in this paper is part of a dialogue system called KomParse (Kl¨uwer et al., 2010), which is an application of a NL dialogue system combined with various question answering technologies in a three-dimensional virtual world named Twinity, a web-based online product of the Berlin startup company Metaversum1 . The KomParse NPCs provide various services through con1 http://www.metaversum.com/ In contrast to existing systems using mainly lexical features, i.e. words, single markers such as punctuation (Verbree et al., ) or combinations of various features (Stolcke et al., 2000) for the dialogue act classification, the results of the interpretation component presented in this paper are based on syntactic and semantic relations. The system first gathers linguistic information coming from different levels of deep linguistic processing similar to (Allen et al., 2007). The retrieved information is used as input for an information extraction component that delivers the relations embedded in the actual utterance (Xu et al., 2007). These relations combined with additional features (a small dialogue context and mood of the sentence) are then utilized as features for the mach"
C10-2065,N09-1064,0,0.0626592,"Missing"
C10-2065,C08-1123,0,0.0478388,"Missing"
C10-2065,P07-1074,1,0.293455,"ystems using mainly lexical features, i.e. words, single markers such as punctuation (Verbree et al., ) or combinations of various features (Stolcke et al., 2000) for the dialogue act classification, the results of the interpretation component presented in this paper are based on syntactic and semantic relations. The system first gathers linguistic information coming from different levels of deep linguistic processing similar to (Allen et al., 2007). The retrieved information is used as input for an information extraction component that delivers the relations embedded in the actual utterance (Xu et al., 2007). These relations combined with additional features (a small dialogue context and mood of the sentence) are then utilized as features for the machine-learning based recognition. The classifier is trained on a corpus originating from a Wizard-of-Oz experiment which was semiautomatically annotated. It contains automatically annotated syntactic relations namely, predicate argument structures, which were checked and corrected manually afterwards. Furthermore these relations are enriched by manual annotation with semantic frame information from VerbNet to gain an additional level of semantic richne"
C10-2065,C98-1013,0,\N,Missing
C10-2155,P07-1073,0,0.0346878,"her relations. In (Xu et al., 2007) we make use of domain relevance values of terms occurring in rules. This method is not applicable to general relations. Parallel to confidence estimation strategies, the learning of negative rules is useful for identifying wrong rules straightforwardly. Yangarber (2003) and Etzioni et al. (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. Examples of one certain domain or class are regarded as negative examples for the other ones. Bunescu and Mooney (2007) follow a classification-based approach to RE. They use positive and negative sentences of a target relation for a SVM classifier. Uszkoreit et al. (2009) exploit negative examples as seeds for learning further negative instances and negative rules. The disadvantage of the above four approaches is that the selected negative domains or classes or negative instances cover only a subset of the negative domains/classes/relations of the target domain/class/relation. 3 DARE Baseline System Our baseline system DARE is a minimally supervised learning system for relation extraction, initialized by so-c"
C10-2155,W06-0204,0,0.755395,"e selected settings, the overall performance measured by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results. 1 Introduction Minimally supervised machine-learning approaches to learning rules or patterns for relation extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff, 1996), (Brin, 1998), (Agichtein and Gravano, 2000), (Yangarber, 2001), (Sudo et al., 2003), (Jones, 2005), (Greenwood and Stevenson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 20"
C10-2155,P03-1029,0,0.622006,"egree depending on the domain and the selected settings, the overall performance measured by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results. 1 Introduction Minimally supervised machine-learning approaches to learning rules or patterns for relation extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff, 1996), (Brin, 1998), (Agichtein and Gravano, 2000), (Yangarber, 2001), (Sudo et al., 2003), (Jones, 2005), (Greenwood and Stevenson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and G"
C10-2155,P07-1074,1,0.955045,"d by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results. 1 Introduction Minimally supervised machine-learning approaches to learning rules or patterns for relation extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff, 1996), (Brin, 1998), (Agichtein and Gravano, 2000), (Yangarber, 2001), (Sudo et al., 2003), (Jones, 2005), (Greenwood and Stevenson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 2003), (Pantel and Pennacchiotti, 2006),"
C10-2155,P03-1044,0,0.167727,"venson, 2006), (Agichtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 2003), (Pantel and Pennacchiotti, 2006), (Etzioni et al., 2005), (Xu et al., 2007) and (Uszkoreit et al., 2009)). In this paper, we present a new approach to estimating or ranking the confidence value of learned rules by utilizing limited closed-world knowledge. As many predecessors, our ranking method is built on the “Duality Principle” (e. g., (Brin, 1998), (Yangarber, 2001) and (Agichtein, 2006)). We extend the validation method by an evaluation of extracted instances against some limited closed-world knowledge, while also allowing cases in which knowledge for informed decisions is not available"
C10-2155,P06-1015,0,0.447985,"chtein, 2006), (Xu et al., 2007), (Xu, 2007)). On the one hand, these approaches show very promising results by utilizing minimal domain knowledge as seeds. On the other hand, they are all confronted with the same problem, i.e., the acquisition of wrong rules because of missing knowledge for their validation during bootstrapping. Various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ”so-called” negative rules for increasing the precision value (e.g., (Brin, 1998), (Agichtein and Gravano, 2000), (Agichtein, 2006), (Yangarber, 2003), (Pantel and Pennacchiotti, 2006), (Etzioni et al., 2005), (Xu et al., 2007) and (Uszkoreit et al., 2009)). In this paper, we present a new approach to estimating or ranking the confidence value of learned rules by utilizing limited closed-world knowledge. As many predecessors, our ranking method is built on the “Duality Principle” (e. g., (Brin, 1998), (Yangarber, 2001) and (Agichtein, 2006)). We extend the validation method by an evaluation of extracted instances against some limited closed-world knowledge, while also allowing cases in which knowledge for informed decisions is not available. In comparison to previous approa"
C98-1056,E95-1032,0,0.250405,"ic underspecification, parallelism, and anaphora. To this purpose it provides parallelism constraints (Niehren and Koller, 1998) of the form X / X &apos; ~ Y / Y &apos; reminiscent to equality up-to constraints (Niehren et al., 1997a), and anaphoric bindings constraints of the form a n t e ( X ) = X q As proved in (Niehren and Koller, 1998), CLLS extends the expressiveness of context unification (Niehren et al., 1997a). It also extends its linguistic coverage (Niehren et al., 1997b) by integrating an analysis of VP ellipses with anaphora as in (Kehler, 1995). Thus, the coverage of CLLS is comparable to Crouch (1995) and Shieber et al. (1996). We illustrate CLLS at a benchmark case for the interaction of scope, anaphora, and ellipsis (8). (8) Mary read a book she liked before Sue did. The paper is organized as follows. First, we introduce CLLS in detail and define its syntax and semantics. We illustrate CLLS in sec. 3 by applying it to the example (8) and compare it to related work in the last section. 2 A Constraint Language for A-Structures (CLLS) CLLS is an ordinary first-order language interpreted over A-structures. A-structures are particular predicate logic tree structures we will introduce. We firs"
C98-1056,P97-1051,0,0.0176282,"c formalisms, such as UDRT (Reyle, 1993) and MRS (Copestake et al., 1997), which use dominance relations similar to ours, and also with theories of Logical Form associated with GB style grammars, such as (May, 1977). In all these frameworks one tends to use variable-coordination (or coindexing) rather than the explicit binding and linking relations we have presented here. We hope that 358 these approaches can potentially benefit from the presented idea of rubber bands for binding and linking, without having to make any dramatic changes. Our definition of parallelism implements some ideas from Hobbs and Kehler (1997) on the behavior of anaphoric links. In contrast to their proposal, our definition of parallelism is not based on an abstract notion of similarity. Furthermore, CLLS is not integrated into a general theory of abduction. We pursue a more modest aim at this stage, as CLLS needs to be connected to ""material"" deduction calculi for reasoning with such underspecified semantic representation in order to make progress on this front. We hope that some of the more ad hoc features of our definition of parallelism (e.g. axiom 5) may receive a justification or improvement in the light of such a deeper unde"
C98-1056,J87-1005,0,0.0128841,"o consistent renaming of bound variables (aequality); a constraint of CLLS is an underspecified description of a A-structure. CLLS solves a capturing problem omnipresent in underspecified scope representations. CLLS features constraints for dominance, lambda binding, parallelism, and anaphoric links. Based on CLLS we present a simple, integrated, amt underspecified treatment of scope, parallelism, and anaphora. 1 Introduction A central concern of semantic underspecification (van Deemter and Peters, 1996) is the underspecification of the scope of variable binding operators such as quantifiers (Hobbs and Shieber, 1987; Alshawi, 1990; Reylc, 1993). This immediately raises the conceptual problem of how to avoid variable-capturing when instantiating underspecified scope representations. In principle, capturing may occur in all formalisms for structural underspecification which represent binding relations by the coordination of variables (Reyle, 1995; Pinkal, 1996; Bos, 1996; Niehren et al., 1997a). Consider for instance the verb phrase in (1) Manfred [w, knows every student] An underspecified description of the compositional semantics of the VP in (1) might be given along the lines of (2): (2) X=Cl(Vx(student"
C98-1056,E93-1025,0,0.028419,"t with parallelism. E.g., (16) is the source clause of the manypronouns-puzzle, a problematic case of interaction of ellipsis and anaphora. (Xu, 1998), where our t r e a t m e n t of ellipsis and anaphora was developed, argues that link chains yield the best explanation for the distribution of strict/sloppy readings involving m a n y pronouns. The basic idea is that an elided pronoun can either be linked to its parallel pronoun in the source clause (referential parallelism) or be linked in a structurally parallel way (structural parallelism). This analysis agrees with the proposal m a d e in (Kehler, 1993; Kehler, 1995). It covers a series of problematic cases in the literature such as the many-pronouns-puzzle, cascaded ellipsis, or the five-reading sentence (17): (17) J o h n revised his paper before the teacher did, and so did Bill The precise interaction of parallelism with binding and linking relations is spelled out in sec. 2.2. 2.2 Syntax and Semantics of CLLS We start with a set of labels E = {@2, lam I &apos; var 0, ana 0, before 2, mary0, read0,...}, ranged over by f i with arity i which may be omitted. The syntax of CLLS is given by: (p ::= [ I X:f(X1,...,Xn) (fnEP,,) X&lt;*Y A(x)=Y I ante(X"
C98-1056,P83-1020,0,0.345776,"on tree-nodes for expressing variable binding. An graphical illustration of the A-structure corresponding to the A-term Ax.like(x,x) is given (5). (5) ( like,.1 &apos;, Ax.like(x,x) Formally, the binding relation of the A-structure in (5) is expressed through the partial function A(5) defined by A(5)(u2) = u0 and A(5)(u3) = u0. We propose a first-order constraint language for A-structures called CLLS which solves the capturing problem of underspecified scope representations in a simple and elegant way. CLLS subsumes dominance constraints (Backofen et al., 1995) as known from syntactic processing (Marcus et al., 1983) with tree-adjoining grammars (Vijay-Shanker, 1992; Rogers and VijayShanker, 1994). Most importantly, CLLS constraints can describe the binding relation of a Astructure in an underspecified manner (in contrast to A-structures like (5), which are always fully specified). The idea is that A-binding behaves like a kind of rubber band that can be arbitraryly enlarged but never broken. E.g., (6) is an underspecified CLLS-description of the Astructure (5). X0&lt;~*X1 A A(X1)=X4A ,_x.0 Xo Xl:lam(X2)A //""lain I X1 (6) X2,a*XaA Xa:like(X4, Xh)A X4:var A Xs:var t ! I var""c""X4 X2 vat ~ X5 The constraint (6)"
C98-1056,P97-1053,1,0.826567,"e, parallelism, and anaphora. 1 Introduction A central concern of semantic underspecification (van Deemter and Peters, 1996) is the underspecification of the scope of variable binding operators such as quantifiers (Hobbs and Shieber, 1987; Alshawi, 1990; Reylc, 1993). This immediately raises the conceptual problem of how to avoid variable-capturing when instantiating underspecified scope representations. In principle, capturing may occur in all formalisms for structural underspecification which represent binding relations by the coordination of variables (Reyle, 1995; Pinkal, 1996; Bos, 1996; Niehren et al., 1997a). Consider for instance the verb phrase in (1) Manfred [w, knows every student] An underspecified description of the compositional semantics of the VP in (1) might be given along the lines of (2): (2) X=Cl(Vx(student(x)~C2(know(Z, x)))) The recta-variable X in (2) denotes some tree representing a predicate logic formula which is 353 underspecified for quantifier scope by means of two place holders C1 and Cs where a subjectquantifier can be filled in, and a place holder Z for the subject-variable. The binding of the object-variable x by the object-quantifier Vx is coordinated through the name"
C98-1056,J90-3001,0,\N,Missing
C98-1056,J92-4004,0,\N,Missing
cheng-xu-2008-fine,H05-1044,0,\N,Missing
cheng-xu-2008-fine,H05-2017,0,\N,Missing
cheng-xu-2008-fine,H05-1043,0,\N,Missing
cheng-xu-2008-fine,W03-1014,0,\N,Missing
cheng-xu-2008-fine,H01-1046,0,\N,Missing
cheng-xu-2008-fine,W06-1652,0,\N,Missing
cheng-xu-2008-fine,P02-1053,0,\N,Missing
cheng-xu-2008-fine,W02-1011,0,\N,Missing
cheng-xu-2008-fine,P97-1023,0,\N,Missing
cheng-xu-2008-fine,schafer-2006-ontonerdie,0,\N,Missing
cotik-etal-2017-annotation,W16-2921,1,\N,Missing
cotik-etal-2017-annotation,W17-1807,0,\N,Missing
cotik-etal-2017-annotation,W16-4210,1,\N,Missing
D16-1065,D15-1198,0,0.636214,"tem P R F1 JAMR(fixed) System 1 System 2 JAMR(fixed) System 1 System 2 .67 .72 .73 .68 .74 .73 .58 .65 .69 .59 .63 .68 .62 .68 .71 .63 .68 .71 Table 3: Comparison between our joint approaches and the pipelined counterparts. Dataset LDC2013E117 LDC2014T12 System P R F1 CAMR* CAMR Our approach CAMR* CAMR CCG-based Our approach .69 .71 .73 .70 .72 .67 .73 .67 .69 .69 .66 .67 .66 .68 .68 .70 .71 .68 .70 .66 .71 Table 4: Final results of various methods. 4.5 Comparison with State-of-the-art We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependencybased parser (Wang et al., 2015b). For comparison 4 We use the latest, fixed version of JAMR, available at https://tiny.cc/jamr. purposes, we give two results from two different versions of dependency-based AMR parser5 : CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing an"
D16-1065,W13-2322,0,0.620283,"w between the two subtasks in a single incremental model. Experiments on the public datasets demonstrate that our joint model significantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources. 1 Introduction Producing semantic representations of text is motivated not only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation. Banarescu et al. (2013) described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node"
D16-1065,P13-2131,0,0.178848,"te inference, it is very natural to employ violation-fixing perceptron here for AMR parsing training. Specifically, we use an improved update method “max-violation” which updates at the worst mistake, 686 4.1 Experiments Dataset and Evaluation Metric Following previous studies on AMR parsing, our experiments were performed on the newswire sections of LDC2013E117 and LDC2014T12, and we also follow the official split for training, development and evaluation. Finally, we also show our parsers performance on the full LDC2014T12 dataset. We evaluate the performance of our parser using Smatch v2.0 (Cai and Knight, 2013), which counts the precision, recall and F1 of the concepts and relations together. 4.2 Development Results Generally, larger beam size will increase the computational cost while smaller beam size may reduce the performance. As a tradeoff, we set the beam size as 4 throughout our experiments. Figure 3 shows the training curves of the averaged violation-fixing perceptron with respect to the performance on the both development sets. As we can see the curves converge very quickly, at around iteration 3. 4.4 0.72 F-measure 0.71 0.7 0.69 0.68 LDC2014T112 LDC2103E117 0.67 0.66 0 1 2 3 4 5 6 7 8 9 10"
D16-1065,P04-1015,0,0.0592557,"cantly outperforms the pipelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations (Collins and Roark, 2004; Hatori et al., 2012; Li and Ji, 2014). However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated. In addition, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments de"
D16-1065,W02-1001,0,0.38374,"1. Name Description 1 Fragment given words 2 3 Span length NER 4 Bias Relative frequency estimates of the probability of a concept fragment given the span of words. The length of the span. 1 if the span corresponds to a named entity, 0 otherwise. 1 for any concept fragment from the alignment table, 0 otherwise. 5 6 7 8 9 10 11 12 13 14 15 16 c c+w c + lem c + pos c + w−1 c + w+1 c + pos−1 c + pos+1 c + w−2 c + w+2 c + pos−2 c + pos+2 To reduce overfitting, we used averaged parameters after training to decode test instances in our experiments. The resulting model is called averaged perceptron (Collins, 2002). Additionally, in our training algorithms, the implementation of the oracle function is rela-tively straightforward. Specifically, when the i-th span is processed in the incremental parsing process, the partial gold-standard AMR graph up to the i-th span consists of the edges and nodes that appear before the end position of the i-th span, over which the gold-standard feature vectors are calculated. 4 c represents the current concept label, w represents the current words, lem represents the current lemmas, pos represents the current POS tags. w−1 denotes the first word to the left of current w"
D16-1065,P14-1134,0,0.701726,"only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation. Banarescu et al. (2013) described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node generation is an important limiting factor in AMR parsing, Werling et al. (2015) proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs. However, the overall architecture is still based on the pipelined model. As a common drawback of the staged architecture, errors in upstream component are often compounded and"
D16-1065,P16-1001,0,0.650129,"nal semantic resources. Dataset System P R F1 LDC2014T12 JAMR(fixed) CAMR* CAMR SMBT-based Our approach .64 .68 .70 .70 .53 .60 .62 .62 .58 .64 .66 .67 .66 Table 5: Final results on the full LDC2014T12 dataset. 5 Related Work Our work is motivated by JAMR (Flanigan et al., 2014), which is based on a pipelined model, resulting in a large drop in overall performance when moving from gold concepts to system concepts. Wang et al. (2015a) uses a two-stage approach; dependency parses are modified by executing a sequence of actions to resolve dis-crepancies between dependency tree and AMR structure. Goodman et al. (2016) improves the transition-based parser with the imitation learning algorithms, achieving almost the same performance as that of Wang et al. 5 The code is available at https://github.com/ Juicechuan/AMRParsing 688 (2015b), which exploits the extended features from additional trained analysers, including co-reference and semantic role labelers. Artzi et al. (2015) introduces a new CCG grammar induction algorithm for AMR parsing, combined with a factor graph to model non-compositional phenomena. Pust et al. (2015) adapts the SBMT parsing framework to AMR parsing by designing an AMR transformation,"
D16-1065,P12-1110,0,0.0117722,"pelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations (Collins and Roark, 2004; Hatori et al., 2012; Li and Ji, 2014). However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated. In addition, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole"
D16-1065,N12-1015,0,0.0303754,"2016. 2016 Association for Computational Linguistics cent”, it should evoke the concept “affect-02”. Obviously, the correct concept choice for the verb “affect” should exploit a larger context, and even the whole semantic structure of the sentence, which is more probable to be unfolded at the downstream relation identification stage. This example indicates that it is necessary to allow for the interaction of information between the two stages. try-01 :ARG1 :ARG0 :ARG1 :mod :name He accent 2 Background 2.1 country affect-02 :ARG0 designed specifically for inexact search in structured learning (Huang et al., 2012). Experimental results show that the proposed joint framework significantly outperforms the pipelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as"
D16-1065,D14-1036,0,0.0398291,"a factor graph to model non-compositional phenomena. Pust et al. (2015) adapts the SBMT parsing framework to AMR parsing by designing an AMR transformation, and adding external semantic resources. More recently, Damonte et al. (2016) also presents an incremental AMR parser based on a simple transition system for dependency parsing. However, compared to our parser, their parser cannot parse non-projective graphs, resulting in a limited coverage. Our work is also inspired by a new computational task of incremental semantic role labeling, in which semantic roles are assigned to incomplete input (Konstas et al., 2014). 6 Conclusions and Future Work In this paper, we present a new approach to AMR parsing by using an incremental model for performing the concept identification and relation identification jointly, which alleviates the error propagation in the pipelined model. In future work, we plan to improve the parsing performance by exploring more features from the coreference resolution, word sense disambiguation system and other external semantic resources. In addition, we are interested in further incorporating the incremental semantic role labeling into our incremental framework to allow bi-directional"
D16-1065,P14-1038,0,0.0994402,"and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources. name :op1 British Figure 1: The AMR graph for the sentence “He tries to affect a British accent.” To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations (Collins and Roark, 2004; Hatori et al., 2012; Li and Ji, 2014). However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated. In addition, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are requi"
D16-1065,N15-1114,0,0.122135,"Missing"
D16-1065,J08-4003,0,0.0403371,"GHT-ARCS (lines 26-27): Add current concept and highest-scoring left arc and right arc to the partial graph. The first three actions are similar in form to those in the Arc-Standard algorithm for transition-based 2 3 The constant B denotes the beam size. The right-to-left order reflects the principle of local priority. 683 Figure 2: An illustrative diagram for CWBS algorithm. Each dotted box corresponds to a connected component in the partial graph, each of which consists one or multiple concept fragments. The rightmost subgraph corresponds to the current concept fragment. dependency parsing (Nivre, 2008; Zhang and Clark, 2008a). The last one is defined to cope with the cases where there may be multiple parents for some nodes in an AMR graph. Note that the “SHIFT” action does not add any edges. This operation is particularly necessary because the partial graphs are not always connected during the search process. In our experiments, we also found that the number of connected components during search process is relatively small, which is generally less than 6. It is important to note that, in order to guarantee the output graph connected, when the last concept fragment is encountered, the “SHIF"
D16-1065,N15-1119,0,0.0396957,"es. 1 Introduction Producing semantic representations of text is motivated not only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation. Banarescu et al. (2013) described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node generation is an important limiting factor in AMR parsing, Werling et al. (2015) proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs. However, the overall architecture is still based on the pipelined model. As a common drawbac"
D16-1065,D15-1136,0,0.517497,"ures generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources. We also evaluate our parser on the full LDC2014T12 dataset. We use the training/development/test split recommended in the release: 10,312 sentences for training, 1,368 sentences for development and 1,371 sentences for testing. For comparison, we include the results of JAMR, CAMR*, CAMR and SMBT-based parser (Pust et al., 2015), which are also trained on the same dataset. The results in Table 5 show that our approach outperforms CAMR*, and obtains comparable performance with CAMR. However, our approach achieves slightly lower performance, compared to the SMBT-based parser, which adds data and features drawn from various external semantic resources. Dataset System P R F1 LDC2014T12 JAMR(fixed) CAMR* CAMR SMBT-based Our approach .64 .68 .70 .70 .53 .60 .62 .62 .58 .64 .66 .67 .66 Table 5: Final results on the full LDC2014T12 dataset. 5 Related Work Our work is motivated by JAMR (Flanigan et al., 2014), which is based"
D16-1065,N15-1040,0,0.667706,"xed) System 1 System 2 .67 .72 .73 .68 .74 .73 .58 .65 .69 .59 .63 .68 .62 .68 .71 .63 .68 .71 Table 3: Comparison between our joint approaches and the pipelined counterparts. Dataset LDC2013E117 LDC2014T12 System P R F1 CAMR* CAMR Our approach CAMR* CAMR CCG-based Our approach .69 .71 .73 .70 .72 .67 .73 .67 .69 .69 .66 .67 .66 .68 .68 .70 .71 .68 .70 .66 .71 Table 4: Final results of various methods. 4.5 Comparison with State-of-the-art We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependencybased parser (Wang et al., 2015b). For comparison 4 We use the latest, fixed version of JAMR, available at https://tiny.cc/jamr. purposes, we give two results from two different versions of dependency-based AMR parser5 : CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources. We also evaluat"
D16-1065,P15-2141,0,0.302746,"xed) System 1 System 2 .67 .72 .73 .68 .74 .73 .58 .65 .69 .59 .63 .68 .62 .68 .71 .63 .68 .71 Table 3: Comparison between our joint approaches and the pipelined counterparts. Dataset LDC2013E117 LDC2014T12 System P R F1 CAMR* CAMR Our approach CAMR* CAMR CCG-based Our approach .69 .71 .73 .70 .72 .67 .73 .67 .69 .69 .66 .67 .66 .68 .68 .70 .71 .68 .70 .66 .71 Table 4: Final results of various methods. 4.5 Comparison with State-of-the-art We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependencybased parser (Wang et al., 2015b). For comparison 4 We use the latest, fixed version of JAMR, available at https://tiny.cc/jamr. purposes, we give two results from two different versions of dependency-based AMR parser5 : CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources. We also evaluat"
D16-1065,P15-1095,0,0.212769,"h their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. Pan et al. (2015) presented Automatic AMR parsing is still in a nascent stage. Flanigan et al. (2014) built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node generation is an important limiting factor in AMR parsing, Werling et al. (2015) proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs. However, the overall architecture is still based on the pipelined model. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream prediction. The downstream components, however, cannot impact earlier decision. For example, for the verb “affect” in the example shown in Figure 1, there exist two possible concepts: “affect-01” and “affect-02”. Comparatively, the first concept has mo"
D16-1065,D08-1059,0,0.189991,"on, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are required as input, as the MSCG algorithm in JAMR. Secondly, we adopt a segment-based decoder similar to the multiple-beam algorithm (Zhang and Clark, 2008b) for concept identification, and then incorporate the CWBS algorithm for relation identification into this framework, combining the two subtasks in a single incremental model. For parameter estimation, “violation-fixing” perceptron is adopted since it is 681 AMR Parsing Task Nodes of an AMR graph are labeled with concepts, and edges are labeled with relations. Concepts can be English words (“He”), PropBank event predicates (“try-01”, “affect-02”), or special keywords (“British”). For example, “affect-02” represents a PropBank roleset that corresponds to the first sense of “affect”. According"
D16-1065,P08-1101,0,0.126709,"on, the nodes in the graph are latent. One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are required as input, as the MSCG algorithm in JAMR. Secondly, we adopt a segment-based decoder similar to the multiple-beam algorithm (Zhang and Clark, 2008b) for concept identification, and then incorporate the CWBS algorithm for relation identification into this framework, combining the two subtasks in a single incremental model. For parameter estimation, “violation-fixing” perceptron is adopted since it is 681 AMR Parsing Task Nodes of an AMR graph are labeled with concepts, and edges are labeled with relations. Concepts can be English words (“He”), PropBank event predicates (“try-01”, “affect-02”), or special keywords (“British”). For example, “affect-02” represents a PropBank roleset that corresponds to the first sense of “affect”. According"
E09-2004,P07-1074,1,0.799928,"develop and implement cognitively enhanced artificial agents, using technologies in natural language processing, question answering, web-based information extraction, semantic web and interaction driven profiling with cognitive modelling (Krenn, 2008). This paper describes a conversational agent “Gossip Galore”, an active self-learning system that can learn, update and interpret information from the web, and can make conversations with users and provide answers to their questions in the domain of celebrity gossip. In more detail, by applying a minimally supervised relation extraction system (Xu et al., 2007; Xu et al., 2008), the agent automatically collects the knowledge from relevant websites, and also communicates with the users using a question-answering engine via a 3D graphic interface. This paper is organized as follows. Section 2 gives an overview of the system architecture and presents the design and functionalities of the components. Section 3 explains the system setup and discusses implementation details, and finally Section 4 draws conclusions. 2 System Overview Figure 1 shows a use case of the system. Given a query “Tell me something about Carla Bruni”, the application would trigger"
E17-3002,W15-0514,0,0.030732,"r, they do not offer effective functionalities for 1) easy access to the argumentative structure of debate content, and 2) quick overviews of the various semantic facets, the polarity and the relevance of the arguments. Some platforms1 allow users to label posts as pro or con arguments, to cite external sources, to assess debate content or to create structured debates across the web, but do not offer any deeper automatic language technologybased analyses. Argumentation mining research, which could help in automatically structuring debates, has only recently been applied to web debate corpora (Boltuzic and Snajder, 2015; Petasis and Karkaletsis, 2016; Egan et al., 2016). Our goal is to address these issues by developing a debate platform that: • Supports debate participants in making substantial and clear contributions • Facilitates an overview of debate contents • Associates relevant information available on the web with debate topics • Connects regional discussions to global deliberations • Supports advanced participation in deliberations, without sacrificing transparency and usability. In this paper, we present the Common Round platform, which implements various functions towards these goals, with followi"
E17-3002,W16-2816,0,0.0376372,"Missing"
E17-3002,P09-1113,0,0.0119288,"odels to recognize standard entity types, such as persons, organizations, locations and date/time expressions. For non-standard concept types, we use SProUT (Drozdzynski et al., 2004), which implements a regular expression-like rule formalism and gazetteers for detecting domain-specific concepts in text. Relation extraction is performed by matching dependency parse trees of sentences to a set of automatically learned dependency patterns (Krause et al., 2012). Relation patterns are learned from corpora manually annotated with event type, argument types, and roles, or using distant supervision (Mintz et al., 2009). For sentiment analysis, we apply a lexicon-based approach that additionally makes use of syntactic information in order to handle negation. Text Analytics and Association with External Material The Common Round platform enriches the contents of debates and posts by extracting information about topics, sentiment, entities and relations. Topic detection helps to find semantically related debates. Sentiment analysis allows determining the emotion level in discussions. By identifying, for example, instances of the relation MayTreatDisorder in a discussion about the legalization of cannabis, we c"
E17-3002,W16-2811,0,\N,Missing
fu-etal-2010-determining,J96-1002,0,\N,Missing
fu-etal-2010-determining,J91-3001,0,\N,Missing
K16-1024,P98-1013,0,0.603699,"Missing"
K16-1024,P10-1143,0,0.227749,"Missing"
K16-1024,W09-3208,0,0.378324,"as such. Analogously, 22 out of the 100 analyzed false positives were cases where the misclassification of the system was plausible to a human rater. This exemplifies that this task has many boundary cases were a positive/negative decision is hard to make even for expert annotators, thus putting the overall performance of all models in Table 3 in perspective. 6 Related work We briefly point out other relevant approaches and efforts from the vast amount of literature. Event coreference In addition to the competitors mentioned in Section 5, approaches for event linking were presented, e.g., by Chen and Ji (2009), who determine link scores with hand-crafted compatibility metrics for event mention pairs and a maximum-entropy model, and feed these to a spectral clustering algorithm. A variation of the eventcoreference resolution task extends the scope to cross-document relations. Cybulska and Vossen (2015) approach this task with various classification models and propose to use a type-specific granularity hierarchy for feature values. Lee et al. (2012) further extend the task definition by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sac"
K16-1024,W09-4303,0,0.401158,"Missing"
K16-1024,P15-1017,0,0.402323,"angeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisio"
K16-1024,cybulska-vossen-2014-using,0,0.0549016,"ctured-perceptron algorithm. Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermingles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency."
K16-1024,W15-0801,0,0.262634,"can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexical-semantic resources (WordNet, FrameNet) and other datasets (VerbOcean"
K16-1024,P15-1061,0,0.0841438,"2; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event men"
K16-1024,N15-1133,0,0.0151487,"output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a"
K16-1024,D13-1137,0,0.015828,"an (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task"
K16-1024,W13-1203,0,0.0233806,"n by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sachan et al. (2015) describe an active-learning based method for the same problem, where they derive a clustering of entities/events by incorporating bits of human judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm. Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermingles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocumen"
K16-1024,liu-etal-2014-supervised,0,0.55287,"quence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexical-semantic resour"
K16-1024,P09-1113,0,0.0614883,"nd Mitamura, 2015). to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, presen"
K16-1024,P08-1030,0,0.0213297,"ent aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performan"
K16-1024,P02-1014,0,0.145167,"η 10−5 dw 300 dp 8 β1 0.2 dc 256 β2 0.999 de 50  10−2 dsim 2 batch size n 3 epochs ≤ 2000 Dropout no `2 reg. no 512 Table 2: Hyperparameter settings. strategy performed worse than the less elaborate algorithm in Figure 2. The pairwise coreference decisions of our model induce a clustering of a document’s event mentions. In order to force the model to output a consistent view on a given document, a strategy for resolving conflicting decisions is needed. We followed the strategy detailed in Figure 3, which builds the transitive closure of all positive links. Additionally, we experimented with Ng and Gardent (2002)’s “BestLink” strategy, which discards all but the highest-scoring antecedent of an anaphoric event mention. Liu et al. (2014) reported that for event linking, BestLink outperforms naive transitive closure, however, in our experiments (Section 5) we come to a different conclusion. 4 ACE++ Table 1: Dataset properties. Figure 2: Generation of examples Pd for a document d with a sequence of event mentions Md . 1: 2: 3: 4: 5: 6: 7: ACE Experimental setting, model training We implemented our model using the TensorFlow framework (Abadi et al., 2015, v0.6), and chose the ACE 2005 dataset (Walker et a"
K16-1024,P11-1115,0,0.0957174,"Missing"
K16-1024,P15-2060,0,0.128067,"ejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make"
K16-1024,W12-4501,0,0.0283451,"n judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm. Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermingles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level informatio"
K16-1024,N15-1120,1,0.821269,"E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference cand"
K16-1024,D12-1045,0,0.185055,"al, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexic"
K16-1024,D12-1110,0,0.00798676,"mple, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model o"
K16-1024,D15-1278,0,0.0151945,"be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint"
K16-1024,J01-4004,0,0.355159,"ters θ = {Ww , Wp , {wc }, {bc }, We , be , Wsim , bsim , Wout , bout } (8) by minimizing the logistic loss over shuffled minibatches with gradient descent using Adam (Kingma and Ba, 2014). 3.3 Example generation and clustering We investigated two alternatives for the generation of examples from documents with recognized event mentions. Figure 2 shows the strategy we found to perform best, which iterates over the event mentions of a document and pairs each mention (the “anaphors”) with all preceding ones (the “antecedent” candidates). This strategy applies to both training and inference time. Soon et al. (2001) propose an alternative strategy, which during training creates positive examples only for the closest actual antecedent of an anaphoric event mention with intermediate event mentions serving as negative antecedent candidates. In our experiments, this 242 1: 2: 3: 4: 5: 6: 7: procedure G ENERATE E XAMPLES(Md ): Md = (m1 , . . . , m|Md |) Pd ← ∅ for i = 2, . . . , |Md |do for j = 1, . . . , i − 1 do Pd ← Pd ∪ {(mi , mj )} return Pd # documents # event instances # event mentions procedure G ENERATE C LUSTERS(Pd , score): Pd = {(mi , mj )}i,j score : Pd 7→ [0, 1] Cd ← {(mi , mj ) ∈ Pd : score(mi"
K16-1024,P10-1081,0,0.031717,"ce. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto"
K16-1024,P11-1053,0,0.0267238,"to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event ty"
K16-1024,swampillai-stevenson-2010-inter,0,0.0721242,"Missing"
K16-1024,M95-1005,0,0.935591,"Missing"
K16-1024,P15-1137,0,0.0418208,"from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint trigger identification and coreference resolution. On the other hand, separating anaphoricity detection from antecedent scoring, as is often done for the task of entity coreference resolution (e.g., by Wiseman et al. (2015)), might result in performance gains; also the generation of sentential features from recurrent neural networks seems promising. Regarding our mediumterm research agenda, we would like to investigate if the model can benefit from more fine-grained information about the discourse structure underlying a text. This could guide the model when encountering the problematic case of pronoun resolution, described in the error analysis. Acknowledgments This research was supported by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (contract 01IW14002) and BBDC"
K16-1024,D15-1206,0,0.0142118,"d classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences. There are several areas for potential future work. As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument set"
K16-1024,D10-1099,0,0.0773609,"combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain. Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011). In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015). 246 7 Conclusion Our proposed model for the ta"
K16-1024,C14-1220,0,0.441852,"rence resolution (Bejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step"
K16-1024,D15-1203,0,0.155321,"make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level. Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event mentions based on the p"
K16-1024,P05-1053,0,0.0500769,"example in (Araki and Mitamura, 2015). to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date). 3 Model design This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture. Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011; Krause et al., 2015) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples: • lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (en"
K16-1024,C98-1013,0,\N,Missing
K16-1024,D15-1247,0,\N,Missing
K16-1024,D14-1159,0,\N,Missing
kluewer-etal-2012-evaluation,W10-4353,0,\N,Missing
kluewer-etal-2012-evaluation,adolphs-etal-2010-question,1,\N,Missing
kluewer-etal-2012-evaluation,W04-2304,0,\N,Missing
kluewer-etal-2012-evaluation,W09-3951,0,\N,Missing
kluewer-etal-2012-evaluation,C10-2065,1,\N,Missing
kluewer-etal-2012-evaluation,J00-3003,0,\N,Missing
kluewer-etal-2012-evaluation,P10-4007,1,\N,Missing
kluewer-etal-2012-evaluation,P97-1035,0,\N,Missing
kluewer-etal-2012-evaluation,J05-1004,0,\N,Missing
krause-etal-2014-language,P07-1074,1,\N,Missing
krause-etal-2014-language,li-etal-2014-annotating,1,\N,Missing
krause-etal-2014-language,li-etal-2012-annotating,1,\N,Missing
krause-etal-2014-language,doddington-etal-2004-automatic,0,\N,Missing
krause-etal-2014-language,hasler-etal-2006-nps,0,\N,Missing
krause-etal-2014-language,P05-1045,0,\N,Missing
krieger-etal-2014-information,P03-1054,0,\N,Missing
krieger-etal-2014-information,P07-1074,1,\N,Missing
krieger-etal-2014-information,P99-1052,0,\N,Missing
L16-1383,W14-2907,0,0.0279161,"Missing"
L16-1383,P98-1013,0,0.0704039,"red via a pattern discovery method based on distant supervision (Mintz et al., 2009; Krause et al., 2012). Thus sar-graphs can be directly applied to free texts for relation extraction. Early work on lexical-semantics resources has focused on gathering information about individual words and their different meanings in varying contexts, the famous example being WordNet (Fellbaum, 1998). Linguistic knowledge resources that go beyond the level of lexical items are scarce and of limited coverage due to significant investment of human effort and expertise required for their construction. FrameNet (Baker et al., 1998) is such a resource and provides fine-grained semantic relations of predicates and their arguments. However, FrameNet does not provide an explicit link to real-world fact types. There is increasing research in automatically creating largescale linguistic resources, often these have been built on top of existing resources. For example, BabelNet (Navigli and Ponzetto, 2012) merged Wikipedia concepts including entities with word senses from WordNet; a similar strategy was pursued in ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczy"
L16-1383,W13-5503,0,0.0237209,"rce and provides fine-grained semantic relations of predicates and their arguments. However, FrameNet does not provide an explicit link to real-world fact types. There is increasing research in automatically creating largescale linguistic resources, often these have been built on top of existing resources. For example, BabelNet (Navigli and Ponzetto, 2012) merged Wikipedia concepts including entities with word senses from WordNet; a similar strategy was pursued in ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Ofte"
L16-1383,W15-1601,0,0.0135592,"ia the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this paper the ongoing effort of linking the data-driven sar-graphs with the curated FrameNet. We also show that by enriching sargraphs with FrameNet data we can mitigate the notorious long-tail distribution of linguistic phrases, which allows us to reach higher coverage in extraction experiments. In"
L16-1383,J14-1002,0,0.0215939,"i, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this"
L16-1383,D15-1112,0,0.034165,"Missing"
L16-1383,J02-3001,0,0.0693277,"ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work an"
L16-1383,P06-1117,0,0.0245124,"on for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this paper the ongoing effort of linking the data-driven sar-graphs with the curated FrameNet. We also show that by enriching sargraphs with FrameNet data we can mitigate the notorious long-tail distribution of linguistic phrases, which allows us to reach hi"
L16-1383,E12-1059,0,0.0233527,"arguments. However, FrameNet does not provide an explicit link to real-world fact types. There is increasing research in automatically creating largescale linguistic resources, often these have been built on top of existing resources. For example, BabelNet (Navigli and Ponzetto, 2012) merged Wikipedia concepts including entities with word senses from WordNet; a similar strategy was pursued in ConceptNet (Speer and Havasi, 2013). Only few approaches have included FrameNet in their linking efforts (Scheffczyk et al., 2006; Bonial et al., 2013; Aguilar et al., 2014). A particular example is UBY (Gurevych et al., 2012), which provides a standardized representation for several combined lexico-semantic resources via the Lexical Markup Framework. None of these approaches linked FrameNet both to knowledge-graph relations and extended it with linguistic patterns at the same time. Much of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas"
L16-1383,W15-4204,1,0.904146,"ch of the recent literature has dealt with the problems of semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014; FitzGerald et al., 2015), i.e., the automatic enrichment of sentences with FrameNetstyle annotation. Often, these systems suffer from a lack of training data. Although several ideas to address this issue have been presented (Giuglea and Moschitti, 2006; Pavlick et al., 2015; Chang et al., 2015), the problem largely remains unsolved. Our approach can support these systems by increasing the amount of available training data. In previous work (Krause et al., 2015), we have linked sargraphs to word-level lexical-semantic resources like BabelNet. We continue this line of work and describe in this paper the ongoing effort of linking the data-driven sar-graphs with the curated FrameNet. We also show that by enriching sargraphs with FrameNet data we can mitigate the notorious long-tail distribution of linguistic phrases, which allows us to reach higher coverage in extraction experiments. In the following, we discuss two ways of linking sar-graphs with FrameNet, which are in spirit of the large-scale efforts mentioned above. We believe that both resources an"
L16-1383,P09-1113,0,0.0504113,"Missing"
L16-1383,P15-2067,0,0.044101,"Missing"
L16-1537,P12-2011,0,0.141426,"of inference types underlying the entailment decisions. Keywords: Entailment Graphs, Relation Extraction, Textual Entailment 1. Introduction The task of relation extraction (RE) is to recognize and extract relations between entities or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. In rule-based RE methods, the patterns are directly applied to extract relation mentions from parsed sentences of free texts (e.g., Yangarber et al. (2000), Alfonseca et al. (2012)). Other methods treat RE as a classification or sequence-labeling problem, but even for those techniques parse tree patterns have proven useful as key classification features (e.g., Zelenko et al. (2003), Bunescu and Mooney (2005)). In order to circumvent manual annotation work needed for supervised learning, recent work in RE concentrates on weakly supervised learning, for example based on techniques of distant supervision (Mintz et al., 2009; Krause et al., 2012). These utilize extensive volumes of pre-existing knowledge for partially labeling large volumes of data, resulting in large numbe"
L16-1537,P10-1124,0,0.304804,"e propose a new approach to structuring extraction patterns by utilizing entailment graphs. The textual entailment paradigm captures the semantic relationship holding between two textual expressions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et a"
L16-1537,P11-1062,0,0.0285902,"ween two textual expressions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et al., 2007). Unlike most other relation extraction systems, Web-DARE can deal with n-ary relations, not only binary relations. Furthermore, just as in the Snowball system"
L16-1537,P12-1013,0,0.203706,"essions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et al., 2007). Unlike most other relation extraction systems, Web-DARE can deal with n-ary relations, not only binary relations. Furthermore, just as in the Snowball system (Agichtein, 2006), Web"
L16-1537,H05-1091,0,0.0646459,"ties or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. In rule-based RE methods, the patterns are directly applied to extract relation mentions from parsed sentences of free texts (e.g., Yangarber et al. (2000), Alfonseca et al. (2012)). Other methods treat RE as a classification or sequence-labeling problem, but even for those techniques parse tree patterns have proven useful as key classification features (e.g., Zelenko et al. (2003), Bunescu and Mooney (2005)). In order to circumvent manual annotation work needed for supervised learning, recent work in RE concentrates on weakly supervised learning, for example based on techniques of distant supervision (Mintz et al., 2009; Krause et al., 2012). These utilize extensive volumes of pre-existing knowledge for partially labeling large volumes of data, resulting in large numbers of unique candidate patterns acquired from suspected mentions of relation instances. Among these patterns, some are semantically equivalent, but differ in their morphological, lexical-semantic or syntactic form. Some express a r"
L16-1537,W14-1610,0,0.113191,"ls H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to as entailment graphs (Berant et al., 2010). Entailment graphs have been built for various types 3367 of expressions, including propositional templates (Berant et al., 2010), typed predicates (Berant et al., 2011; Berant et al., 2012), open IE propositions (Levy et al., 2014), and text fragments (Kotlerman et al., 2015). A sample graph from Berant et al. (2010) is depicted in Figure 1, where → denotes a unidirectional and ↔ a bidirectional entailment (i.e., paraphrase) relation. et al., 2008) for annotating relation mentions in candidate sentences and learns pattern candidates from sentence parses generated using MaltParser (Nivre et al., 2007). Unlike most other relation extraction systems, Web-DARE can deal with n-ary relations, not only binary relations. Furthermore, just as in the Snowball system (Agichtein, 2006), WebDARE rules assign the semantic role labels"
L16-1537,P09-1113,0,0.109532,"Missing"
L16-1537,N13-1008,0,0.217571,"Missing"
L16-1537,E06-1052,0,0.504574,"wledge for partially labeling large volumes of data, resulting in large numbers of unique candidate patterns acquired from suspected mentions of relation instances. Among these patterns, some are semantically equivalent, but differ in their morphological, lexical-semantic or syntactic form. Some express a relation that entails the target relation or that is entailed by the target relation. Others are semantically unrelated to the target relation. The basic assumption made in this work is that patterns are truly reliable if they express a relation that semantically entails the target relation (Romano et al., 2006). This includes all patterns that express the target relation explicitly or a semantically equivalent relation. As an example, the pattern “org|B UYER bought org|ACQUIRED” can be considered to be semantically equivalent to the pattern “org|B UYER purchased org|ACQUIRED”, whereas “per|SPOUSE divorced per|SPOUSE” is not semantically equivalent to “per|SPOUSE married per|SPOUSE”, but entails a marriage relation. We propose a new approach to structuring extraction patterns by utilizing entailment graphs, hierarchical structures representing entailment relations, and present a novel resource of ent"
L16-1537,W11-0201,0,0.223261,"evaluating automatically generated entailment graphs and systems for recognizing textual entailment. 2. Related Work While relation extraction would clearly benefit from considering semantic relationships between patterns, there has been only a limited amount of prior work in structuring patterns. Matrix factorization approaches cluster semantically similar patterns based on argument co-occurrence information (e.g., Riedel et al. (2013)). Other approaches focus on the tree structure of the patterns, and compute similarity metrics based on graph matching techniques or tree edit distance (e.g., Thomas et al. (2011), Liu et al. (2013)). We propose a new approach to structuring extraction patterns by utilizing entailment graphs. The textual entailment paradigm captures the semantic relationship holding between two textual expressions T (text) and H (hypothesis): T entails H if the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). Textual entailment is defined as a semantic relation between exactly two text expressions. With entailment being a transitive relation, entailment relations holding among a set of expressions can be represented in a hierarchical structure, referred to"
L16-1537,C00-2136,0,0.154294,"ce as well as an analysis of inference types underlying the entailment decisions. Keywords: Entailment Graphs, Relation Extraction, Textual Entailment 1. Introduction The task of relation extraction (RE) is to recognize and extract relations between entities or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. In rule-based RE methods, the patterns are directly applied to extract relation mentions from parsed sentences of free texts (e.g., Yangarber et al. (2000), Alfonseca et al. (2012)). Other methods treat RE as a classification or sequence-labeling problem, but even for those techniques parse tree patterns have proven useful as key classification features (e.g., Zelenko et al. (2003), Bunescu and Mooney (2005)). In order to circumvent manual annotation work needed for supervised learning, recent work in RE concentrates on weakly supervised learning, for example based on techniques of distant supervision (Mintz et al., 2009; Krause et al., 2012). These utilize extensive volumes of pre-existing knowledge for partially labeling large volumes of data,"
li-etal-2012-annotating,remus-etal-2010-sentiws,0,\N,Missing
li-etal-2012-annotating,cheng-xu-2008-fine,1,\N,Missing
li-etal-2012-annotating,W05-0308,0,\N,Missing
li-etal-2012-annotating,W06-0301,0,\N,Missing
li-etal-2012-annotating,H05-2017,0,\N,Missing
li-etal-2012-annotating,H05-1043,0,\N,Missing
li-etal-2012-annotating,W03-1014,0,\N,Missing
li-etal-2012-annotating,W08-0122,0,\N,Missing
li-etal-2012-annotating,J04-3002,0,\N,Missing
li-etal-2012-annotating,H05-1045,0,\N,Missing
li-etal-2012-annotating,P10-1059,0,\N,Missing
li-etal-2012-annotating,W02-1011,0,\N,Missing
li-etal-2012-annotating,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
li-etal-2012-annotating,schulz-etal-2010-multilingual,0,\N,Missing
li-etal-2014-annotating,li-etal-2012-annotating,1,\N,Missing
P02-1056,P01-1019,0,0.0162156,"Missing"
P02-1056,A97-1035,0,0.0548928,"SG feature structures) is available in XML format with hyperlinks to full feature structure representations externally stored in corresponding data files. Fig. 1 gives an overview of the architecture of the WHITEBOARD Annotation Machine (WHAM). Applications feed the WHAM with input texts and a specification describing the components and configuration options requested. The core WHAM engine has an XML markup storage (external “offline” representation), and an internal “online” multi-level annotation chart (index-sequential access). Following the trichotomy of NLP data representation models in (Cunningham et al., 1997), the XML markup contains additive information, while the multi-level chart contains positional and abstraction-based information, e.g., feature structures representing NLP entities in a uniform, linguistically motivated form. Applications and the integrated components access the WHAM results through an object-oriented programming (OOP) interface which is designed as general as possible in order to abstract from component-specific details (but preserving shallow and deep paradigms). The interfaces of the actually integrated components form subclasses of the generic interface. New components ca"
P02-1056,P01-1034,0,0.116035,"Missing"
P02-1056,W97-0802,0,0.0694652,"Missing"
P02-1056,C02-1093,1,\N,Missing
P03-2019,C94-2144,1,0.720999,"ral edge automatically causes the failure of several, more specialized edges, without applying the unifiability test. Such information can in fact be precompiled. This and other optimization techniques are described in (Krieger and Piskorski, 2003). When compared to symbol-based finite state approaches, our method leads to smaller grammars and automata, which usually better approximate a given language. 3 XTDL – The Formalism in SProUT XTDL combines two well-known frameworks, viz., typed feature structures and regular expressions. XTDL is defined on top of TDL, a definition language for TFSs (Krieger and Schäfer, 1994) that is used as a descriptive device in several grammar systems (LKB, PAGE, PET). Apart from the integration into the rule definitions, we also employ TDL in SProUT for the establishment of a type hierarchy of linguistic entities. In the example definition below, the morph type inherits from sign and introduces three more morphologically motivated attributes with the corresponding typed values: morph := sign & [ POS atom, STEM atom, INFL infl ]. A rule in XTDL is straightforwardly defined as a recognition pattern on the left-hand side, written as a regular expression, and an output descriptio"
P03-2019,W03-2415,0,0.0311393,"roUT offers three online components: a tokenizer, a gazetteer, and a morphological analyzer. The tokenizer maps character sequences to tokens and performs fine-grained token classification. The gazetteer recognizes named entities based on static named entity lexica. The morphology unit provides lexical resources for English, German (equipped with online shallow compound recognition), French, Italian, and Spanish, which were compiled from the full form lexica of MMorph (Petitpierre and Russell, 1995). Considering Slavic languages, a component for Czech presented in (Hajiþ, 2001), and Morfeusz (Przepiórkowski and Wolinski, 2003) for Polish. For Asian languages, we integrated Chasen (Asahara and Matsumoto, 2000) for Japanese and Shanxi (Liu, 2000) for Chinese. The XTDL-based grammar engineering platform has been used to define grammars for English, German, French, Spanish, Chinese and Japanese allowing for named entity recognition and extraction. To guarantee a comparable coverage, and to ease evaluation, an extension of the MUC-7 standard for entities has been adopted. ne-person := enamex & [ TITLE list-of-strings, GIVEN_NAME list-of-strings, SURNAME list-of-strings, P-POSITION list-of-strings, NAME-SUFFIX string, DE"
P03-2019,piskorski-etal-2002-flexible,1,\N,Missing
P07-1074,W06-0204,0,0.879054,"and Hong Li Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation models (subject-verbobject, chain m"
P07-1074,W06-0202,0,0.108538,"Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation models (subject-verbobject, chain model, linked chain model and sub"
P07-1074,P03-1029,0,0.672803,"Xu, Hans Uszkoreit and Hong Li Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation mode"
P07-1074,C00-2136,0,0.0322115,"the seed-driven bottom-up pattern acquisition is presented. Section 5 describes our experiments with pattern ranking, filtering and rule induction. Section 6 presents the experiments and evaluations for the two application domains. Section 7 provides a conclusion and an outline of future work. 2 System Architecture Given the framework, our system architecture can be depicted as follows: Figure 1. Architecture This architecture has been inspired by several existing seed-oriented minimally supervised machine learning systems, in particular by Snowball (Agichtein and Gravano, 2000) and ExDisco (Yangarber et al., 2000). We call our system DARE, standing for “Domain Adaptive Relation Extraction based on Seeds”. DARE contains four major components: linguistic annotation, classifier, rule learning and relation extraction. The first component only applies once, while the last three components are integrated in a bootstrapping loop. At each iteration, rules will be learned based on the seed and then new relation instances will be extracted by applying the learned rules. The new relation instances are then used as seeds for the next iteration of the learning cycle. The cycle terminates when no new relations can b"
P07-1074,P03-1044,0,0.584108,"Complexity Feiyu Xu, Hans Uszkoreit and Hong Li Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic interaction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules. Stevenson and Greenwood (2006) present a systematic investigation of the pattern representation models and point out that substructures of the linguistic representation and the access to the embedded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered"
P07-1074,xu-etal-2002-domain,1,0.798353,"erms “prize” and “win” respectively. Given n different domains, the domain relevance score (DR) of a term t in a domain di is: DR(t, di)= 0, if df(t, di) =0; df(t,di ) df(t,di ) × LOG(n × n ) , otherwise N×D ∑df(t,d j ) j=1 where • df(t, di): is the document frequency of a term t in the domain di • D: the number of the documents in di • N: the total number of the terms in di Here the domain relevance of a term is dependent both on its document frequency and its document frequency distribution in other domains. Terms mentioned by more documents within the domain than outside are more relevant (Xu et al., 2002). In the case of n=3 such different domains might be, e.g., management succession, book review or biomedical texts. Every domain corpus should ideally have the same number of documents and similar average document size. In the calculation of the trustworthiness of the origin, we follow Agichtein and Gravano (2000) and Yangarber (2003). Thus, the relevance of a pattern is dependent on the relevance of its terms and the score value of the most trustworthy seed from which it origins. Finally, the score of a pattern p is calculated as follows: T score(p)= ∑ DR(t i ) × max{score( s ) :s ∈ Seeds} i"
P10-4007,adolphs-etal-2010-question,1,0.873423,"Missing"
P10-4007,W08-1301,0,0.035559,"Missing"
P10-4007,P07-1074,1,0.665395,".C.E. chatbot that uses its own understanding and generation components (Wallace and Bush, 2001). &lt;PREDICATE, ARG1, ARG2, [message-type]&gt; The following examples show the structure used for different input: • ”Who is the boyfriend of Madonna?” &lt;hasBoyfriend, Madonna, ?, [wh]&gt; • ”I want to buy a sofa.” &lt;buy, I, &quot;a sofa&quot;, [declarative]&gt; 5.2 Information Extraction Both scenarios make use of state-of-the-art information extraction approaches to extract the important pieces from the user input. While the bartender depends on relation extraction to detect the fact or relation questioned by the user (Xu et al., 2007), the sales agent uses information extraction methods to recognize user wishes and demands. As a result, the questioned fact or the demanded object feature equals the ontology structure containing the knowledge needed to handle the user input. The input “Do you have any red couches?” for example needs to get processed by the system in such a way that the information regarding the sofa with red color is extracted. This is done by the system in a data-driven way. The input analysis first tries to find a demanded object in the input via asking the ontology: Every object which can be discussed in"
P15-1058,D07-1074,0,0.138324,"Missing"
P15-1058,C10-1032,0,0.0663769,"Missing"
P15-1058,D11-1072,0,0.0796333,"Missing"
P15-1058,H93-1061,0,0.552476,"Missing"
P15-1058,Q14-1019,0,0.402417,"Missing"
P15-1058,S13-2040,0,0.208108,"Missing"
P15-1058,C10-1145,0,0.0251144,"Missing"
P15-1058,P10-4014,0,0.721567,"Missing"
P15-1058,J14-4005,0,0.133748,"Missing"
P15-1058,P10-1154,0,0.0170223,"Missing"
P15-1058,S07-1016,0,0.2072,"Missing"
P15-1058,S13-1003,0,0.256707,"Missing"
P15-1058,W04-0811,0,0.178741,"Missing"
P15-1058,E06-1002,0,\N,Missing
P15-1058,J14-1003,0,\N,Missing
P15-4001,ai-etal-2014-sprinter,1,0.822127,"ng and individualized learning. Natural language processing (NLP) technologies play a growing important role for CALL (Hubbard, 2009). They can help to identify errors in student input and provide feedback so that the learners can be aware of them (Heift, 2013; Nagata, 1993). Furthermore, they can help to build models of the achieved proficiency of the learners and provide materials and tasks appropriate. In this paper, we demonstrate an implementation of a framework of computer assisted pronunciation training for L2. The current version, which is a further development of the Sprinter system (Ai et al., 2014), automatically recognises and classifies the pronunciation errors of learners and provides visual and audio feedback with respect to the error types. The framework contains two subsystems: 1) the annotation tool which provides linguists an easy environment for annotating pronunciation errors in learners’ speech data; 2) the speech verification tool which identifies learners’ prosody and pronunciation errors and helps to correct them. The remainder of the paper is as follows: Section 2 describes the system architecture; Section 3 and 4 explain how each subsystem works; Section 5 evaluates the"
P15-4001,ai-charfuelan-2014-mat,1,\N,Missing
P15-4008,doddington-etal-2004-automatic,0,0.0800995,"tically correct if there are no parsing or other preprocessing errors, and it is semantically correct if its source sentences express the target relation. Correspondingly, we label a dependency pattern as “INCORRECT” if it is grammatically incorrect, or if its sentences do not express the target relation. Typically, the annotators aim to identify one or more of the error classes discussed in Section 5 to decide whether a pattern is incorrect. For deciding whether a sentence expresses a given relation, we use the ACE annotation guidelines’ conceptual definition of relations and their mentions (Doddington et al., 2004), and define the semantics of relations based on Freebase descriptions. In contrast to the ACE tasks, we also consider n-ary relations in addition to binary relations. Sentences must express the target relation explicitly, e.g., “Obama was awarded the Nobel Peace Prize.” explicitly expresses the relation award honor. We treat implicit mentions as semantically incorrect, e.g., the previous example only implies an award nomination. A third feedback category, “CORRECT, BUT TOO SPECIFIC ”, was added based on our initial analysis of the dataset, and applies to dependency patterns mostly found in th"
P15-4008,P09-1113,0,0.170511,"Missing"
P15-4008,Q13-1030,0,0.0289449,"Missing"
P15-4008,P05-1047,0,0.0242473,"d allows searching for specific patterns or sentences. The center part visualizes the currently selected dependency pattern in AVM notation. In this notation, the IN PUT element contains the dependency pattern, and the OUTPUT element lists the relation arguments extracted by this pattern. In the example pattern shown in Figure 2, these correspond to the spouses and the wedding date. Thus, the patterns also contain the semantic role label information of the target relation for the corresponding linguistic arguments, which is not included in most traditional pattern extraction approaches (e.g., Stevenson and Greenwood (2005)). Pattern Extraction In this section, we briefly describe our approach for extracting relation-specific dependency patterns in a distantly supervised setting, called WebDARE (Krause et al., 2012). In contrast to most other approaches, we consider not only binary, but arbitrary n-ary relations, with n &gt;= 2. For example, we can define a 4-ary marriage relation with the spouses as essential (required) arguments, and optional arguments such as the wedding date and location. Given a knowledge base (KB) containing such relations and their arguments, we select a set of seed relation instances from t"
P15-4008,D12-1042,0,0.0708671,"Missing"
P15-4008,P07-1074,1,0.780952,"which matches this sentence, the pattern extraction algorithm first identifies the argument mentions of the seed relation instance occurring in the sentence, and then determines and composes the set of shortest paths connecting the arguments in the dependency parse in a bottom-up manner. Figure 1 visualizes the pattern extraction process for an example sentence expressing the marriage relation. The extracted pattern is shown in attribute-value-matrix (AVM) notation in Figure 1c. For more details on the algorithm we refer the interested reader to the DARE pattern extraction method described in Xu et al. (2007). 3 The area below the representation of the pattern lists the source sentences that it was observed in, as well as some statistics about the frequency of the pattern. Sentences are formatted to highlight the important elements of the pattern. Relation arguments are marked in red, content words occurring in the pattern are marked in blue. Listing the source sentences is important because it enables the human expert to verify both the extracted dependency pattern (e.g., to detect a parse error), and the semantic correctness of the pattern, i.e., whether the sentences express the target relation"
P16-4007,P14-5007,0,0.190071,"Missing"
P16-4007,C10-1011,0,\N,Missing
P16-4007,P07-1074,1,\N,Missing
P98-1058,E95-1032,0,0.0153383,"ent of semantic underspecification, parallelism, and anaphora. To this purpose it provides parallelism constraints (Niehren and Koller, 1998) of the form X/X',,~Y/Y I reminiscent to equality up-to constraints (Niehren et al., 1997a), and anaphoric bindings constraints of the form ante(X)=X'. As proved in (Niehren and Koller, 1998), CLLS extends the expressiveness of context unification (Niehren et al., 1997a). It also extends its linguistic coverage (Niehren et al., 1997b) by integrating an analysis of VP ellipses with anaphora as in (Kehler, 1995). Thus, the coverage of CLLS is comparable to Crouch (1995) and Shieber et al. (1996). We illustrate CLLS at a benchmark case for the interaction of scope, anaphora, and ellipsis (8). (8) Mary read a book she liked before Sue did. The paper is organized as follows. First, we introduce CLLS in detail and define its syntax and semantics. We illustrate CLLS in sec. 3 by applying it to the example (8) and compare it to related work in the last section. 2 A Constraint Language for A-Structures (CLLS) CLLS is an ordinary first-order language interpreted over A-structures. A-structures are particular predicate logic tree structures we will introduce. We firs"
P98-1058,P97-1051,0,0.0164823,"to the link from X12 to inated by X2, which only makes the anaphoric X16, and hence leads to the node of the parallel content 212 of the pronoun she within the NP element Sue (sloppy reading, see (21)). explicit. The anaphoric relationship between the pronoun she and Mary is represented by the linking relation between X12 and X16. (X20 rep357 (20)~ x , these approaches can potentially benefit from the presented idea of rubber bands for binding and linking, without having to make any dramatic changes. Our definition of parallelism implements some I"""" ~""r, ary.,, X~6""~. ' ~/sue * _X ideas from Hobbs and Kehler (1997) on the behavior of anaphoric links. In contrast to their proposal, our definition of parallelism is not based on an abstract notion of similarity. Furthermore, CLLS is not integrated into a general theory of abduction. We pursue a more modest aim at this stage, as CLLS needs to be connected to ""material"" deduction calculi for reasoning with such underspecified semantic representation in order to make progress on this front. We hope that some of the more ad hoc 4 Related Work features of our definition of parallelism (e.g. axiom 5) may receive a justification or improveCLLS allows a uniform an"
P98-1058,J87-1005,0,0.0164314,"o consistent renaming of bound variables (aequality); a constraint of CLLS is an underspecified description of a A-structure. CLLS solves a capturing problem omnipresent in underspecified scope representations. CLLS features constraints for dominance, lambda binding, parallelism, and anaphoric links. Based on CLLS we present a simple, integrated, and underspecified treatment of scope, parallelism, and anaphora. 1 Introduction A central concern of semantic underspecification (van Deemter and Peters, 1996) is the underspecification of the scope of variable binding operators such as quantifiers (Hobbs and Shieber, 1987; Alshawi, 1990; Reyle, 1993). This immediately raises the conceptual problem of how to avoid variable-capturing when instantiating underspecified scope representations. In principle, capturing may occur in all formalisms for structural underspecification which represent binding relations by the coordination of variables (Reyle, 1995; Pinkal, 1996; Bos, 1996; Niehren et al., 1997a). Consider for instance the verb phrase in (1) Manfred [vF knows every student] An underspecified description of the compositional semantics of the VP in (1) might be given along the lines of (2): (2) X--Cl(Vx(studen"
P98-1058,E93-1025,0,0.0280859,"aphors interact with parallelism. E.g., (16) is the source clause of the manypronouns-puzzle, a problematic case of interaction of ellipsis and anaphora. (Xu, 1998), where our treatment of ellipsis and anaphora was developed, argues that link chains yield the best explanation for the distribution of strict/sloppy readings involving many pronouns. The basic idea is that an elided pronoun can either be linked to its parallel pronoun in the source clause (referential parallelism) or be linked in a structurally parallel way (structural parallelism). This analysis agrees with the proposal made in (Kehler, 1993; Kehler, 1995). It covers a series of problematic cases in the literature such as the many-pronouns-puzzle, cascaded ellipsis, or the five-reading sentence (17): (17) John revised his paper before the teacher did, and so did Bill The precise interaction of parallelism with binding and linking relations is spelled out in sec. 2.2. 2.2 Syntax and Semantics of CLLS We start with a set of labels E = {@2, lam I ' var 0 ' ana 0 ' before 2, maryO, r e a d O , , , .}, ranged over by ]ji, with arity i which may be omitted. The syntax of CLLS is given by: ::= X J ( X l , . . . , X , ) (]J""ES) I I X<*Y"
P98-1058,P83-1020,0,0.373464,"function on tree-nodes for expressing variable binding. An graphical illustration of the A-structure corresponding to the A-term Ax.like(x,x) is given (5). (5) ( ', Axlike(x,x) Formally, the binding relation of the A-structure in (5) is expressed through the partial function A(5) defined by A(5)(v2) = v0 and A(5)(v3) = v0. We propose a first-order constraint language for A-structures called CLLS which solves the capturing problem of underspecified scope representations in a simple and elegant way. CLLS subsumes dominance constraints (Backofen et al., 1995) as known from syntactic processing (Marcus et al., 1983) with tree-adjoining grammars (Vijay-Shanker, 1992; Rogers and VijayShanker, 1994). Most importantly, CLLS constraints can describe the binding relation of a Astructure in an underspecified manner (in contrast to A-structures like (5), which are always fully specified). The idea is that A-binding behaves like a kind of rubber band that can be arbitraryly enlarged but never broken. E.g., (6) is an underspecified CLLS-description of the Astructure (5). Xo,~*X~ AA(X~)=X4A . ~ . ? Xo Xl:lam(X2)A //lain I X1 (6) X2,~*X3A ' * X2 Z3:,ke(X ,Xs)^ X4:var A X5:var ,I var,,~.~X4 vat ~ X5 The constraint (6"
P98-1058,P97-1053,1,0.881837,"e, parallelism, and anaphora. 1 Introduction A central concern of semantic underspecification (van Deemter and Peters, 1996) is the underspecification of the scope of variable binding operators such as quantifiers (Hobbs and Shieber, 1987; Alshawi, 1990; Reyle, 1993). This immediately raises the conceptual problem of how to avoid variable-capturing when instantiating underspecified scope representations. In principle, capturing may occur in all formalisms for structural underspecification which represent binding relations by the coordination of variables (Reyle, 1995; Pinkal, 1996; Bos, 1996; Niehren et al., 1997a). Consider for instance the verb phrase in (1) Manfred [vF knows every student] An underspecified description of the compositional semantics of the VP in (1) might be given along the lines of (2): (2) X--Cl(Vx(student(x)-+C2(know(Z, x)))) The meta-variable X in (2) denotes some tree representing a predicate logic formula which is 353 underspecified for quantifier scope by means of two place holders C1 and C2 where a subjectquantifier can be filled in, and a place holder Z for the subject-variable. The binding of the object-variable x by the object-quantifier Vx is coordinated through the nam"
P98-1058,J90-3001,0,\N,Missing
P98-1058,J92-4004,0,\N,Missing
piskorski-etal-2002-flexible,J94-3001,0,\N,Missing
piskorski-etal-2002-flexible,C94-2144,0,\N,Missing
piskorski-etal-2002-flexible,W98-1305,0,\N,Missing
R11-1003,P05-1061,0,0.0279728,"adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which match with the seeds. RE applies acquired rules"
R11-1003,P03-1029,0,0.0794554,"nce in another domain. 1 Introduction Domain adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which"
R11-1003,H05-1091,0,0.0432423,"in. 1 Introduction Domain adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which match with the seeds. RE"
R11-1003,P00-1002,0,0.0464215,"different effects of negative examples depending on the domain data properties and (iv) the potential of reusing rules from one domain for improving the relation extraction performance in another domain. 1 Introduction Domain adaptation is very important for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small s"
R11-1003,de-marneffe-etal-2006-generating,0,0.0127206,"Missing"
R11-1003,P07-1074,1,0.951825,"2010a; Kozareva and Hovy, 2010b)). The advantage of the minimally supervised approaches for IE rule learning is that only initial seed knowledge is needed. Therefore the adaptation might be limited to substituting the seed examples. However, different domains/corpora exhibit rather different properties of their learning/extraction data with respect to the learning algorithm. Depending on the domain, the need for improving precision by utilizing negative examples may differ. An important research goal is the exploitation of more benign domains for improving extraction in less suitable domains. Xu et al. (2007) and Xu (2007) present a minimally supervised learning system for relation extraction, initialized by a so-called semantic seed, i.e., examples of the target relations. We dub our system DARE for Domain Adaptive Relation Extraction. The system supports the domain adaptation with a compositional rule representation and a bottom-up rule discovery strategy. In this way, DARE can handle target relations of various complexities and arities. Relying on a few examples of a target relation as semantic seed dispenses with the costly acquisition of domain knowledge through experts or specialized resourc"
R11-1003,W06-0204,0,0.729222,"rtant for information extraction (IE) systems. IE systems in the real world are often required to work for new domains and new tasks within a limited adaptation or tuning time. Thus, automatic learning of relation extraction rules for a new domain or a new task has been established as a relevant subarea in IE research and development (Muslea, 1999; Tsujii, 2000; Uszkoreit, 2011), in particular for minimally supervised or semi-supervised bootstrapping approaches (e.g., (Brin, 1998; Agichtein and Gravano, 2000; Yangarber, 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; McDonald et al., 2005; Greenwood and Stevenson, 2006; Jones, 2005; Xu et al., 2007; Xu, 2007; Kozareva and 17 Proceedings of Recent Advances in Natural Language Processing, pages 17–24, Hissar, Bulgaria, 12-14 September 2011. extraction (RE). Rule learning and RE feed each other in a bootstrapping framework. The bootstrapping starts from so-called “semantic seeds”, which is a small set of instances of the target relation. The rules are extracted from sentences automatically annotated with semantic entity types and parsing results (e.g., dependency structures), which match with the seeds. RE applies acquired rules to texts in order to discover m"
R11-1003,C96-1079,0,0.0407676,"ARE can handle target relations of various complexities and arities. Relying on a few examples of a target relation as semantic seed dispenses with the costly acquisition of domain knowledge through experts or specialized resources. In practice, this does not work equally well for any given domain. Xu (2007) and Uszkoreit et al. (2009) concede that DARE’s performance strongly depends on the specific type of relation and domain. In our experiments, we apply DARE to the extraction of two different 4-ary relations from different domains (Nobel Prize awards and MUC-6 management succession events (Grishman and Sundheim, 1996)). In the data set of the first domain, the connectivity between relation instances and linguistic patterns (rules) approximates the small world property (Amaral et al., 2005). In MUC-6 data on the other hand, the redundancy of both mentions of instances and patterns as well as their connectivity are very low. This paper investigates the application of an existing seed-based minimally supervised learning algorithm to different social domains exhibiting different properties of the available data. A systematic analysis studies the respective data properties of the three domains including the dis"
R11-1003,P03-1044,0,0.0769525,"Missing"
R11-1003,P10-1150,0,0.0496944,"Missing"
R11-1003,N10-1087,0,0.0473886,"Missing"
R11-1003,W06-0200,0,\N,Missing
R11-1052,W06-0204,0,0.0333742,"f the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance profile. 1 Introduction Seed-based minimally supervised machine learning within a bootstrapping framework has been widely applied to various information extraction tasks (e.g., (Hearst, 1992; Riloff, 1996; Brin, 1998; Agichtein and Gravano, 2000; Sudo et al., 2003; Greenwood and Stevenson, 2006; Blohm and Cimiano, 2007)). The power of this approach is that it needs only a small set of examples of either patterns or relation instances and can learn 1 http://dare.dfki.de/ 378 Proceedings of Recent Advances in Natural Language Processing, pages 378–384, Hissar, Bulgaria, 12-14 September 2011. • Offline linguistic annotation: This component automatically annotates the corpus texts with named entity information and dependency tree structures using standard NLP tools. All annotations are stored in XML format. formance. As a web service, it offers a very user-friendly visualization of the"
R11-1052,C92-2082,0,0.0557225,"les and their applications. META-DARE is also an analysis tool which gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance profile. 1 Introduction Seed-based minimally supervised machine learning within a bootstrapping framework has been widely applied to various information extraction tasks (e.g., (Hearst, 1992; Riloff, 1996; Brin, 1998; Agichtein and Gravano, 2000; Sudo et al., 2003; Greenwood and Stevenson, 2006; Blohm and Cimiano, 2007)). The power of this approach is that it needs only a small set of examples of either patterns or relation instances and can learn 1 http://dare.dfki.de/ 378 Proceedings of Recent Advances in Natural Language Processing, pages 378–384, Hissar, Bulgaria, 12-14 September 2011. • Offline linguistic annotation: This component automatically annotates the corpus texts with named entity information and dependency tree structures using standard NLP tools. All annotations a"
R11-1052,R11-1003,1,0.886343,"hich learns relation extraction rules for dealing with relations of various complexity by utilizing some relation examples as semantic seed in the initialization and has achieved very promising results for the extraction of complex relations. In the recent years, more and more researchers are interested in understanding the underlying process behind this approach and attempt to identify relevant learning parameters to improve the system performance. Xu (2007) investigates the role of the seed selection in connection with the data properties in a careful way with our DARE system. Xu (2007) and Li et al. (2011) describe the applications of DARE system in different domains for different relation extraction tyes, for example, the NobelPrize-Winning event, management succession relations defined in MUC-6, marriage relationship, etc. Uszkoreit et al. (2009) describe a further empirical analysis of the seed construction and its influence on the learning performance and show that size, arity and distinctiveness of the seed examples play various important roles for the learning performance. Thus, the system demonstrated here, called META-DARE, serves as a monitoring and analysis system for conducting vario"
R11-1052,P03-1029,0,0.0125486,"gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance profile. 1 Introduction Seed-based minimally supervised machine learning within a bootstrapping framework has been widely applied to various information extraction tasks (e.g., (Hearst, 1992; Riloff, 1996; Brin, 1998; Agichtein and Gravano, 2000; Sudo et al., 2003; Greenwood and Stevenson, 2006; Blohm and Cimiano, 2007)). The power of this approach is that it needs only a small set of examples of either patterns or relation instances and can learn 1 http://dare.dfki.de/ 378 Proceedings of Recent Advances in Natural Language Processing, pages 378–384, Hissar, Bulgaria, 12-14 September 2011. • Offline linguistic annotation: This component automatically annotates the corpus texts with named entity information and dependency tree structures using standard NLP tools. All annotations are stored in XML format. formance. As a web service, it offers a very user"
R11-1052,P07-1074,1,0.912011,"pecificity to start experiments on the example domains. Moreover, it provides a detailed survey of all learning iterations including the learned rules and extracted instances and their respective properties. Finally, it delivers a qualitative analysis of the learning perThis paper demonstrates a web-based online system, called META-DARE1 . META-DARE is built to assist researchers to obtain insights into seed-based minimally supervised machine learning for relation extraction. META-DARE allows researchers and students to conduct experiments with an existing machine learning system called DARE (Xu et al., 2007). Users can run their own learning experiments by constructing initial seed examples and can monitor the learning process in a very detailed way, namely, via interacting with each node in the learning graph and viewing its content. Furthermore, users can study the learned relation extraction rules and their applications. META-DARE is also an analysis tool which gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a"
R11-1052,C10-2155,1,0.746712,". The following example shows two sentences from which relation 3 is extracted. (4) Visualization of Evaluation Results 1. Canadian economist Robert Mundell won the Nobel in economics for introducing foreign trade, capital movements, and currency swings into 3 http://dare.dfki.de/graph.jsp?f_id= example 382 id 1 2 3 4 instance number 1 1 2 3 2 12 2 1 3 1 prize area chemistry chemistry peace medicine chemistry peace literature physics economics year 1999 1998 1998 As illustrated in Figure 7 and 8, META-DARE also highlights the dangerous or bad rules and wrong relation instance. As described in Xu et al. (2010), the acquired rules are divided into four groups according to the extraction results: 1998 • useless, if the rule does not extract any instances. • good, if the rule extracts only correct instances. • dangerous, if the rule extract both correct and wrong instances. Table 2: Different seed constructions id 1 2 3 4 bootstrapping steps 7 10 6 5 extracted instances sum 4-arity 372 156 374 156 373 159 374 163 learned rules 1151 1146 1147 1117 • bad, if the rule extract only bad instances. In the learning graph, the rules from different group are colored in the following way: • useless rules: not f"
R11-1095,R11-1003,1,0.827965,"eyer” are the authors of publications about the same technology, they might be identified as name variants of the same person by our method. 3.2 DARE7 (Domain Adaptive Relation Extraction) is a minimally supervised machine learning framework for extracting relations of various complexity. It consists two major parts: 1) rule learning, 2) relation extraction. Rule learning and relation extraction feed each other in a bootstrapping framework. The bootstrapping starts from socalled “semantic seed” as a search query, which is a small set of instances of the target relation. (Uszkoreit, 2011) and (Li et al., 2011) describe the application and evaluation of DARE on different corpora for different relation extraction tasks. Currently DARE provides linguistic components which process English and German free texts. In T ECH WATCH T OOL, DARE is used to learn linguistic patterns to recognize sentences that potentially contain the trend information and also relations between persons and organizations. To learn patterns from trend sentences, we used the corpus offered by the project partner ThyssenKrupp AG, which is annotated with trend sentences and terms by the experts. From the annotation, we acquire examp"
R11-1095,P07-1074,1,0.774237,"we used the corpus offered by the project partner ThyssenKrupp AG, which is annotated with trend sentences and terms by the experts. From the annotation, we acquire examples as seed for DARE to learn patterns, e.g., NLP Tools In T ECH WATCH T OOL, named entity (NE) recognition and information extraction (IE) tools are applied to extract named entities (persons, organizations, etc.) and to detect relations or mentions of trends. Two tools are integrated in our system: 1. SProUT as named entity (Drozdzynski et al., 2004) and recognizer 2. DARE as relation extractor and trend sentence detector (Xu et al., 2007; Xu, 2007). 3.1 SProUT • (“lithium-ion battery”, “car”, “future”) SProUT6 (Shallow Processing with Unification and Typed Feature Structures) is a platform for development of multilingual shallow text processing and information extraction systems. It is a generic rule-based recognizer to extract named entities or concept terms. Users can write corresponding recognition patterns and specify linguistic resources, such as lexicons, gazetteers and tokenizers. The platform provides linguistic processing resources for several languages including English, German, etc. SProUT uses typed feature struct"
S15-2056,E06-1002,0,0.0362967,"Missing"
S15-2056,D11-1072,0,0.101095,"Missing"
S15-2056,H93-1061,0,0.0294046,"Missing"
S15-2056,Q14-1019,0,0.0474194,"Missing"
S15-2056,P10-1040,0,0.0324483,"Missing"
S15-2056,P10-4014,0,0.0880334,"Missing"
S15-2056,S15-2049,0,\N,Missing
S17-1026,W04-3205,0,0.10063,"cktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), and two lexical aligners based on Wordnet (Fellbaum, 1998)4 and VerbOcean (Chklovski and Pantel, 2004). As output, it produces a binary decision (entailment, non-entailment) along with a computed confidence score. As the RTE engine was originally designed for sentences, rather than patterns, we converted each pattern into its textual representation. The variables expressing type and semantic role of the entities linked by the pattern were excluded in this representation, as the resulting variable alignments would skew the RTE engine’s entailment decision. For our experiments, we used the original MultiAlign implementation as well as an adapted version, in which we made some changes to the Word"
S17-1026,P15-1034,0,0.0497792,"semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressing that two entities l"
S17-1026,W14-3348,0,0.0515783,"makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), and two lexical aligners based on Wordnet (Fellbaum, 1998)4 and VerbOcean (Chklovski and Pantel, 2004). As output, it produces a binary decision (entailment, non-entailment) along with a computed confidence score. As the RTE engine was originally designed for sentences, rather than patterns, we converted each pattern into its textual representation. The variables expressing type and semantic role of the entities linked by the pattern were excluded in this representation, as the resulting variable alignments would skew the RTE engine’s entailment decision. For our experiments, we used the ori"
S17-1026,L16-1537,1,0.852149,"xperiments, we split the evaluation set into a development set for optimizing the graph building parameters and a test set for the final evaluation. In our experiments, we tested several strategies for selecting patterns and measured performance over the annotated relation mentions in the evaluation dataset. For evaluating the graphbased methods, we selected all patterns entailing the base patterns [PERSON1 &lt;marry&gt; PERSON2 ] (for marriage) and [ORGANIZATION1 &lt;acquire&gt; For evaluating our method on the relation extraction task, we conducted experiments on two freely available datasets: TEG-REP (Eichler et al., 2016) and FB15k-237 (Toutanova et al., 2015). On the TEG-REG dataset, we carried out a detailed evaluation of several pattern filtering strategies with respect to two semantic relations. On the FB15k-237 corpus, we evaluate the scalability of our method to other semantic relations. 5.1 TEG-REP The TEG-REP corpus contains automatically derived relation extraction patterns as well as goldstandard entailment graphs created from these patterns for three relations typically considered in RE tasks: marriage, acquisition, and award honor. The patterns underlying this corpus are a subset of the patterns us"
S17-1026,P12-1013,0,0.116832,"Sample set of RTE decisions (YES: entailment, NO: no entailment) with associated confidence Figure 3: Sample outputs using greedy (left) and global (right) graph optimizer to a set of decisions that is invalid given the transitivity of the entailment relation. For deriving a consistent graph, we applied two different strategies: First, a simple greedy strategy that assumes each computed positive entailment relation with a confidence exceeding a pre-defined threshold to be valid, and adds missing entailment edges to ensure transitive closure. Second, the global graph optimization algorithm by Berant et al. (2012), which searches for the best graph under a global transitivity constraint, approaching the optimization problem by Integer Linear Programming. The selection of the optimization strategy is crucial, as illustrated in Figure 3, which shows two sample outputs from each of the two strategies for the decisions in Figure 2. 4 2. Generate an entailment graph EG expressing entailment relations among the patterns in P . 3. Choose a base pattern5 , expressing the target relation explicitly and select all patterns entailing the base pattern according to EG. 4. Apply the selected patterns to extract rela"
S17-1026,P10-1124,0,0.194142,"proach taken in the RTE challenges (Dagan et al., 2005). 220 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 220–229, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics 2 Related Work for individual T/H pairs. We propose to exploit entailment relationships holding among RE patterns by structuring the candidate set in an entailment graph. Entailment graphs are hierarchical structures representing entailment relations among textual expressions and have previously been generated for various types of expressions (Berant et al., 2010, 2012; Mehdad et al., 2013; Levy et al., 2014; Kotlerman et al., 2015). Entailment graphs can be constructed by determining entailment relationships between pairs of expressions or, as proposed by Kolesnyk et al. (2016), by generating entailed sentences from source sentences. Our work of building entailment graphs based on RE patterns is related to the work by Nakashole et al. (2012), who create a taxonomy of binary relation patterns. For their syntactic patterns, they compute partial orders of generalization and subsumption based on the set of mentions extracted by each pattern. In contrast"
S17-1026,D15-1075,0,0.0473261,"t is transitive, all edges are omitted that can be recovered in the transitive closure. 221 Figure 1: Subgraph showing entailment relations for the pattern [PERSON1 &lt;marry&gt; PERSON2 ] of an RTE engine based on multi-level alignments. This RTE engine, referred to as MultiAlign, is available through the RTE platform EXCITEMENT (Magnini et al., 2014) and achieved state-of-the-art performance on several RTE corpora (Noh et al., 2015). We opted for this RTE system because it makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), and two lexical aligners based on Wordnet (Fellbaum, 1998)4 and VerbOcean (Chklovski and Pantel, 2004). As output, it p"
S17-1026,N06-1039,0,0.0590712,"This is motivated by the fact that entailment is semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it fa"
S17-1026,P14-5008,1,0.818781,", 2010). 3.2 RTE Engine For recognizing entailment relations between individual T/H pairs of patterns, we make use 3 For reasons of simplicity, the figure shows the text representation of the patterns, which are in fact represented as dependency structures. Since entailment is transitive, all edges are omitted that can be recovered in the transitive closure. 221 Figure 1: Subgraph showing entailment relations for the pattern [PERSON1 &lt;marry&gt; PERSON2 ] of an RTE engine based on multi-level alignments. This RTE engine, referred to as MultiAlign, is available through the RTE platform EXCITEMENT (Magnini et al., 2014) and achieved state-of-the-art performance on several RTE corpora (Noh et al., 2015). We opted for this RTE system because it makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning"
S17-1026,W11-0201,0,0.0273779,"ct that entailment is semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressi"
S17-1026,N13-1018,0,0.0152046,"llenges (Dagan et al., 2005). 220 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 220–229, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics 2 Related Work for individual T/H pairs. We propose to exploit entailment relationships holding among RE patterns by structuring the candidate set in an entailment graph. Entailment graphs are hierarchical structures representing entailment relations among textual expressions and have previously been generated for various types of expressions (Berant et al., 2010, 2012; Mehdad et al., 2013; Levy et al., 2014; Kotlerman et al., 2015). Entailment graphs can be constructed by determining entailment relationships between pairs of expressions or, as proposed by Kolesnyk et al. (2016), by generating entailed sentences from source sentences. Our work of building entailment graphs based on RE patterns is related to the work by Nakashole et al. (2012), who create a taxonomy of binary relation patterns. For their syntactic patterns, they compute partial orders of generalization and subsumption based on the set of mentions extracted by each pattern. In contrast to their work, we construct"
S17-1026,W15-4007,0,0.0134313,"ate if the models trained on T/H pairs for one relation are general enough to be used for computing entailment relations among pattern candidates for other semantic relations. To this end, we used the FB15k237 corpus (Toutanova et al., 2015), which contains knowledge-base relation triples and textual mentions of Freebase entity pairs. For our experiments on this corpus, we generated candidate patterns by extracting the first 1000 tuples matching a particular relation from the pattern files in the corpus, and then extracting all patterns linking any of the tuples in the textual triples used by Toutanova and Chen (2015). This way, our candidate pattern set contains both patterns expressing the target relation as well as patterns expressing other relations. For creating the entailment graph, we converted all patterns into a textual representation, removed patterns with no lexical item, and, from the remaining patterns, built an entailment graph applying the RTE engine described in 3.2 with the model trained on the marriage relation and the best parameter setting derived based on the TEG-REP corpus. For evaluating the result, we selected 10 relations, defined a base pattern for each of them, and checked, for e"
S17-1026,D15-1174,0,0.133243,"et into a development set for optimizing the graph building parameters and a test set for the final evaluation. In our experiments, we tested several strategies for selecting patterns and measured performance over the annotated relation mentions in the evaluation dataset. For evaluating the graphbased methods, we selected all patterns entailing the base patterns [PERSON1 &lt;marry&gt; PERSON2 ] (for marriage) and [ORGANIZATION1 &lt;acquire&gt; For evaluating our method on the relation extraction task, we conducted experiments on two freely available datasets: TEG-REP (Eichler et al., 2016) and FB15k-237 (Toutanova et al., 2015). On the TEG-REG dataset, we carried out a detailed evaluation of several pattern filtering strategies with respect to two semantic relations. On the FB15k-237 corpus, we evaluate the scalability of our method to other semantic relations. 5.1 TEG-REP The TEG-REP corpus contains automatically derived relation extraction patterns as well as goldstandard entailment graphs created from these patterns for three relations typically considered in RE tasks: marriage, acquisition, and award honor. The patterns underlying this corpus are a subset of the patterns used by Moro et al. (2013) and were acqui"
S17-1026,D12-1104,0,0.0324044,"the candidate set in an entailment graph. Entailment graphs are hierarchical structures representing entailment relations among textual expressions and have previously been generated for various types of expressions (Berant et al., 2010, 2012; Mehdad et al., 2013; Levy et al., 2014; Kotlerman et al., 2015). Entailment graphs can be constructed by determining entailment relationships between pairs of expressions or, as proposed by Kolesnyk et al. (2016), by generating entailed sentences from source sentences. Our work of building entailment graphs based on RE patterns is related to the work by Nakashole et al. (2012), who create a taxonomy of binary relation patterns. For their syntactic patterns, they compute partial orders of generalization and subsumption based on the set of mentions extracted by each pattern. In contrast to their work, we construct pattern-based entailment graphs using RTE technology. This is motivated by the fact that entailment is semantic and not mention-based, i.e., one pattern can entail another pattern even if they extract disjoint sets of mentions in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in vario"
S17-1026,S15-1022,1,0.856577,"s of patterns, we make use 3 For reasons of simplicity, the figure shows the text representation of the patterns, which are in fact represented as dependency structures. Since entailment is transitive, all edges are omitted that can be recovered in the transitive closure. 221 Figure 1: Subgraph showing entailment relations for the pattern [PERSON1 &lt;marry&gt; PERSON2 ] of an RTE engine based on multi-level alignments. This RTE engine, referred to as MultiAlign, is available through the RTE platform EXCITEMENT (Magnini et al., 2014) and achieved state-of-the-art performance on several RTE corpora (Noh et al., 2015). We opted for this RTE system because it makes use of external knowledge resources and, unlike more recent systems based on neural networks (Rocktäschel et al., 2015; Bowman et al., 2015), is able to cope with the restricted amount of training data available for the task. MultiAlign uses shallow parsing for linguistic preprocessing and logistic regression for entailment classification. Features for the classifier are generated on the basis of multilevel alignments using four aligners: a lemma aligner (aligning identical lemmas found in T and H), an aligner based on the paraphrase tables provi"
S17-1026,D11-1135,0,0.0349594,"in a given text corpus. The task of estimating the quality of automatically learned extraction patterns has been dealt with in various ways, for example based on integrity constraints (Agichtein, 2006), frequency heuristics (Krause et al., 2012) or lexical semantic criteria (Moro et al., 2013). Another line of research in RE groups similar patterns, e.g., by merging patterns based on syntactic criteria (Banko et al., 2007; Shinyama and Sekine, 2006; Thomas et al., 2011; Angeli et al., 2015), by clustering patterns that are semantically related (Kok and Domingos, 2008; Yates and Etzioni, 2009; Yao et al., 2011), or by identifying patterns associated to a given seed relation (Bauer et al., 2014). Such approaches help gain generalization; however, their ability to express semantic relationships is limited, as they cannot capture the asymmetric nature of these relationships. For example, clustering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressing that two entities linked by patterns P1 to P3 are mentions of the marriage relation, whereas this is not necessarily true of entities linked"
S17-1026,N13-1008,0,0.0620861,"tering can help us identify pattern P4 below as being semantically related to patterns P1 to P3 in section 1. P4: PERSON1 &lt;love&gt; PERSON2 However, it falls short of expressing that two entities linked by patterns P1 to P3 are mentions of the marriage relation, whereas this is not necessarily true of entities linked by pattern P4. Similarly, clustering can identify patterns P1 and P3 as semantically related. However, it cannot express that the relation expressed by pattern P3 entails the relation expressed by pattern P1, but not vice versa. These asymmetric relationships have been considered by Riedel et al. (2013), who learns latent feature vectors for patterns based on matrix factorization, and have also been studied extensively in the context of recognizing textual entailment (RTE). RTE is the task of determining, for two textual expressions T (text) and H (hypothesis), whether the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). In RE, RTE systems have been applied to validate a given relation instance (Wang and Neumann, 2008) and to extract instances entailing a given target relation (Romano et al., 2006; Bar-Haim et al., 2007; Roth et al., 2009). As illustrated above,"
S17-1026,E06-1052,0,0.0373823,"ce versa. These asymmetric relationships have been considered by Riedel et al. (2013), who learns latent feature vectors for patterns based on matrix factorization, and have also been studied extensively in the context of recognizing textual entailment (RTE). RTE is the task of determining, for two textual expressions T (text) and H (hypothesis), whether the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). In RE, RTE systems have been applied to validate a given relation instance (Wang and Neumann, 2008) and to extract instances entailing a given target relation (Romano et al., 2006; Bar-Haim et al., 2007; Roth et al., 2009). As illustrated above, RE can clearly benefit from considering semantic relationships holding among extraction patterns. However, previous work in RE has either focussed on grouping related patterns without considering non-symmetric relations, or, on computing entailment decisions 3 3.1 Entailment Graph Generation Pattern-Based Entailment Graphs A pattern-based entailment graph refers to a directed graph, in which each node represents a unique RE pattern, and each edge (→) denotes an entailment relationship. Bidirectional edges (↔) denote that the pa"
S17-1026,P09-2015,0,0.0353095,"e been considered by Riedel et al. (2013), who learns latent feature vectors for patterns based on matrix factorization, and have also been studied extensively in the context of recognizing textual entailment (RTE). RTE is the task of determining, for two textual expressions T (text) and H (hypothesis), whether the meaning of H can be inferred from the meaning of T (Dagan and Glickman, 2004). In RE, RTE systems have been applied to validate a given relation instance (Wang and Neumann, 2008) and to extract instances entailing a given target relation (Romano et al., 2006; Bar-Haim et al., 2007; Roth et al., 2009). As illustrated above, RE can clearly benefit from considering semantic relationships holding among extraction patterns. However, previous work in RE has either focussed on grouping related patterns without considering non-symmetric relations, or, on computing entailment decisions 3 3.1 Entailment Graph Generation Pattern-Based Entailment Graphs A pattern-based entailment graph refers to a directed graph, in which each node represents a unique RE pattern, and each edge (→) denotes an entailment relationship. Bidirectional edges (↔) denote that the patterns represented by the two nodes are con"
S17-1026,W14-1610,0,\N,Missing
thomas-etal-2017-streaming,C10-1032,0,\N,Missing
thomas-etal-2017-streaming,C10-1011,0,\N,Missing
thomas-etal-2017-streaming,P09-1113,0,\N,Missing
thomas-etal-2017-streaming,P07-1074,1,\N,Missing
thomas-etal-2017-streaming,P14-5010,0,\N,Missing
thomas-etal-2017-streaming,doddington-etal-2004-automatic,0,\N,Missing
thomas-etal-2017-streaming,P14-5007,0,\N,Missing
thomas-etal-2017-streaming,D13-1100,0,\N,Missing
W11-2915,P05-1022,0,0.149875,"Missing"
W11-2915,W08-1301,0,0.0169911,"do et al., 2003; Greenwood and Stevenson, 2006)). The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances (Xu et al., 2007). Since the minimally supervised learning starts its bootstrapping from a few semantic examples, no treebanking or any other annotation is required for new domains. In addition to this inherently domain-adaptable rule-learning component, the framework also employs two language analysis modules: a namedentity (NE) recognizer (Drozdzynski et al., 2004) and a parser (Lin, 1998; de Marneffe and Manning, 2008). NE recognizers are adapted to new domains–if needed–by adding rules for new NE types and extending the gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The out"
W11-2915,D10-1068,0,0.0167683,"rd making deep linguistic grammars useful for relation extraction, whereas up to now most minimally supervised approaches to RE have employed shallower robust parsers. The hope behind these attempts is to improve precision without losing too much recall. After reclaiming recall through our parse reranking, next steps in this line of research will be dedicated to balancing off the deficits in coverage by data-driven lexicon extension in the spirit of (Zhang et al., 2010) and by exploiting the chart for partial parses involving the relevant types of named entities. Furthermore, the approach of (Dridan and Baldwin, 2010) to learning a parse selection model in an unsupervised way by utilizing the constraints of HSPG grammars might also be interesting for domain adaptive parse selection for relation extraction. At some point we may then be in a position to conduct a fair empirical comparison between deep-linguistic parsing with hand-crafted grammars on the one hand and purely statistical parsing on the other. An error analysis may then indicate the chances for hybrid approaches. However, before targeting these medium-term goals we plan to investigate whether our approach can also be applied to other parsers wit"
W11-2915,P10-1074,0,0.0388319,"Missing"
W11-2915,P06-2034,0,0.0499667,"Missing"
W11-2915,W06-0204,0,0.0712755,"Missing"
W11-2915,C02-2025,0,0.0824744,"uistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component (Toutanova et al., 2005b), which had been trained on a generic HPSG treebank (Oepen et al., 2002). The parse ranking had attracted our The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of R"
W11-2915,C96-1079,0,0.059077,"erformance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambig"
W11-2915,W10-2105,0,0.0515468,"Missing"
W11-2915,I05-1018,0,0.0300548,"election of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by"
W11-2915,W09-2205,0,0.0249408,"e relation extraction in contrast to text understanding does not need the entire and correct syntactic structure for the detection of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between th"
W11-2915,W07-2202,0,0.0200102,"ct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between the performance on a gold-stand treebank and the usefulness in real-world applications. All four domain-adapted parsers achieve similar IE perfor125 References than parsing accuracy, we consider worth sharing. The presented results may also be viewed as a step forward"
W11-2915,D08-1050,0,0.0271645,"of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between the performance on a gold-stand treebank and the usefulness in real-world applications. All four domain-adapted parsers achieve similar IE perfo"
W11-2915,P06-1043,0,0.025829,"not necessarily correspond to a better parse ranking for other purposes or for generic parsing. This should not be surprising since relation extraction in contrast to text understanding does not need the entire and correct syntactic structure for the detection of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treeba"
W11-2915,P03-1029,0,0.0867986,"Missing"
W11-2915,N10-1004,0,0.026464,"that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by the following (Toutano"
W11-2915,P05-1073,0,0.167111,"e gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component (Toutanova et al., 2005b), which had been trained on a generic HPSG treebank (Oepen et al., 2002). The parse ranking had attracted our The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less"
W11-2915,W10-1905,0,0.143777,"r the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by the following (Toutanova et al., 2005b), P"
W11-2915,P07-1074,1,0.945583,"it, Sebastian Krause DFKI, LT-Lab, Germany {feiyu,lihong,Yi.Zhang,uszkoreit,sebastian.krause}@dfki.de Abstract exploit domain knowledge with minimal human effort. Many IE systems benefit from combining generic NLP components with task-specific extraction methods. Various machine learning approaches have been employed for adapting the IE methods to new domains and extraction tasks (e.g., (Yangarber, 2001; Sudo et al., 2003; Greenwood and Stevenson, 2006)). The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances (Xu et al., 2007). Since the minimally supervised learning starts its bootstrapping from a few semantic examples, no treebanking or any other annotation is required for new domains. In addition to this inherently domain-adaptable rule-learning component, the framework also employs two language analysis modules: a namedentity (NE) recognizer (Drozdzynski et al., 2004) and a parser (Lin, 1998; de Marneffe and Manning, 2008). NE recognizers are adapted to new domains–if needed–by adding rules for new NE types and extending the gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handc"
W11-2915,C10-2155,1,0.922211,"Therefore, the confidence values can be utilized as feedback to the parser to help it to rerank its readings. Figure 1: DARE core architecture Relying entirely on semantic seeds as domain knowledge, DARE can accommodate new relation types and domains with minimal effort. Since we had already reported on experiments applying the framework to different relation types and corpora including MUC-6 data in the cited papers, including comparisons with other ML approaches to RE (Xu, 2007; Uszkoreit et al., 2009), we omit a comparative discussion here. For confidence estimation, the method proposed by Xu et al. (2010) is adopted.1 Actually, in (2) we propose an extended version of the rule scoring, since the rule scoring in (Xu et al., 2010) did not consider the case when a learned rule does not extract any new instances. Thus, given the scoring of instances, the confidence value of a rule is the average score of all instances (Iextracted ) extracted by this rule or the average score of seed instances (Irule ) from which they are learned. Through the factor δ we reduce the score of rules that have not proven yet their potential for extracting instances. 4.2 Reranking Architecture and Method Figure 2 depict"
W11-2915,P09-1043,1,0.850491,"and t is the HPSG reading; T (w) is the set of all possible readings for a given sentence w licensed by the grammar; hf1 , . . . , fn i and hλ1 , . . . , λn i are feature functions and their corresponding weights. In practice, the effective features are defined on the HPSG derivation trees (without details from the feature structures), and the best readings are decoded efficiently from a packed parse forest with dynamic programming (Zhang et al., 2007). Although there are indications that parsers with hand-written grammars usually suffer less from the shift of domain than statistical parsers (Zhang and Wang, 2009; Plank and van Noord, 2010), the effect can still be observed, say in the preference of lexical selection. The issue is not that the correct analysis would be ruled out by the constraints in the treebank-induced grammar, but rather that it is not favored by the statistical ranking model, since the statistical distribution of the syntactic structures in the training corpus is different from the target application domain. This issue is recently acknowledged in most parsing systems and known as the domain adaptation task. 3 DARE and Confidence Estimation DARE (Xu et al., 2007; Xu, 2007) is a min"
W11-2915,W07-2207,1,0.836424,"m. Section 6 discusses related work. Finally, Section 7 summarizes the results and suggests directions for further research. 2 where w is the given input sentence and t is the HPSG reading; T (w) is the set of all possible readings for a given sentence w licensed by the grammar; hf1 , . . . , fn i and hλ1 , . . . , λn i are feature functions and their corresponding weights. In practice, the effective features are defined on the HPSG derivation trees (without details from the feature structures), and the best readings are decoded efficiently from a packed parse forest with dynamic programming (Zhang et al., 2007). Although there are indications that parsers with hand-written grammars usually suffer less from the shift of domain than statistical parsers (Zhang and Wang, 2009; Plank and van Noord, 2010), the effect can still be observed, say in the preference of lexical selection. The issue is not that the correct analysis would be ruled out by the constraints in the treebank-induced grammar, but rather that it is not favored by the statistical ranking model, since the statistical distribution of the syntactic structures in the training corpus is different from the target application domain. This issue"
W11-2915,N10-1002,1,0.846736,"similar IE perfor125 References than parsing accuracy, we consider worth sharing. The presented results may also be viewed as a step forward toward making deep linguistic grammars useful for relation extraction, whereas up to now most minimally supervised approaches to RE have employed shallower robust parsers. The hope behind these attempts is to improve precision without losing too much recall. After reclaiming recall through our parse reranking, next steps in this line of research will be dedicated to balancing off the deficits in coverage by data-driven lexicon extension in the spirit of (Zhang et al., 2010) and by exploiting the chart for partial parses involving the relevant types of named entities. Furthermore, the approach of (Dridan and Baldwin, 2010) to learning a parse selection model in an unsupervised way by utilizing the constraints of HSPG grammars might also be interesting for domain adaptive parse selection for relation extraction. At some point we may then be in a position to conduct a fair empirical comparison between deep-linguistic parsing with hand-crafted grammars on the one hand and purely statistical parsing on the other. An error analysis may then indicate the chances for hy"
W11-2915,P02-1062,0,\N,Missing
W15-4204,P09-1113,0,0.153013,"Missing"
W15-4204,P98-1013,0,0.707625,"hese relations in natural language text. Lexicalsemantic resources focus on linkage at the level of individual lexical items. For example, BabelNet integrates entity information from Wikipedia with word senses from WordNet, UWN is a multilingual WordNet built from various resources, and UBY integrates several linguistic resources by linking them at the word-sense level. Linguistic knowledge resources that go beyond the level of lexical items are scarce and of limited coverage due to significant investment of human effort and expertise required for their construction. Among these are FrameNet (Baker et al., 1998), which provides fine-grained semantic relations of predicates and their arguments, and VerbNet (Schuler, 2005), which models verb-class specific syntactic and semantic preferences. What is missing, therefore, is a large-scale, preferably automatically constructed linguistic resource that links language expressions at the phrase or sentence level to the semantic relations of knowledge bases, as well as to existing terminological resources. Such a repository would be very useful for many information extraction tasks, e.g., for relation extraction and knowledge base population. We aim to fill th"
W15-4204,Q14-1019,0,0.0348974,"ing sar-graphs to the linguistic LOD cloud, this mapping allows us to augment the lexico-syntactic and semantic information specified in sar-graphs with lexical semantic knowledge from the linked resources. In particular, we introduce new vertices for synonyms, and add new edges based on the lexical semantic relations specified in BabelNet. In Figure 1, these additional graph elements are represented as dashed vertices and edges. To link sar-graph vertices to Babelnet, we disambiguate content words in our pattern extraction pipeline (see Section 5), using the graph-based approach described by Moro et al. (2014). The disambiguation is performed on a per-sentence basis, considering all content words in the sentence as potentially ambiguous mentions if they correspond to at least one candidate meaning in BabelNet. This includes multi-token sequences containing at least one noun. The candidate senses (synset identifiers) of all mentions in a sentence are linked to each other via their BabelNet relations to create a graph. The approach then iteratively prunes low-probability candidate senses from the graph to select the synset assignment that maximizes the semantic agreement within a given sentence. Once"
W15-4204,E12-1059,0,0.106744,"Missing"
W15-4204,C98-1013,0,\N,Missing
W15-4405,ai-etal-2014-sprinter,1,0.880256,"Missing"
W15-4405,D11-1142,0,0.0572628,"Missing"
W15-4405,D12-1104,0,0.0139533,"Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 26–33, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2.2 Supervision by teacher Named-entity recognition Identified entity mentions Relation extraction Texts Relation instances Answers generation & multiple choices compilation A key part of our approach is the application of a pattern-based relation extraction (RE) system. Such systems, e.g., NELL (Carlson et al., 2010; Mitchell et al., 2015), PATTY (Nakashole et al., 2012), DARE (Xu et al., 2007), rely on lexicosyntactic patterns that pose restrictions on the surface level or grammatical level of sentences. Their underlying assumption is that whenever a given sentence matches a given pattern (i.e., a sentence template), the sentence expresses the pattern’s corresponding semantic relation. This assumption does not always hold, hence the system output usually contains a certain amount of noise, which makes a human-in-the-loop necessary for high-precision applications. Typically, RE systems associate patterns with a confidence score of some kind, allowing downstre"
W15-4405,P05-1045,0,0.00821473,"a teacher to compile actual reading-comprehension exercises suitable for presentation to learners, given only our approach’s resulting candidates. 2.1 Relation Extraction Reading-comprehension exercises The reading-comprehension exercises generated by our approach ask for relational facts mentioned in a text. In order to automatically identify these, we apply a series of processing steps, as depicted in Figure 1. In the first step, the input texts, e.g., news articles, are processed by a standard component for named-entity recognition (e.g., the well-known Stanford Named Entity Recognizer by Finkel et al. (2005)), in order to identify persons, organization, locations, etc. mentioned in the text, followed by application of a relation extraction system for the identification of facts. The information about mentioned facts and entities is passed on to a further processing step in which a mentioned instance of a semantic relation is transformed into a natural-language statement, paraphrasing the original occurrence of the fact. Furthermore, the information about named-entity occurrences is used to create false statements about relations between the entities. For each fact identified in a text, four choic"
W15-4405,P14-1084,0,0.0295347,"Missing"
W15-4405,W12-2039,0,0.0280881,"ticated reading-comprehension capabilities of language learners. An area receiving particular focus in the literature is the task of reading comprehension. Typically, language learners are asked to provide a short free-text summary for, e.g., a news article. A teacher then has to manually verify whether the learner was capable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, com"
W15-4405,W00-0603,0,0.0168134,"pable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, compared to short-answer summaries, the benefit that once created such tests require relatively few work on the teacher’s side in order to assess a learner’s skill level. At the core of our approach is the application of existing informationextraction approaches, mainly from the sub-area of relation extraction, for the identif"
W15-4405,S13-1041,0,0.0129197,"language learners. An area receiving particular focus in the literature is the task of reading comprehension. Typically, language learners are asked to provide a short free-text summary for, e.g., a news article. A teacher then has to manually verify whether the learner was capable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, compared to short-answer summaries, the benefit"
W15-4405,W14-3505,0,0.0165894,"area receiving particular focus in the literature is the task of reading comprehension. Typically, language learners are asked to provide a short free-text summary for, e.g., a news article. A teacher then has to manually verify whether the learner was capable of understanding the text and correctly summarized the main content. Some CALL systems support the teacher in this task by automatically scoring the learner’s summary wrt. the original article text or compared to a teacherprovided gold-standard summary, see for example (Hahn and Meurers, 2012; Madnani et al., 2013; Horbach et al., 2013; Koleva et al., 2014). Equally relevant to our work is the approach of Gates (2008), who automatically generated WHquestions for reading-comprehension tests through a transformation of the parse tree of selected sentences from the article text, as well as Riloff and Thelen (2000), who developed a rule-based system for the automatic answering of questions in a reading-comprehension setting. Our focus is the automatic generation of multiple-choice reading-comprehension exercises. This exercise type is a standard tool for educational tests and has, compared to short-answer summaries, the benefit that once created suc"
W15-4405,krause-etal-2014-language,1,0.834374,"same meaning. For example, a sentence template “wife had a kid from husband” is formed from one of the patterns used in the marriage relation. The paraphrasing engine takes this template as input and provides templates with the same words in natural language, e.g., “from husband wife had a kid”. Both templates are treated as valid and randomly chosen to create answers. 2.4 Human supervision As already noted earlier, employing automatic information-extraction methods has the disadvantage of inevitable noise in the system output. with sans-serif font represent entity placeholders. 28 articles (Krause et al., 2014), and measured the productivity of our approach for automatic question and answer generation. For these first experiments, we used the available gold-standard entity annotation. For the relation-extraction part, we applied the RE patterns of Moro et al. (2013) to automatically extract the relations between the annotated entities in the text. These patterns are based on the dependency-grammar analysis of sentences and were extracted from a large web corpus, hence they should provide enough variation for both the detection of relation mentions in texts as well as the generation of statements abo"
W15-4405,C12-2127,0,0.0624695,"ally generated by filling arguments into sentence templates. These templates are created based on patterns that were used for relation extraction in the previous step, i.e., RE patterns are utilized for two purposes in our approach. Depending on the specific kind of RE pattern, this step involves a few straight-forward processing steps, e.g., for the case of surface-level RE patterns it involves restoring correct inflections of poten27 tially lemmatized lexical pattern elements; for the case of depenency-grammar based patterns it additionally includes a step of tree linearization, see, e.g., (Wang and Zhang, 2012). In the following, we present some example sentence templates, used for the generation of multiplechoice tests1 : • marriage relation: another in the source text. The best case would be that they appear in the same sentence from which a relationship is extracted. Consider the following example sentence from which the instance marriage(Madonna, Ritchie) is extracted: Example 2: If Penn was Madonna’s temperamental match and boyfriend Carlos Leon, father of Lourdes, her physical ideal, Ritchie --- who reportedly calls his new wife ‘Madge’ in private --- is a man who holds his own against his hig"
W15-4405,W13-1722,0,0.0425492,"Missing"
W15-4405,P07-1074,1,0.707314,"on Natural Language Processing Techniques for Educational Applications, pages 26–33, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2.2 Supervision by teacher Named-entity recognition Identified entity mentions Relation extraction Texts Relation instances Answers generation & multiple choices compilation A key part of our approach is the application of a pattern-based relation extraction (RE) system. Such systems, e.g., NELL (Carlson et al., 2010; Mitchell et al., 2015), PATTY (Nakashole et al., 2012), DARE (Xu et al., 2007), rely on lexicosyntactic patterns that pose restrictions on the surface level or grammatical level of sentences. Their underlying assumption is that whenever a given sentence matches a given pattern (i.e., a sentence template), the sentence expresses the pattern’s corresponding semantic relation. This assumption does not always hold, hence the system output usually contains a certain amount of noise, which makes a human-in-the-loop necessary for high-precision applications. Typically, RE systems associate patterns with a confidence score of some kind, allowing downstream components to trade p"
W16-4210,D14-1164,0,0.0149434,"istorical patient data. Automated information extraction could allow the development of alert systems, which help the clinicians in their daily routine and thus would increase patients safety. However, the first step towards any information extraction is the definition of information of interest, such as diseases, medications or dosing size. This information is then defined within an annotation schema and is used to manually annotate a gold standard corpus to train and evaluate information extraction methods. Unfortunately, manual annotation is time consuming (Kim et al., 2008) and expensive (Angeli et al., 2014). In particular in the medical domain, expert knowledge is often required which makes the annotation process even more difficult and costly. Therefore existing schemata and corpora could be used in order to save time and effort for the annotation of new data. On the other hand, existing schemata might not cover the information of interest. Furthermore, most of the existing and assessable clinical data sets are in English language. The existing German-language clinical data sets are not freely available. Consequently, we aim to create a new gold standard corpus for German data. This work introd"
W16-4210,W13-1904,0,0.119648,"ols could support physicians to better access patient data. However, annotated data sets are required for the development and testing of information extraction methods. Most of the existing annotated clinical data sets are in English language. There are only a few data sets that have been created for non-English languages, such as for Swedish (Skeppstedt et al., 2014), French (N´ev´eol et al., 2015) or Polish (Mykowiecka et al., 2009). For German, only a few sources and clinical corpora exist and will be introduced in the following. The two most relevant sources for this work are described in Bretschneider et al. (2013) and Toepfer et al. (2015). Bretschneider et al. (2013) focused on the classification of sentences in radiology reports as either pathological and non-pathological based on the given findings. Toepfer et al. (2015) addressed the extraction of fine-grained information from German transthoracic echocardiography reports. The presented terminology involves three main types: objects, attributes and values. Unfortunately, both data sets are not publicly available. Another very interesting corpus is the FraMed corpus which is described in Wermter and Hahn (2004). The authors present a German-language"
W16-4210,faessler-etal-2014-disclose,0,0.534753,"pfer et al. (2015) addressed the extraction of fine-grained information from German transthoracic echocardiography reports. The presented terminology involves three main types: objects, attributes and values. Unfortunately, both data sets are not publicly available. Another very interesting corpus is the FraMed corpus which is described in Wermter and Hahn (2004). The authors present a German-language medical text corpus containing manually supplied sentence boundary, token segmentation and part-of-speech (POS) tags. Due to the fact that the corpus cannot be legally accessed by a third party, Faessler et al. (2014) present an freely available tool for segmentation and POS tagging for German clinical data, based on models trained on the FraMed corpus. Further relevant sources for German clinical data are for instance the German Specialist Lexicon (Weske-Heck et al., 2002) or the German MeSH1 . A good overview is also provided in the work of Schulz et al. (2013). 3 Utilized Data Sources This section presents the two data sources used for this work. Firstly, a biomedical knowledge source is presented which is used to automatically pre-annotate data to reduce annotation time. Secondly, the textual data whic"
W16-4210,wermter-hahn-2004-annotated,0,0.607558,"ces for this work are described in Bretschneider et al. (2013) and Toepfer et al. (2015). Bretschneider et al. (2013) focused on the classification of sentences in radiology reports as either pathological and non-pathological based on the given findings. Toepfer et al. (2015) addressed the extraction of fine-grained information from German transthoracic echocardiography reports. The presented terminology involves three main types: objects, attributes and values. Unfortunately, both data sets are not publicly available. Another very interesting corpus is the FraMed corpus which is described in Wermter and Hahn (2004). The authors present a German-language medical text corpus containing manually supplied sentence boundary, token segmentation and part-of-speech (POS) tags. Due to the fact that the corpus cannot be legally accessed by a third party, Faessler et al. (2014) present an freely available tool for segmentation and POS tagging for German clinical data, based on models trained on the FraMed corpus. Further relevant sources for German clinical data are for instance the German Specialist Lexicon (Weske-Heck et al., 2002) or the German MeSH1 . A good overview is also provided in the work of Schulz et a"
W16-5113,W13-1904,0,0.0769879,", 2015; Costumero et al., 2014; Afzal et al., 2014). Beside NegEx and Context a wide range of other methods exist, e.g. based on syntactic techniques (Huang and Lowe, 2007; Mehrabi et al., 2015; Sohn et al., 2012; Cotik et al., 2016) or machine learning techniques (Uzuner et al., 2009). However, in clinical context simple methods, such as NegEx work very reliably for the task they have been designed for. 2 http://www.clips.ua.ac.be/NeSpNLP2010/program.html 116 Other research has been dedicated to clinical negation detection together with the detection of pathological entities in German texts. Bretschneider et al. (2013) classify sentences containing pathological and non-pathological findings in German radiology reports. Their approach uses a syntacto-semantic parsing approach. Gros and Stede (2013) present Negtopus, a system that identifies negations and their scope in medical diagnoses written in German and in English. Chapman et al. (2013) translate NegEx triggers into Swedish, French and German. The work reports, among others, the frequency of occurrence of German triggers in an annotated corpus of German medical text (Wermter and Hahn, 2004), that, as far as we know, is not available for public use. Both"
W16-5113,W16-2921,1,0.917461,"ical texts, which also addresses negation detection. A widely used tool for negation and speculation detection is Negex (Chapman et al., 2001). The method uses a simple algorithm based on regular expressions to detect triggers that indicate negation or speculation. Next it uses a window of words preceding or following each relevant term to determine if the term is under the scope of negation or speculation or not. NegEx has been extended to Context (Harkema et al., 2009) and adapted to Swedish, French, Spanish and other languages with good results (Skeppstedt, 2011; Del´eger and Grouin, 2012; Cotik et al., 2016; Stricker et al., 2015; Costumero et al., 2014; Afzal et al., 2014). Beside NegEx and Context a wide range of other methods exist, e.g. based on syntactic techniques (Huang and Lowe, 2007; Mehrabi et al., 2015; Sohn et al., 2012; Cotik et al., 2016) or machine learning techniques (Uzuner et al., 2009). However, in clinical context simple methods, such as NegEx work very reliably for the task they have been designed for. 2 http://www.clips.ua.ac.be/NeSpNLP2010/program.html 116 Other research has been dedicated to clinical negation detection together with the detection of pathological entities"
W16-5113,W10-3001,0,0.0912888,"Missing"
W16-5113,W15-2914,0,0.0388162,"Missing"
W16-5113,W16-4210,1,0.798227,"Missing"
W16-5113,wermter-hahn-2004-annotated,0,0.0469593,"ith the detection of pathological entities in German texts. Bretschneider et al. (2013) classify sentences containing pathological and non-pathological findings in German radiology reports. Their approach uses a syntacto-semantic parsing approach. Gros and Stede (2013) present Negtopus, a system that identifies negations and their scope in medical diagnoses written in German and in English. Chapman et al. (2013) translate NegEx triggers into Swedish, French and German. The work reports, among others, the frequency of occurrence of German triggers in an annotated corpus of German medical text (Wermter and Hahn, 2004), that, as far as we know, is not available for public use. Both publications, (Gros and Stede, 2013) and (Chapman et al., 2013), are related to our work. However, Negtopus focuses currently only on negation terms. It has been evaluated on a set of only 12 cardiology reports for German negation detection. NegEx with the German trigger set has not been evaluated and thus its performance is still unknown to us. 3 Methods The adaptation of NegEx to German requires having a set of triggers written in German. In order to evaluate the new system, a gold standard data set is necessary, consisting of"
W16-5113,W10-3111,0,0.0420113,"ether the finding is within the scope of negation or speculation. In comparison to English, German clinical data differs in various characteristics which have to be taken into account for the successful application of an algorithm detecting non-factuality. First of all, German is a richly inflected language (e.g. no can be translated as kein, keiner, keine etc.). Furthermore, German includes discontinuous triggers, such as kann ... ausgeschlossen werden ...1 (can be ruled out). Triggers may precede, but may also follow the negated expression, as presented in Table 1. Regarding this situation, Wiegand et al. (2010) state, that the detection of negation scope in German language is more difficult than in other languages, such as English. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 Dots indicate potential positions of the finding: (kann ... finding... ausgeschlossen werden, ... finding... kann ausgeschlossen werden) 115 Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining (BioTxtM 2016), pages 115–124, Osaka, Japan, December 12th 2016. precede frei von Besc"
W16-5113,S12-1035,0,\N,Missing
xu-etal-2002-domain,J90-1003,0,\N,Missing
xu-etal-2002-domain,J93-1007,0,\N,Missing
xu-etal-2002-domain,C00-2136,0,\N,Missing
xu-etal-2002-domain,C92-2082,0,\N,Missing
xu-etal-2002-domain,P01-1025,0,\N,Missing
xu-etal-2008-adaptation,W06-0202,0,\N,Missing
xu-etal-2008-adaptation,P07-1074,1,\N,Missing
