2007.jeptalnrecital-poster.25,2003.mtsummit-papers.6,0,0.038714,"ns morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme unit´es du syst`eme de traduction et de n’introduire les cat´egories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’int´egrer les informations syntaxiques dans le mod`ele de traduction lui-mˆeme. De plus, nous proposons de combiner cette approche avec les ` notre connaism´ethodes classiques de r´e´evaluation de listes de n meilleures hypoth`eses. A sance, cette approche n’a pas ´et´e ´evalu´ee sur une large tˆache (elle a ´et´e appliqu´ee par (Hwang et al., 2007) `a la tˆach"
2007.jeptalnrecital-poster.25,W06-2606,0,0.0276563,"re en compte les contraintes syntaxiques ou les d´ependances `a long terme entre les mots. Il apparaˆıt donc n´ecessaire d’utiliser des m´ethodes dans lesquelles les propri´et´es structurelles des langues sont explicitement repr´esent´ees. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme unit´es du syst`eme de traduction et de n’introduire les cat´egories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’int´egrer les informations syntaxiques dans le mod`ele de traduction"
2007.jeptalnrecital-poster.25,W05-0821,0,0.0315029,"s syntaxiques ou les d´ependances `a long terme entre les mots. Il apparaˆıt donc n´ecessaire d’utiliser des m´ethodes dans lesquelles les propri´et´es structurelles des langues sont explicitement repr´esent´ees. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme unit´es du syst`eme de traduction et de n’introduire les cat´egories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’int´egrer les informations syntaxiques dans le mod`ele de traduction lui-mˆeme. De plus, nous propos"
2007.jeptalnrecital-poster.25,2003.mtsummit-tttt.3,0,0.177459,"ique, mod´elisation linguistique dans un espace continu, analyse morpho-syntaxique, d´esambigu¨ısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un th`eme de recherche depuis plusieurs d´ecennies et diff´erentes approches ont ´et´e propos´ees, telles que la traduction par r`egles, la traduction `a base d’exemples ou la traduction statistique. Les travaux r´ecents en traduction statistique confirment que les mod`eles fond´es sur des s´equences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fond´es sur des mots (Brown et al., 1993). En utilisant des s´equences de mots, les syst`emes de traduction parviennent a` pr´eserver certaines contraintes locales sur l’ordre des mots. L’entraˆınement d’un tel mod`ele n´ecessite l’alignement d’un corpus parall`ele. Les r´egularit´es du langage naturel comme celles de la syntaxe, ou, encore a` un niveau sup´erieur, celles de la s´emantique sont ainsi, en principe, implicitement captur´ees par les mod`eles. 253 ´chelotte, H. Bonneau-Maynard, A. Allauzen H. Schwenk, D. De Depuis"
2007.jeptalnrecital-poster.25,N04-1021,0,0.0224246,"ucturation syntaxique pour restituer le sens de l’´enonc´e d’origine. La mod´elisation du langage comme une source markovienne (mod`ele de langage n-gramme), avec comme unit´e le mot ou la s´equence de mots, ne permet pas de prendre en compte les contraintes syntaxiques ou les d´ependances `a long terme entre les mots. Il apparaˆıt donc n´ecessaire d’utiliser des m´ethodes dans lesquelles les propri´et´es structurelles des langues sont explicitement repr´esent´ees. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme u"
2007.jeptalnrecital-poster.25,P02-1038,0,0.0392758,"derni`eres ann´ees, les travaux en traduction statistique ont ´etendu avec succ`es l’unit´e qu’´etait le mot a` la s´equence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unit´e se d´efinit alors comme un groupe de mots successifs ˜f de la langue source. Sa ˜ dans la phrase cible. Les s´equences traduction est ´egalement une s´equence de mots e de mots peuvent ˆetre extraites automatiquement `a partir de donn´ees bilingues align´ees au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de d´ecomposer le probl`eme de la mani`ere suivante (Och & Ney, 2002) : e∗ = arg max p(e|f ) = arg max{exp e ! λi hi (e, f )} (1) i o` u chaque fonction hi quantifie l’ad´equation des phrases f et e1 . Les coefficients λi pond`erent l’importance relative de ces fonctions. 2.1 D´ ecodeur Moses Moses2 est un syst`eme de traduction automatique `a base de s´equences de mots a` l’´etat de l’art. Il est distribu´e librement avec les scripts n´ecessaires `a l’entraˆınement d’un syst`eme de traduction complet, ainsi qu’une mise en œuvre efficace d’un algorithme de recherche de type recherche en faisceau pour produire les traductions. Le d´ecodeur Moses peut ´egalement"
2007.jeptalnrecital-poster.25,W99-0604,0,0.119817,", approche statistique, mod´elisation linguistique dans un espace continu, analyse morpho-syntaxique, d´esambigu¨ısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un th`eme de recherche depuis plusieurs d´ecennies et diff´erentes approches ont ´et´e propos´ees, telles que la traduction par r`egles, la traduction `a base d’exemples ou la traduction statistique. Les travaux r´ecents en traduction statistique confirment que les mod`eles fond´es sur des s´equences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fond´es sur des mots (Brown et al., 1993). En utilisant des s´equences de mots, les syst`emes de traduction parviennent a` pr´eserver certaines contraintes locales sur l’ordre des mots. L’entraˆınement d’un tel mod`ele n´ecessite l’alignement d’un corpus parall`ele. Les r´egularit´es du langage naturel comme celles de la syntaxe, ou, encore a` un niveau sup´erieur, celles de la s´emantique sont ainsi, en principe, implicitement captur´ees par les mod`eles. 253 ´chelotte, H. Bonneau-Maynard, A. Allauzen H. S"
2007.jeptalnrecital-poster.25,P02-1040,0,0.0799804,"s large, puisqu’un syst`eme de traduction inclut toujours un mod`ele de langage cible hi (e, f ) = p(e). 2 http://www.statmt.org/moses/ 255 ´chelotte, H. Bonneau-Maynard, A. Allauzen H. Schwenk, D. De deux sens, les probabilit´es de traduction des mots dans les deux sens, une mesure de distorsion, deux p´enalit´es d’insertion de mots et de s´equences de mots, et la probabilit´e calcul´ee par le mod`ele de langage de la langue cible. L’approche couramment employ´ee pour optimiser les poids λi des fonctions caract´eristiques est la maximisation sur un corpus de d´eveloppement de la mesure BLEU (Papineni et al., 2002). Pour cela, l’outil d’optimisation num´erique Condor (Berghen & Bersini, 2005) est int´egr´e `a l’algorithme it´eratif suivant : 1. Partant d’un jeu de poids initial, les listes des n = 1000 meilleures hypoth`eses sont g´en´er´ees avec Moses (une liste par phrase source). 2. Ces listes sont r´e´evalu´ees en utilisant le jeu de poids courant. 3. Les meilleures hypoth`eses sont extraites et ´evalu´ees. ` partir du score BLEU aisni calcul´e, Condor calcule un nouveau jeu de poids 4. A (l’algorithme retourne alors a` l’´etape 2), sauf si un maximum local est d´etect´e ce qui met fin `a l’algorith"
2007.jeptalnrecital-poster.25,2006.iwslt-papers.2,1,0.898341,"Missing"
2007.mtsummit-papers.18,J96-1002,0,0.0353272,"iscussion of future research issues. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f that maximizes the posterior probability: e∗ = argmax Pr(e|f ) e X = argmax Pr(e, A|f ) e ≈ argmax max Pr(e, A|f ) e (1) A A (2) In the above equations, A denotes a correspondence between source and target words and is called an alignment. The Moses decoder makes the so-called maximum approximation as in Equation 2. The Pr(e, A|f ) probability is modeled by a combination of feature functions, according to the maximum entropy framework (Berger et al., 1996): X Pr(e, A|f ) ∝ exp λi fi (e, A|f ) (3) i The translation process involves segmenting the source sentence into source phrases f˜; translating each source phrase into a target phrase e˜, and optionally reordering the target phrases to produce the target sentence e∗ . A phrase is here defined as a group of words that should be translated together (Koehn et al., 2003; Och and Ney, 2003). The segmentation stage is not modeled explicitly by any feature function, which amounts to considering every segmentation equally likely. A phrase table provides several scores that quantitize the relevance of"
2007.mtsummit-papers.18,N03-2002,0,0.0323071,"there is no significant difference after rescoring the n-best lists with the continuous space language model. We conjecture that both approaches correct the same translation problems. The results reported in Table 4 were obtained by rescoring with word language models, even in the last row. We believe that it is necessary to use the enriched representation also in the language models in order to take full advantage of the disambiguation in the translation model. Rescoring with simple POS language models was tried, but without success. We are now working on the use of factored language models (Bilmes and Kirchhoff, 2003) that simultaneously use the word and POS information. Figure 5 shows comparative translation examples from the baseline and the enriched translation systems. In the first example, the baseline system outputs “durante los u´ ltimos sesiones” where the enriched translation system produces “en los u´ ltimos per´ıodos de sesiones”, a better translation that may be attributed to the introduction of the masculine word “per´ıodos”, allowing the system to build a syntactically correct sentence. In the second example, the syntactical error “no puede ser un cierto reconocimiento” produced by the baseli"
2007.mtsummit-papers.18,W07-0409,1,0.882047,"Missing"
2007.mtsummit-papers.18,2005.mtsummit-posters.19,0,0.0305021,"These n-best lists are then rescored with the continuous space language model. 1. The n-best lists are reranked using the current set of weights. The current hypothesis is extracted and scored against the reference translations. 2. The obtained BLEU score is passed to Condor, which either computes a new set of weights (the algorithm then proceeds to step 1) or detects that a local maximum has been reached and the algorithm stops iterating. Each of the two passes uses its own set of eight weights and is tuned separately, a feature shared with other systems, for instance (L¨oo¨ f et al., 2006; Cettolo et al., 2005). The second pass is often taken as an opportunity to compute several feature functions on the n-best list, yet after several experiments we chose not to follow this direction. The described system is thus voluntarily simple, with the hope that it will generalize well to new data. We believe that adding many feature functions, especially some that could just be ad hoc fixes to phenomena from the development data, in conjunction with performing a numerical optimization of the λi that is unaware of the highly discontinuous nature of BLEU (Papineni et al., 2002), bear the risk of heavily over-fit"
2007.mtsummit-papers.18,P07-2045,0,0.0664233,"Missing"
2007.mtsummit-papers.18,N03-1017,0,0.0260625,"Missing"
2007.mtsummit-papers.18,koen-2004-pharaoh,0,0.0334108,"science. From the pioneer works to today’s research, many paradigms have been explored, for instance rulebased, example-based, knowledge-based and statistical approaches to machine translation. Statistical machine translation (SMT) seems today to be the preferred approach of many industrial and academic research laboratories, each of them developing their own set of tools. In 1999 however, a summer workshop at Johns-Hopkins University hosted the creation of the EGYPT toolkit1 , on which the widely used training tool Giza++ (Och and Ney, 2003) is based. Later, the Pharaoh phrase-based decoder (Koehn, 2004) became available and distributed in binary form2 , but as far as we know, Pharaoh was not widely used. More recently, another workshop3 released an open source toolkit, which includes a decoder, Moses (Koehn and al., 2007), and a comprehensive set of softwares and scripts to build a complete SMT system—namely determining word alignments, extracting phrases, performing the translation and tuning system parameters. In this paper, we describe the development of a state-of-theart SMT system based on the Moses suite. Several new features were added, in particular a two-pass decoding strategy using"
2007.mtsummit-papers.18,J03-1002,0,0.0151761,"rst natural language processing applications investigated in computer science. From the pioneer works to today’s research, many paradigms have been explored, for instance rulebased, example-based, knowledge-based and statistical approaches to machine translation. Statistical machine translation (SMT) seems today to be the preferred approach of many industrial and academic research laboratories, each of them developing their own set of tools. In 1999 however, a summer workshop at Johns-Hopkins University hosted the creation of the EGYPT toolkit1 , on which the widely used training tool Giza++ (Och and Ney, 2003) is based. Later, the Pharaoh phrase-based decoder (Koehn, 2004) became available and distributed in binary form2 , but as far as we know, Pharaoh was not widely used. More recently, another workshop3 released an open source toolkit, which includes a decoder, Moses (Koehn and al., 2007), and a comprehensive set of softwares and scripts to build a complete SMT system—namely determining word alignments, extracting phrases, performing the translation and tuning system parameters. In this paper, we describe the development of a state-of-theart SMT system based on the Moses suite. Several new featu"
2007.mtsummit-papers.18,P02-1040,0,0.0814218,"for instance (L¨oo¨ f et al., 2006; Cettolo et al., 2005). The second pass is often taken as an opportunity to compute several feature functions on the n-best list, yet after several experiments we chose not to follow this direction. The described system is thus voluntarily simple, with the hope that it will generalize well to new data. We believe that adding many feature functions, especially some that could just be ad hoc fixes to phenomena from the development data, in conjunction with performing a numerical optimization of the λi that is unaware of the highly discontinuous nature of BLEU (Papineni et al., 2002), bear the risk of heavily over-fitting the development data. Some experimental evidence for this are provided in the results section. Overall, there are roughly 60 million words of texts available to train the target language models. This is a quite limited amount in comparison to tasks like the N IST machine translation evaluations for which several billion words of newspaper texts are available. Therefore, specific techniques must be deployed to make the most of the limited resources. We use MERT, which is distributed along with the Moses decoder, to tune the first pass. The weights were ad"
2007.mtsummit-papers.18,2006.iwslt-papers.2,1,0.614623,"Missing"
2010.amta-papers.18,N06-1013,0,0.629907,"to the first problem is to use discriminative models, which are able to consider arbitrary features of the involved words. In this framework, the alignment task is casted as a classification problem: a binary classifier predicts, for each possible assignment, whether it should be included or not in the alignment. Discriminative models can also consider predictions provided by other alignment models as features, and therefore constitute a solution to the second problem: by applying these features to learn symmetrization decisions in light of a global view of the data. By applying these ideas (Ayan and Dorr, 2006) obtained promising results. However, their model remains unable to model interactions between alignment decisions which are, intuitively, of great help to correctly prevent or encourage certain configurations in the predicted alignment. To overcome this shortcoming, we propose to extend their model by introducing a stacked classification layer (Wolpert, 1992) that operates globally and, hence, enables arbitrary features, describing interactions between alignment decisions, to be taken into consideration. The main contribution of this work is a reexamination of (Ayan and Dorr, 2006) work which"
2010.amta-papers.18,P06-1009,0,0.0912765,"nt task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference and training. However the underlying graphical structure is similar to the directed hidden Markov model (HMM) used in generative alignment, hence only one-to-many alignments can be obtained, and the symmetrization step is still needful. In (Niehues and Vogel, 2008), the alignment matrix is directly modeled by a more complex CRF structure, which allows to get rid of the symme"
2010.amta-papers.18,J93-2003,0,0.0208821,"p. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation directions. Theses alignments are then symmetrized during a post-processing step to obtain a many-to-many symmetric alignment. Training these models only requires sentence-aligned bitext and is performed in an unsupervised way with the EM algorithm. This approach has two main caveats, leaving room for improving the alignment quality and, consequently, the translation quality. Firstly, the generative paradigm is not well suited to incorporate arbitrary and possibly interdependent information sources. Secondly, the"
2010.amta-papers.18,P03-1012,0,0.0307885,"tained by the discriminative matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form o"
2010.amta-papers.18,P08-2007,0,0.0743953,"stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task. 1 Introduction The translation quality of phrase-based machine translation systems depends heavily on the quality of the translation model, the so-called phrase table consisting of a set of aligned phrase-pairs in mutual translation relationship. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation di"
2010.amta-papers.18,N07-2007,0,0.396858,"nt matrix is typically sparse, with a majority of inactive links, the classification task we consider is unbalanced. To avoid learning a biased classifier with high tendency toward labeling all links as inactive, we use a set of input alignments to reduce the set of links to be predicted to a subset of the alignment matrix: a point that has not been proposed by at least one input alignment will be labeled as inactive; the others are labeled by the classifier. The union of all input alignments is hence used to reduce the search space and avoid biasing the classifier as in (Ayan and Dorr, 2006; Elming and Habash, 2007). Input alignments are pre-computed separately using GIZA++. During inference, the model assigns a probability to each proposed alignment link. The final output matrix consists of active links whose probability exceeds a threshold p (optimized on a development set using a grid search). This parameter is used to control the density of the resulting alignment and therefore the balance between its precision and recall. In this work, we used a maximum entropy (ME) classifier to estimate the probability of a link of A: Align 2 Align 1 AAlign Target ADist AJump Figure 1: Features extracted to label"
2010.amta-papers.18,J07-3002,0,0.0746646,"Missing"
2010.amta-papers.18,N06-2013,0,0.0585018,"Missing"
2010.amta-papers.18,H05-1012,0,0.020578,"native matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imp"
2010.amta-papers.18,P06-1141,0,0.0209513,"the model simple, interactions between individual predictions cannot be modeled, and global decisions cannot be made. In order to incorporate structure and dependencies into the ME model, without sacrificing efficient, model-optimal predictions, we use a stacked generalization method (Wolpert, 1992). Stacked generalization is an approximation approach to structured learning. It allows to indirectly model dependencies between predicted labels at a low computational cost. It has been successfully applied to NLP problems, like dependency parsing (Martins et al., 2008), named entity recognition (Krishnan and Manning, 2006) and sequential partitioning problems (Cohen and Carvalho, 2005). In stacked learning, all labels are jointly predicted in two steps. (1) For each training example (xi , y˜i ), the entire set of observations x = [x1 , . . . , xn ] is considered to extract features, that are then fed to a first-level classifier. This classifier is used to assign a label yi to each observation xi without taking dependencies between labels into consideration; then (2) observations are augmented with predictions of the local classifier y = [y1 , . . . , yn ] to generate an extended representation of the training c"
2010.amta-papers.18,N06-1015,0,0.0149343,"ir and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference an"
2010.amta-papers.18,P05-1057,0,0.0168931,"ght of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of t"
2010.amta-papers.18,2006.amta-papers.11,0,0.0391831,"Missing"
2010.amta-papers.18,D08-1017,0,0.0309813,"s are assumed to be independent. While this keeps the model simple, interactions between individual predictions cannot be modeled, and global decisions cannot be made. In order to incorporate structure and dependencies into the ME model, without sacrificing efficient, model-optimal predictions, we use a stacked generalization method (Wolpert, 1992). Stacked generalization is an approximation approach to structured learning. It allows to indirectly model dependencies between predicted labels at a low computational cost. It has been successfully applied to NLP problems, like dependency parsing (Martins et al., 2008), named entity recognition (Krishnan and Manning, 2006) and sequential partitioning problems (Cohen and Carvalho, 2005). In stacked learning, all labels are jointly predicted in two steps. (1) For each training example (xi , y˜i ), the entire set of observations x = [x1 , . . . , xn ] is considered to extract features, that are then fed to a first-level classifier. This classifier is used to assign a label yi to each observation xi without taking dependencies between labels into consideration; then (2) observations are augmented with predictions of the local classifier y = [y1 , . . . , yn ] t"
2010.amta-papers.18,H05-1011,0,0.0168663,"t have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of tractability and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment"
2010.amta-papers.18,W08-0303,0,0.0353835,"usually the probability of the hypothesized alignment, is decomposable under some independence assumptions. In (Blunsom and Cohn, 2006) word alignment is considered as a sequence labeling problem, in which, source words are tagged with target positions using a linear chain conditional random field (CRF). The linear chain assumption enables exact inference and training. However the underlying graphical structure is similar to the directed hidden Markov model (HMM) used in generative alignment, hence only one-to-many alignments can be obtained, and the symmetrization step is still needful. In (Niehues and Vogel, 2008), the alignment matrix is directly modeled by a more complex CRF structure, which allows to get rid of the symmetrization step, at the expense of an approximate inference and a complicated two-step training. Many of these discriminative models do not entirely dispense with the generative models, but rather integrate their predictions as supplementary features. 3 Maximum Entropy for Alignment Matrix Modeling In this section, we present the task of word alignment as a binary classification problem, in which we model the alignment matrix directly. We also explain how to improve the expressivity o"
2010.amta-papers.18,J03-1002,0,0.0614447,"delivering improved performance in a large scale Arabic to English translation task. 1 Introduction The translation quality of phrase-based machine translation systems depends heavily on the quality of the translation model, the so-called phrase table consisting of a set of aligned phrase-pairs in mutual translation relationship. Since finding the optimal phrase alignment in parallel sentences is NPhard (DeNero and Klein, 2008), most practical approaches rely on pre-computed word alignments to restrict the search space and use a heuristic to extract phrase pairs that are consistent with them (Och and Ney, 2003). Phrase extraction therefore boils down to the problem of word alignments, that consists in finding a many-to-many correspondence between source and target words of a bilingual sentence-pair. Many approaches have been proposed to solve this problem. The most widely used in practice are generative IBM models (Brown et al., 1993) which allow to construct directional one-to-many alignments in both translation directions. Theses alignments are then symmetrized during a post-processing step to obtain a many-to-many symmetric alignment. Training these models only requires sentence-aligned bitext an"
2010.amta-papers.18,P03-1021,0,0.0261178,"ng we used a freely available toolkit3 . The model parameters are estimated using L-BFGS (Byrd et al., 1994) to maximize the regularized log-likelihood on a training corpus. A Gaussian prior is used during optimization to prevent overfitting. GIZA++ (Och and Ney, 2003) is used to train our generative alignments, with the additional parallel data made available by NIST MT Eval’09 constrained training condition. We used Moses4 with SRILM5 with the same data in our translation experiments. A 4-gram back-of language model is estimated using all English available data. Minimum Error-Rate Training (Och, 2003) is carried on to tune the parameters of the translation system on the NIST MT’06 test set. Translations are evaluated on NIST MT’08 test set. Arabic pre-processing scheme and remappings Arabic is a morphologically complex, highlyinflected language. This makes normalization necessary to reduce the sparsity of the data. We use MADA+TOKAN6 for morphological analysis, disambiguation and tokenization for Arabic. Given previous experiments on the NIST MT’09 task, we use the D2 tokenization scheme that showed to perform best under large resource conditions (Habash 3 http://homepages.inf.ed.ac.uk/lzh"
2010.amta-papers.18,P02-1040,0,0.0816489,"e one hand, we present a careful study of the impact of several novel features on the performance; on the other hand, we investigate the use of the stacking technique to improve the alignment quality. By conjoining these techniques, we were able to greatly reduce the AER as compared to previously published work, and to achieve better BLEU results. In this paper, we also contrast alignments obtained by the symmetrization heuristic with those obtained by the discriminative matrix model, in the light of their Alignment Error Rate (AER) and their impact on translation quality as measured by BLEU (Papineni et al., 2002) on NIST MT08 largescale task. The rest of the paper is organized as follows: after reviewing the related work in Section 2, we present our approach in Section 3, focusing on the design of our feature set, and on our implementation of stacking. We then present experimental results both in terms of AER and BLEU in Section 4. 2 Related Work Several discriminative approaches of word alignment have been carried out recently (Cherry and Lin, 2003; Ittycheriah and Roukos, 2005; Liu et al., 2005), attempting to reach a good balance between the expressivity of the model and its complexity (in terms of"
2010.amta-papers.18,H05-1010,0,0.0234131,"ity and the possibility of performing exact inference and learning). In one type of approaches, a word alignment between two sentences is evaluated with a global score using a non-decomposable discriminative scoring function. This scheme enables to take into consideration the complete observation of the sentence-pair and the hypothesized word alignment when extracting features (Moore, 2005). However, as no restriction on the form of considered alignments is imposed, the size of the resulting search space makes the search intractable and requires the application of a heuristic beam search. In (Taskar et al., 2005), tractability of the search problem is achieved by casting the word alignment task as a maximum weighted matching problem. This comes at the price of constraining possible alignments to one-to-one matchings and making local decisions with no global interactions. These limitations are fixed in (Lacoste-Julien et al., 2006), by modeling alignment as a quadratic assignment problem which is NP-hard in general. In another type of approaches, word alignment is viewed as a classification problem of the cells in the alignment matrix. The scoring function, which is usually the probability of the hypot"
2010.iwslt-evaluation.13,W10-1704,1,0.845666,"is first described in Section 2, while Section 3 reports our work on Turkish pre-processing and on the use of continuous space language models. 2. TALK task 2.1. n-code SMT system 1. Introduction LIMSI took part in the IWSLT 2010 evaluation for two different tasks: Talk and BTEC. The goal of the new Talk task is to translate public speeches on a variety of topics, from English to French. Since the allowed training data includes the parallel corpora distributed by the ACL 2010 Workshop on Statistical Machine Translation (WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our"
2010.iwslt-evaluation.13,P07-2045,0,0.00317713,"(WMT), our starting system is the one submitted to the evaluation campaign [1]. We enhanced our inhouse n-code SMT system with an additional reordering model which is estimated as a standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on biling"
2010.iwslt-evaluation.13,J06-4004,1,0.810779,"standard n-gram language model over generalized translation units (partof-speech in the described experiments). In order to add more closely related training data, the use of Wikipedia as an additionnal source of monolingual text for the target language model was also evaluated. For the BTEC task, the LIMSI participated in the Turkish to English translation track with a system based on the open source Moses system [2]. The linguistic discrepancies between these two languages appear both Our in-house n-code SMT system implements the bilingual n-gram approach to statistical Machine Translation [3]. A translation hypothesis t given a source sentence s is defined as the sentence which maximizes a linear combination of feature functions: tˆI1 = arg max tI1 ( M X m=1 λm hm (sJ1 , tI1 ) ) , (1) where sJ1 and tI1 respectively denote the source and the target sentences, and λm is the weight associated with the feature function hm . The most important feature is the log-score of the translation model based on bilingual units called tuples. The probability assigned to a sentence pair by the translation model is estimated by using the n-gram assumption: p(sJ1 , tI1 ) = K Y k=1 p((s, t)k |(s, t)k"
2010.iwslt-evaluation.13,P03-1021,0,0.0080791,"Figure 1: Tuple extraction from a sentence pair. The resulting sequence of tuples (1) is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalizatio"
2010.iwslt-evaluation.13,N04-4026,0,0.0409215,"is further refined to avoid NULL words in source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order. In addition to the translation model, eleven feature functions are optimally combined using a discriminative training framework [4]: a target-language model; four lexicon models; two lexicalized reordering models [5] aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correpond to the relative frequencies of All the available textual corpora are processed and normalized using in-house tools. Previous experiments revealed that using better normalization tools provides a significant reward in BLEU . The downside is the need to post-proc"
2010.iwslt-evaluation.13,C10-2023,1,0.883793,"Missing"
2010.iwslt-evaluation.13,W07-0704,1,0.827751,"Missing"
2010.iwslt-evaluation.13,P10-1047,0,0.0285202,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.2,0,0.0280005,"Missing"
2010.iwslt-evaluation.13,2009.iwslt-evaluation.6,0,0.0375883,"Missing"
2010.iwslt-evaluation.13,P06-1001,0,0.0377565,"Missing"
2010.iwslt-evaluation.13,popovic-ney-2004-towards,0,0.0520656,"Missing"
2010.iwslt-evaluation.13,H05-1085,0,0.0623309,"Missing"
2010.iwslt-evaluation.13,P08-1087,0,0.0340659,"Missing"
2010.iwslt-evaluation.13,P07-1017,0,0.0300031,"Missing"
2010.iwslt-evaluation.13,J04-2003,0,0.0867503,"Missing"
2010.iwslt-evaluation.13,corston-oliver-gamon-2004-normalizing,0,0.068379,"Missing"
2010.iwslt-evaluation.13,2005.mtsummit-papers.11,0,0.0224187,"Missing"
2010.iwslt-evaluation.13,E06-1006,0,0.0399082,"Missing"
2010.iwslt-evaluation.13,N04-4015,0,0.0798783,"Missing"
2010.iwslt-evaluation.13,2001.mtsummit-papers.45,0,0.0857026,"Missing"
2010.iwslt-evaluation.13,D10-1076,1,0.891217,"Missing"
2010.iwslt-evaluation.13,W06-3102,1,\N,Missing
2010.iwslt-evaluation.13,2009.iwslt-evaluation.5,0,\N,Missing
2011.eamt-1.41,P05-1071,0,0.0328206,"d by the translation systems, we select a subset of the LDC resources made available by the NIST MT’09 constrained track2 . In order to validate the obtained results on training corpora of varying sizes, we consider two training conditions, one with 30K parallel sentence pairs, and another with 130K. For each condition, we report below the AER, the BLEU scores on the test set, along with the size of the obtained phrase tables. A 4-gram back-off language model, estimated with SRILM3 is trained on the NIST MT’09 constrained English data. All Arabic sentences are pre-processed using MADA+TOKAN4 (Habash and Rambow, 2005), and segmented according to the D2 tokenization scheme. The IBM Arabic-English Word Alignment Corpus (Ittycheriah et al., 2006) is used to train both CRF and MaxEnt aligners and evaluate them using Alignment Error Rate (AER). 5.1 Translation Models Construction In section 3, we have described a generic algorithm that constructs the translation model in three steps: word alignment, phrase-pairs extraction, and scoring. In this section, we compare different instantiations of these steps, and report the translation performance of the resulting models. In the word alignment step, we experiment tw"
2011.eamt-1.41,N03-1017,0,0.449227,"p of related work. Section 3 revisits the standard translation model procedures and its extensions to weighted matrices. Our own approach is introduced in Section 4 and experimentally contrasted to various baselines in 5. We discuss further prospects in Section 6. 2 Related work As pointed out in the introduction, the construction of the translation model starts with a word alignment step during which relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignment outputs a probability distribution over all possible alignments. However, the most common practice (Koehn et al., 2003) is to use only the 1-best, Viterbi alignment, while discarding all the other informations contained in this distribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best"
2011.eamt-1.41,D09-1106,0,0.125085,"Missing"
2011.eamt-1.41,N06-1013,0,0.0703931,"y many alignments. The authors of (Liu et al., 2009) estimate link probabilities by calculating relative frequencies over a list of N-best alignments produced by generative models, and show some improvements in translation quality. However, using small N-best lists as samples is known to yield poor estimates of the alignment posteriors, as these lists usually contain too few variations. In this paper, we argue that better estimation of alignment probabilities helps achieving clearer improvements. Our solution is to directly model the weighted alignment matrices using a discriminative aligner (Ayan and Dorr, 2006; Tomeh et al., 2010). The rest of this paper is organized as follows: we start in Section 2 by a recap of related work. Section 3 revisits the standard translation model procedures and its extensions to weighted matrices. Our own approach is introduced in Section 4 and experimentally contrasted to various baselines in 5. We discuss further prospects in Section 6. 2 Related work As pointed out in the introduction, the construction of the translation model starts with a word alignment step during which relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignmen"
2011.eamt-1.41,W06-3123,0,0.0194356,"ranslations of one another, are first extracted. 2) Phrase pairs accumulated over the entire training corpus are collected and scored using relative frequencies estimates. The collection of phrase-pairs and their scores constitutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words instead of phrases: a fixe"
2011.eamt-1.41,J93-2003,0,0.0367641,"ley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this problem is described in (Deng and Byrne, 2005), where a word-to-phrase HMM alignment model is proposed, which constitutes a competitive model to IBM4. Under this model, the necessary statistics can be computed efficiently with the forward algorithm. The phrase pair induction procedure described in (Deng and Byrne, 2005), benefits from this efficiency to estimate a phrase-to-phrase posterior distribution, which is used further in the extraction and scoring of phrases. In"
2011.eamt-1.41,D10-1053,0,0.0306266,"Missing"
2011.eamt-1.41,P08-2007,0,0.0178677,"titutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words instead of phrases: a fixed segmentation, based on word boundaries, is used, and the resulting model is simpler to train using EM. Then, for each word-aligned sentence in the training corpus, an additional step is required to identify the set of phrase-p"
2011.eamt-1.41,W06-3105,0,0.023653,"e entire training corpus are collected and scored using relative frequencies estimates. The collection of phrase-pairs and their scores constitutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words instead of phrases: a fixed segmentation, based on word boundaries, is used, and the resulting model is simpler"
2011.eamt-1.41,H05-1022,0,0.0207298,"rly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this problem is described in (Deng and Byrne, 2005), where a word-to-phrase HMM alignment model is proposed, which constitutes a competitive model to IBM4. Under this model, the necessary statistics can be computed efficiently with the forward algorithm. The phrase pair induction procedure described in (Deng and Byrne, 2005), benefits from this efficiency to estimate a phrase-to-phrase posterior distribution, which is used further in the extraction and scoring of phrases. In (de Gispert et al., 2010), a similar procedure"
2011.eamt-1.41,P06-1121,0,0.0388355,"s contained in this distribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 19"
2011.eamt-1.41,W08-0509,0,0.0307294,"1 , ei1 ) can be calculated and used as a fractional count. Only phrase-pairs with a fractional count above certain threshold tp 7 are extracted. The same fractional counts are used for scoring with relative fractional frequencies. In both configurations, only phrasepairs that do not exceed a length limit of 7, on the source or the target side, are retained and scored. 5.2 Results and Discussion In this section, we describe five alignment systems and compare their performance (see Table 5.2). MGIZA++8 These alignments are produced by the multi-threaded and optimized alignment toolkit MGIZA++ (Gao and Vogel, 2008), which implements the IBM models. This tool only outputs deterministic alignment matrices in configuration (i). These models also deliver features for the discriminative word aligners described below. MGIZA++ IBM4 represents the performance of the standard baseline: one IBM4 alignment in each direction, which are symmetrized with grow-diagfinal-and heuristic. This system deliver competitive BLEU scores of 35.9 and 40.2 on the 30K and 130K respectively, with a much smaller phrase table than all the other systems. N-best WAM9 These alignments build weighted matrices, by averaging link occurence"
2011.eamt-1.41,W02-1018,0,0.0676035,"rase-pairs, that are translations of one another, are first extracted. 2) Phrase pairs accumulated over the entire training corpus are collected and scored using relative frequencies estimates. The collection of phrase-pairs and their scores constitutes the translation model. During the extraction step, we would like to use a phrase alignment model that enables the compuc 2011 European Association for Machine Translation. tation of corpus level statistics related to the joint segmentation and alignment of source and target sentences. Unfortunately, generative models designed for this purpose (Marcu and Wong, 2002; Birch et al., 2006) fail to deliver good performance due to three key difficulties (DeNero et al., 2006). First, exploring the whole space of phraseto-phrase alignment is intractable, which makes phrase alignment a NP-hard (DeNero and Klein, 2008) problem. Second, including a latent segmentation variable in the model increases the risk of overfitting during EM training. Third, spurious segmentation ambiguity tends to populate the phrase table with more entries, each having too few translation options. A practical solution is to reconfigure the phrase alignment problem in terms of words inste"
2011.eamt-1.41,D08-1022,0,0.0172246,"ich relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignment outputs a probability distribution over all possible alignments. However, the most common practice (Koehn et al., 2003) is to use only the 1-best, Viterbi alignment, while discarding all the other informations contained in this distribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translat"
2011.eamt-1.41,W08-0303,0,0.0203569,"N-best WAM by ≈ 0.8 BLEU point. The weighted matrix configuration performs even better than the standard one and increases BLEU scores by another ≈ 0.3 BLEU point. Improvements are persistent but less apparent on the larger task. We notice that the phrase table extracted from the weighted matrix is considerably larger than the standard one (by a factor of at least 3). PostCAT also slightly decreases the AER as compared to the MGIZA++ baseline. CRF12 The alignment matrix is modeled with a conditional random field (CRF), of which the graphical structure is quite complex and contains many loops (Niehues and Vogel, 2008). Therefore, neither training nor inference can be performed exactly, and the loopy belief propagation algorithm is used to approximate the posteriors. The CRF approach differs from our MaxEnt model (Section 4) in two aspects: first, MaxEnt training only optimizes the log-likelihood, whereas CRF training also aims at minimizing the AER. Second, while both models use the same set of features, MaxEnt 10 http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html http://code.google.com/p/geppetto/ 12 We thank J. Niehues (KIT) for sharing his implementation. 11 Translation task: 30K Translation model constru"
2011.eamt-1.41,P03-1021,0,0.0110619,"using a combination of `1 and `2 terms, allowing for efficient feature selection while maintaining numerical stability. 5 Experiments In our experiments, we aim (1) to compare the standard translation model training method with the method based on weighted alignment matrices; and (2) to contrast different approches to populate the matrices with link posterior probabilities. For this purpose we build several phrasebased, Arabic to English, translation systems us309 ing Moses1 in its default configuration. In order to tune the parameters of the translation systems, Minimum Error-Rate Training (Och, 2003) is applied on the development corpus, for which we used the NIST MT’06 evaluation’s test set, containing 1,797 Arabic sentences (46K words) with four English references (53K words). The performance of each system is assessed by calculating the multi-reference BLEU on NIST MT’08 evaluation’s test set, which contains 1,360 Arabic sentences (43K words), each with four references (53K words). For training the various models used by the translation systems, we select a subset of the LDC resources made available by the NIST MT’09 constrained track2 . In order to validate the obtained results on tra"
2011.eamt-1.41,2010.amta-papers.18,1,0.92306,"e authors of (Liu et al., 2009) estimate link probabilities by calculating relative frequencies over a list of N-best alignments produced by generative models, and show some improvements in translation quality. However, using small N-best lists as samples is known to yield poor estimates of the alignment posteriors, as these lists usually contain too few variations. In this paper, we argue that better estimation of alignment probabilities helps achieving clearer improvements. Our solution is to directly model the weighted alignment matrices using a discriminative aligner (Ayan and Dorr, 2006; Tomeh et al., 2010). The rest of this paper is organized as follows: we start in Section 2 by a recap of related work. Section 3 revisits the standard translation model procedures and its extensions to weighted matrices. Our own approach is introduced in Section 4 and experimentally contrasted to various baselines in 5. We discuss further prospects in Section 6. 2 Related work As pointed out in the introduction, the construction of the translation model starts with a word alignment step during which relevant phrase-pairs are extracted and their probabilities are estimated. Yet, word alignment outputs a probabili"
2011.eamt-1.41,P03-1041,0,0.151405,"ng et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this problem is described in (Deng and Byrne, 2005), where a word-to-phrase HMM alignment model is proposed, which constitutes a competitive model to IBM4. Under this model, the necessary statistics can be computed efficiently with the forward algorithm. The phrase pair induction procedure described in (Deng and Byrne, 2005), benefits from this efficiency to estimate a phrase-to-phrase posterior distribution, which is used further in the extraction and scoring of phrases. In (de Gispert et al., 201"
2011.eamt-1.41,2008.amta-papers.18,0,0.078552,"al researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2005), they do not compete with Model 4 in terms of performance. A solution to this prob"
2011.eamt-1.41,D07-1078,0,0.0278246,"istribution, which seems to adversely impact the quality of the resulting translation model. In fact, several researchers have shown that incorporating more information from the posterior distribution helps reducing the propagation of errors and improves performance. In (Mi and Huang, 2008), some gains are achieved by exploiting a packed forest, which compactly encodes exponentially many parses, to extract rules for a syntaxbased translation system, instead of using only 306 the 1-best tree. This compact representation has already been shown to be efficient and effective (Galley et al., 2006; Wang et al., 2007). Similarly, N-best alignments are used to extract phrase-pairs as in (Xue et al., 2006; Venugopal et al., 2008); in the latter, a probability distribution over N-best alignments and parses is used to generate posterior fractional counts for rules in a syntaxbased translation model. Due to the difficulty of computing statistics under IBM3 and IBM4 models, the previously described approaches use N-best alignments as samples to approximate word-to-word alignment posterior probabilities. While simpler models, such as HMM and IBM1, allow for such a computation (Brown et al., 1993; Venugopal et al."
2011.eamt-1.41,2006.amta-papers.2,0,\N,Missing
2011.eamt-1.41,2010.iwslt-papers.14,0,\N,Missing
2011.iwslt-evaluation.7,J04-2004,0,0.757535,"hods for adapting statistical models using both in-domain and out-of-domain data are actively sought and several proposals have been studied in the literature (see below). The IWSLT’11 “TED” task offers a nice test case for adaptation techniques, since the volume of talk data is, by far, outnumbered by the other sources of data, be they parallel or monolingual. LIMSI took part in the IWSLT 2011 TED task in the MT track for English to French with the intent to improve our understanding of adaptation techniques for SMT. Our submission is based on the n-gram based approach to Machine Translation [1, 2], a framework in which it is relatively simple to re-implement and compare various adaptation strategies. Several proposal have been put forward to adapt SMT systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the l"
2011.iwslt-evaluation.7,D10-1044,0,0.0409521,"anslation [1, 2], a framework in which it is relatively simple to re-implement and compare various adaptation strategies. Several proposal have been put forward to adapt SMT systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered va"
2011.iwslt-evaluation.7,W07-0717,0,0.0565975,"systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that"
2011.iwslt-evaluation.7,W07-0733,0,0.0438259,"systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that"
2011.iwslt-evaluation.7,2010.eamt-1.30,0,0.014856,"systems: in the typical situation where a small amount of indomain data is backed up by larger out-of-domain corpora, various ways to combine the two source of informations can be entertained. The most simple-minded approach is to pool all the available data into one single mixed-domain training corpus; carefully selecting the out-of-domain data based on their similarity with the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that"
2011.iwslt-evaluation.7,2008.iwslt-papers.6,0,0.0267923,"ith the in-domain texts, at the level of sentences [3], or at the level of phrases however proves to be more effective. Pooling can also be performed directly at the level of models using various mixture modeling strategies [4, 5, 6]. Depending on the available resources, this approach can be applied to the sole language or translation model, or to both models. In the less favorable case where only monolingual data is available, self-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that we reproduced in various past experiments [8], is that performing an ad-hoc linear combination of models seems to be more effective than tuning the weights of a log-linear model combination with MERT [9]. This finding seems to contradict the findings of [5]. We have found again the same effect, and try to provide some analysis for this unexpected behavior. Another co"
2011.iwslt-evaluation.7,P03-1021,0,0.126132,"elf-training techniques using an out-of-domain SMT system to build an artificial indomain parallel corpus have also delivered improved performance in several studies [7]. Following the study of [4], we have considered various ways to build mixture models. If all adaptation strategies were indeed useful, a rather paradoxical finding, that was already mentioned in the Foster et al’s study, and that we reproduced in various past experiments [8], is that performing an ad-hoc linear combination of models seems to be more effective than tuning the weights of a log-linear model combination with MERT [9]. This finding seems to contradict the findings of [5]. We have found again the same effect, and try to provide some analysis for this unexpected behavior. Another contribution of the paper is an empirical study of adaptation for Neural Network Language models, which was found here to improve the performance of the non-adapted models. The rest of the paper is organized as follows. In Sections 2 and 3, we describe our decoder, then the various sources of data that have been used to train our baseline systems. Section 4 presents the experimental results achieved during the development period whe"
2011.iwslt-evaluation.7,N04-4026,0,0.16466,"using the n-gram assumption: p(sJ1 , tI1 ) = K Y Figure 1: Tuple extraction from a sentence pair. p((s, t)k |(s, t)k−1 . . . (s, t)k−n+1 ), k=1 where s refers to a source symbol (resp. t for target) and (s, t)k to the k th tuple of the given bilingual sentence pair. It is worth noticing that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. In addition to the translation model, eleven feature functions are combined: a target-language model (see Section 3.2 for details); four lexicon models; two lexicalized reordering models [10] aiming at predicting the orientation of the next translation unit; a “weak” distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework [9] (Minimum Error R"
2011.iwslt-evaluation.7,P02-1040,0,0.0841673,"Missing"
2011.iwslt-evaluation.7,J92-4003,0,0.064522,"ower order models [18, 19]. All LMs except the one trained on the news corpora from 2010-2011 were first linearly interpolated. The associated coefficients were estimated so as to minimize the perplexity evaluated on dev2010-2011. The resulting LM and the 20102011 LM were finaly interpolated with newstest2008 as development data. This procedure aims to avoid overestimating the weight associated to the 2010-2011 LM. 3.4. The SOUL Model We give here a brief overview of the SOUL LM; refer to [17] for the complete training procedure. Following the classical work on distributed word representation [20], we assume that the output vocabulary is structured by a clustering tree, where each word belongs to only one class and its associated sub-classes. If wi denotes the i-th word in a sentence, the sequence c1:D (wi ) = c1 , . . . , cD encodes the path for the word wi in the clustering tree, with D the depth of the tree, cd (wi ) a class or sub-class assigned to wi , and cD (wi ) the leaf associated with wi (the word itself). The n-gram probability of wi given its history h can then be estimated as follows using the chain rule: P (wi |h) = P (c1 (wi )|h) D Y P (cd (wi )|h, c1:d−1 ) d=2 Figure 2"
2011.iwslt-evaluation.7,2011.iwslt-evaluation.1,0,0.0176011,"on and target language models that have already been adapted, when the SOUL model has only seen News data. Adapting the SOUL model with in-domain data does even slightly better: compared to the initial WMT baseline, the total accumulated improvement of adaptation is approximately +2.5 bleu points. Most of the results presented above have been obtained as the result of post-evaluation analyses. Our primary submission for the official TED task uses two separate bilingual models, as well as two separated target language models, and a non-adapted SOUL LM; the corresponding results are reported in [21]. 5. Conclusion In this paper, we presented LIMSI’s submission for IWSLT’2011 text translation task. These results were obtained using our in-house n-code system, which implements th n-gram based approach to SMT. One convenient feature of n-code is its ability to handle a arbitrary number of bilingual and target side language models, a facility which makes domain adaptation straightforward: it suffices to incorporate all the available in- and out-of-domain models in the loglinear combination and let the tuning procedure determine the best mixture weights. In particular, models computed for oth"
2011.iwslt-evaluation.7,W08-0310,1,0.93088,"Missing"
2011.iwslt-evaluation.7,J06-4004,0,\N,Missing
2011.iwslt-evaluation.7,W11-2135,1,\N,Missing
2011.iwslt-papers.10,N04-1021,0,0.430021,"mi 2749, 21027 Ispra, Italy marco.turchi@jrc.ec.europa.eu {nadi,wisniews,allauzen,yvon}@limsi.fr P (a, e|f ) = Z(f, λ)−1 exp Alexandre Allauzen † (1) i=1 where Z is a normalization constant and each hi is a feature function that decomposes over atomic phrase translations and λi is the corresponding feature weight which scales each feature’s contribution to the final model score. Typical features include language model, reordering and conditional phrase translation probabilities, word and phrase penalties and lexical weights. Additional feature functions are also investigated in the literature [1, 2]. For a PBSMT to produce a good translation, two conditions must be met: (i) good translations must exist in the search space of the decoder, and (ii) the model score must be (positively) correlated with translation quality. The first condition mainly depends on the coverage of the phrase translation candidates that are stored in the phrase table. A maximal coverage can be achieved by including all possible phrase pairs encountered in the training corpus: in this setting, the model scores are the only information used to select suitable translations during decoding. Given the sheer number of p"
2011.iwslt-papers.10,N09-1025,0,0.0418301,"mi 2749, 21027 Ispra, Italy marco.turchi@jrc.ec.europa.eu {nadi,wisniews,allauzen,yvon}@limsi.fr P (a, e|f ) = Z(f, λ)−1 exp Alexandre Allauzen † (1) i=1 where Z is a normalization constant and each hi is a feature function that decomposes over atomic phrase translations and λi is the corresponding feature weight which scales each feature’s contribution to the final model score. Typical features include language model, reordering and conditional phrase translation probabilities, word and phrase penalties and lexical weights. Additional feature functions are also investigated in the literature [1, 2]. For a PBSMT to produce a good translation, two conditions must be met: (i) good translations must exist in the search space of the decoder, and (ii) the model score must be (positively) correlated with translation quality. The first condition mainly depends on the coverage of the phrase translation candidates that are stored in the phrase table. A maximal coverage can be achieved by including all possible phrase pairs encountered in the training corpus: in this setting, the model scores are the only information used to select suitable translations during decoding. Given the sheer number of p"
2011.iwslt-papers.10,W07-0414,0,0.0186814,"s to find the best path, of which we employ two in our experiments. The first method is constrained decoding, as implemented in M OSES2 : the lattice is searched for the path with the highest model score that exactly matches the reference, and thus has a local BLEU score of one. However, if the reference is not attainable the sentence is discarded. The second method relaxes this constraint by using an oracle decoder that searches for the hypothesis that explicitly optimizes an approximation of the BLEU score at the sentence level as an objective. We implemented the lattice oracle decoder from [21], which, while being less conservative than constrained decoding (all source sentences are decoded), is agnostic about the model score which, therefore, needs to be optimized 1 δ (x) = w.Φ(x) − ρ, where w is the classifier h 2 http://www.statmt.org/moses/ weight vector. indirectly by pruning the lattice input of the decoder. 5. Feature Functions One of the main motivation of this work is to incorporate features into phrase pairs extraction, so as to smooth the conventional, alignment-based, phrase scores. We consider features from the literature [6, 16, 22, 4, 23], which evaluate various aspec"
2011.iwslt-papers.10,N03-1017,0,0.475537,"imal coverage can be achieved by including all possible phrase pairs encountered in the training corpus: in this setting, the model scores are the only information used to select suitable translations during decoding. Given the sheer number of possible phrase pairs, the vast majority of which are in fact irrelevant, taking all possible phrase pairs into account is impractical, and all methods for constructing phrase tables comprise a first step where the quality of each phrase pair is estimated, and where phrase pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, t"
2011.iwslt-papers.10,P03-1041,0,0.542925,"implemented the lattice oracle decoder from [21], which, while being less conservative than constrained decoding (all source sentences are decoded), is agnostic about the model score which, therefore, needs to be optimized 1 δ (x) = w.Φ(x) − ρ, where w is the classifier h 2 http://www.statmt.org/moses/ weight vector. indirectly by pruning the lattice input of the decoder. 5. Feature Functions One of the main motivation of this work is to incorporate features into phrase pairs extraction, so as to smooth the conventional, alignment-based, phrase scores. We consider features from the literature [6, 16, 22, 4, 23], which evaluate various aspects of the association between a source and a target chunk. Most features are data-driven and language-independent, based on statistical word alignment and language models. A small set of language-dependent morpho-syntactic features is also used. Weighted Alignment Matrix (WAM) feature is a score computed using discriminative Weighted Alignment Matrices [6] similar to the model-based phrase pair posterior metric described in [16]. Each cell in a weighted matrix [5] contains the posterior probability of aligning the corresponding source and target words. A phrase pa"
2011.iwslt-papers.10,D07-1103,0,0.376938,"ssible phrase pairs into account is impractical, and all methods for constructing phrase tables comprise a first step where the quality of each phrase pair is estimated, and where phrase pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translati"
2011.iwslt-papers.10,D09-1106,0,0.0879548,"pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, wher"
2011.iwslt-papers.10,2011.eamt-1.41,1,0.911426,"pairs that look too bad are filtered out. For this purpose, the standard approach [3] relies on binary scores deduced from the underlying word alignment and discards all phrase pairs that are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, wher"
2011.iwslt-papers.10,J93-2003,0,0.0146232,"at are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, where feature weights are set so as to optimize an automatic quality measure, such as BLEU, on held-out data via Minimum Error-Rate Training (MERT) [9]. As an attempt to improve these proced"
2011.iwslt-papers.10,W02-1018,0,0.0303579,"at are not consistent with it. This technique, however, does not let the user control the size of the resulting phrase table. More flexibility is gained by employing pruning techniques that need to be applied a posteriori as in [4], where a second scoring step is used to filter large phrase tables. An alternative is to use weighted alignment matrices, assigning each phrase a smooth score in the interval [0, 1] [5, 6]. Unlike computing the model score, which typically combines several features, the phrase extraction approach is mostly heuristic and relies primarily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, where feature weights are set so as to optimize an automatic quality measure, such as BLEU, on held-out data via Minimum Error-Rate Training (MERT) [9]. As an attempt to improve these proced"
2011.iwslt-papers.10,P03-1021,0,0.0895713,"marily on word or phrase alignments [7, 8]. These alignments are error-prone and they are obtained as the result of complex optimization programs maximizing an objective function (the likelihood of the training data) that correlates only indirectly with the translation quality. If the same can be said of the feature functions used in the model score, the combined model is however enhanced during tuning to better correlate with translation quality, where feature weights are set so as to optimize an automatic quality measure, such as BLEU, on held-out data via Minimum Error-Rate Training (MERT) [9]. As an attempt to improve these procedures, we study in this paper novel extraction and scoring procedures that : (1) can straightforwardly handle arbitrary feature functions; (2) have a direct relationship to translation quality; and (3) give the user a finer control over the size of the phrase table. This study has both practical and methodological implications. From a practical perspective, the scenario we consider is the use of a small set of parallel sentences, from which we would like to extract as much phrases as possible, so as to ensure the larger possible coverage. In this setting,"
2011.iwslt-papers.10,N07-2053,0,0.258182,"obtain optimal solution, each optimization iteration that involves training a standard phrase table with parameters {λk , τ }, should tune its weights with MERT, which is expensive and hence omitted in their experiments. A similar model is used in [22] to add features to extraction, without any parameter tuning. Our work is similar in respect of incorporating additional features to extraction, whereas our formulation of the problem in the supervised classification framework, unlike [16], allows much less expensive incorporation of the translation quality measure, which is ignored in [22]. In [27] the standard features in the log-linear translation model tuned with MERT is used to score phrase pairs already existing in the phrase table and employ a competitive linking algorithm to keep the best one-to-one phrase matching while discarding the rest. Contrarily tou our approach, feature weights selected by MERT, although directly optimizing translation quality, they are learned for a given phrase table and do not generalize to unseen phrase pairs. In [28], the whole set of phrasal translation rules that should be extracted from a sentence-pair is predicted at once instead of predicting on"
2011.iwslt-papers.10,P10-1147,0,0.0234293,"ation framework, unlike [16], allows much less expensive incorporation of the translation quality measure, which is ignored in [22]. In [27] the standard features in the log-linear translation model tuned with MERT is used to score phrase pairs already existing in the phrase table and employ a competitive linking algorithm to keep the best one-to-one phrase matching while discarding the rest. Contrarily tou our approach, feature weights selected by MERT, although directly optimizing translation quality, they are learned for a given phrase table and do not generalize to unseen phrase pairs. In [28], the whole set of phrasal translation rules that should be extracted from a sentence-pair is predicted at once instead of predicting one phrase pair at a time. Word and phrase level features are incorporated into a discriminative model for extraction. Manually annotated word alignments are used to automatically obtain training extraction sets, whereas we use the oracle decoder. Another related line of research is phrase table pruning, which is carried out by first assigning scores to phrase pairs, by ways of statistical significance tests [4, 17], or by computing the decoder usage statistics"
2011.iwslt-papers.10,N07-2006,0,0.249995,", the whole set of phrasal translation rules that should be extracted from a sentence-pair is predicted at once instead of predicting one phrase pair at a time. Word and phrase level features are incorporated into a discriminative model for extraction. Manually annotated word alignments are used to automatically obtain training extraction sets, whereas we use the oracle decoder. Another related line of research is phrase table pruning, which is carried out by first assigning scores to phrase pairs, by ways of statistical significance tests [4, 17], or by computing the decoder usage statistics [29]. Our method takes advantage of different pruning criteria and integrates them into the filtering procedure. The introduction of the translation process into the definition of useful phrase pairs has also been investigated in the literature. 8. Conclusions and Future Work In this paper we presented a novel translation quality informed procedure for both extraction and scoring of phrase pairs. The model at the center of our procedure combines arbitrary features to assess phrase quality. It is parametrized with a threshold that allows improved control over the size of the resulting phrase table,"
2011.iwslt-papers.10,2010.amta-papers.28,0,0.210183,"equire to examine all the (nonoptimal) derivations of our test data, which is clearly unrealistic. A nice walk-around is to use single-class classification [10] techniques, which aim at learning concepts in the absence of counter examples, by distinguishing one class of (positive) instances from all other possible instances. Such techniques can handle arbitrary feature functions to represent candidate phrase pairs, thus making the extraction procedure more robust to alignment errors. A useful by-product of the model is the computation of an accuracy-based feature, analogous to the proposal in [11]. In short, our main contribution can be viewed as a novel translation quality informed procedure for both extraction and scoring of phrase pairs. The rest of the paper is organized as follows. In Section 2, we motivate the formulation of phrase pair extraction as a single-class classification problem and describe a practical extraction pipeline. The One-Class SVM (OC-SVM) [12] and the Mapping Convergence (MC) [13] algorithms, which are used to train the single-class classifier are presented in Section 3. In Section 4, we describe the oracle decoder used to label positive examples. Our feature"
2011.iwslt-papers.10,P10-1049,0,0.0397474,"res. 266 Source phrase: األوضاع الراهنة 37.3 Standard Z-scores SCC BLEU 36.8 36.3 F: STD 35.8 STD WAM F: STD+SCC 35.3 0 0.01 0.02 0.03 0.04 0.05 Fraction of selected unlabeled data (U - P) Candidate target translations Figure 5: BLEU: SCC scores as a new feature Figure 6: Z-scores: comparison of different scoring methods construct high precision phrase tables with the best phrase pairs, recall oriented phrase tables require more sophisticated decision procedures to retrieve good translations in the large set of candidates that are difficult to distinguish and ignored by standard methods. In [30] an oracle decoder is used to compute forced phrasal alignment, that are then used in a leaving-one-out smoothing technique, which results in a better estimation of translation probabilities. In [11] an oracle decoder is used to identify the best hypothesis in the n-best list output of the decoder and use an average edit-distance between phrase pairs occurring in the oracle hypothesis and other phrase pairs in other hypothesis in the n-best list, to compute a translation quality-based feature that is added to the phrase table. Unlike these methods, our procedure applies the oracle decoder to t"
2011.iwslt-papers.10,D09-1039,0,0.0202296,"n to learning from positive examples, exploits unlabeled data to improve the accuracy of the classifier. In step (4), the best classifier, output of the previous step, is applied to the unlabeled phrase pairs (U − P ), estimating to what extent they resemble the positive samples, and which ones ought to be extracted. The distance to the decision boundary (the hyperplane in the SVM feature space) is interpreted as a confidence measure, and used for two purposes: it is thresholded to extract phrase pairs; and injected into the final phrase table as an accuracy-based feature function, similar to [11, 15]. Final phrase table contains all phrase pairs labeled as positive either by the oracle decoder or by the learned classifier. Any subset of the calculated features, in addition to the standard phrase translation probabilities (normalized frequencies) can be used to score phrase pairs in the output phrase table. Training phrase translation model needs to address precision and recall issues, following an information retrieval scheme [16]. High precision requires that extracted phrase pairs are accurate, while high recall seeks to increase coverage by extracting as much valid phrase pairs as poss"
2011.iwslt-papers.10,P08-1010,0,0.586115,"used for two purposes: it is thresholded to extract phrase pairs; and injected into the final phrase table as an accuracy-based feature function, similar to [11, 15]. Final phrase table contains all phrase pairs labeled as positive either by the oracle decoder or by the learned classifier. Any subset of the calculated features, in addition to the standard phrase translation probabilities (normalized frequencies) can be used to score phrase pairs in the output phrase table. Training phrase translation model needs to address precision and recall issues, following an information retrieval scheme [16]. High precision requires that extracted phrase pairs are accurate, while high recall seeks to increase coverage by extracting as much valid phrase pairs as possible. Precision of standard phrase tables can be improved by filtering out most of the entries, using some statistical significance test [4, 17]. On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors [6]. The algorithm presented here attempts to circumvent alignment errors and increase accuracy by integrating multiple features and combining them discriminatively. A"
2011.iwslt-papers.10,2009.mtsummit-papers.17,1,0.874158,"of the calculated features, in addition to the standard phrase translation probabilities (normalized frequencies) can be used to score phrase pairs in the output phrase table. Training phrase translation model needs to address precision and recall issues, following an information retrieval scheme [16]. High precision requires that extracted phrase pairs are accurate, while high recall seeks to increase coverage by extracting as much valid phrase pairs as possible. Precision of standard phrase tables can be improved by filtering out most of the entries, using some statistical significance test [4, 17]. On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors [6]. The algorithm presented here attempts to circumvent alignment errors and increase accuracy by integrating multiple features and combining them discriminatively. At the same time, the threshold on the classifier score presents a control point over the balance between precision and recall, and introduces an additional parameter that can be tuned via grid search, for an optimal performance on a specific translation task. 262 3. Learning the Single-Class Classifier 3"
2011.jeptalnrecital-long.37,P89-1018,0,0.0143928,"Missing"
2011.jeptalnrecital-long.37,J93-2003,0,0.0424327,"Missing"
2011.jeptalnrecital-long.37,J07-2003,0,0.147443,"Missing"
2011.jeptalnrecital-long.37,D10-1053,0,0.0327985,"Missing"
2011.jeptalnrecital-long.37,W06-3105,0,0.0435905,"Missing"
2011.jeptalnrecital-long.37,P08-2007,0,0.0275874,"Missing"
2011.jeptalnrecital-long.37,H05-1022,0,0.0442315,"Missing"
2011.jeptalnrecital-long.37,N07-2007,0,0.0538593,"Missing"
2011.jeptalnrecital-long.37,P06-1121,0,0.104305,"Missing"
2011.jeptalnrecital-long.37,W08-0509,0,0.0323651,"Missing"
2011.jeptalnrecital-long.37,J10-3007,0,0.0303273,"Missing"
2011.jeptalnrecital-long.37,P07-2045,0,0.00753602,"Missing"
2011.jeptalnrecital-long.37,N03-1017,0,0.0408729,"Missing"
2011.jeptalnrecital-long.37,P10-1052,1,0.864066,"Missing"
2011.jeptalnrecital-long.37,2010.iwslt-papers.14,0,0.0214156,"Missing"
2011.jeptalnrecital-long.37,D09-1106,0,0.0297969,"Missing"
2011.jeptalnrecital-long.37,D08-1022,0,0.0391956,"Missing"
2011.jeptalnrecital-long.37,J03-1002,0,0.0174968,"Missing"
2011.jeptalnrecital-long.37,P02-1040,0,0.0830726,"Missing"
2011.jeptalnrecital-long.37,2010.amta-papers.18,1,0.848277,"Missing"
2011.jeptalnrecital-long.37,P03-1041,0,0.0578449,"Missing"
2011.jeptalnrecital-long.37,2008.amta-papers.18,0,0.031129,"Missing"
2011.jeptalnrecital-long.37,C96-2141,0,0.461889,"Missing"
2011.jeptalnrecital-long.37,D07-1078,0,0.0573657,"Missing"
2011.jeptalnrecital-long.37,P10-1049,0,0.028198,"Missing"
2011.jeptalnrecital-long.37,2002.tmi-tutorials.2,0,0.067176,"Missing"
2014.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.822181,"to adapt standard MT systems to ASR output. Finally, the impact of re-scoring n-best translation hypotheses using SOUL models is presented in the closing section. 1 https://www.ted.com/ 106 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 dataset dev2010 tst2010 WER (del., ins.) 15.0 (4.0, 3.5) 12.7 (3.3, 2.7) Table 1: Case-insensitive recognition results on the 2010 dev and tst data, scored using sclite. LM. The first decoding pass is carried out with a modified version of our 2011 Quaero system for broadcast data in English [8, 9] in which a language model trained on the provided ASR texts including the IWSLT14 TED LM transcriptions (3.2M words) was interpolated with the baseline 78k-word language model. The first decoding pass is done in 1xRT. The acoustic models in the first pass were trained on the data distributed in Quaero as well as on data from other sources from previous European or national projects and from the LDC. All acoustic and other language model training data predate December 31, 2010. The Euronews data provided by the organizers was not used. The second pass decoding used the same interpolated langua"
2014.iwslt-evaluation.15,J04-2004,0,0.0584651,"model training data predate December 31, 2010. The Euronews data provided by the organizers was not used. The second pass decoding used the same interpolated language model with acoustic models trained only on 180 hours of transcribed TED talks predating December 31, 2010 to better target the TED data. The case-insensitive recognition results on the 2010 dev and tst data are given in Table 1 scoring with the NIST sclite scoring using the provided stm and no glm. 3. MT systems: adaptation to speech data 3.1. Machine Translation with N-code N CODE implements the bilingual n-gram approach to SMT [10, 11, 12] that is closely related to the standard phrase-based approach [13]. In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bil"
2014.iwslt-evaluation.15,2002.tmi-tutorials.2,0,0.045489,"y the organizers was not used. The second pass decoding used the same interpolated language model with acoustic models trained only on 180 hours of transcribed TED talks predating December 31, 2010 to better target the TED data. The case-insensitive recognition results on the 2010 dev and tst data are given in Table 1 scoring with the NIST sclite scoring using the provided stm and no glm. 3. MT systems: adaptation to speech data 3.1. Machine Translation with N-code N CODE implements the bilingual n-gram approach to SMT [10, 11, 12] that is closely related to the standard phrase-based approach [13]. In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bilingual units called tuples. The best translation is selected by max"
2014.iwslt-evaluation.15,N04-4026,0,0.0177956,"tuples. The best translation is selected by maximizing a linear combination of feature functions using the following inference rule: e∗ = argmax e,a K X λk fk (f , e, a), (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ) and where a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target n-gram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models [14, 15] aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuplebonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match the target word order by unfolding the word alignments [12]. Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved [11] and n-gram translation models are then estimated over the training corpus composed of tupl"
2014.iwslt-evaluation.15,W08-0310,1,0.802984,"peech (POS), rather than surface word forms, to increase their generalization power [12]. 3.2. MT baseline This section describes the MT systems trained on written material that served as a benchmark for the succeeding experiments aiming at improving the translation quality for speech transcriptions. All the parallel corpora used in our translation systems have been preprocessed to remove excessively long sentences as well as sentences with an important length difference between the source and the target. The common preprocessing also included tokenization using the in-house tool described in [16] and word alignments using MGIZA++ [17] and Moses’s grow-diag-final-and heuristic for alignment symmetrization. All the MT systems developed in this study make use of the N-code system described above for translation model training and for decoding. Since the N-code system uses factored models, the training corpora have been tagged with part-of-speech (POS) labels using TreeTagger [18]. The target language model used discriminative log-linear interpolation approach to combine the model trained on TED monolingual data provided by the organizers and the bigger LM trained on WMT data (SRILM [19]"
2014.iwslt-evaluation.15,W08-0509,0,0.0156267,"orms, to increase their generalization power [12]. 3.2. MT baseline This section describes the MT systems trained on written material that served as a benchmark for the succeeding experiments aiming at improving the translation quality for speech transcriptions. All the parallel corpora used in our translation systems have been preprocessed to remove excessively long sentences as well as sentences with an important length difference between the source and the target. The common preprocessing also included tokenization using the in-house tool described in [16] and word alignments using MGIZA++ [17] and Moses’s grow-diag-final-and heuristic for alignment symmetrization. All the MT systems developed in this study make use of the N-code system described above for translation model training and for decoding. Since the N-code system uses factored models, the training corpora have been tagged with part-of-speech (POS) labels using TreeTagger [18]. The target language model used discriminative log-linear interpolation approach to combine the model trained on TED monolingual data provided by the organizers and the bigger LM trained on WMT data (SRILM [19] toolkit was used for both models). Our"
2014.iwslt-evaluation.15,W14-3302,0,0.031887,"some lines, removing comments between square brackets and between parentheses, etc. Those notes are added by transcribers in order to facilitate the understanding of the text by human readers, but are useless and even confusing in the context of automatic speech translation. 107 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 3.2.1. Impact of the out-of-domain corpora We tried to improve the performance of the baseline system trained on in-domain data only, by adding various bilingual corpora from the WMT Evaluation Campaign [20]: NewsCommentary (NC), Europarl (EPPS) and Gigaword filtered as in [21] (GIGA). All those models were tuned on the same manually transcribed development set (dev2010). As can be seen in Table 2, only the filtered Gigaword corpus actually helped improve the performance of the baseline system. In accordance with these results, we used only this corpus as the additional out-of-domain corpus for our final system. Table 2: Baseline MT experiments with written corpora. training corpora TED TED + NC + EPPS TED + NC + EPPS + GIGA TED + GIGA BLEU dev2010 test2010 28.8 33.2 29.5 33.0 29.6 34.0 29.7 34.4"
2014.iwslt-evaluation.15,2011.iwslt-papers.7,0,0.028841,"em, on the other hand, is expected to produce fully punctuated text as its output and is typically trained on punctuated sources. The performance on the manually transcribed test data, that does not contain any recognition errors, is nevertheless degraded dramatically if the punctuation is removed from the source side of the test (BLEU=25.5, as compared to BLEU=33.0 for the punctuated test, see Table 3). 108 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 Possible solutions to this problem have been explored, for example, in [22]. One solution is to build a new MT system based on the training corpora with unpunctuated source side: the system is thus trained to implicitly insert punctuation as part of the general translation process (implicit punctuation). Another solution is to produce automatic punctuation for the source language and to insert some punctuation marks to speech recognition output before translation (explicit punctuation in source): this approach has the advantage of allowing to keep the MT system unchanged. Our experiments with both approaches are shown in Table 4. We trained a new MT system unpunctuat"
2014.iwslt-evaluation.15,P06-2093,1,0.819533,"Missing"
2014.iwslt-evaluation.15,N12-1005,1,0.872164,"Missing"
2014.iwslt-evaluation.15,N12-1047,0,0.0643549,"Missing"
2014.iwslt-evaluation.15,2011.iwslt-evaluation.7,1,0.899941,"Missing"
2014.iwslt-papers.6,W07-0717,0,0.0553536,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,W09-0432,0,0.0614921,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,D11-1033,0,0.151975,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,E12-1055,0,0.102234,"omain data. To avoid the dilution of domain-specific knowledge, most approaches consider various kinds of data weighting schemes in order to balance the importance of in-domain vs out-of-domain data. In such adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding p"
2014.iwslt-papers.6,D07-1045,0,0.28546,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,N12-1005,1,0.601782,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,E14-1003,0,0.0172429,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,D14-1179,0,0.0491922,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,P14-1066,0,0.182463,"uch adaptation scenarios, the NLP component needs to be retrained, entirely or partly, to integrate these new samples, which can be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among lingu"
2014.iwslt-papers.6,P13-1045,0,0.351451,"an be very time consuming or even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among linguistic units. This lack of structure hinders the generalization power of statistical models and reduces their"
2014.iwslt-papers.6,D13-1176,0,0.0769434,"even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among linguistic units. This lack of structure hinders the generalization power of statistical models and reduces their ability to adapt to other domains. By c"
2014.iwslt-papers.6,P14-1129,0,0.0712479,"even unrealistic in many situations. This is especially problematic for SMT systems, that are typically made of several layers of statistical models. DA for SMT has therefore received considerable attention in the recent years (for instance [3, 4, 5, 6]). This situation is compounded when, as we do here, SMT systems rely on Continuous Space Language Models (CSLMs) or Translation Models (CSTMs), which have recently gained a lot of popularity [7, 8, 9, 10, 11, 12]. As demonstrated for many NLP tasks [13], such as language modelling [7, 14, 15, 16], syntactic parsing [17] and machine translation [8, 9, 18, 19], CSLMs and CSTMs can remedy to two well-know issues of statistical modelling for linguistic data. Typical statistical models use discrete random variables to represent the realization of words, phrases or phrase pairs. The corresponding parameter estimates are based on relative frequencies and are unreliable for rare events. Furthermore, the resulting representations ignore morphological, syntactic and semantic relationships that exist among linguistic units. This lack of structure hinders the generalization power of statistical models and reduces their ability to adapt to other domains. By c"
2014.iwslt-papers.6,D07-1080,0,0.853065,"licitly capture some similarity relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and anal"
2014.iwslt-papers.6,N12-1047,0,0.597667,"licitly capture some similarity relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and anal"
2014.iwslt-papers.6,D11-1125,0,0.701771,"relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and analyzes our main results. We final"
2014.iwslt-papers.6,P12-1002,0,0.24795,"relationships, thereby introducing some smoothing in the probability estimates. The adaptation of Continuous Models for SMT has thus far received little attention. We study here the following practical situation: a large scale, state-of-the-art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and analyzes our main results. We final"
2014.iwslt-papers.6,federico-etal-2012-iwslt,0,0.316459,"art SMT system is available and needs to be ported to a new domain, using a small in-domain parallel corpus. In this setting, our main contribution is the definition and evaluation of new loss functions, that aim at discriminatively adapting the CSTMs to the new data. These objective functions derive from both the max-margin [20, 21] and pair-wise ranking [22, 23] approaches. In our experiments, the baseline, out-of-domain system is preliminarily trained for the News translation task, and the CSTMs must be adapted to the lecture translation task as defined in recent IWSLT evaluation campaigns [24]. The rest of the paper is organized as follows. Section 2 briefly describes the model structure that will be used in our experiments. Section 3 proposes new discriminative loss functions on N -best lists, along with the corresponding adaptation algorithms. The next section gives details about our experimental conditions and analyzes our main results. We finally provide a short review of similar works both on Discriminative Machine Translation and on Continuous Space Translation Models, before concluding with some perspectives for future work. 2. Continuous space translation models This sectio"
2014.iwslt-papers.6,J06-4004,0,0.318812,"e pair is: P (s, t) = L Y i=1 P (ui |ui−1 i−n+1 ), (1) where ui−1 i−n+1 denotes the tuple sequence ui−n+1 , . . . , ui−1 . The complete model for a sentence pair thus involves latent variables that specify the reordering applied to the source sentence, as well as its segmentation into translation units. These latent variables define the derivation of the source sentence that generates the target sentence. They are omitted for the sake of clarity. During the training step, the segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments (see [25, 26] for details). During the inference step, the SMT decoder will compute and output the best derivation. In this model, the elementary units are bilingual pairs, which means that the underlying vocabulary, hence the number of parameters, can be quite large, even for small translation tasks. Due to data sparsity issues, such models face severe estimation problems. Equation (1) can therefore be factored by decomposing tuples in two (source and target) parts and in two equivalent ways: P (ui |ui−1 i−n+1 ) i−1 i−1 i−1 i−1 = P (ti |sii−n+1 , ti−n+1 )P (si |si−1 i−n+1 , ti−n+1 ) (2) i = P (si |ti−n+1"
2014.iwslt-papers.6,J92-4003,0,0.0936148,"otes an aligned sentence pair, where the source words are reordered. Each decomposition involves two bilingual conditional distributions that can also be decomposed at the level of words, using again the n-gram assumption. 2.2. Continuous translation modeling with SOUL The n-gram distributions described in Section 2.1 are defined over potentially large vocabularies. As proposed in [9], these distributions can be estimated using the SOUL model introduced in [27]. Following [28], the SOUL model combines the feed-forward neural network approach for n-gram models [7] with a class-based prediction [29]. Structuring the output layer with word-class information makes the estimation of distributions over the entire vocabulary computationally feasible. Neural network architectures are also interesting as they can easily handle larger contexts than typical n-gram models. In the SOUL architecture, enlarging the context mainly consists in increasing the size of the projection layer, which corresponds to a simple look-up operation. Increasing the context length at the input layer thus causes only a linear growth in complexity in the worst case [14]. 2.3. Training and initialization issues The word-"
2014.iwslt-papers.6,2012.iwslt-papers.3,0,0.145403,"Missing"
2014.iwslt-papers.6,D13-1140,0,0.182472,"Missing"
2014.iwslt-papers.6,P05-1012,0,0.155117,"nce. The basic feature functions used in this study are very similar to those used by standard phrase-based SMT systems (see [30] for instance). When reranking with a continuous space model, Fλ (.) is augmented to also include an additional feature denoted fθ (s, h). As explained in Section 2.2, fθ (s, h) typically As explained above, each hypothesis hi produced by the decoder is scored according to (4). Its quality can also be evaluated by the sentence-level approximation of the BLEU score sBLEU (hi ). Let h∗ denote the hypothesis with the best sentence BLEU score. A max-margin loss function [33, 34, 20] for estimating θ can then be formulated as follows: Lmm (θ, s) = −Gλ,θ (s, h∗ ) + max (Gλ,θ (s, hj ) + costα (hj )) , 1≤j≤N 2 The following parameters can be initialized given a source and target language monolingual models: the source and target word embeddings respectively, and the structured output layer’s structure. 3 See however [31, 32, 19] for early attempts to integrate Neural Network Translation Models within the decoder. (5)  where costα (hj ) = α sBLEU (h∗ ) − sBLEU (hj ) . The parameter α mitigates the contribution of the cost function 4 http://www.statmt.org/moses/ 194 Proceedin"
2014.iwslt-papers.6,W02-1001,0,0.126019,"he structured output layer’s structure. 3 See however [31, 32, 19] for early attempts to integrate Neural Network Translation Models within the decoder. (5)  where costα (hj ) = α sBLEU (h∗ ) − sBLEU (hj ) . The parameter α mitigates the contribution of the cost function 4 http://www.statmt.org/moses/ 194 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 to the objective function. When alpha &gt; 0, the objective defined in (5) is a general max-margin training criterion; taking α = 0 corresponds to the structured perceptron loss [35]. This objective function aims to discriminatively learn to give the highest model score to the hypothesis h∗ having the best sentence level BLEU. Moreover, the margin term enforces the scoring difference between h∗ and the rest of the N -best list to be greater than the margin. However, a source sentence can have, among the N -best list, several good translations that differ only slightly from the best hypothesis. The max-margin objective function defined above nevertheless considers that all hypotheses, except the best one, are wrong. The ranking-based approach defined below tries to correct"
2014.iwslt-papers.6,P02-1040,0,0.0899927,"continuous space translation models, i.e to adapt the parameters θ. The baseline and out-of-domain system is trained in the condition of the shared translation task of WMT 2013 evaluation campaign.5 This system includes CSTMs that will be used as starting points for adaptation. The official development and test sets respectively contain 934 and 1, 664 sentence pairs. Following [9], these sets are swapped, the tuning of the feature weights λ is carried out on 1, 664 sentences of the latter, while the final test is on 934 sentences of the former. Translations are evaluated using the BLEU score [36]. For a fair comparison, all BLEU scores reported are obtained after a tuning phase on the dev set, including the baseline system. For Algorithm 1, (θ, λ) are selected by maximizing the BLEU score on the dev set (line 7). 4.2. Baseline system and models The n-gram-based system used here is based on an open source implementation6 of the bilingual n-gram approach to Statistical Machine Translation [37]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using an n-gram model of (source, target) pairs as described in section 2.1. Training this mode"
2014.iwslt-papers.6,J04-2004,0,0.550335,"apped, the tuning of the feature weights λ is carried out on 1, 664 sentences of the latter, while the final test is on 934 sentences of the former. Translations are evaluated using the BLEU score [36]. For a fair comparison, all BLEU scores reported are obtained after a tuning phase on the dev set, including the baseline system. For Algorithm 1, (θ, λ) are selected by maximizing the BLEU score on the dev set (line 7). 4.2. Baseline system and models The n-gram-based system used here is based on an open source implementation6 of the bilingual n-gram approach to Statistical Machine Translation [37]. In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using an n-gram model of (source, target) pairs as described in section 2.1. Training this model requires to reorder source sentences so as to match the target word order. This is performed by a non-deterministic finite-state reordering model, which uses part-of-speech information generated by the TreeTagger to generalize reordering patterns beyond lexical regularities. In addition to the TM, fourteen feature functions are included that are similar to the standard phrase-based system: target-l"
2014.iwslt-papers.6,P13-1126,0,0.0203937,"d in-domain data, while in all other cases the baseline system only uses out-of-domain data. SMT system is entirely re-trained from scratch to integrate in-domain data (from word alignments to the large scale target language model), and all four CSTMs defined by (2) are adapted using the CLL criterion. This experiment shows that we can achieve slightly better performance by only adapting two CSTMs with the proposed objective function. 5. Related work Most recent works in domain adaptation for SMT focuses on the modification of the sufficient statistics required by conventional discrete models [3, 4, 38], or on data selection [5, 6]. Our work owes much to recent contributions in discriminative training and tuning of SMT systems. While perceptron-based learning has been first introduced in [39, 40], margin-based algorithms such as MIRA [20, 21] are nowadays considered as more efficient to train FeatureRich Translation systems. This property is especially relevant in our case, since we intend to learn a large set of parameters (θ). Another trend considers the optimization problem as ranking [41, 39, 22, 23]. Note that the ranking task corresponds to the integration of the CSTM that is actually"
2014.iwslt-papers.6,P06-1096,0,0.0387168,"rge scale target language model), and all four CSTMs defined by (2) are adapted using the CLL criterion. This experiment shows that we can achieve slightly better performance by only adapting two CSTMs with the proposed objective function. 5. Related work Most recent works in domain adaptation for SMT focuses on the modification of the sufficient statistics required by conventional discrete models [3, 4, 38], or on data selection [5, 6]. Our work owes much to recent contributions in discriminative training and tuning of SMT systems. While perceptron-based learning has been first introduced in [39, 40], margin-based algorithms such as MIRA [20, 21] are nowadays considered as more efficient to train FeatureRich Translation systems. This property is especially relevant in our case, since we intend to learn a large set of parameters (θ). Another trend considers the optimization problem as ranking [41, 39, 22, 23]. Note that the ranking task corresponds to the integration of the CSTM that is actually used for N -best reranking. In this work, the proposed objective functions borrow from these two lines of research to both adapt the CSTM (θ) and tune its contribution (λ) to the whole SMT system."
2014.iwslt-papers.6,N04-1023,0,0.0489942,"uses on the modification of the sufficient statistics required by conventional discrete models [3, 4, 38], or on data selection [5, 6]. Our work owes much to recent contributions in discriminative training and tuning of SMT systems. While perceptron-based learning has been first introduced in [39, 40], margin-based algorithms such as MIRA [20, 21] are nowadays considered as more efficient to train FeatureRich Translation systems. This property is especially relevant in our case, since we intend to learn a large set of parameters (θ). Another trend considers the optimization problem as ranking [41, 39, 22, 23]. Note that the ranking task corresponds to the integration of the CSTM that is actually used for N -best reranking. In this work, the proposed objective functions borrow from these two lines of research to both adapt the CSTM (θ) and tune its contribution (λ) to the whole SMT system. To the best of our knowledge, the most similar work on discriminative training or adaptation of neural network models is [12]. In this article, the authors propose to estimate the parameters of a neural network towards the expected BLEU score, while tuning λ by standard tools. Algorithm 1 is very similar to the o"
2014.iwslt-papers.6,P14-2023,0,0.100364,"this article, the authors propose to estimate the parameters of a neural network towards the expected BLEU score, while tuning λ by standard tools. Algorithm 1 is very similar to the optimization algorithm they describe, except that in our case, the feature weights λ are regularly updated for a better and tighter integration of the CSTM into the SMT system. Moreover, their proposed model only considers phrase pairs in isolation, while we use a probabilistic model of the joint distribution of sentence pairs. Expected BLEU training was also applied to recurrent neural network language model in [42]. In [13], the authors also introduce a ranking-type objective function that only aims to estimate word embeddings in a multitask-learning framework. Furthermore, [17] investigates the use of a large-margin criterion to train a recursive neural network for syntactic parsing. Interestingly, their model is also used to rerank N -best derivations generated by a conventional probabilistic context-free grammar. However, as showed by experimental results, the max-margin criterion alone is less adapted to machine translation. One explanation is that the N -best lists generated by the SMT system are n"
2014.iwslt-papers.6,N13-1048,0,0.21861,"Missing"
2014.iwslt-papers.6,D13-1106,0,0.12517,"Missing"
2015.jeptalnrecital-long.23,J04-2004,0,0.634087,"Missing"
2015.jeptalnrecital-long.23,N12-1047,0,0.150827,"Missing"
2015.jeptalnrecital-long.23,D14-1179,0,0.0687613,"Missing"
2015.jeptalnrecital-long.23,W02-1001,0,0.147491,"Missing"
2015.jeptalnrecital-long.23,P14-1129,0,0.116526,"Missing"
2015.jeptalnrecital-long.23,2014.iwslt-papers.6,1,0.781554,"Missing"
2015.jeptalnrecital-long.23,W14-3307,1,0.895942,"Missing"
2015.jeptalnrecital-long.23,federico-etal-2012-iwslt,0,0.0530621,"Missing"
2015.jeptalnrecital-long.23,D11-1125,0,0.196228,"Missing"
2015.jeptalnrecital-long.23,P12-1092,0,0.129908,"Missing"
2015.jeptalnrecital-long.23,D13-1176,0,0.10623,"Missing"
2015.jeptalnrecital-long.23,J10-4005,0,0.0560098,"Missing"
2015.jeptalnrecital-long.23,N12-1005,1,0.932129,"Missing"
2015.jeptalnrecital-long.23,J06-4004,0,0.523008,"Missing"
2015.jeptalnrecital-long.23,P05-1012,0,0.228448,"Missing"
2015.jeptalnrecital-long.23,C90-3038,0,0.54057,"Missing"
2015.jeptalnrecital-long.23,2012.iwslt-papers.3,0,0.3952,"Missing"
2015.jeptalnrecital-long.23,P02-1040,0,0.104932,"Missing"
2015.jeptalnrecital-long.23,C12-2104,0,0.0539265,"Missing"
2015.jeptalnrecital-long.23,D07-1045,0,0.562968,"Missing"
2015.jeptalnrecital-long.23,P13-1045,0,0.17715,"Missing"
2015.jeptalnrecital-long.23,P10-1040,0,0.0114571,"Missing"
2015.jeptalnrecital-long.23,D13-1140,0,0.16983,"Missing"
2015.jeptalnrecital-long.23,D07-1080,0,0.334531,"Missing"
2015.jeptalnrecital-long.23,P13-1017,0,0.27926,"Missing"
2015.jeptalnrecital-long.23,2002.tmi-tutorials.2,0,0.48302,"Missing"
2015.jeptalnrecital-long.4,N10-1066,0,0.0751533,"Missing"
2015.jeptalnrecital-long.4,P05-1022,0,0.0847008,"Missing"
2015.jeptalnrecital-long.4,J13-1005,0,0.0499864,"Missing"
2015.jeptalnrecital-long.4,P08-1085,0,0.0604285,"Missing"
2015.jeptalnrecital-long.4,A00-2013,0,0.166506,"Missing"
2015.jeptalnrecital-long.4,J00-4006,0,0.0394026,"Missing"
2015.jeptalnrecital-long.4,D12-1127,0,0.0294932,"Missing"
2015.jeptalnrecital-long.4,J94-2001,0,0.681755,"Missing"
2015.jeptalnrecital-long.4,C14-1110,0,0.0433613,"Missing"
2015.jeptalnrecital-long.4,D13-1032,0,0.0356679,"Missing"
2015.jeptalnrecital-long.4,W96-0213,0,0.79269,"Missing"
2015.jeptalnrecital-long.4,P09-1057,0,0.0433144,"Missing"
2015.jeptalnrecital-long.4,P14-2043,0,0.0337427,"Missing"
2015.jeptalnrecital-long.4,P05-1044,0,0.0758103,"Missing"
2015.jeptalnrecital-long.4,H05-1060,0,0.0593043,"Missing"
2015.jeptalnrecital-long.4,Q13-1001,0,0.0354823,"Missing"
2015.jeptalnrecital-long.4,C12-1170,0,0.0540179,"Missing"
2015.jeptalnrecital-long.4,D14-1187,1,0.887622,"Missing"
2016.jeptalnrecital-long.16,J90-2002,0,0.901275,"Missing"
2016.jeptalnrecital-long.16,C12-1039,0,0.0640547,"Missing"
2016.jeptalnrecital-long.16,J01-2001,0,0.407438,"Missing"
2016.jeptalnrecital-long.16,H05-1085,0,0.0756639,"Missing"
2016.jeptalnrecital-long.16,N04-4015,0,0.073887,"Missing"
2016.jeptalnrecital-long.16,Q15-1012,0,0.0347121,"Missing"
2016.jeptalnrecital-long.16,Q13-1021,0,0.041249,"Missing"
2016.jeptalnrecital-long.16,P08-1084,0,0.061609,"Missing"
2016.jeptalnrecital-long.16,P06-1124,0,0.149894,"Missing"
2016.jeptalnrecital-long.16,P15-1171,0,0.0618043,"Missing"
2017.jeptalnrecital-court.17,W02-1002,0,0.116147,"Missing"
2017.jeptalnrecital-court.17,W16-3905,0,0.0323013,"Missing"
2017.jeptalnrecital-court.17,2015.jeptalnrecital-long.4,1,0.852163,"Missing"
2017.jeptalnrecital-court.17,2015.jeptalnrecital-court.29,0,0.103588,"Missing"
2017.jeptalnrecital-court.17,C12-1149,0,0.111228,"Missing"
2017.jeptalnrecital-court.17,W13-4917,0,0.0214041,"Missing"
2017.jeptalnrecital-court.17,L16-1680,0,0.0514621,"Missing"
2017.jeptalnrecital-court.17,W11-0328,0,0.0411679,"Missing"
2017.jeptalnrecital-court.17,D13-1117,0,0.0567552,"Missing"
2017.jeptalnrecital-court.17,D14-1187,1,0.727723,"Missing"
2017.jeptalnrecital-court.17,F14-1016,1,0.734185,"Missing"
2017.jeptalnrecital-long.3,D15-1041,0,0.0609068,"Missing"
2017.jeptalnrecital-long.3,P16-1186,0,0.0359439,"Missing"
2017.jeptalnrecital-long.3,N12-1047,0,0.0692171,"Missing"
2017.jeptalnrecital-long.3,J81-4005,0,0.759957,"Missing"
2017.jeptalnrecital-long.3,P14-1129,0,0.0586615,"Missing"
2017.jeptalnrecital-long.3,N16-1030,0,0.103176,"Missing"
2017.jeptalnrecital-long.3,N12-1005,1,0.825803,"Missing"
2017.jeptalnrecital-long.3,D15-1176,0,0.0709461,"Missing"
2017.jeptalnrecital-long.3,P16-1100,0,0.0365912,"Missing"
2017.jeptalnrecital-long.3,K15-1031,0,0.0372914,"Missing"
2017.jeptalnrecital-long.3,W15-3016,1,0.902307,"Missing"
2017.jeptalnrecital-long.3,P02-1040,0,0.0968632,"Missing"
2017.jeptalnrecital-long.3,P16-1162,0,0.109324,"Missing"
2017.jeptalnrecital-long.3,D13-1140,0,0.0422827,"Missing"
2018.jeptalnrecital-court.29,N15-1083,0,0.0325011,"Missing"
2018.jeptalnrecital-court.29,P16-1186,0,0.0312611,"Missing"
2018.jeptalnrecital-court.29,P14-1129,0,0.0647279,"Missing"
2018.jeptalnrecital-court.29,P15-1001,0,0.101297,"Missing"
2018.jeptalnrecital-court.29,D17-1198,0,0.0277294,"Missing"
2018.jeptalnrecital-court.29,D13-1140,0,0.0757565,"Missing"
2018.jeptalnrecital-court.29,N16-1145,0,0.0245688,"Missing"
2018.jeptalnrecital-court.34,W12-2205,0,0.061664,"Missing"
2018.jeptalnrecital-court.34,W12-2202,0,0.0480299,"Missing"
2018.jeptalnrecital-court.34,francois-etal-2014-flelex,0,0.0715058,"Missing"
2018.jeptalnrecital-court.34,F14-1009,0,0.0722236,"Missing"
2018.jeptalnrecital-court.34,S12-1066,0,0.0540236,"Missing"
2018.jeptalnrecital-court.34,D09-1094,0,0.0868457,"Missing"
2018.jeptalnrecital-court.34,P13-3015,0,0.0322297,"Missing"
2018.jeptalnrecital-court.34,S12-1046,0,0.0613916,"Missing"
2019.jeptalnrecital-court.16,N15-1184,0,0.0960502,"Missing"
2019.jeptalnrecital-court.16,D17-1030,0,0.0244308,"Missing"
2019.jeptalnrecital-court.16,W16-0503,0,0.0365571,"Missing"
2019.jeptalnrecital-court.16,E17-2068,0,0.0283837,"Missing"
2019.jeptalnrecital-court.16,W18-3408,0,0.0543004,"Missing"
2019.jeptalnrecital-court.16,W13-3512,0,0.13608,"Missing"
2019.jeptalnrecital-court.16,K16-1006,0,0.0205486,"Missing"
2019.jeptalnrecital-court.16,W12-3018,0,0.0244959,"Missing"
2019.jeptalnrecital-court.16,W18-2301,0,0.0611837,"Missing"
2019.jeptalnrecital-court.16,N19-1048,0,0.045986,"Missing"
2019.jeptalnrecital-long.1,D17-1118,0,0.0233716,"Missing"
2019.jeptalnrecital-long.1,P16-2009,0,0.0597491,"Missing"
2019.jeptalnrecital-long.1,W11-2508,0,0.082252,"Missing"
2019.jeptalnrecital-long.1,P16-1141,0,0.0420971,"Missing"
2019.jeptalnrecital-long.1,D18-1527,0,0.0213038,"Missing"
2019.jeptalnrecital-long.1,W14-2517,0,0.0478355,"Missing"
2019.jeptalnrecital-long.1,C18-1117,0,0.0232723,"Missing"
2019.jeptalnrecital-long.1,N18-1044,0,0.0614922,"Missing"
2019.jeptalnrecital-long.1,P17-2071,0,0.0336545,"Missing"
2020.finnlp-1.2,2020.lrec-1.592,0,0.0319447,"Missing"
2020.finnlp-1.2,N18-1202,0,0.0741645,"Missing"
2020.finnlp-1.2,P19-1072,0,0.0469083,"Missing"
2020.finnlp-1.2,D19-1007,0,0.0258404,"Missing"
2020.finnlp-1.2,W17-6804,0,0.0434814,"Missing"
2020.finnlp-1.2,D19-5103,0,0.0379255,"Missing"
2020.finnlp-1.2,N19-1423,0,0.0163741,"Missing"
2020.finnlp-1.2,Q16-1003,0,0.0437909,"Missing"
2020.finnlp-1.2,W11-2508,0,0.0292399,"Missing"
2020.finnlp-1.2,P16-1141,0,0.0382245,"Missing"
2020.finnlp-1.2,P19-1379,0,0.0216388,"Missing"
2020.finnlp-1.2,W14-2517,0,0.0537407,"Missing"
2020.jeptalnrecital-taln.26,D19-1572,0,0.0323304,"Missing"
2020.jeptalnrecital-taln.26,P18-1031,0,0.0541897,"Missing"
2020.jeptalnrecital-taln.26,P19-1340,0,0.0367789,"Missing"
2020.jeptalnrecital-taln.26,P18-1249,0,0.0395933,"Missing"
2020.jeptalnrecital-taln.26,P07-2045,0,0.0108736,"Missing"
2020.jeptalnrecital-taln.26,P10-1023,0,0.119362,"Missing"
2020.jeptalnrecital-taln.26,D14-1162,0,0.0892081,"Missing"
2020.jeptalnrecital-taln.26,N18-1202,0,0.0990203,"Missing"
2020.jeptalnrecital-taln.26,P10-1114,0,0.0829177,"Missing"
2020.jeptalnrecital-taln.26,D17-1039,0,0.0479498,"Missing"
2020.jeptalnrecital-taln.26,W13-4917,0,0.0975746,"112 500 paires de phrases annotées avec les étiquettes entailment, contradiction ou neutre. FLUE intègre la partie française de ce corpus. Analyse syntaxique et étiquetage morphosyntaxique Nous considérons deux tâches d’analyse syntaxique : analyse en constituants et en dépendances, ainsi que l’étiquetage morphosyntaxique. Pour cela, nous utilisons le French Treebank (Abeillé et al., 2003), une collection de phrases du Monde annotées manuellement en constituants et dépendances syntaxiques. Nous utilisons la version de ce corpus de la campagne d’évaluation SPMRL 2014 décrite par Seddah et al. (2013), qui contient 14759, 1235 et 2541 phrases pour respectivement l’entraînement, le développement et l’évaluation. 271 Désambiguïsation lexicale des verbes et des noms La désambiguïsation lexicale consiste à assigner un sens, parmi un inventaire donné, à des mots d’une phrase. Pour la désambiguïsation lexicale de verbes, nous utilisons les données de FrenchSemEval (Segonne et al., 2019). Il s’agit d’un corpus d’évaluation dont les occurrences de verbes ont été annotées manuellement avec les sens de Wiktionary. 10 Pour la désambiguïsation lexicale des noms, nous utilisons la partie française de l"
2020.jeptalnrecital-taln.26,W19-0422,1,0.888597,"Missing"
2020.jeptalnrecital-taln.26,P16-1162,0,0.146858,"Missing"
2020.jeptalnrecital-taln.26,tiedemann-2012-parallel,0,0.0825572,"Missing"
2020.jeptalnrecital-taln.26,2019.gwc-1.14,1,0.893471,"Missing"
2020.jeptalnrecital-taln.26,W18-5446,0,0.0474061,"Missing"
2020.jeptalnrecital-taln.26,N18-1101,0,0.0610743,"Missing"
2020.jeptalnrecital-taln.26,D19-1382,0,0.0395373,"Missing"
2020.jeptalnrecital-taln.26,N19-1131,0,0.0313441,"Missing"
2020.jeptalnrecital-taln.31,D19-5103,0,0.0442072,"Missing"
2020.jeptalnrecital-taln.31,W19-5504,0,0.0375019,"Missing"
2020.jeptalnrecital-taln.31,W14-2517,0,0.233226,"Missing"
2020.jeptalnrecital-taln.31,2020.lrec-1.302,1,0.812956,"Missing"
2020.jeptalnrecital-taln.31,2020.lrec-1.592,0,0.0575452,"Missing"
2020.jeptalnrecital-taln.31,N18-1202,0,0.125029,"Missing"
2020.jeptalnrecital-taln.31,W09-0214,0,0.117457,"Missing"
2020.jeptalnrecital-taln.31,P19-1072,0,0.0365119,"Missing"
2020.jeptalnrecital-taln.31,D19-1007,0,0.0428067,"Missing"
2020.jeptalnrecital-taln.31,W17-6804,0,0.0595376,"Missing"
2020.lrec-1.302,2020.osact-1.2,0,0.0214996,"QuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this app"
2020.lrec-1.302,Q19-1038,0,0.0214599,"19), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors o"
2020.lrec-1.302,W06-1615,0,0.0880109,"2 1 985 XNLI-FR Diverse genres 392 702 2 490 5 010 French Treebank Daily newspaper 14 759 1 235 2 541 FrenchSemEval Diverse genres 55 206 - 3 199 Noun Sense Disambiguation Diverse genres 818 262 - 1 445 Table 2: Descriptions of the datasets included in our FLUE benchmark. 4.1. Text Classification CLS The Cross Lingual Sentiment CLS (Prettenhofer and Stein, 2010) dataset consists of Amazon reviews for three product categories: books, DVD, and music in four languages: English, French, German, and Japanese. Each sample contains a review text and the associated rating from 1 to 5 stars. Following Blitzer et al. (2006) and Prettenhofer and Stein (2010), ratings with 3 stars are removed. Positive reviews have ratings higher than 3 and negative reviews are those rated lower than 3. There is one train and test set for each product category. The train and test sets are balanced, including around 1 000 positive and 1 000 negative reviews for a total of 2 000 reviews in each dataset. We take the French portion to create the binary text classification task in FLUE and report the accuracy on the test set. 4.2. Paraphrasing PAWS-X The Cross-lingual Adversarial Dataset for Paraphrase Identification PAWS-X (Yang et al"
2020.lrec-1.302,P17-1152,0,0.0617991,"Missing"
2020.lrec-1.302,D18-1269,0,0.0239485,"gement. The paraphrasing task is to identify whether the sentences in these pairs are semantically equivalent or not. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the Fr"
2020.lrec-1.302,P19-4007,0,0.0544476,"Missing"
2020.lrec-1.302,W13-4905,0,0.054327,"shared task organizers. Our word representations are a concatenation of word embeddings and tag embeddings learned together with the model parameters on the French Treebank data itself, and at most one of (fastText, CamemBERT, FlauBERTBASE , FlauBERTBASE , mBERT) word vector. As Dozat and Manning (2016), we use word and tag dropout (d = 0.5) on word and tag embeddings but without dropout on BERT representations. We performed a fairly comprehensive grid search on hyperparameters for each model tested. Results The results are reported in Table 7. The best published results in this shared task (Constant et al., 2013) were involving an ensemble of parsers with additional resources for modelling multi word expressions (MWE), typical of the French treebank annotations. The monolingual French BERT models (CamemBERT, FlauBERT) perform better and set the new state of the art on this dataset with a single parser and without specific modelling for MWEs. One can observe that both FlauBERT models perform marginally better than CamemBERT, while all of them outperform mBERT by a large margin. 2484 Model UAS LAS Best published (Constant et al., 2013) 89.19 85.86 No pre-training fastText pre-training mBERT CamemBERT Fl"
2020.lrec-1.302,2020.findings-emnlp.292,0,0.0289963,"ethods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages paral"
2020.lrec-1.302,N19-1423,0,0.620298,"is-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, c"
2020.lrec-1.302,eisele-chen-2010-multiun,0,0.103871,"Missing"
2020.lrec-1.302,D19-1572,0,0.0171919,"architecture, while ELMo adopts a bidirectional LSTM to build the final embedding for each input token from the concatenation of the left-to-right and rightto-left representations. Another fundamental difference lies in how each model can be tuned to different downstream tasks: ELMo delivers different word vectors that can be interpolated, whereas ULMFiT enables robust fine-tuning of the whole network w.r.t. the downstream tasks. The ability of fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP do"
2020.lrec-1.302,P18-1031,0,0.289639,"incent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified eval"
2020.lrec-1.302,P18-1249,0,0.141589,"Table 5. The results confirm the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001"
2020.lrec-1.302,P19-1340,0,0.0189308,"m the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001), batch size (8) and ign"
2020.lrec-1.302,P07-2045,0,0.0134554,"to extract the text or download them directly from their websites. The total size of the uncompressed text before preprocessing is 270 GB. More details can be found in Appendix A.1. Data preprocessing For all sub-corpora, we filtered out very short sentences as well as repetitive and nonmeaningful content such as telephone/fax numbers, email addresses, etc. For Common Crawl, which is our largest sub-corpus with 215 GB of raw text, we applied aggressive cleaning to reduce its size to 43.4 GB. All the data were Unicode-normalized in a consistent way before being tokenized using Moses tokenizer (Koehn et al., 2007). The resulting training corpus is 71 GB in size. Our code for downloading and preprocessing data is made publicly available.13 3.2. Models and Training Configurations Model architecture FlauBERT has the same model architecture as BERT (Devlin et al., 2019), which consists of a multi-layer bidirectional Transformer (Vaswani et al., 2017). Following Devlin et al. (2019), we propose two model sizes: • FlauBERTBASE : L = 12, H = 768, A = 12, • FlauBERTLARGE : L = 24, H = 1024, A = 16, where L, H and A respectively denote the number of Transformer blocks, the hidden size, and the number of selfatt"
2020.lrec-1.302,2005.mtsummit-papers.11,0,0.015061,"Missing"
2020.lrec-1.302,W19-5303,0,0.226301,"ding FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual emb"
2020.lrec-1.302,L16-1147,0,0.0701355,"Missing"
2020.lrec-1.302,H93-1061,0,0.0518853,"rt of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based"
2020.lrec-1.302,P10-1023,0,0.0572014,"(04-20-2018) openly available via Dbnary (S´erasset, 2012). For a given sense of a target key, the sense inventory offers a definition along with one or more examples. For this task, we considered the examples of the sense inventory as training examples and tested our model on the evaluation dataset. Noun Sense Disambiguation We propose a new challenging task for the WSD of French, based on the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from"
2020.lrec-1.302,S13-2040,0,0.0327036,"ated version to French provided in XNLI. Following Conneau et al. (2018), we report the test accuracy. 4.4. Parsing and Part-of-Speech Tagging Syntactic parsing consists in assigning a tree structure to a sentence in natural language. We perform parsing on the French Treebank (Abeill´e et al., 2003), a collection of sentences extracted from French daily newspaper Le Monde, and manually annotated with both constituency and dependency syntactic trees and part-of-speech tags. Specifically, we use the version of the corpus instantiated for the SPMRL 2013 shared task and described by Seddah et al. (2013). This version is provided with a standard split representing 14 759 sentences for the training corpus, and respectively 1 235 and 2 541 sentences for the development and evaluation sets. 4.5. Word Sense Disambiguation Tasks Word Sense Disambiguation (WSD) is a classification task which aims to predict the sense of words in a given context according to a specific sense inventory. We used two French WSD tasks: the FrenchSemEval task (Segonne et al., 2019), which targets verbs only, and a modified version of the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), whi"
2020.lrec-1.302,2020.findings-emnlp.92,0,0.0305509,"ls for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Prot"
2020.lrec-1.302,N19-4009,0,0.0205028,"nventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training da"
2020.lrec-1.302,D14-1162,0,0.0923659,"ding Evaluation), are shared to the research community for further reproducible experiments in French NLP. Keywords: FlauBERT, FLUE, BERT, Transformer, French, language model, pre-training, NLP benchmark, text classification, parsing, word sense disambiguation, natural language inference, paraphrase. 1. Introduction A recent game-changing contribution in Natural Language Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b),"
2020.lrec-1.302,N18-1202,0,0.806323,"-grenoble-alpes.fr {vincent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT a"
2020.lrec-1.302,P10-1114,0,0.0398144,"Missing"
2020.lrec-1.302,P18-2124,0,0.0233078,"f fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch ("
2020.lrec-1.302,D17-1039,0,0.213586,"uage Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), RoBERTa (Liu et al., 2019). Using these pre-trained models in a transfer learning fashion has shown to yield striking improvements across a wide range of NLP tasks. One can easily build state-of-the-art NLP systems thanks to the publicly available pre-trained weights, saving time, energy, and resources. As a consequence, unsupervised language model pre-training has be"
2020.lrec-1.302,W13-4917,0,0.0460081,"Missing"
2020.lrec-1.302,W19-0422,1,0.895578,"Missing"
2020.lrec-1.302,P16-1162,0,0.0200296,"(attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et al., 2007, Moses), as in XLM (Lample and Conneau, 2019), before the application of BPE. We use fastBPE,15 a very efficient implementation to extract the BPE units and encode the corpora. 15 2481 https://github.com/glample/fastBPE FlauBERTBASE is trained on 32 GPUs Nvidia V100 in 410 hours and FlauBERTLARGE is trained on 128 GPUs in 390 hours, both with the effective batch size of 8192 sequences. Finally, we summarize the differences between Fl"
2020.lrec-1.302,serasset-2012-dbnary,0,0.036519,"Missing"
2020.lrec-1.302,skadins-etal-2014-billions,0,0.0456176,"Missing"
2020.lrec-1.302,tiedemann-2012-parallel,0,0.180846,"peline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual embeddings was also proposed in (McCann et al., 2017), but in a supervised fashion as they u"
2020.lrec-1.302,W18-1819,0,0.025239,"RTLARGE : warmup steps of 30k, peak learning rate of 3e−4, β1 = 0.9, β2 = 0.98,  = 1e−6 and weight decay of 0.01. Training FlauBERTLARGE Training very deep Transformers is known to be susceptible to instability (Wang et al., 2019b; Nguyen and Salazar, 2019; Xu et al., 2019; Fan et al., 2019). Not surprisingly, we also observed this difficulty when training FlauBERTLARGE using the same configurations as BERTLARGE and RoBERTaLARGE , where divergence happened at an early stage. Several methods have been proposed to tackle this issue. For example, in an updated implementation of the Transformer (Vaswani et al., 2018), layer normalization is applied before each attention layer by default, rather than after each residual block as in the original implementation (Vaswani et al., 2017). These configurations are called pre-norm and post-norm, respectively. It was observed by Vaswani et al. (2018), and again confirmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et"
2020.lrec-1.302,L18-1166,1,0.830909,"French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training data and the evaluation data in the UFSAC format (Vial et al., 2018). 5. Experiments and Results In this section, we present FlauBERT fine-tuning results on the FLUE benchmark. We compare the performance of FlauBERT with Multilingual BERT (Devlin et al., 2019, mBERT) and CamemBERT (Martin et al., 2019) on all tasks. In addition, for each task we also include the best non-BERT model for comparison. We made use of the open source libraries (Lample and Conneau, 2019, XLM) and (Wolf et al., 2019, Transformers)"
2020.lrec-1.302,2019.gwc-1.14,1,0.892947,"Missing"
2020.lrec-1.302,W18-5446,0,0.470157,"escribe our methodology to build FlauBERT – French Language Understanding via Bidirectional Encoder Representations from Transformers, a French BERT1 model that outperforms multilingual/cross-lingual models in several downstream NLP tasks, under similar configurations. FlauBERT relies on freely available datasets and is made publicly available in different versions.2 For further reproducible experiments, we also provide the complete processing and training pipeline as well as a general benchmark for evaluating French NLP systems. This evaluation setup is similar to the popular GLUE benchmark (Wang et al., 2018), and is named FLUE (French Language Understanding Evaluation). 2. 2.1. Related Work Pre-trained Language Models Self-supervised3 pre-training on unlabeled text data was first proposed in the task of neural language modeling (Bengio et al., 2003; Collobert and Weston, 2008), where it was shown that a neural network trained to predict next word from prior words can learn useful embedding representations, called word embeddings (each word is represented by a fixed vector). These representations were shown to play an important role in NLP, yielding state-of-the-art performance on multiple tasks ("
2020.lrec-1.302,P19-1176,0,0.104394,"2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors of GLUE have therefore introduced SuperGLUE (Wang et al., 2019a): a new benchmark built on the principles of GLUE, including more challenging and diverse set of tasks. A Chinese version of GLUE9 is also developed to evaluate model performance in Chinese NLP tasks. As of now, we have not learned of any such benchmark for French. 3. Building FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse"
2020.lrec-1.302,N18-1101,0,0.0161936,"ot. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the French part of the XNLI corpus to form the development and test sets for the NLI task in FLUE. The train set is obta"
2020.lrec-1.302,D19-1382,0,0.0391611,"Missing"
2020.lrec-1.302,N19-1131,0,0.100143,"firmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et al. (2019) and Fan et al. (2019) who successfully trained architectures of more than 40 layers. The idea is to randomly drop a number of (attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et a"
2021.acl-long.100,2020.cl-3.2,0,0.0283815,"n-contextualised embeddings for this task (Schlechtweg et al., 2020). Semantic change across languages. While this topic is actively researched in the linguistic and sociology research communities (Boberg, 2012), it is fairly new in the NLP literature. Many authors apply diachronic embeddings models to more than one language (Hamilton et al., 2016; Schlechtweg et al., 2020). However, prior work comparing the evolution of word usage across languages is very limited. Some work studies variations between languages or dialects, without looking into the temporal dimension (Hovy and Purschke, 2018; Beinborn and Choenni, 2020). Uban et al. (2019) compare present meanings of cognate words across 5 Romance languages to differentiate true cognates from false friends and measure the divergence between languages. In a temporal fashion, Martinc et al. (2020a) study the evolution of 4 word pairs in an English-Slovenian corpus of newspaper articles. Finally, Frossard et al. (2020) propose a list of cognates for analysing the similarities in the evolution of English and French, along with a preliminary analysis focusing on the differences in word frequency over time. 3 Diachronic Words Embeddings Before presenting systems b"
2021.acl-long.100,P17-2094,0,0.069033,"stics and the 11th International Joint Conference on Natural Language Processing, pages 1247–1258 August 1–6, 2021. ©2021 Association for Computational Linguistics guage model (M-BERT, Devlin et al., 2019).1 We also propose an anchored-alignment strategy to tackle the bilingual setting for non-contextual embeddings. Then, we suggest a metric to measure the divergence of word usage between two languages, the bilingual divergence. Given the lack of a bilingual dataset annotated with semantic divergence, we generate a corpus of synthetic semantic drift across two languages using EuroSense (Delli Bovi et al., 2017), a sense-disambiguated and aligned bilingual corpus. To do so, we define a set of monolingual and bilingual semantic change scenarios and evaluate our different approaches on them. Finally, we apply our systems to newspaper corpora in two languages, English and French, covering the same time period, from 1987 to 2006. We classify all words of a bilingual lexicon into the scenarios defined for the synthetic drift generation. To sum up, we extend the most appropriate methods from the literature of diachronic semantic change to build a framework for the measure of semantic divergence across lang"
2021.acl-long.100,N19-1423,0,0.269356,"are comparable across both time and languages, and to detect and classify semantic divergence in a bilingual setting. We compare: (i) diachronic word embeddings, which allow static embeddings such as CBOW (Mikolov et al., 2013) to drift through time, and (ii) contextualised embeddings, relying on a pre-trained multilingual lan1247 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1247–1258 August 1–6, 2021. ©2021 Association for Computational Linguistics guage model (M-BERT, Devlin et al., 2019).1 We also propose an anchored-alignment strategy to tackle the bilingual setting for non-contextual embeddings. Then, we suggest a metric to measure the divergence of word usage between two languages, the bilingual divergence. Given the lack of a bilingual dataset annotated with semantic divergence, we generate a corpus of synthetic semantic drift across two languages using EuroSense (Delli Bovi et al., 2017), a sense-disambiguated and aligned bilingual corpus. To do so, we define a set of monolingual and bilingual semantic change scenarios and evaluate our different approaches on them. Final"
2021.acl-long.100,2020.lrec-1.107,0,0.170667,"tweg et al., 2020). However, prior work comparing the evolution of word usage across languages is very limited. Some work studies variations between languages or dialects, without looking into the temporal dimension (Hovy and Purschke, 2018; Beinborn and Choenni, 2020). Uban et al. (2019) compare present meanings of cognate words across 5 Romance languages to differentiate true cognates from false friends and measure the divergence between languages. In a temporal fashion, Martinc et al. (2020a) study the evolution of 4 word pairs in an English-Slovenian corpus of newspaper articles. Finally, Frossard et al. (2020) propose a list of cognates for analysing the similarities in the evolution of English and French, along with a preliminary analysis focusing on the differences in word frequency over time. 3 Diachronic Words Embeddings Before presenting systems based on contextualised embeddings, we introduce two methods using noncontextualised ones, as they are known to perform best for the task of semantic change detection (Schlechtweg et al., 2020). We use the continuous bag of words (CBOW) architecture of Word2Vec (Mikolov et al., 2013); we apply two different training methods to train it in a diachronic"
2021.acl-long.100,2020.acl-main.365,0,0.0315098,"Missing"
2021.acl-long.100,W11-2508,0,0.0452171,"s of a bilingual lexicon into the scenarios defined for the synthetic drift generation. To sum up, we extend the most appropriate methods from the literature of diachronic semantic change to build a framework for the measure of semantic divergence across languages (Sections 3 and 4), for which we propose a definition of the task, a measure of semantic divergence (Section 5), and a process to evaluate the presented methods (Section 6). 2 Related Work Diachronic embedding models. The first approaches to diachronic modeling were based on relative word frequencies and distributional similarities (Gulordava and Baroni, 2011). Following the generalisation of word embeddings, diachronic word embeddings models emerged (Tahmasebi et al., 2018). A first line of work, led by Kim et al. (2014), learns an embedding matrix on the first time slice of a temporal corpus, and incrementally fine-tune it at each time step. This method has the advantage of simplicity but face a greater sensitivity to noise (Shoemark et al., 2019; Kaiser et al., 2020). Another method, proposed by Hamilton et al. (2016) and Kulkarni et al. (2015), train word embeddings on each time slice independently and align the representation spaces to make th"
2021.acl-long.100,P16-1141,0,0.330562,"models. The first approaches to diachronic modeling were based on relative word frequencies and distributional similarities (Gulordava and Baroni, 2011). Following the generalisation of word embeddings, diachronic word embeddings models emerged (Tahmasebi et al., 2018). A first line of work, led by Kim et al. (2014), learns an embedding matrix on the first time slice of a temporal corpus, and incrementally fine-tune it at each time step. This method has the advantage of simplicity but face a greater sensitivity to noise (Shoemark et al., 2019; Kaiser et al., 2020). Another method, proposed by Hamilton et al. (2016) and Kulkarni et al. (2015), train word embeddings on each time slice independently and align the representation spaces to make the embeddings comparable. Finally, Rudolph and Blei (2018); Jawahar and Seddah (2019) and Bamler and Mandt (2017) define probabilistic models of word embeddings, able to capture the drifts by training embeddings jointly on all time slices. 1 Code is available at https://github.com/ smontariol/BilingualSemanticChange These methods average all the senses of a word into a unique vector at each time step. Pre-trained language models such as BERT (Devlin et al., 2019) all"
2021.acl-long.100,D18-1469,0,0.0239283,"still outperformed by non-contextualised embeddings for this task (Schlechtweg et al., 2020). Semantic change across languages. While this topic is actively researched in the linguistic and sociology research communities (Boberg, 2012), it is fairly new in the NLP literature. Many authors apply diachronic embeddings models to more than one language (Hamilton et al., 2016; Schlechtweg et al., 2020). However, prior work comparing the evolution of word usage across languages is very limited. Some work studies variations between languages or dialects, without looking into the temporal dimension (Hovy and Purschke, 2018; Beinborn and Choenni, 2020). Uban et al. (2019) compare present meanings of cognate words across 5 Romance languages to differentiate true cognates from false friends and measure the divergence between languages. In a temporal fashion, Martinc et al. (2020a) study the evolution of 4 word pairs in an English-Slovenian corpus of newspaper articles. Finally, Frossard et al. (2020) propose a list of cognates for analysing the similarities in the evolution of English and French, along with a preliminary analysis focusing on the differences in word frequency over time. 3 Diachronic Words Embedding"
2021.acl-long.100,W19-4705,0,0.0219862,"ic word embeddings models emerged (Tahmasebi et al., 2018). A first line of work, led by Kim et al. (2014), learns an embedding matrix on the first time slice of a temporal corpus, and incrementally fine-tune it at each time step. This method has the advantage of simplicity but face a greater sensitivity to noise (Shoemark et al., 2019; Kaiser et al., 2020). Another method, proposed by Hamilton et al. (2016) and Kulkarni et al. (2015), train word embeddings on each time slice independently and align the representation spaces to make the embeddings comparable. Finally, Rudolph and Blei (2018); Jawahar and Seddah (2019) and Bamler and Mandt (2017) define probabilistic models of word embeddings, able to capture the drifts by training embeddings jointly on all time slices. 1 Code is available at https://github.com/ smontariol/BilingualSemanticChange These methods average all the senses of a word into a unique vector at each time step. Pre-trained language models such as BERT (Devlin et al., 2019) allow each occurrence of a word to have a contextualised vector representation. These models, pre-trained on large datasets, improved the state of the art on numerous NLP tasks. Similarly, contextualised embeddings ca"
2021.acl-long.100,2020.semeval-1.8,0,0.0785305,"(Section 6). 2 Related Work Diachronic embedding models. The first approaches to diachronic modeling were based on relative word frequencies and distributional similarities (Gulordava and Baroni, 2011). Following the generalisation of word embeddings, diachronic word embeddings models emerged (Tahmasebi et al., 2018). A first line of work, led by Kim et al. (2014), learns an embedding matrix on the first time slice of a temporal corpus, and incrementally fine-tune it at each time step. This method has the advantage of simplicity but face a greater sensitivity to noise (Shoemark et al., 2019; Kaiser et al., 2020). Another method, proposed by Hamilton et al. (2016) and Kulkarni et al. (2015), train word embeddings on each time slice independently and align the representation spaces to make the embeddings comparable. Finally, Rudolph and Blei (2018); Jawahar and Seddah (2019) and Bamler and Mandt (2017) define probabilistic models of word embeddings, able to capture the drifts by training embeddings jointly on all time slices. 1 Code is available at https://github.com/ smontariol/BilingualSemanticChange These methods average all the senses of a word into a unique vector at each time step. Pre-trained la"
2021.acl-long.100,W14-2517,0,0.175799,"antic change to build a framework for the measure of semantic divergence across languages (Sections 3 and 4), for which we propose a definition of the task, a measure of semantic divergence (Section 5), and a process to evaluate the presented methods (Section 6). 2 Related Work Diachronic embedding models. The first approaches to diachronic modeling were based on relative word frequencies and distributional similarities (Gulordava and Baroni, 2011). Following the generalisation of word embeddings, diachronic word embeddings models emerged (Tahmasebi et al., 2018). A first line of work, led by Kim et al. (2014), learns an embedding matrix on the first time slice of a temporal corpus, and incrementally fine-tune it at each time step. This method has the advantage of simplicity but face a greater sensitivity to noise (Shoemark et al., 2019; Kaiser et al., 2020). Another method, proposed by Hamilton et al. (2016) and Kulkarni et al. (2015), train word embeddings on each time slice independently and align the representation spaces to make the embeddings comparable. Finally, Rudolph and Blei (2018); Jawahar and Seddah (2019) and Bamler and Mandt (2017) define probabilistic models of word embeddings, able"
2021.acl-long.100,C18-1117,0,0.0137995,"to detect the divergence in its usage and connotation between communities, one might draw spurious results when analysing texts of this period. Diachronic semantic change detection is an emerging field in Natural Language Processing, building upon the growing number of digitised texts with temporal metadata publicly available in various languages. It opens new perspectives of improvement for downstream tasks (using timeaware word representation for tasks ranging from text classification to information retrieval in temporal corpora) or for socio-linguistic and historical linguistics analysis (Kutuzov et al., 2018). The goal of this paper is to extend the analysis of lexical semantic change across two languages, aiming at estimating the degree of diachronic semantic divergence between a word and its translation across time in a bilingual corpus. We propose an experimental framework to learn word representations that are comparable across both time and languages, and to detect and classify semantic divergence in a bilingual setting. We compare: (i) diachronic word embeddings, which allow static embeddings such as CBOW (Mikolov et al., 2013) to drift through time, and (ii) contextualised embeddings, relyi"
2021.acl-long.100,2020.lrec-1.592,0,0.142225,"NLP literature. Many authors apply diachronic embeddings models to more than one language (Hamilton et al., 2016; Schlechtweg et al., 2020). However, prior work comparing the evolution of word usage across languages is very limited. Some work studies variations between languages or dialects, without looking into the temporal dimension (Hovy and Purschke, 2018; Beinborn and Choenni, 2020). Uban et al. (2019) compare present meanings of cognate words across 5 Romance languages to differentiate true cognates from false friends and measure the divergence between languages. In a temporal fashion, Martinc et al. (2020a) study the evolution of 4 word pairs in an English-Slovenian corpus of newspaper articles. Finally, Frossard et al. (2020) propose a list of cognates for analysing the similarities in the evolution of English and French, along with a preliminary analysis focusing on the differences in word frequency over time. 3 Diachronic Words Embeddings Before presenting systems based on contextualised embeddings, we introduce two methods using noncontextualised ones, as they are known to perform best for the task of semantic change detection (Schlechtweg et al., 2020). We use the continuous bag of words"
2021.acl-long.100,2020.semeval-1.6,1,0.834086,"Missing"
2021.acl-long.100,2021.naacl-main.369,1,0.685714,"ddings, able to capture the drifts by training embeddings jointly on all time slices. 1 Code is available at https://github.com/ smontariol/BilingualSemanticChange These methods average all the senses of a word into a unique vector at each time step. Pre-trained language models such as BERT (Devlin et al., 2019) allow each occurrence of a word to have a contextualised vector representation. These models, pre-trained on large datasets, improved the state of the art on numerous NLP tasks. Similarly, contextualised embeddings can be applied to semantic change detection (Giulianelli et al., 2020; Montariol et al., 2021) using several aggregation techniques to measure the degree of semantic change of a word from all its contextualised representations over time. However, these methods are still outperformed by non-contextualised embeddings for this task (Schlechtweg et al., 2020). Semantic change across languages. While this topic is actively researched in the linguistic and sociology research communities (Boberg, 2012), it is fairly new in the NLP literature. Many authors apply diachronic embeddings models to more than one language (Hamilton et al., 2016; Schlechtweg et al., 2020). However, prior work compari"
2021.acl-long.100,2020.lrec-1.706,0,0.0106215,"sentences using each of the two original words in the successive time slices of a temporal corpus (Rosenfeld and Erk, 2018; Shoemark et al., 2019). However, as advised by Schlechtweg and Schulte im Walde (2020), it is preferable to use the natural polysemy of words for the synthetic drift to be as close as possible to reality: instead of controlling the proportion of sentences containing two unrelated words merged as a pseudo-word, we use sentences containing several senses of a unique word. To this end, we need a bilingual sense-annotated corpus with consistent annotations between languages (Pasini and Camacho-Collados, 2020). The EuroSense corpus4 (Delli Bovi et al., 2017) is derived from the Europarl corpus, a large public corpus of proceedings of the European Parliament. It has a full and a refined version. We use the latter to build our synthetic corpus; it is half the size of the first one but more reliable. The framework BabelNet (Navigli and Ponzetto, 2012) is used for annotation. EuroSense contains parallel text in 21 European languages. We focus on the two languages with the highest amount of annotations in the refined corpus: English and French. An example of aligned sentences in these languages can be f"
2021.acl-long.100,W19-4725,0,0.0267778,"sequence of T embeddings ul in each language (for CBOW and m-BERT with averaging), or a (t) vector of T cluster distributions cl (for m-BERT with clustering). We compute the distance between representations: the cosine distance between noncontextual embeddings and the JSD between clus3 We define the cosine distance as (1 - cosine similarity). ters distributions. d(t1 , t2 , l1 , l2 ) =   cos(u(t1 ) , u(t2 ) ) l1 (averaging l2 or CBOW)  JSD(c(t1 ) , c(t2 ) ) (clustering) l1 l2 (1) In a monolingual setting, we use two metrics commonly used to measure the drifts of a word in each language (Rodina et al., 2019): the incremental drift, from each time slice to the next one, and the inceptive drift, from the beginning of the period to each time slice. We obtain drift vectors in RT −1 for each word in each language, by computing d(t1 , t2 , l, l). In a bilingual setting, drift measures can be computed for each word pair (one word and its translation). First, we compute the distance inside each word pair at each time step. We call it the bilingual (t) distance: sB = d(t, t, l1 , l2 ) for t = 1, 2, . . . , T . Second, the temporal drift of this distance is measured similarly to the monolingual drift, eith"
2021.acl-long.100,N18-1044,0,0.0251392,"actice in the literature of monolingual semantic change detection (Shoemark et al., 1250 2019; Schlechtweg and Schulte im Walde, 2020). It allows us to control exactly the shape and degree of semantic change in the corpus and thus gain a deeper understanding of the impact of each modeling choice. To create synthetic semantic change, common practice involve to merge two words that do not share a common sense, creating a pseudo-word; then, generate synthetic change by controlling the proportion of sentences using each of the two original words in the successive time slices of a temporal corpus (Rosenfeld and Erk, 2018; Shoemark et al., 2019). However, as advised by Schlechtweg and Schulte im Walde (2020), it is preferable to use the natural polysemy of words for the synthetic drift to be as close as possible to reality: instead of controlling the proportion of sentences containing two unrelated words merged as a pseudo-word, we use sentences containing several senses of a unique word. To this end, we need a bilingual sense-annotated corpus with consistent annotations between languages (Pasini and Camacho-Collados, 2020). The EuroSense corpus4 (Delli Bovi et al., 2017) is derived from the Europarl corpus, a"
2021.acl-long.100,P19-1072,0,0.0631883,"Missing"
2021.acl-long.100,2020.semeval-1.1,0,0.423968,"d language models such as BERT (Devlin et al., 2019) allow each occurrence of a word to have a contextualised vector representation. These models, pre-trained on large datasets, improved the state of the art on numerous NLP tasks. Similarly, contextualised embeddings can be applied to semantic change detection (Giulianelli et al., 2020; Montariol et al., 2021) using several aggregation techniques to measure the degree of semantic change of a word from all its contextualised representations over time. However, these methods are still outperformed by non-contextualised embeddings for this task (Schlechtweg et al., 2020). Semantic change across languages. While this topic is actively researched in the linguistic and sociology research communities (Boberg, 2012), it is fairly new in the NLP literature. Many authors apply diachronic embeddings models to more than one language (Hamilton et al., 2016; Schlechtweg et al., 2020). However, prior work comparing the evolution of word usage across languages is very limited. Some work studies variations between languages or dialects, without looking into the temporal dimension (Hovy and Purschke, 2018; Beinborn and Choenni, 2020). Uban et al. (2019) compare present mean"
2021.acl-long.100,D19-1007,0,0.195036,"e the presented methods (Section 6). 2 Related Work Diachronic embedding models. The first approaches to diachronic modeling were based on relative word frequencies and distributional similarities (Gulordava and Baroni, 2011). Following the generalisation of word embeddings, diachronic word embeddings models emerged (Tahmasebi et al., 2018). A first line of work, led by Kim et al. (2014), learns an embedding matrix on the first time slice of a temporal corpus, and incrementally fine-tune it at each time step. This method has the advantage of simplicity but face a greater sensitivity to noise (Shoemark et al., 2019; Kaiser et al., 2020). Another method, proposed by Hamilton et al. (2016) and Kulkarni et al. (2015), train word embeddings on each time slice independently and align the representation spaces to make the embeddings comparable. Finally, Rudolph and Blei (2018); Jawahar and Seddah (2019) and Bamler and Mandt (2017) define probabilistic models of word embeddings, able to capture the drifts by training embeddings jointly on all time slices. 1 Code is available at https://github.com/ smontariol/BilingualSemanticChange These methods average all the senses of a word into a unique vector at each tim"
2021.acl-long.100,R19-1139,0,0.0257183,"Missing"
2021.acl-long.100,W19-4720,0,0.0194006,"or this task (Schlechtweg et al., 2020). Semantic change across languages. While this topic is actively researched in the linguistic and sociology research communities (Boberg, 2012), it is fairly new in the NLP literature. Many authors apply diachronic embeddings models to more than one language (Hamilton et al., 2016; Schlechtweg et al., 2020). However, prior work comparing the evolution of word usage across languages is very limited. Some work studies variations between languages or dialects, without looking into the temporal dimension (Hovy and Purschke, 2018; Beinborn and Choenni, 2020). Uban et al. (2019) compare present meanings of cognate words across 5 Romance languages to differentiate true cognates from false friends and measure the divergence between languages. In a temporal fashion, Martinc et al. (2020a) study the evolution of 4 word pairs in an English-Slovenian corpus of newspaper articles. Finally, Frossard et al. (2020) propose a list of cognates for analysing the similarities in the evolution of English and French, along with a preliminary analysis focusing on the differences in word frequency over time. 3 Diachronic Words Embeddings Before presenting systems based on contextualis"
2021.acl-long.100,P15-1063,0,0.0587259,"Missing"
2021.jeptalnrecital-taln.7,P19-1044,0,0.0285091,"Missing"
2021.jeptalnrecital-taln.7,2020.acl-main.365,0,0.0370478,"Missing"
2021.jeptalnrecital-taln.7,2020.acl-main.51,0,0.0646712,"Missing"
2021.jeptalnrecital-taln.7,W11-2508,0,0.0810717,"Missing"
2021.jeptalnrecital-taln.7,P16-1141,0,0.0563023,"Missing"
2021.jeptalnrecital-taln.7,P19-1403,0,0.0404282,"Missing"
2021.jeptalnrecital-taln.7,W17-2705,0,0.0439959,"Missing"
2021.jeptalnrecital-taln.7,2020.lrec-1.592,0,0.0732816,"Missing"
2021.jeptalnrecital-taln.7,2021.naacl-main.369,1,0.622768,"Missing"
2021.jeptalnrecital-taln.7,P19-1072,0,0.0451721,"Missing"
2021.jeptalnrecital-taln.7,N18-2027,0,0.0355929,"Missing"
2021.jeptalnrecital-taln.7,D19-1007,0,0.0511268,"Missing"
allauzen-bonneau-maynard-2008-training,popovic-ney-2006-pos,0,\N,Missing
allauzen-bonneau-maynard-2008-training,paroubek-2000-language,0,\N,Missing
allauzen-bonneau-maynard-2008-training,J93-2004,0,\N,Missing
allauzen-bonneau-maynard-2008-training,W06-3125,0,\N,Missing
allauzen-bonneau-maynard-2008-training,W07-0409,1,\N,Missing
allauzen-bonneau-maynard-2008-training,D07-1091,0,\N,Missing
C18-1261,N15-1083,0,0.0364878,"Missing"
C18-1261,P16-1186,0,0.0383718,"Missing"
C18-1261,P14-1129,0,0.032427,"Missing"
C18-1261,P15-1001,0,0.113196,"Missing"
C18-1261,D17-1198,0,0.0258593,"Missing"
C18-1261,D13-1140,0,0.0736509,"Missing"
C18-1261,N16-1145,0,0.0239749,"Missing"
D10-1076,W09-0417,1,0.829036,"this article are evaluated on the Arabic to English NIST 2009 constrained task. For the continuous space language model, the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector in"
D10-1076,J92-4003,0,0.180781,"irical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of word-classes (Brown et al., 1992; Niesler, 1997), of generalized back-off strategies (Bilmes et al., 1997) or the explicit integration of morphological information in the random-forest model (Xu and Jelinek, 2004; Oparin et al., 2008). One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This 778 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778–788, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational L"
D10-1076,P96-1041,0,0.032208,"L Y P (wl |w1l−1 ) l=1 Modeling the joint distribution of several discrete random variables (such as words in a sentence) is Many approaches to this problem have been proposed over the last decades, the most widely used being back-off n-gram language models. n-gram models rely on a Markovian assumption, and despite this simplification, the maximum likelihood estimate (MLE) remains unreliable and tends to underestimate the probability of very rare n-grams, which are hardly observed even in huge corpora. Conventional smoothing techniques, such as KneserNey and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of wor"
D10-1076,P07-2045,0,0.0086703,"atallah Mesyats Langlois 1 1 vector init. 4160th 3651st 3487th 3378th 3558th best list is accordingly reordered to produce the final translations. The different language models discussed in this article are evaluated on the Arabic to English NIST 2009 constrained task. For the continuous space language model, the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. #"
D10-1076,J10-4005,0,0.0044806,"n the small vocabulary tasks occurred more than several hundreds times in the training corpus, which was more than sufficient to guide the model towards satisfactory projection matrices. This finally suggests that there still exists room for improvement if we can find more efficient initialization strategies than starting from one or several random points. 4.4 Statistical machine translation experiments As a last experiment, we compare the various models on a large scale machine translation task. Statistical language models are key component of current statistical machine translation systems (Koehn, 2010), where they both help disambiguate lexical choices in the target language and influence the choice of the right word ordering. The integration of a neural network language model in such a system is far from easy, given the computational cost of computing word probabilities, a task that is performed repeatedly during the search of the best translation. We then had to resort to a two pass decoding approach: the first pass uses a conventional back-off language model to produce a n-best list (the n most likely translations and their associated scores); in the second pass, the probability of the n"
D10-1076,H93-1021,0,0.0464304,"These two models differ only by the activation function of their hidden layer (linear for the LBL model and tangent hyperbolic for the standard model) and by their definition of the prediction space: for the LBL model, the context space and the prediction space are the same (R = Who , and thus H = m), while in the standard model, the prediction space is defined independently from the context space. This restriction drastically reduces the number of free parameters of the LBL model. It is finally noteworthy to outline the similarity of this model with standard maximum entropy language models (Lau et al., 1993; Rosenfeld, 1996). Let x denote the binary vector formed by stacking the (n-1) 1-of-V encodings of the history words; then the conditional probability distributions estimated in the model are proportional to exp F (x), where F is an affine transform of x. The main difference with MaxEnt language models are thus the restricted form of the feature functions, which only test one history word, and the particular representation of F , which is defined as: T F (x) = RWih R0 v + Rbih + bho where, as before, R0 is formed by concatenating (n − 1) copies of the projection matrix R. 2.3 Training and inf"
D10-1076,P03-1021,0,0.0834137,"the training data consists in the parallel corpus used to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector initialization standard model which achieves a 0.9 BLEU improvement. While this results is similar to the one o"
D10-1076,P02-1040,0,0.104017,"to train the translation model (previously described in section 3.1). The development data is again the 2006 NIST test set and the test data is the official 2008 NIST test set. Our system is built using the open-source Moses toolkit (Koehn et al., 2007) with default settings. To set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using Kneser-Ney smoothing described in (Allauzen et al., 2009). The weights used during the reranking are tuned using the Minimum Error Rate Training algorithm (Och, 2003). Performance is measured based on the BLEU (Papineni et al., 2002) scores, which are reported in Table 4. Table 4: BLEU scores on the NIST MT08 test set with different language models. Vc size all 10000 all Model baseline log bilinear standard iterative reinit. reinit. standard one vector init. # epochs 6 13 6 11 14 9 BLEU 37.8 38.2 38.3 38.4 38.4 38.6 38.7 All the experimented neural language models yield to a significant BLEU increase. The best result is obtained by the one vector initialization standard model which achieves a 0.9 BLEU improvement. While this results is similar to the one obtained with the standard model, the training time is reduced here"
D10-1076,P06-2093,0,0.69272,"Missing"
D10-1076,P06-1124,0,0.0109844,"f several discrete random variables (such as words in a sentence) is Many approaches to this problem have been proposed over the last decades, the most widely used being back-off n-gram language models. n-gram models rely on a Markovian assumption, and despite this simplification, the maximum likelihood estimate (MLE) remains unreliable and tends to underestimate the probability of very rare n-grams, which are hardly observed even in huge corpora. Conventional smoothing techniques, such as KneserNey and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. n-gram language models rely on a discrete space representation of the vocabulary, where each word is associated with a discrete index. In this model, the morphological, syntactic and semantic relationships which structure the lexicon are completely ignored, which negatively impact the generalization performance of the model. Various approaches have proposed to overcome this limitation, notably the use of word-classes (Brown et al., 1992; Niesler, 199"
D10-1076,W04-3242,0,\N,Missing
D10-1091,2007.mtsummit-papers.3,0,0.0519115,"Missing"
D10-1091,W09-0437,0,0.394639,"l., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et al., 2009): search errors, induction errors and model errors. The former corresponds to cases where the hypothesis with the best score is missed by the search procedure, either because of the use of an ap2 the 3 the ttl option of Moses, defaulting to 20. dl option of Moses, whose default value is 7. 933 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics proximate search method or because of the restrictions of the search space. Induction errors correspond to ca"
D10-1091,W05-0909,0,0.0448439,"ses and can not be evaluated in isolation. Because of its nondecomposability, maximizing BLEU-4 is hard; in particular, the phrase-level decomposability of the evaluation metric is necessary in our approach. To circumvent this difficulty, we propose to evaluate the similarity between a translation hypothesis and a reference by the number of their common words. This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ). This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty). We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle"
D10-1091,P07-1020,0,0.0173698,"y accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various constraints that are typically used in conventional PBTS. In 17 The best BLEU-4 oracle they achieve on E"
D10-1091,D08-1064,0,0.028483,"igh 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a n×m binary matrix describing possible translation links between source words and target words7 , this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6 Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7 The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the j th word of the reference, 0 otherwise. 935 is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of possible translation links between potential phrases of the source and of the target, find the subset of links so that the unigram precision and recall are the highest possible. The corresponding oracle hypothesis can then be easily generated by selecting the target phrases that are aligned with one s"
D10-1091,P08-2007,0,0.041734,"Missing"
D10-1091,W07-0414,0,0.320042,"rious causes of errors is of primary interest for SMT system developers: for lack of such diagnoses, it is difficult to figure out which components of the system require the most urgent attention. Diagnoses are however, given the tight intertwining among the various component of a system, very difficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report meas"
D10-1091,P01-1030,0,0.0693371,"f (Leusch et al., 2008) and concur in making the oracle decoding problem for phrase-based models more complex than it is for word-based models: it can be proven, using arguments borrowed from (De Nero and Klein, 2008), that this problem is NP-hard even for the simple unigram precision measure. 2.3 An Integer Program for Oracle Decoding To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist. ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models. Following the latter reference, we introduce the following variables: fi,j (resp. ek,l ) is a binary indicator variable that is true when the phrase contains all spans from betweenword position i to j (resp. k to l) of the source (resp. target) sentence. We also introduce a binary variable, denoted ai,j,k,l , to describe a possible link between source phrase fi,j and target phrase ek,l . These variables are built from the entries of the phrase table according to selection strategies introduced in Secti"
D10-1091,N03-1010,0,0.0268074,"is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT"
D10-1091,W08-0302,0,0.0314872,"roximation of the BLEU-4 oracle score. We have shown that this approximation could be computed fairly accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various cons"
D10-1091,N03-1017,0,0.0262102,"el corresponds to the set of all possible sequences of 1 Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simpl"
D10-1091,P07-2045,0,0.0154746,"oring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et a"
D10-1091,koen-2004-pharaoh,0,0.0452448,"and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are com"
D10-1091,H05-1021,0,0.0253438,"r uses a distortion limit to constrain the set of possible reorderings. This constraint “enforces (...) that the last word of a phrase chosen for translation cannot be more than d9 words from the leftmost untranslated word in the source” (Lopez, 2009) and is expressed as: ∀aijkl , ai0 j 0 k0 l0 s.t. k > k 0 , aijkl · ai0 j 0 k0 l0 · |j − i0 + 1 |≤ d, The maximum distortion limit strategy (Lopez, 2009) is also easily expressed and take the following form (assuming this constraint is parameterized by d): ∀l &lt; m − 1, ai,j,k,l ·ai0 ,j 0 ,l+1,l0 · |i0 − j − 1 |&lt; d Implementing the “local” or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints: X X ∀i, k, ai0 ,j 0 ,k0 ,l0 − ai0 ,j 0 ,k0 ,l0 ≤ d i0 ≤i k0 ≤k Similarly, It is possible to simulate decoding under the so-called IBM(d) reordering constraints10 by considering the following constraints: X ∀µ ≤ m, max ai,j,k,l · j − ai,j,k,l · (j − i) ≤ d i,k,l j≤µ 9 This i,j,k,l corresponds to the dl parameter of Moses In these constraints, the first factor corresponds to the rightmost translated word of the source and the second one to the number of translated source words. The constraints simply enfor"
D10-1091,D08-1088,0,0.441812,"scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a n×m binary matrix describing possible translation links between source words and target words7 , this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6 Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7 The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the j th word of the reference, 0 otherwise. 935 is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of po"
D10-1091,N09-2003,0,0.199476,"ally generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5 The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice is the BLEU-4 score (Papineni et al., 2002) used in most machine translation evaluations. However, using this metric in the oracle decoding problem raises several issues. First, BLEU-4 is a metric defined at the corpus level and is hard to interpret at the sentence level. More importantly, BL"
D10-1091,P06-1096,0,0.271375,"Missing"
D10-1091,E09-1061,0,0.0614161,"compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed. 1 1.1 Phrase-Based Machine Translation Principle A Phrase-Based Translation System (PBTS) consists of a ruleset and a scoring function (Lopez, 2009). The ruleset, represented in the phrase table, is a set of phrase1 pairs {(f, e)}, each pair expressing that the source phrase f can be rewritten (translated) into a target phrase e. Translation hypotheses are generated by iteratively rewriting portions of the source sentence as prescribed by the ruleset, until each source word has been consumed by exactly one rule. The order of target words in an hypothesis is uniquely determined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1 Following"
D10-1091,J03-1002,0,0.0140971,"ined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1 Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004)."
D10-1091,P03-1021,0,0.032078,"utive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source"
D10-1091,P02-1040,0,0.0801182,"ifficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report measurements performed on several standard benchmarks. The main contributions of this paper are twofold. We first introduce an ILP program able to efficiently find the best hypothesis a PBTS can achieve. This program can be easily extended to test various improvements to 4 We omit here optimization err"
D10-1091,2007.tmi-papers.28,0,0.0659647,"Missing"
D10-1091,P06-1091,0,0.0995125,"ems to generate good candidate translations, irrespective of their ability to score them properly. We believe that studying this problem is interesting for various reasons. First, as described in Section 3.4, comparing the best hypothesis a system could have generated and the hypothesis it actually generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5 The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice"
D10-1091,W08-0305,0,0.082699,"Missing"
D10-1091,W05-0834,0,0.024682,"not directly comparable with ours17 , it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a similar technique, which is to guide a heuristic search in an hypergraph representing possible translation hypotheses with n-gram counts matches, which amounts to decoding with a n-gram model trained on the sole reference translation. Additional tricks are presented in this article to speed-up decoding. Computing oracle BLEU scores is also the subject of (Zens and Ney, 2005; Leusch et al., 2008), yet with a different emphasis. These studies are concerned with finding the best hypotheses in a word graph or in a consensus network, a problem that has various implications for multi-pass decoding and/or system combination techniques. The former reference describes an exponential approximate algorithm, while the latter proves the NPcompleteness of this problem and discuss various heuristic approaches. Our problem is somewhat more complex and using their techniques would require us to built word graphs containing all the translations induced by arbitrary segmentations"
D10-1091,lavie-etal-2004-significance,0,\N,Missing
D10-1091,W09-0401,0,\N,Missing
D15-1025,C08-1098,0,0.0143748,"sults published in (Mueller et al., 2013). Results show that a character-level encoding yields better results than the conventional word-level encoding. Moreover, the structured inference allows the model to achieve accuracy reasonably close to the performance of a high-order CRF that uses handcrafted features. Finally, the model that uses the concatenation of both the character and word-level embeddings outperforms the state-of-the-art system on the more difficult task, without any feature engineering. To give an idea of how a simple model would perform on such task, the reader can refer to (Schmid and Laws, 2008) and (Mueller et al., 2013). For instance in the former, by choosing the most probable tag position-by-position, the error rate on the development set of the TIGER dataset Experimental settings All the models are implemented4 with the Theano library (Bergstra et al., 2010). For optimization, we use Adagrad (Duchi et al., 2011), with a learning rate of 0.1. The other hyperparameters are: the window sizes, dc and dw , respectively set to 5 and 9, the dimension of character embeddings, word embeddings and of the hidden layer, nc , nf and nh , that are respectively of 100, 200 and 2005 . The model"
D15-1025,J13-1005,0,0.0575539,"Missing"
D15-1025,W13-3512,0,0.0360803,"Missing"
D15-1025,D13-1032,0,0.0492387,"Missing"
D15-1121,P14-2023,0,0.0715303,"train the CTM provide an explanation for this difference: in training, the N -best lists contain hypotheses with an overoptimistic BLEU score, to be compared with the ones observed on unseen data. As a result, adding the CTM significantly 7 www.statmt.org/wmt14/medical-task/ ncode.limsi.fr/ 9 The threshold δ is set to 250 for 300-best and to 500 for 600-best lists, while α is set empirically. 8 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The initial values of λ also needs some investigation. Gao et al. (2014) and Auli"
D15-1121,D13-1106,0,0.0314769,"hieve impressive results, and with which efficient tricks are available to speed up both training and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation’s cost, we prefer the NCE selfnormalized output which is very efficient both in training and inference. Another form of selfnormalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N -best rescoring is used in this work to facilitate the discriminative training, other CTM’s integration into SMT systems exist, such as lattice reranking (Auli et al., 2013) or direct decoding with CTM (Niehues and Waibel, 2012; Devlin et al., 2014; Auli and Gao, 2014). 6 Conclusions In this paper, we have proposed a new discriminative training procedure for continuous-space translation models, which correlates better with translation quality than conventional training methods. This procedure has been validated using an n-gram-based CTM, but the general idea could be applied to other continuous models which compute a score for each translation hypothesis. The core of the method lays in the definition of a new objective function inspired both from max-margin and P"
D15-1121,J04-2004,0,0.0322429,"his proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the 1046 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1046–1052, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source sentence has been reordered beforehand. 2.1 n-gram-based Machine Translation Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingua"
D15-1121,N12-1047,0,0.0218534,"ers, by alternatively updating θ through SGD on the training corpus, and updating λ using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are s"
D15-1121,P14-1129,0,0.0598631,"Missing"
D15-1121,2014.iwslt-papers.6,1,0.601875,"h) < α∆i,k SBLEU (h). A pair of hypotheses is thus deemed critical when a large difference in SBLEU is not reflected by the difference of scores, which falls below a threshold. This threshold is defined by the difference between their sentence-level BLEU, multiplied by α. Our loss function L(θ) is defined with respect to this critical set and can be written as:4 X α∆i,k SBLEU (h) − ∆i,k Gλ,θ (s, hi ) (i,k)∈Cδα (s) Initialization is an important issue when optimizing NN. Moreover, our training procedure depends heavily on the log-linear coefficients λ. To initialize θ, preliminary experiments (Do et al., 2014; Do et al., 2015) show that it is more efficient to start from a NN pre-trained using NCE, while the discriminative loss is used only in a finetuning phase. Given the pre-trained CTM’s scores, we initialize λ by optimizing it on the development set. This strategy forces the training of θ to focus on errors made by the system as a whole. 4 Experiments 4.1 Tasks and Corpora The discriminative optimization framework is evaluated both in a training and in an adaptation scenario. In the training scenario, the CTM is trained on the same parallel data as the one used for the baseline system. In the"
D15-1121,2015.jeptalnrecital-long.23,1,0.625921,"(h). A pair of hypotheses is thus deemed critical when a large difference in SBLEU is not reflected by the difference of scores, which falls below a threshold. This threshold is defined by the difference between their sentence-level BLEU, multiplied by α. Our loss function L(θ) is defined with respect to this critical set and can be written as:4 X α∆i,k SBLEU (h) − ∆i,k Gλ,θ (s, hi ) (i,k)∈Cδα (s) Initialization is an important issue when optimizing NN. Moreover, our training procedure depends heavily on the log-linear coefficients λ. To initialize θ, preliminary experiments (Do et al., 2014; Do et al., 2015) show that it is more efficient to start from a NN pre-trained using NCE, while the discriminative loss is used only in a finetuning phase. Given the pre-trained CTM’s scores, we initialize λ by optimizing it on the development set. This strategy forces the training of θ to focus on errors made by the system as a whole. 4 Experiments 4.1 Tasks and Corpora The discriminative optimization framework is evaluated both in a training and in an adaptation scenario. In the training scenario, the CTM is trained on the same parallel data as the one used for the baseline system. In the adaptation scenari"
D15-1121,N13-1025,0,0.0162701,"training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are scored using a sentence-level approximation of BLEU denoted SBLEU (hi ). Let ri be the rank of hypothesis hi when hypotheses are sorted according to their sentence-level BLEU. Critical hypotheses are de2 http://www.statmt.org/moses/ Cδα (s) = {(i, k) : 1 ≤ k, i ≤ N, rk − ri ≥ δ, ∆i,k Gλ,θ (s, h) < α∆i,k SBLEU (h). A pair of hypotheses is thus deemed critical when a large difference in SBLEU is not reflected by the diffe"
D15-1121,N13-1048,0,0.0883008,"do for P mini-batch do . λ is fixed Compute the sub-gradient of L(θ) for each sentence s in the mini-batch Update θ end for Update λ on development set . θ is fixed end for cumulates, over all contexts c and word w, the CTM log-score log bθ (w, c). Gλ,θ depends both on the NN parameters θ and on the log-linear coefficients λ. We propose to train these two sets of parameters, by alternatively updating θ through SGD on the training corpus, and updating λ using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Sim"
D15-1121,P14-1066,0,0.0193361,"n) measured on the N -best lists used to train the CTM provide an explanation for this difference: in training, the N -best lists contain hypotheses with an overoptimistic BLEU score, to be compared with the ones observed on unseen data. As a result, adding the CTM significantly 7 www.statmt.org/wmt14/medical-task/ ncode.limsi.fr/ 9 The threshold δ is set to 250 for 300-best and to 500 for 600-best lists, while α is set empirically. 8 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The initial values of λ also needs some"
D15-1121,P12-1031,0,0.168758,"for each iteration do for P mini-batch do . λ is fixed Compute the sub-gradient of L(θ) for each sentence s in the mini-batch Update θ end for Update λ on development set . θ is fixed end for cumulates, over all contexts c and word w, the CTM log-score log bθ (w, c). Gλ,θ depends both on the NN parameters θ and on the log-linear coefficients λ. We propose to train these two sets of parameters, by alternatively updating θ through SGD on the training corpus, and updating λ using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update θ. After each training iteration of the CTM, λs are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in M OSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins"
D15-1121,D11-1125,0,0.205097,"addressing problems (a) and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et"
D15-1121,P12-1092,0,0.0232118,"w that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize"
D15-1121,N12-1005,1,0.90363,"two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speed up training and inference, a popular and effective choice being the Noise Contrastive Estimation (NCE) introduced in (Gutmann a"
D15-1121,J06-4004,0,0.0883567,"Missing"
D15-1121,2012.iwslt-papers.3,0,0.0569966,"c)), where aθ (w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s. Each hypothesis h is composed of a target sentence t along with its associated derivation and is evaluated as f"
D15-1121,P02-1040,0,0.0998119,"2012). This technique is readily applicable for CTMs and has been adopted here. We therefore assume that the NN outputs a positive score bθ (w, c) for each word w given its context c; this score is simply computed as bθ (w, c) = exp(aθ (w, c)), where aθ (w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the re"
D15-1121,D07-1045,0,0.0228854,"Missing"
D15-1121,P12-1002,0,0.0968245,"and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced i"
D15-1121,P13-1045,0,0.0266231,"this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function r"
D15-1121,D13-1140,0,0.0299684,"he activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s. Each hypothesis h is composed of a target sentence t along with its associated derivation and is evaluated as follows: Gλ,θ (s, h) ="
D15-1121,D07-1080,0,0.255024,"paper, we study an alternative training regime aimed at addressing problems (a) and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Transla"
D15-1121,P13-1017,0,0.0241565,"ce gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alt"
D15-1121,2002.tmi-tutorials.2,0,0.0658344,"ay, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the 1046 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1046–1052, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source sentence has been reordered beforehand. 2.1 n-gram-based Machine Translation Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pai"
D15-1121,D14-1179,0,\N,Missing
E17-2003,P16-1186,0,0.115799,"poses an objective function that replaces the conventional log-likelihood by a binary classification task, discriminating between the real examples provided by the data, and negative examples sampled from a chosen noise distribution. This allows the model to learn indirectly from the data distribution. NCE was first applied to language modeling by (Mnih and Teh, 2012), and then to various models, often in the context of machine translation (Vaswani et al., 2013; Baltescu and Blunsom, 2015; Zoph et al., 2016). However, recently, a comparative study of methods for training large vocabulary LMs (Chen et al., 2016) highlighted the inconsistency of NCE training when dealing with very large vocabularies, showing very different perplexity results for close loss values. In another work (J´ozefowicz et al., 2016), NCE was shown far less data-efficient than the theoretically similar importance sampling. Introduction Statistical language models (LMs) play an important role in many tasks, such as machine translation and speech recognition. Neural models, with various neural architectures (Bengio et al., 2001; Mikolov et al., 2010; Chelba et al., 2014; J´ozefowicz et al., 2016), have recently achieved great succ"
E17-2003,P14-1129,0,0.079821,"Missing"
E17-2003,D13-1140,0,0.0339774,"as the bigram distribution, can solve these issues and provide stable and data-efficient learning. 1 Introduced by (Gutmann and Hyv¨arinen, 2010), NCE proposes an objective function that replaces the conventional log-likelihood by a binary classification task, discriminating between the real examples provided by the data, and negative examples sampled from a chosen noise distribution. This allows the model to learn indirectly from the data distribution. NCE was first applied to language modeling by (Mnih and Teh, 2012), and then to various models, often in the context of machine translation (Vaswani et al., 2013; Baltescu and Blunsom, 2015; Zoph et al., 2016). However, recently, a comparative study of methods for training large vocabulary LMs (Chen et al., 2016) highlighted the inconsistency of NCE training when dealing with very large vocabularies, showing very different perplexity results for close loss values. In another work (J´ozefowicz et al., 2016), NCE was shown far less data-efficient than the theoretically similar importance sampling. Introduction Statistical language models (LMs) play an important role in many tasks, such as machine translation and speech recognition. Neural models, with v"
E17-2003,P15-1001,0,0.0384443,"e output vocabularies cause a computational bottleneck due to the output normalization. Different solutions have been proposed, as shortlists (Schwenk, 2007), hierarchical softmax (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011), or self-normalisation techniques (Devlin et al., 2014; Andreas et al., 2015; Chen et al., 2016). Sampling-based techniques explore a different solution, where a limited number of negative examples are sampled to reduce the normalization cost. The resulting model is theoretically unnormalized. Apart from importance sampling (Bengio and S´en´ecal, 2008; Jean et al., 2015), the noise contrastive estimation (NCE) In this paper, we focus on a small task to provide an in-depth analysis of the results. NCE relies on the definition of an artificial classification task that must be monitored. Indeed, using a unigram noise distribution as usually advised leads to an ineffective solution, where the model almost systematically classifies words in the noise class. This can be explained by the inability to sample rare words from the noise distribution, yielding inconsistent updates for the most frequent words. We explore other noise distributions and show that designing a"
E17-2003,N16-1145,0,0.0618613,"s and provide stable and data-efficient learning. 1 Introduced by (Gutmann and Hyv¨arinen, 2010), NCE proposes an objective function that replaces the conventional log-likelihood by a binary classification task, discriminating between the real examples provided by the data, and negative examples sampled from a chosen noise distribution. This allows the model to learn indirectly from the data distribution. NCE was first applied to language modeling by (Mnih and Teh, 2012), and then to various models, often in the context of machine translation (Vaswani et al., 2013; Baltescu and Blunsom, 2015; Zoph et al., 2016). However, recently, a comparative study of methods for training large vocabulary LMs (Chen et al., 2016) highlighted the inconsistency of NCE training when dealing with very large vocabularies, showing very different perplexity results for close loss values. In another work (J´ozefowicz et al., 2016), NCE was shown far less data-efficient than the theoretically similar importance sampling. Introduction Statistical language models (LMs) play an important role in many tasks, such as machine translation and speech recognition. Neural models, with various neural architectures (Bengio et al., 2001"
F13-1033,P08-1024,0,0.0509722,"Missing"
F13-1033,P96-1041,0,0.0629406,"Missing"
F13-1033,N12-1047,0,0.0840101,"Missing"
F13-1033,P05-1033,0,0.20525,"Missing"
F13-1033,N09-1025,0,0.0523268,"Missing"
F13-1033,2009.eamt-1.10,1,0.89492,"Missing"
F13-1033,D08-1113,0,0.0420633,"Missing"
F13-1033,N10-1128,0,0.0524758,"Missing"
F13-1033,N10-1112,0,0.0607107,"Missing"
F13-1033,D11-1125,0,0.0471838,"Missing"
F13-1033,J10-4005,0,0.0549819,"Missing"
F13-1033,P07-2045,0,0.00926885,"Missing"
F13-1033,N03-1017,0,0.0205266,"Missing"
F13-1033,H05-1021,0,0.0736623,"Missing"
F13-1033,W11-2168,1,0.869656,"Missing"
F13-1033,P06-1096,0,0.0733762,"Missing"
F13-1033,J06-4004,0,0.421516,"Missing"
F13-1033,P03-1021,0,0.111567,"Missing"
F13-1033,P02-1040,0,0.0880358,"Missing"
F13-1033,2010.iwslt-evaluation.1,0,0.0423573,"Missing"
F13-1033,H05-1095,0,0.0566484,"Missing"
F13-1033,P12-1002,0,0.0264726,"Missing"
F13-1033,E12-1013,1,0.863456,"Missing"
F13-1033,takezawa-etal-2002-toward,0,0.0489745,"Missing"
F13-1033,N04-4026,0,0.12837,"Missing"
F13-1033,J03-1005,0,0.122695,"Missing"
F13-1033,P06-1091,0,0.0473605,"Missing"
F13-1033,D07-1080,0,0.0524939,"Missing"
F13-1033,2002.tmi-tutorials.2,0,0.416816,"Missing"
F14-1016,C04-1080,0,0.0983607,"Missing"
F14-1016,H92-1026,0,0.486931,"Missing"
F14-1016,J92-4003,0,0.582882,"Missing"
F14-1016,W06-2920,0,0.058481,"Missing"
F14-1016,D10-1056,0,0.0296351,"Missing"
F14-1016,J03-4003,0,0.197779,"Missing"
F14-1016,P11-1061,0,0.0364566,"Missing"
F14-1016,N13-1014,0,0.0240925,"Missing"
F14-1016,D07-1033,0,0.0471334,"Missing"
F14-1016,P07-2045,0,0.00361318,"Missing"
F14-1016,P08-1068,0,0.105394,"Missing"
F14-1016,D12-1127,0,0.0348292,"Missing"
F14-1016,J94-2001,0,0.460186,"Missing"
F14-1016,N13-1039,0,0.0244234,"Missing"
F14-1016,petrov-etal-2012-universal,0,0.0265866,"Missing"
F14-1016,W09-1119,0,0.040984,"Missing"
F14-1016,J03-3002,0,0.0974294,"Missing"
F14-1016,N12-1052,0,0.0210751,"Missing"
F14-1016,W11-0328,0,0.265713,"Missing"
F14-1016,Q13-1001,0,0.0261175,"Missing"
F14-1016,Q14-1005,0,0.0213256,"Missing"
F14-1016,H01-1035,0,0.174923,"Missing"
F14-1023,D07-1090,0,0.0356393,"Missing"
F14-1023,N12-1047,0,0.0684646,"Missing"
F14-1023,P13-1031,0,0.0612609,"Missing"
F14-1023,P12-1092,0,0.082064,"Missing"
F14-1023,D13-1176,0,0.0585586,"Missing"
F14-1023,N12-1005,1,0.879962,"Missing"
F14-1023,W12-2701,1,0.89996,"Missing"
F14-1023,C90-3038,0,0.665071,"Missing"
F14-1023,P13-1045,0,0.0313874,"Missing"
F14-1023,P10-1040,0,0.0139435,"Missing"
F14-1023,P13-1017,0,0.0293458,"Missing"
N12-1005,N03-2002,0,0.0449754,"earned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented here in the framework of the n-gram based models, taking advantage of a specific hierarchic"
N12-1005,J92-4003,0,0.190024,"n the size of the output vocabulary (i.e. the number of words that have to be predicted). One practical solution is to restrict the output vocabulary to a short-list composed of the most frequent words (Schwenk, 2007). However, the usual size of the short-list is under 20k, which does not seem sufficient to faithfully represent the translation models of section 2. 3.2 Principles of SOUL To circumvent this problem, Structured Output Layer (SOUL) LMs are introduced in (Le et al., 2011a). Following Mnih and Hinton (2008), the SOUL model combines the neural network approach with a class-based LM (Brown et al., 1992). Structuring the output layer and using word class information makes the estimation of distributions over the entire vocabulary computationally feasible. wi-1 To meet this goal, the output vocabulary is structured as a clustering tree, where each word belongs to only one class and its associated sub-classes. If wi denotes the ith word in a sentence, the sequence c1:D (wi ) = c1 , . . . , cD encodes the path for word wi in the clustering tree, with D being the depth of the tree, cd (wi ) a class or sub-class assigned to wi , and cD (wi ) being the leaf associated with wi (the word itself). The"
N12-1005,J93-2003,0,0.0566446,"e, which, given a source sentence s, selects the target sentence t and the underlying alignment a maximizing the following term: K X  1 P (t, a|s) = exp λk fk (s, t, a) , Z(s) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P (t|s), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those"
N12-1005,J04-2004,0,0.412345,"ural architecture developed and explain how it can be made to handle large vocabulary tasks as well as language models over bilingual units. We finally report, in Section 4, experimental results obtained on a large-scale English to French translation task. 40 2 Variations on the n-gram approach Even though n-gram translation models can be integrated within standard phrase-based systems (Niehues et al., 2011), the n-gram based framework provides a more convenient way to introduce our work and has also been used to build the baseline systems used in our experiments. In the ngram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006), translation is divided in two steps: a source reordering step and a translation step. Source reordering is based on a set of learned rewrite rules that nondeterministically reorder the input words so as to match the target order thereby generating a lattice of possible reorderings. Translation then amounts to finding the most likely path in this lattice using a n-gram translation model 2 of bilingual units. 2.1 The standard n-gram translation model n-gram translation models (TMs) rely on a specific decomposition of the joint probability P (s, t"
N12-1005,P96-1041,0,0.836328,"e random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those units in both languages. This lack of structure hinders the generalization power of the model and reduces its ability to adapt to other domains. Another consequence is that phrase-based models usually consider a very restricted context1 . This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. 1 typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 39 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, c Montr´eal, Canada, June 3-8, 2012."
N12-1005,W06-1607,0,0.0733779,"an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented her"
N12-1005,D07-1091,0,0.0607795,"Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the other hand. In comparison, in our approach, the representation and the prediction are learned in a joined fashion. Other ways to address the data sparsity issues faced by translation model were also proposed in the literature. Smoothing is obviously one possibility (Foster et al., 2006). Another is to use factored language models, introduced in (Bilmes and Kirchhoff, 2003), then adapted for translation models in (Koehn and Hoang, 2007; Crego and Yvon, 2010). Such approaches require to use external linguistic analysis tools which are error prone; moreover, they did not seem to bring clear improvements, even when translating into morphologically rich languages. 6 Conclusion In this paper, we have presented possible ways to use a neural network architecture as a translation model. A first contribution was to produce the first largescale neural translation model, implemented here in the framework of the n-gram based models, taking advantage of a specific hierarchical architecture (SOUL). By considering several decompositions o"
N12-1005,P07-2045,0,0.0140012,"eral issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network (see eg. 1 typically a small number of preceding phrase pairs for the n-gram based approach (Crego and Mari˜no, 2006), or no context at all, for the standard approach of (Koehn et al., 2007). 39 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics (Collobert et al., 2011)). An influential proposal, in this respect, is the work of (Bengio et al., 2003) on continuous space language models. In this approach, n-gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations. Experimental results, reported for instance in (Schwenk, 2007) show significant improvem"
N12-1005,2011.iwslt-evaluation.7,1,0.825382,"ights are estimated from the automatically generated word 6 geek.kyloo.net/software alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7 . For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to some of the best official submissions. 4.3 Small task evaluation Table 2 summarizes the results obtained with the baseline and different SOUL models, TMs and a target LM. The first comparison concerns the standard n-gram TM, defined by equation (2), when estimated conventionally or as a SOUL model. Adding the latter model yields a slight BLEU improvement of 0.5 point over the baseline. When the SOUL TM"
N12-1005,J06-4004,0,0.680784,"Missing"
N12-1005,W11-2124,0,0.23067,"system. The rest of this paper is organized as follows. We first recollect, in Section 2, the n-gram based approach, and discuss various implementations of this framework. We then describe, in Section 3, the neural architecture developed and explain how it can be made to handle large vocabulary tasks as well as language models over bilingual units. We finally report, in Section 4, experimental results obtained on a large-scale English to French translation task. 40 2 Variations on the n-gram approach Even though n-gram translation models can be integrated within standard phrase-based systems (Niehues et al., 2011), the n-gram based framework provides a more convenient way to introduce our work and has also been used to build the baseline systems used in our experiments. In the ngram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006), translation is divided in two steps: a source reordering step and a translation step. Source reordering is based on a set of learned rewrite rules that nondeterministically reorder the input words so as to match the target order thereby generating a lattice of possible reorderings. Translation then amounts to finding the most likely"
N12-1005,J04-4002,0,0.136322,"derlying alignment a maximizing the following term: K X  1 P (t, a|s) = exp λk fk (s, t, a) , Z(s) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P (t|s), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological, syntactic and semantic relationships that exist among those units in both languages. This lack of structure hinders the generalization p"
N12-1005,P03-1021,0,0.161984,"are included: a target-language model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011); a distance-based distortion model; and finally a word-bonus model and a tuple-bonus model. The four lexicon models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6 geek.kyloo.net/software alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7 . For the small task, the target LM is a standard 4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models (Kneser and Ney, 1995; Chen and Goodman, 1996), while for the large task, the target LM is obtained by linear interpolation of several 4-gram models (see (Lavergne et al., 2011) for details). As for the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results obtained with this large-scale system were found to be comparable to so"
N12-1005,D07-1045,0,0.820912,"Missing"
N12-1005,P06-1124,0,0.0126685,"e sequences of tokens (typically words) w1L in V + as follows: P (w1L ) = L Y i−1 P (wi |wi−n+1 ) (6) i=1 Modeling the joint distribution of several discrete random variables (such as words in a sentence) is difficult, especially in NLP applications where V typically contains dozens of thousands words. In spite of the simplifying n-gram assumption, maximum likelihood estimation remains unreliable and tends to underestimate the probability of very rare n-grams. Smoothing techniques, such as Kneser-Ney and Witten-Bell backoff schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order dis42 tributions, thus providing an estimate for the probability of these unseen events. One of the most successful alternative to date is to use distributed word representations (Bengio et al., 2003), where distributionally similar words are represented as neighbors in a continuous space. This turns n-grams distributions into smooth functions of the word representations. These representations and the associated estimates are jointly computed in a multi-layer neural network architecture. Figure 2 provides a partial representation"
N12-1005,N04-4026,0,0.250662,"In a nutshell, the TM is implemented as a stochastic finite-state transducer trained using a ngram model of (source, target) pairs as described in section 2.1. Training this model requires to reorder source sentences so as to match the target word order. This is performed by a non-deterministic finitestate reordering model, which uses part-of-speech information generated by the TreeTagger to generalize reordering patterns beyond lexical regularities. In addition to the TM, fourteen feature functions are included: a target-language model; four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011); a distance-based distortion model; and finally a word-bonus model and a tuple-bonus model. The four lexicon models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word 6 geek.kyloo.net/software alignments. The weights associated to feature functions are optimally combined using the Minimum Error Rate Training (MERT) (Och, 2003). All the results in BLEU are obtained as an average of 4 optimization runs7 . For the small task,"
N12-1005,2010.iwslt-evaluation.4,0,0.0733703,"ork To the best of our knowledge, the first work on machine translation in continuous spaces is (Schwenk et al., 2007), where the authors introduced the model referred here to as the the standard n-gram translation model in Section 2.1. This model is an extension of the continuous space language model of (Bengio et al., 2003), the basic unit is the tuple (or equivalently the phrase pair). The resulting vocabulary being too large to be handled by neural networks without a structured output layer, the authors had thus to restrict the set of the predicted units to a 8k short-list . Moreover, in (Zamora-Martinez et al., 2010), the authors propose a tighter integration of a continuous space model with a n-gram approach but only for the target LM. A different approach, described in (Sarikaya et al., 2008), divides the problem in two parts: first the continuous representation is obtained by an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the translation model on the"
N12-1005,2002.tmi-tutorials.2,0,0.312467,"ine translation (SMT) is based on the following inference rule, which, given a source sentence s, selects the target sentence t and the underlying alignment a maximizing the following term: K X  1 P (t, a|s) = exp λk fk (s, t, a) , Z(s) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ), and Z is a normalizing factor. The phrase-based approach differs from other approaches by the hidden variables of the translation process: the segmentation of a parallel sentence pair into phrase pairs and the associated phrase alignments. This formulation was introduced in (Zens et al., 2002) as an extension of the word based models (Brown et al., 1993), then later motivated within a discriminative framework (Och and Ney, 2004). One motivation for integrating more feature functions was to improve the estimation of the translation model P (t|s), which was initially based on relative frequencies, thus yielding poor estimates. This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the morphological"
N12-1005,W11-2135,1,\N,Missing
R19-1092,C18-1151,0,0.0671443,"Missing"
R19-1092,C18-1117,0,0.146961,"Missing"
R19-1092,D17-1118,0,0.270374,"Missing"
R19-1092,D15-1025,1,0.884043,"Missing"
R19-1092,D17-1257,0,0.0536444,"Missing"
R19-1092,N15-1184,0,0.197629,"Missing"
R19-1092,W13-3512,0,0.106759,"Missing"
R19-1092,W11-2508,0,0.0947356,"Missing"
R19-1092,P16-1141,0,0.104729,"Missing"
R19-1092,W14-2517,0,0.274215,"Missing"
R19-1092,W18-3408,0,0.0626348,"Missing"
R19-1092,P17-2071,0,\N,Missing
S19-1002,S16-1081,0,0.0135791,"on similarity judgments. ison of existing static and dynamic embeddingWe combine direct Usim assessments, made by based meaning representation methods on the usthe embedding-based methods, with a substituteage similarity (Usim) task, which involves estibased Usim approach. Building up on previous mating the semantic proximity of word instances work that used manually selected in-context subin different contexts (Erk et al., 2009). Usim stitutes as a proxy for Usim (Erk et al., 2013; Mcdiffers from a classical Semantic Textual SimiCarthy et al., 2016), we propose to automatize the larity task (Agirre et al., 2016) by the focus on annotation collection step in order to scale up the a particular word in the sentence. We evalumethod and make it operational on unrestricted ate on this task word and context representations text. We exploit annotations assigned to words obtained using pre-trained uncontextualized word 9 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 9–21 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Due to its high reliance on context, Usim can be viewed as a semantic textual similarity (STS) (Agirre et al., 201"
S19-1002,ide-etal-2008-masc,0,0.015982,"Missing"
S19-1002,P09-1002,0,0.636984,"g adapted to every new context of ity. The best representations are used as features use. in supervised models for Usim prediction, trained In this work, we perform an extensive comparon similarity judgments. ison of existing static and dynamic embeddingWe combine direct Usim assessments, made by based meaning representation methods on the usthe embedding-based methods, with a substituteage similarity (Usim) task, which involves estibased Usim approach. Building up on previous mating the semantic proximity of word instances work that used manually selected in-context subin different contexts (Erk et al., 2009). Usim stitutes as a proxy for Usim (Erk et al., 2013; Mcdiffers from a classical Semantic Textual SimiCarthy et al., 2016), we propose to automatize the larity task (Agirre et al., 2016) by the focus on annotation collection step in order to scale up the a particular word in the sentence. We evalumethod and make it operational on unrestricted ate on this task word and context representations text. We exploit annotations assigned to words obtained using pre-trained uncontextualized word 9 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 9–21 c Min"
S19-1002,U12-1006,0,0.225826,"sical word-averaging approach with traditional word embeddings (Pennington et al., 2014), to more recent contextualized word representations (Peters et al., 2018; Devlin et al., 2018). We explore the contribution of each separate method for Usim prediction, and use the best performing ones as features in supervised models. These are trained on sentence pairs labelled with Usim judgments (Erk et al., 2009) to predict the similarity of new word instances. Previous attempts to automatic Usim prediction involved obtaining vectors encoding a distribution of topics for every target word in context (Lui et al., 2012). In this work, Usim was approximated by the cosine similarity of the resulting topic vectors. We show how contextualized representations, and the supervised model that uses them as features, outperform topic-based methods on the graded Usim task. We combine the embedding-based direct Usim assessment methods with substitute-based representations obtained using an unsupervised lexical substitution model. McCarthy et al. (2016) showed it is possible to model usage similarity using manual substitute annotations for words in context. In this setting, the set of substitutes proposed for a word inst"
S19-1002,J13-3003,0,0.634806,"entations are used as features use. in supervised models for Usim prediction, trained In this work, we perform an extensive comparon similarity judgments. ison of existing static and dynamic embeddingWe combine direct Usim assessments, made by based meaning representation methods on the usthe embedding-based methods, with a substituteage similarity (Usim) task, which involves estibased Usim approach. Building up on previous mating the semantic proximity of word instances work that used manually selected in-context subin different contexts (Erk et al., 2009). Usim stitutes as a proxy for Usim (Erk et al., 2013; Mcdiffers from a classical Semantic Textual SimiCarthy et al., 2016), we propose to automatize the larity task (Agirre et al., 2016) by the focus on annotation collection step in order to scale up the a particular word in the sentence. We evalumethod and make it operational on unrestricted ate on this task word and context representations text. We exploit annotations assigned to words obtained using pre-trained uncontextualized word 9 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 9–21 c Minneapolis, June 6–7, 2019. 2019 Association for Comput"
S19-1002,D08-1094,0,0.144398,"Missing"
S19-1002,J16-2003,1,0.905227,"similarity of new word instances. Previous attempts to automatic Usim prediction involved obtaining vectors encoding a distribution of topics for every target word in context (Lui et al., 2012). In this work, Usim was approximated by the cosine similarity of the resulting topic vectors. We show how contextualized representations, and the supervised model that uses them as features, outperform topic-based methods on the graded Usim task. We combine the embedding-based direct Usim assessment methods with substitute-based representations obtained using an unsupervised lexical substitution model. McCarthy et al. (2016) showed it is possible to model usage similarity using manual substitute annotations for words in context. In this setting, the set of substitutes proposed for a word instance describe its specific meaning, while similarity of substitute annotations for different instances points to their semantic proximity.1 We follow up on this work and propose a way to use substitutes for Usim prediction on unrestricted text, bypassing the need for manual annotations. Our method relies on substitute annotations proposed by the context2vec model (Melamud et al., 2016), which uses word and context representat"
S19-1002,S07-1009,0,0.532242,"013) with the GOLD substitutes and Usim scores assigned by the annotators. The Usim score is high for similar instances, and decreases for instances that describe different meanings. The semantic proximity of two instances is also reflected in the similarity of their substitutes sets. For comparison, we also give in the Table the substitutes selected for these instances by the automatic context2vec substitution method used in our experiments (more details in Section 4.2). The LexSub and Usim Datasets We use the training and test datasets of the SemEval-2007 Lexical Substitution (LexSub) task (McCarthy and Navigli, 2007), which contain instances of target words in sentential context handlabelled with meaning-preserving substitutes. A subset of the LexSub data (10 instances x 56 lemmas) has additionally been annotated with graded pairwise Usim judgments (Erk et al., 2013). Each sentence pair received a rating (on a scale of 15) by multiple annotators, and the average judgment for each pair was retained. McCarthy et al. (2016) derive two additional scores from Usim annotations that denote how easy it is to partition a lemma’s usages into sets describing distinct senses: Uiaa, the inter-annotator agreement for a"
S19-1002,P08-1028,0,0.0581255,"on par with BERT on the graded and binary Usim tasks, when using embedding-based representations and clean lexical substitutes. 2 Related Work Usage similarity is a means for representing word meaning which involves assessing in-context semantic similarity, rather than mapping to word senses from external inventories (Erk et al., 2009, 2013). This methodology followed from the gradual shift from word sense disambiguation models that would select the best sense in context from a dictionary, to models that reason about meaning by solely relying on distributional similarity (Erk and Pad´o, 2008; Mitchell and Lapata, 2008), or allow multiple sense interpretations (Jurgens, 2014). In Erk et al. (2009), the idea is to model meaning in context in a way that captures different degrees of similarity to a word sense, or between word instances. 1 McCarthy et al. use the substitute annotations as features for predicting Usim, clustering instances and estimating the partitionability of words into senses. This offers a way to distinguish between lemmas with distinct senses and others with fuzzy semantics, which would be more challenging in annotation tasks and automatic processing. 10 Sentences The local papers took phot"
S19-1002,P15-2070,0,0.124288,"Missing"
S19-1002,D14-1162,0,0.0941852,"e for no charge and may not ] be placed in city cans ] the tag consists of a tiny chip , [about the [size of a match head that serves ] as a ] portable database . this is at least 26 weeks by the [week in [which the approved match with the child ] is made ]. Figure 1: We use contextualized word representations built from the whole sentence or smaller windows around the target word for usage similarity estimation, combined with automatic substitute annotations. grass clippings can be brought out to the landfill at anytime for no *charge* and may not be placed in city cans . embeddings (GloVe) (Pennington et al., 2014), with and without dimensionality reduction (SIF) Traditional word embeddings, like Word2Vec and al., 2017); context obGloVe, merge different meanings of a word in astarted(Arora So what out as a et perfectly lovely stroll ended [representations up as [a public from agrandma bidirectional LSTM (context2vec) wrestlingtained match between and] baby girl], with single vector representation (Mikolov et al., 2013; So what started (Melamud et al., 2016); word em-out as a perfectly lovely s baby girl loving very secondcontextualized of it . Pennington et al., 2014). These pre-trained emas [a public w"
S19-1002,N18-1202,0,0.719609,"a So what out as a et perfectly lovely stroll ended [representations up as [a public from agrandma bidirectional LSTM (context2vec) wrestlingtained match between and] baby girl], with single vector representation (Mikolov et al., 2013; So what started (Melamud et al., 2016); word em-out as a perfectly lovely s baby girl loving very secondcontextualized of it . Pennington et al., 2014). These pre-trained emas [a public wrestling match between g beddings derived from a LSTM bidirectional lanbeddings are fixed, and stay the same indepenbaby girl], with baby girl loving very s guage model (ELMo) (Peters et al., 2018) and gendently of the context of use. Current contextualerated by a Transformer (BERT) (Devlin et al., ized sense representations, like ELMo and BERT, 2018); doc2vec (Le and Mikolov, 2014) and go to the other extreme and model meaning as Universal Sentence Encoder representations (Cer word usage (Peters et al., 2018; Devlin et al., et al., 2018). All these embedding-based meth2018). They provide a dynamic representation of ods provide direct assessments of usage similarword meaning adapted to every new context of ity. The best representations are used as features use. in supervised models for"
S19-1002,jurgens-2014-analysis,0,\N,Missing
S19-1002,E14-1057,0,\N,Missing
S19-1002,N13-1092,0,\N,Missing
S19-1002,W15-1501,0,\N,Missing
S19-1002,K16-1006,0,\N,Missing
S19-1002,D18-2029,0,\N,Missing
W07-0409,J93-2003,0,0.00881613,"Missing"
W07-0409,2003.mtsummit-papers.6,0,0.029494,"mising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat65 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65–71, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics egories. The same idea has already been applied in (Hwang et al., 2007) to the Basic Travel Expression Corpus (BTEC). To our knowledge, th"
W07-0409,W06-2606,0,0.132853,"ge model) discards linguistic properties such as long term word dependency and word-order or phrase-order syntactic constraints. Therefore, explicit introduction of structure in the language models becomes a major and promising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat65 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65–"
W07-0409,W05-0821,0,0.314467,"stic properties such as long term word dependency and word-order or phrase-order syntactic constraints. Therefore, explicit introduction of structure in the language models becomes a major and promising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat65 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65–71, c Rochester, New York, Apri"
W07-0409,N03-1017,0,0.0336336,"Missing"
W07-0409,P00-1056,0,0.147669,"., 2002) on a development set (Och and Ney, 2002). For this purpose, the public numerical optimization tool Condor (Berghen and Bersini, 2005) is integrated in the following iterative algorithm: 0. Using good general purpose weights, the Moses decoder is used to generate 1000-best lists. 1. The 1000-best lists are reranked using the current set of weights. 2. The current hypothesis is extracted and scored. Moses1 is an open-source, state-of-the-art phrasebased decoder. It implements an efficient beamsearch algorithm. Scripts are also provided to train a phrase-based model. The popular Giza++ (Och and Ney, 2000b) tool is used to align the parallel corpora. The baseline system uses 8 feature functions hi , namely phrase translation probabilities in both directions, lexical translation probabilities in both directions, a distortion feature, a word and a phrase 1 penalty and a trigram target language model. Additional features can be added, as described in the following sections. The weights λi are typically optimized so as to maximize a scoring function on a development set (Och and Ney, 2002). The moses decoder can output n-best lists, producing either distinct target sentences or not (as different s"
W07-0409,P02-1038,0,0.0171446,"ntences the one with the highest probability is chosen. The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process: e∗ = arg max Pr(e|f ) X = arg max{exp( λi hi (e, f ))} e (1) i where the feature functions hi are the system models characterizing the translation process, and the coefficients λi act as weights. 2.1 Moses decoder 2.2 Weight optimization A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al., 2002) on a development set (Och and Ney, 2002). For this purpose, the public numerical optimization tool Condor (Berghen and Bersini, 2005) is integrated in the following iterative algorithm: 0. Using good general purpose weights, the Moses decoder is used to generate 1000-best lists. 1. The 1000-best lists are reranked using the current set of weights. 2. The current hypothesis is extracted and scored. Moses1 is an open-source, state-of-the-art phrasebased decoder. It implements an efficient beamsearch algorithm. Scripts are also provided to train a phrase-based model. The popular Giza++ (Och and Ney, 2000b) tool is used to align the par"
W07-0409,N04-1021,0,0.0424268,"ering words to recover its syntactic structure. Modeling language generation as a word-based Markovian source (an ngram language model) discards linguistic properties such as long term word dependency and word-order or phrase-order syntactic constraints. Therefore, explicit introduction of structure in the language models becomes a major and promising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphos"
W07-0409,P02-1040,0,0.0806419,"nce f . Among all possible target language sentences the one with the highest probability is chosen. The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process: e∗ = arg max Pr(e|f ) X = arg max{exp( λi hi (e, f ))} e (1) i where the feature functions hi are the system models characterizing the translation process, and the coefficients λi act as weights. 2.1 Moses decoder 2.2 Weight optimization A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al., 2002) on a development set (Och and Ney, 2002). For this purpose, the public numerical optimization tool Condor (Berghen and Bersini, 2005) is integrated in the following iterative algorithm: 0. Using good general purpose weights, the Moses decoder is used to generate 1000-best lists. 1. The 1000-best lists are reranked using the current set of weights. 2. The current hypothesis is extracted and scored. Moses1 is an open-source, state-of-the-art phrasebased decoder. It implements an efficient beamsearch algorithm. Scripts are also provided to train a phrase-based model. The popular Giza++ (Och and"
W08-0310,allauzen-bonneau-maynard-2008-training,1,0.882861,"Missing"
W08-0310,D07-1091,0,0.12789,"ond pass, the use of the Neural Network LMs, if used with an appropriate (tuned) weight, yields a small, yet consistent improvement of B LEU for all pairs. Performance on the news task are harder to analyze, due to the lack of development data. Throwing in large set of in-domain data was obviously helpful, even though we are currently unable to adequately measure this effect. 4 Experiments with factored models Even though these models were not used in our submissions, we feel it useful to comment here our (negative) experiments with factored models. 4.1 Overview In this work, factored models (Koehn and Hoang, 2007) are experimented with three factors : the surface form, the lemma and the part of speech (POS). The translation process is composed of different mapping steps, which either translate input factors into output factors, or generate additional output factors from existing output factors. In this work, four mapping steps are used with two decoding paths. The first path corresponds to the standard and direct mapping of surface forms. The second decoding path consists in two translation steps for respectively POS tag and the lemmas, followed by a generation step which produces the surface form give"
W08-0310,W07-0733,0,0.0277945,"stical Machine Translation, pages 107–110, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the news-commentary parallel data, as depicted on Figure 1. This setup was found to be more favorable than training on Europarl data only (for obvious mismatching domain reasons) and than training on news-commentary data only, most probably because of a lack of coverage. Another, alternative way of benefitting from the coverage of the Europarl corpus and the relevance of the news-commentary corpus is to use two phrase-tables in parallel, an interesting feature of Moses. (Koehn and Schroeder, 2007) found that this was the best way to “adapt” a translation system to the news-commentary task. These results are corroborated in (Déchelotte, 2007)1 , which adapts a “European Parliament” system using a “European and Spanish Parliaments” development set. However, we were not able to reproduce those findings for this evaluation. This might be caused by the increase of the number of feature functions, from 14 to 26, due to the duplication of the phrase table and the lexicalized reordering model. 2.2 2.2.1 Language Models Europarl language models The training of Europarl language models (LMs) was"
W08-0310,N03-1017,0,0.00383999,"data, translating French, German and Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of L IMSI’s translation systems. They are fairly standard phrase-based ∗ Univ. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of L IMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were"
W08-0310,P07-2045,0,0.0336357,"d Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of L IMSI’s translation systems. They are fairly standard phrase-based ∗ Univ. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of L IMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl pa"
W08-0310,P02-1038,0,0.0651661,"s. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl parallel data only. For the news condition, they were trained on the Europarl data merged with 107 Proceedings of the Third Workshop on Statistical Machine Translation, pages 107–110, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the news-commentary parallel data,"
W08-0310,J04-4002,0,0.016144,"l data and on News data, translating French, German and Spanish from and to English, amounting a total of twelve evaluation conditions. Figure 1 presents the generic overall architecture of L IMSI’s translation systems. They are fairly standard phrase-based ∗ Univ. Montréal, felipe@iro.umontreal.ca Figure 1: Generic architecture of L IMSI’s SMT systems. Depending on the condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the eur"
W08-0310,P03-1021,0,0.0607674,"condition, the decoder generates either the final output or n-best lists. In the latter case, the rescoring incorporates the same translation features, except for a better target language model (see text). translation systems (Och and Ney, 2004; Koehn et al., 2003) and use Moses (Koehn et al., 2007) to search for the best target sentence. The search uses the following models: a phrase table, providing 4 scores and a phrase penalty, a lexicalized reordering model (7 scores), a language model score and a word penalty. These fourteen scores are weighted and linearly combined (Och and Ney, 2002; Och, 2003); their respective weights are learned on development data so as to maximize the B LEU score. In the following, we detail several aspects of our systems. 2.1 Translation models The translation models deployed in our systems for the europarl condition were trained on the provided Europarl parallel data only. For the news condition, they were trained on the Europarl data merged with 107 Proceedings of the Third Workshop on Statistical Machine Translation, pages 107–110, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics the news-commentary parallel data, as depicted"
W09-0417,2007.tmi-papers.28,0,0.0611781,"Missing"
W09-0417,2007.mtsummit-papers.11,0,0.0245406,"ights, since they dispense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate trans"
W09-0417,J04-2004,0,0.0544024,"mate such large LMs, a vocabulary was first defined for both languages by including all tokens in the WMT parallel data. This initial vocabulary of 130K words was then extended by adding the most frequent words observed in the additional training data. This procedure yielded a vocabulary of one million words in both languages. 2.5 A N-code baseline N-code implements the n-gram-based approach to Statistical Machine Translation (Mariño et al., 2006). In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training such a model requires to reorder source sentences so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models a"
W09-0417,P96-1041,0,0.135168,"es so as to match the target word order. This is also performed via a stochastic finite-state reordering model, which uses part-of-speech information to generalise reordering patterns beyond lexical regularities. The reordering model is trained on a version of the parallel corpora where the source sentences have been reordered via the unfold heuristics (Crego and Mariño, 2007). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney backoff (Chen and Goodman, 1996). The maximum tuple size was also set to 7. Language model training The training data were divided into several sets based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tu"
W09-0417,W08-0310,1,0.911154,"Missing"
W09-0417,W08-0302,0,0.0772343,"f the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source phrase based on the target context. In fact, attempts at using source contexts in phrase-based SMT have to date failed to sho"
W09-0417,P07-2045,0,0.0111904,"Missing"
W09-0417,2008.eamt-1.17,1,0.839871,"ense with the lexical reordering model; these weights were tuned on the same dataset, using an in-house implementation of the simplex algorithm. 3 3.1 of each entry of the phrase table; and by (ii) adding one or several contextual scores to the phrase table. Using standard MERT, the corresponding weights can be optimized on development data. A typical contextual score corresponds to p(e|f , C(f )), where C(f ) is some contextual information about the source phrase f . An external disambiguation system can be used to provide one global context score (Stroppa et al., 2007; Carpuat and Wu, 2007; Max et al., 2008)); alternatively, several scores based on single features can be estimated using relative frequencies (Gimpel and Smith, 2008): Extensions A context-aware system In phrase-based translation, source phrases are translated irrespective of their (source) context. This is often not perceived as a limitation as (i) typical text domains usually contain only few senses for polysemous words, thus limiting the use of word sense disambiguation (WSD); and (ii) using long-span target language models (4-grams and more) often capture sufficient context to select the more appropriate translation for a source"
W09-0417,J06-4004,1,0.915103,"Missing"
W09-0417,P03-1021,0,0.00849652,"ts based on dates on genres (resp. 7 and 9 sets for English and French). On each set, a standard 4-gram LM was estimated from the 1M word vocabulary with in-house tools using absolute discounting interpolated with lower order models. The resulting LMs were then linearly interpolated using interpolation coefficients chosen so as to minimise perplexity of the development set (dev2009a). Due to memory limitations, the final LMs were pruned using perplexity as pruning criterion. 2.6 Tuning procedure The Moses-based systems were tuned using the implementation of minimum error rate training (MERT) (Och, 2003) distributed with the Moses decoder, using the development corpus (dev2009a). For the context-less systems, tuning concerned the 14 usual weights; tuning the Out of vocabulary word and perplexity To evaluate our vocabulary and LMs, we used the official devtest and test sets. The out-of-vocabulary (OOV) rate was drastically reduced by increasing 101 22 weights of the context-aware systems (see 3.1) proved to be much more challenging, and the weights used in our submissions are probably far from optimal. The N-code systems only rely on 9 weights, since they dispense with the lexical reordering m"
W09-0417,E03-1076,0,\N,Missing
W09-0417,C08-1098,0,\N,Missing
W09-0417,E09-3008,0,\N,Missing
W09-0417,J04-2003,0,\N,Missing
W09-0417,H05-1085,0,\N,Missing
W10-1704,W09-0417,1,0.716697,", 2003) distributed with the Moses decoder, using the development corpus (news-test2008). The N -code systems were also tuned by the same implementation of MERT, which was slightly modified to match the requirements of our decoder. The BLEU score is used as objective function for MERT and to evaluate test performance. The interpolation experiment for FrenchEnglish was tuned on news-test2008a (first 1025 lines). Optimization was carried out over newstest2008b (last 1026 lines). Language Models The English and French language models (LMs) are the same as for the last year’s French-English task (Allauzen et al., 2009) and are heavily tuned to the newspaper/newswire genre, using the first part of the WMT09 official development data (dev2009a). We used all the authorized news corpora, including the French and English Gigaword corpora, for translating both into French (1.4 billion tokens) and English (3.7 billion tokens). To estimate such LMs, a vocabulary was defined for both languages by including all tokens in the WMT parallel data. This initial vocabulary of 130K words was then extended with the most frequent words observed in the training data, yielding a vocabulary of one million words in both languages"
W10-1704,J04-2004,0,0.140666,"lgorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder in the form of word lattices (Crego and no, 2006). French-English systems 4.1 Baseline N -coder systems For this language pair, we used our in-house N -code system, which implements the n-grambased approach to SMT. In a nutshell, the translation model is implemented as a stochastic finitestate transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orien"
W10-1704,corston-oliver-gamon-2004-normalizing,0,0.0994184,"ies both at training and decoding time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Tra"
W10-1704,W08-0310,1,0.8106,"Missing"
W10-1704,H05-1085,0,0.0320767,"g time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages"
W10-1704,E09-3008,0,0.0133135,"to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54–59, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computation"
W10-1704,E03-1076,0,0.457998,", German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 54–59, c Uppsala, Sweden, 15-16 July 2010. 201"
W10-1704,N04-4026,0,0.276697,"the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Such models require to estimate reordering probabilities for each phrase pairs, typically distinguishing three case, depending whether the current phrase is translated monotone, swapped or discontiguous with respect to the 4.2 A bilingual POS-based reordering model For this year evaluation, we also experimented with an additional reordering model, which is estimated as a standard n-gram language model, over generalized translation units. In the experiments reported below, we generalized tuples using POS tags, instead of raw word forms. Figure 1 displays the same sequence of tuples when built"
W10-1704,J06-4004,1,0.805664,"Missing"
W10-1704,J04-2003,0,0.0577646,"s a number of difficulties both at training and decoding time. When aligning parallel texts at the word level, German compound words typically tend to align with more than one English word; this, in turn, tends to increase the number of possible translation counterparts for each English type, and to make the corresponding alignment scores less reliable. In decoding, new compounds or unseen morphological variants of existing words artificially increase the number outof-vocabulary (OOV) forms, which severely hurts the overall translation quality. Several researchers have proposed normalization (Niessen and Ney, 2004; Corston-oliver and Gamon, 2004; Goldwater and McClosky, 2005) and compound splitting (Koehn and Knight, 2003; Stymne, 2008; Stymne, 2009) methods. Our approach here is similar, yet uses different implementations; we also studied the joint effect of combining both techniques. 2 3.1 Reducing the lexical redundancy 1 Introduction System architecture and resources In German, determiners, pronouns, nouns and adjectives carry inflection marks (typically suffixes) In this section, we describe the main characteristics of the phrase-based systems developed for this 54 Proceedings of the Joint 5th Wor"
W10-1704,P03-1021,0,0.100273,"o SMT. In a nutshell, the translation model is implemented as a stochastic finitestate transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, our system implements eight feature functions which are optimally combined using a discriminative training framework (Och, 2003): a target-language model; two lexicon models, which give complementary translation scores for each tuple; two lexicalized reordering models aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Such models require to estimate reordering probabilities for each phrase pairs, typically distinguishing three case, depending whether"
W10-1704,C08-1098,0,0.0783579,"vocabulary and to improve the robustness of the alignment probabilities, we considered various normalization strategies for the different word classes. In a nutshell, normalizing amounts to collapsing several German forms of a given lemma into a unique representative, using manually written normalization patterns. A pattern typically specifies which forms of a given morphological paradigm should be considered equivalent when translating into English. These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags. Table 1 displays the analysis of an example sentence. 2 In most cases, normalization patterns replace a word form by its lemma; in order to partially preserve some inflection marks, we introduced two generic suffixes, +s and +en which respectively denote plural and genitive wherever needed. Typical normalization rules take the following form: • For articles, adjectives, and pronouns (Indefinite , possessive, demonstrative, relative and reflexive), if a token has; – Genitive case: replace with lemma+en (Ex. des, der, des, der → d+en) – Pl"
W10-1704,P07-2045,0,\N,Missing
W11-2135,W10-1704,1,0.806759,"the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require to postprocess the output to undo the pre-processing. As in our last year’s experiments (Allauzen et al., 2010), this pre-processing step could be achieved with a two-step decoding. However, by stacking two decoding steps, we may stack errors as well. Thus, for this direction, we used the German tokenizer provided by the organizers. 3.2 contains large portions that are not useful for translating news text. The first filter aime"
W11-2135,J92-4003,0,0.317321,"Missing"
W11-2135,J04-2004,0,0.208795,"is estimated and tuned as described in Section 4.1. Moreover, we also introduce in Section 4.2 the use of the SOUL language model (LM) (Le et al., 2011) in SMT. Based on neural networks, the SOUL LM can handle an arbitrary large vocabulary and a high order markovian assumption (up to 10-gram in this work). Finally, experimental results are reported in Section 5 both in terms of BLEU scores and translation edit rates (TER) measured on the provided newstest2010 dataset. 2 System Overview Our in-house n-code SMT system implements the bilingual n-gram approach to Statistical Machine Translation (Casacuberta and Vidal, 2004). Given a 1 This kind of characters was used for Teletype up to the seventies or early eighties. 309 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309–315, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics source sentence sJ1 , a translation hypothesis tˆ1I is defined as the sentence which maximizes a linear combination of feature functions: ( ) M tˆ1I = arg max t1I ∑ λm hm (sJ1 ,t1I ) (1) m=1 a word-aligned corpus (using MGIZA++2 with default settings) in such a way that a unique segmentation of the bilingual corpus is achi"
W11-2135,W08-0310,1,0.792506,"Missing"
W11-2135,D10-1044,0,0.0594607,"Missing"
W11-2135,D08-1076,0,0.0565273,"iew of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11"
W11-2135,P03-1021,0,0.271147,"s (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT), see details in Section 5.4), using the provided newstest2009 data as development set. 2.1 Training Our translation model is estimated over a training corpus composed of tuple sequences using classical smoothing techniques. Tuples are extracted from 310 The resulting sequence of tuples (1) is further refined to avoid NULL words in the source side of the tuples (2). Once the whole bilingual training data is segmented into tuples, n-gram language model probabilities can be estimated. In this example, note that the English source words perfect and translations"
W11-2135,P02-1040,0,0.0853055,"n models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010)), which aims at reducing the lexical redundancy and splitting complex compounds. Using the same pre-processing scheme to translate from English to German would require t"
W11-2135,W10-1748,0,0.0323177,"for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold. First, we have shown that n-gram based systems can achieve state-of-the-art performance on large scale tasks in terms of automatic metrics such as BLEU. Then, as already sho"
W11-2135,C08-1098,0,0.0510396,"action and reordering rules. 3 Data Pre-processing and Selection We used all the available parallel data allowed in the constrained task to compute the word alignments, except for the French-English tasks where the United Nation corpus was not used to train our translation models. To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English. Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994). For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). 3.1 Tokenization 4 We took advantage of our in-house text processing tools for the tokenization and detokenization steps (D´echelotte et al., 2008). Previous experiments have demonstrated that better normalization tools provide better BLEU scores (Papineni et al., 2002). Thus all systems are built in “true-case.” As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which poses a number of difficulties both at training and decoding time. Thus, to translate from German to English, the G"
W11-2135,2011.eamt-1.33,1,0.771526,"system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a generalpurpose finite state automata framework (Sokolov and Yvon, 2011). The last two approaches, hereafter referred to as ZHN and BBN, replace the BLEU objective function, with the usual BLEU score on expected n-gram counts (Rosti et al., 2010) and with an expected BLEU score for normal n-gram counts (Zens et al., 2007), respectively. All expecta314 Conclusion In this paper, we described our submissions to WMT’11 in the French-English and GermanEnglish shared translation tasks, in both directions. For this year’s participation, we only used n-code, our open source Statistical Machine Translation system based on bilingual n-grams. Our contributions are threefold."
W11-2135,N04-4026,0,0.357669,"estimated by using the n-gram assumption: K p(sJ1 ,t1I ) = ∏ p((s,t)k |(s,t)k−1 . . . (s,t)k−n+1 ) k=1 Figure 1: Tuple extraction from a sentence pair. where s refers to a source symbol (t for target) and (s,t)k to the kth tuple of the given bilingual sentence pair. It is worth noticing that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. In addition to the translation model, eleven feature functions are combined: a target-language model (see Section 4 for details); four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase-based system: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003) (Minimu"
W11-2135,D07-1080,0,0.0221974,"ever, showed any consistent and significant improvement for the majority of setups tried (with the exception of the BBN approach, that had almost always improved over n-best MERT, but for the sole French to English translation direction). Additional experiments with 9 complementary translation models as additional features were performed with lattice-MERT, but neither showed any substantial improvement. In the view of these rather inconclusive experiments, we chose to stick to the classical MERT for the submitted results. Optimization Issues Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al., 2007), MERT is the most widely used algorithm for system optimization. However, standard MERT procedure is known to suffer from instability of results and very slow training cycle with approximate estimates of one decoding cycle for each training parameter. For this year’s evaluation, we experimented with several alternatives to the standard n-best MERT procedure, namely, MERT on word lattices (Macherey et al., 2008) and two differentiable variants to the BLEU objective function optimized during the MERT cycle. We have recast the former in terms of a specific semiring and implemented it using a gen"
W11-2135,D07-1055,0,0.0524218,"Missing"
W11-2142,J04-2004,0,0.0163665,"e a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all 360 source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor. 2.3 LIMSI-CNRS Single System 2.3.1 System overview The LIMSI system is built with n-code2 , an open source statistical machine translation system based on bilingual n-grams. 2.3.2 n-code Overview In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model w"
W11-2142,J07-2003,0,0.0225833,"and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase"
W11-2142,W08-0310,1,0.899949,"Missing"
W11-2142,P07-1019,0,0.0206925,"or Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded ph"
W11-2142,P02-1040,0,0.102913,"Missing"
W11-2142,E03-1076,0,0.0231201,"l table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogeneous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 For the German→English task, RWTH conducted experiments comparing the standard phrase extraction with the phrase training technique described in Section 2.1.3. Further experiments included the use of additional language model training data, reranking of n-best lists generated by the phrase-based system, and different optimization criteria. A considerable increase in translation quality can be achieved by application of German compound splitting (Koehn and Knight, 20"
W11-2142,W07-0732,1,0.818478,"a statistical post editing (SPE) component. The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k − 800k entries per language pair). The basic setup of the SPE component is identical to the one described in (L. Dugast and Koehn, 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures − limiting unwanted statistical effects − were applied: • Named entities are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by t"
W11-2142,E06-1005,1,0.83205,"d 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. A deeper description will be also given in the WMT11 system combination paper of RWTH Aachen University. For this task only the A2L framework has been used. 4 Experiments We tried different system combinations with different sets of single systems and different optimization criteria. As RWTH has two different translation systems, we pu"
W11-2142,W09-0435,1,0.836207,"ystem applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract also phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences. Therefore,"
W11-2142,J03-1002,1,0.00747756,"joint translation by combining the knowledge of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algor"
W11-2142,P03-1021,0,0.147772,"etups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tab"
W11-2142,P07-2045,0,0.0131162,"parallel corpus (whose target is identical to the source). This was added to the parallel text in order to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W11-2142,2007.tmi-papers.21,0,0.0203552,"lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net/ ger (Schmid, 1994). In addition, the system applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract als"
W11-2142,N04-4026,0,0.0225696,"ew In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the news"
W11-2142,W05-0836,1,0.901096,"rd heuristic phrase extraction techniques, performing force alignment phrase training (FA) gives an improvement in BLEU on newstest2008 and newstest2009, but a degradation in TER. The addition of LDC Gigaword corpora (+GW) to the language model training data shows improvements in both BLEU and TER. Reranking was done on 1000-best lists generated by the the best available 359 Preprocessing System Overview The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation. Optimization with regard to the BLEU score is done using Minimum Error Rate Training as described by Venugopal et al. (2005). The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a GIZA++ Word Alignment. We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained on the Gigaword corpus. Reordering is performed based on continuous and non-continuous POS rules to cover short and long-range reorderings. The long-range reordering rules were also applied to the training corpus and phrase extraction was performed on the resulting reordering lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net"
W11-2142,W10-1738,1,0.832324,"are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institut"
W11-2142,P10-1049,1,0.823271,"ons. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and"
W11-2142,W06-3110,1,0.850765,"ase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and IBM-1 lexicon models in both normal and inverse direction. These models are combined in a log-linear fashion and the scaling factors are tuned in the same manner as the baseline system (using TER−4BLEU on newstest2009). The final table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each"
W11-2142,2008.iwslt-papers.8,1,0.820865,"preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hie"
W11-2142,D08-1076,0,\N,Missing
W11-2168,P07-1020,0,0.013269,"a two step process, where a set of possible source reorderings, represented as a parse forest, are associated with possible target sentences, using, as we do, a finitestate translation model. This translation model is trained discriminatively by marginalizing out the (unobserved) reordering variables; inference can be performed effectively by intersecting the input parse forest with a transducer representing translation options. A third strategy is to consider a simpler class of derivation process, which only partly describe the mapping between f and e. This is, for instance, the approach of (Bangalore et al., 2007), where a simple bag-of-word representation of the target sentence is computed using a battery of boolean classifiers (one for each target word). In this approach, discriminative training is readily applicable, as the required supervision is overtly present in example source-target pairs (f , e); however, a complementary reshaping/reordering step is necessary to turn the bag-of-word into a full-fledged translation. This work was recently revisited in (Mauser et al., 2009), where a conditional model predicting the presence of each target phrase provides a supplementary score for the standard “l"
W11-2168,D08-1023,0,0.22415,"rmed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented by performing training and inference over a graph"
W11-2168,P08-1024,0,0.660098,"The model thus defines the probability of a segmented target e e = eeI1 given the segmented and reordered source sentence ˜f = fe1I . To complete the model, one just needs to define a distribution over source segmentations P (˜f |f ). Given the deterministic relationship between e and e e expressed by the “unsegmentation” function φ which maps e e with e = φ(e e), we then have: X P (e|f ) = P (e e, ˜f |f ) ˜ f ,e e|φ(e e)=e = X P (e e, |˜f , f )P (˜f |f ) ˜ f ,e e|φ(e e)=e = X P (e e, |˜f )P (˜f |f ) ˜ f ,e e|φ(e e)=e 2 Assuming first order dependencies. This is a significant difference with (Blunsom et al., 2008), as we do not need to introduce latent variables during training. 3 544 In practice, we will only consider a restricted number of possible segmentation/reorderings of the source, denoted L(f ), and compute the best translation e∗ as φ(e e∗ ), where: e e∗ = arg max P (e e|f ) e e e, |˜f , f )P (˜f |f ) ≈ arg max P (e (1) ˜ f ∈L(f ),e e Even with these simplifying assumptions, this approach raises several challenging computational problems. First, training a CRF is quadratic in the number of labels, of which we will have plenty (typically hundreds of thousands). A second issue is decoding: as w"
W11-2168,J90-2002,0,0.716657,". Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to mak"
W11-2168,J04-2004,0,0.319641,"ity of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems. 1 To overcome the NP-hard problems that derive from the need to consider all possible permutations of the source sentence, we make here a radical simplification and consider training the translation model given a fixed segmentation and reordering. This idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and"
W11-2168,D08-1024,0,0.0537665,"Missing"
W11-2168,N09-1025,0,0.0413879,"use of a target language model in training and/or decoding. 5 Related work Discriminative learning approaches have proven successful for many NLP tasks, notably thanks to their ability to cope with flexible linguistic representations and to accommodate potentially redundant descriptions. This is especially appealing for machine translation, where the mapping between a source word or phrase and its target correlate(s) seems to involve an large array of factors, such as its morphology, its syntactic role, its meaning, its lexical context, etc. (see eg. (Och et al., 2004; Gimpel and Smith, 2008; Chiang et al., 2009), for inspiration regarding potentially useful features in SMT). Discriminative learning requires (i) a parameterized scoring function and (ii) a training objective. The scoring function is usually assumed to be linear and ranks candidate outputs y for input x according to θT G(x, y), where θ is the parameter vector. θ 549 and G deterministically imply the input/output mapping as x → arg maxy θT G(x, y). Given a set of training pairs {xi , y i , i = 1 . . . N }, parameters are learned by optimizing some regularized loss function of θ, so as to make the inferred input/output mapping faithfully"
W11-2168,P05-1033,0,0.130734,"extremely large. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented"
W11-2168,P05-1066,0,0.0417081,"f tuples such as: (demanda, said ) or (de nouveau, again). p(ul |ui−1 . . . ui−n+1 ). i=1 The probability of a sentence pair (f , e) is then either recovered by marginalization, or approximated 543 la femme voil´ee e: the veiled dame ˜f : la voil´ee femme demanda de nouveau said again Figure 1: The tuple extraction process The original (top) and reordered (bottom) French sentence aligned with its translation. At test time, the source text is reordered so as to match the reordering implied by the disentanglement procedure. Various proposals has been made to perform such source side reordering (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). sid"
W11-2168,W02-1001,0,0.0556563,"word aligned sentences (f , e), but lack the explicit derivation h from f to e that is required to train the model in a fully supervised way. The approach of (Liang et al., 2006a) circumvents the issue by assuming that the hidden derivation h can be approximated through forced decoding. Assuming that h is in fact observed as the optimal (Viterbi) derivation h∗ from f to e given the current parameter value10 , it is straightforward to recast the training of a phrase-based system as a standard structured learning problem, thus amenable to training algorithms such as the averaged perceptron of (Collins, 2002). This approximation is however not genuine, and the choice of the most appropriate derivation seems to raises intriguing issues (Watanabe et al., 2007; Chiang et al., 2008). The authors of (Blunsom et al., 2008; Blunsom and Osborne, 2008) consider models for which it is computationally possible to marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists"
W11-2168,P07-1033,0,0.0599041,"Missing"
W11-2168,P08-2007,0,0.0160335,"is idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallu"
W11-2168,W06-3105,0,0.0286103,"sed approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically inform"
W11-2168,N06-4004,0,0.0285908,"systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to make estimation less sensible to data sparsity issues and ("
W11-2168,N10-1128,0,0.0385918,"o marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists in the model, thus raising the issue of reference reachability, see discussion in Section 3. sociated with training and inference remains very high, especially when using a target side language model, which seems to preclude the application to large-scale translation tasks11 . The recent work of (Dyer and Resnik, 2010) proceeds from a similar vein: translation is however modeled as a two step process, where a set of possible source reorderings, represented as a parse forest, are associated with possible target sentences, using, as we do, a finitestate translation model. This translation model is trained discriminatively by marginalizing out the (unobserved) reordering variables; inference can be performed effectively by intersecting the input parse forest with a transducer representing translation options. A third strategy is to consider a simpler class of derivation process, which only partly describe the"
W11-2168,P08-1112,0,0.059883,"Missing"
W11-2168,W08-0509,0,0.0149225,"ing (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). side language model (LM), word and phrase penalties, etc). 2.2 Translating with CRFs A discriminative version of the n-gram approach consists in modeling P (e|f ) instead of P (e, f ), which can be efficiently performed with CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). Assuming matched sequences of observations (x = L xL 1 ) and labels (y = y1 ), CRFs express the conditional probability of labels as: P (y1L |xL 1) = 1 L exp(θT G(xL 1 , y1 )), Z(xL 1 ; θ) where θ is a parameter vector and G denotes a vector of feature functions testing various properties of x and y. In the linear-"
W11-2168,W08-0302,0,0.0125396,"segmentations, and the use of a target language model in training and/or decoding. 5 Related work Discriminative learning approaches have proven successful for many NLP tasks, notably thanks to their ability to cope with flexible linguistic representations and to accommodate potentially redundant descriptions. This is especially appealing for machine translation, where the mapping between a source word or phrase and its target correlate(s) seems to involve an large array of factors, such as its morphology, its syntactic role, its meaning, its lexical context, etc. (see eg. (Och et al., 2004; Gimpel and Smith, 2008; Chiang et al., 2009), for inspiration regarding potentially useful features in SMT). Discriminative learning requires (i) a parameterized scoring function and (ii) a training objective. The scoring function is usually assumed to be linear and ranks candidate outputs y for input x according to θT G(x, y), where θ is the parameter vector. θ 549 and G deterministically imply the input/output mapping as x → arg maxy θT G(x, y). Given a set of training pairs {xi , y i , i = 1 . . . N }, parameters are learned by optimizing some regularized loss function of θ, so as to make the inferred input/outp"
W11-2168,P07-1019,0,0.034987,"e. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability is crucial to achieve good performance and has to be preserved no matter the difficulties that occur when one moves away from conventional phrase-based systems (Chiang, 2005; Huang and Chiang, 2007; Blunsom and Osborne, 2008; K¨aa¨ ri¨ainen, 2009). It thus seems appropriate to include a LM feature function in our model or alternatively to define: P (e e|˜f ) = 1 PLM (e e) exp(θT G(˜f , e e)), ˜ Z(f ; θ) where PLM Pis the target language model and e) exp(θT G(˜f , e e)). ImpleZ(˜f ; θ) = e e PLM (e menting this approach implies to deal with the lack of synchronization between the units of the translation models, which are variable-length (possibly empty) tuples, and the units of the language models, which are plain words. In practice, this extension is implemented by performing training"
W11-2168,D09-1107,0,0.0313224,"Missing"
W11-2168,N03-1017,0,0.093172,"is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual"
W11-2168,N04-1022,0,0.0760989,": it suffices to perform the search in π2 (S ◦R)◦ −log(D)◦T ◦F ◦L, where L represents a n-gram language model. When combining several models, notably a source segmentation model and/or a target language model for rescoring, we have made sure to rescale the (log)probabilities so as to balance the language model scores with the CRF scores, and to use a fixed word bonus to make hypotheses of different length more comparable. All these parameters are tuned as part of the decoder development process. It is finally noteworthy that, in our architecture, alternative decoding strategies, such as MBR (Kumar and Byrne, 2004) are also readily implemented. 4 Experiments 4.1 Corpora and metrics For these experiments, we have used a medium size training corpus, extracted from the datasets made available for WMT 20116 evaluation campaign, and have focused on one translation direction, from French to English7 . Translation model training uses the entire NewsCommentary subpart of the WMT’2011 training 6 7 statmt.org/wmt11 Results in the other direction suggest similar conclusions. le : the/θle,the ∗ : the/0 0 ∗ : the/θthe ∗ : the/0 0 1 0 ∗ : cat/θthe,cat 1 chat : cat/θchat,cat DET : the/θDET,the Figure 2: Feature matche"
W11-2168,P10-1052,1,0.823642,"th a very small number of different labels. A first simplification is thus to consider that the set of possible “labels” ee for a source sequence fe is limited to those that are seen in training: all the other associations (fe, ee) are deemed impossible, which amounts to setting the corresponding parameter value to −∞. A second speed-up is to enforce sparsity in the model, through the use of a `1 regularization term (Tibshirani, 1996): on the one hand, this greatly reduces the memory usage; furthermore, sparse models are also prone to various optimization of the forward-backward computations (Lavergne et al., 2010). As discussed in (Ng, 2004; Turian et al., 2007), this feature selection strategy is well suited to the task at hand, where the number of possible features is extremely large. Optimization is performed using the Rprop algorithm4 (Riedmiller and Braun, 1993), which provides the memory efficiency needed to cope with the very large feature sets considered here. Training with a target language model One of the main strength of the phrase-based “log-linear” models is their ability to make use of powerful target side language models trained on very large amounts of monolingual texts. This ability i"
W11-2168,P06-1096,0,0.128,"Missing"
W11-2168,N06-1014,0,0.677324,"ing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools that exists for such tasks, most notably linear CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006). It also enables to easily integrate linguistically informed (describing morphological or morpho-syntactical properties of phrases) and/or contextual features into the translation model. In return, in addition to having a better trained model, we also expect (i) to make estimation less sensible to dat"
W11-2168,W02-1018,0,0.0546135,"on and consider training the translation model given a fixed segmentation and reordering. This idea is not new, and is one of the grounding principle of n-gram-based approaches (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) in SMT. The novelty here is that we will use this assumption to recast machine translation (MT) in the familiar terms of a sequence labeling task. Introduction A weakness of existing phrase-based SMT systems, that has been repeatedly highlighted, is their lack of a proper training procedure. Attempts to design probabilistic models of phrase-to-phrase alignments (e.g. (Marcu and Wong, 2002)) have thus far failed to overcome the related combinatorial problems (DeNero and Klein, 2008) and/or to yield improved training heuristics (DeNero et al., 2006). Phrase extraction and scoring thus rely on a chain of heuristics see (Koehn et al., 2003), which evolve phrase alignments from “symmetrized” word-toword alignments obtained with IBM models (Brown et al., 1990) and the like (Liang et al., 2006b; Deng and Byrne, 2006; Ganchev et al., 2008). Phrase scoring is also mostly heuristic and relies on an opThis reformulation allows us to make use of the efficient training and inference tools t"
W11-2168,J06-4004,1,0.936184,"Missing"
W11-2168,D09-1022,0,0.0228507,"s of derivation process, which only partly describe the mapping between f and e. This is, for instance, the approach of (Bangalore et al., 2007), where a simple bag-of-word representation of the target sentence is computed using a battery of boolean classifiers (one for each target word). In this approach, discriminative training is readily applicable, as the required supervision is overtly present in example source-target pairs (f , e); however, a complementary reshaping/reordering step is necessary to turn the bag-of-word into a full-fledged translation. This work was recently revisited in (Mauser et al., 2009), where a conditional model predicting the presence of each target phrase provides a supplementary score for the standard “log-linear” model. This line of research has been continued notably in (K¨aa¨ ri¨ainen, 2009), which introduces an exponential model of bag of phrases (allowing some overlap), that enables to capture localized dependencies between target words, while preserving (to some extend) the efficiency of training and inference. Supervision is here indirectly provided by word alignment and correlated phrase extraction processes implemented in conventional phrase-based systems (Koehn"
W11-2168,P02-1040,0,0.0865962,"ng. Various statistics regarding these corpora are reproduced on Table 1. All the training corpora were aligned using MGIZA++ with standard parameters8 , and processed in the standard tuple extraction pipeline. The development and test corpora were also processed analogously. For the sake of comparison, we also trained a standard n-gram-based and a Moses system (Koehn et al., 2007) with default parameters and a 3-gram target LM trained using only the target side of our parallel corpus. The development set (test 2009) was used to tune these two systems. All performance are measured using BLEU (Papineni et al., 2002). 8 As part of a much larger batch of texts. 4.2 Features The baseline system is composed only of translation features [trs] and target bigram features [t2g]. The former correspond to functions of the form e, i) = I(fei = s ∧ eei = t), where s gus,t (˜f , e and t respectively denote source and target phrases and I() is the indicator function. These are also generalized to part-of-speech and also to any possible source phrase, giving rise to features such as e, i) = I(e ei = t). Target bigram features gu∗,t = (˜f , e e, i) = correspond to functions of the form gbt,t0 (˜f , e I(e ei−1 = t ∧ eei"
W11-2168,N04-4026,0,0.119029,"Missing"
W11-2168,D07-1080,0,0.0463638,"he approach of (Liang et al., 2006a) circumvents the issue by assuming that the hidden derivation h can be approximated through forced decoding. Assuming that h is in fact observed as the optimal (Viterbi) derivation h∗ from f to e given the current parameter value10 , it is straightforward to recast the training of a phrase-based system as a standard structured learning problem, thus amenable to training algorithms such as the averaged perceptron of (Collins, 2002). This approximation is however not genuine, and the choice of the most appropriate derivation seems to raises intriguing issues (Watanabe et al., 2007; Chiang et al., 2008). The authors of (Blunsom et al., 2008; Blunsom and Osborne, 2008) consider models for which it is computationally possible to marginalize out all possible derivations of a given translation. As demonstrated in these papers, this approach is tractable even when the derivation process is a based on synchronous context-free grammars, rather that finitestate devices. However, the computational cost as10 If one actually exists in the model, thus raising the issue of reference reachability, see discussion in Section 3. sociated with training and inference remains very high, es"
W11-2168,P10-1049,0,0.099124,"Missing"
W11-2168,C04-1073,0,0.0361538,"anda, said ) or (de nouveau, again). p(ul |ui−1 . . . ui−n+1 ). i=1 The probability of a sentence pair (f , e) is then either recovered by marginalization, or approximated 543 la femme voil´ee e: the veiled dame ˜f : la voil´ee femme demanda de nouveau said again Figure 1: The tuple extraction process The original (top) and reordered (bottom) French sentence aligned with its translation. At test time, the source text is reordered so as to match the reordering implied by the disentanglement procedure. Various proposals has been made to perform such source side reordering (Collins et al., 2005; Xia and McCord, 2004), or even learning reordering rules based on syntactic or morphosyntactic information (Crego and Mari˜no, 2007). The latter approach amounts to accumulate reordering patterns during the training; test source sentences are then non-deterministically reordered in all possible ways yielding a word graph. This graph is then monotonously decoded, where the score of a translation hypothesis combines information from the translation models as well as from other information sources (lexicalized reordering model, target 1 Here, using the MGIZA++ package (Gao and Vogel, 2008). side language model (LM),"
W11-2168,P07-2045,0,\N,Missing
W11-2168,2009.eamt-smart.4,0,\N,Missing
W11-2168,N04-1021,0,\N,Missing
W12-2701,D07-1090,0,0.0636077,"am Model? On the Future of Language Modeling for HLT, pages 1–10, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics neural network language models (NNLMs) that has often been overlooked is the ability of the latter to fare with extended contexts (Schwenk and Koehn, 2008; Emami et al., 2008); in comparison, standard n-gram LMs rarely use values of n above n = 4 or 5, mainly because of data sparsity issues and the lack of generalization of the standard estimates, notwithstanding the complexity of the computations incurred by the smoothing procedures (see however (Brants et al., 2007) for an attempt to build very large models with a simple smoothing scheme). The recent attempts of Mikolov et al. (2011b) to resuscitate recurrent neural network architectures goes one step further in that direction, as a recurrent network simulates an unbounded history size, whereby the memory of all the previous words accumulates in the form of activation patterns on the hidden layer. Significant improvements in ASR using these models were reported in (Mikolov et al., 2011b; Mikolov et al., 2011a). It must however be emphasized that the use of a recurrent structure implies an increased compl"
W12-2701,J92-4003,0,0.308759,"iculty with the neural network approach is the complexity of inference and training, which largely depends on the size of the output vocabu2 The test sets used in MT experiments are made of various News extracts. Their content is thus not homogeneous and using words from previous sentences doesn’t seem to be relevant. 4 lary ,i.e. of the number of words that have to be predicted. To overcome this problem, Le et al. (2011a) have proposed the structured Output Layer (SOUL) architecture. Following (Mnih and Hinton, 2008), the SOUL model combines the neural network approach with a class-based LM (Brown et al., 1992). Structuring the output layer and using word class information makes the estimation of distribution over large output vocabulary computationally feasible. In the SOUL LM, the output vocabulary is structured in a clustering tree, where every word is associated to a unique path from the root node to a leaf node. Denoting wi the ith word in a sentence, the sequence c1:D (wi ) = c1 , . . . , cD encodes the path for word wi in this tree, with D the tree depth, cd (wi ) the class or sub-class assigned to wi , and cD (wi ) the leaf associated with wi , comprising just the word itself. The probabilit"
W12-2701,P96-1041,0,0.146216,"mply note that our parallel training data includes a large Web corpus, referred to as the GigaWord parallel corpus. After various preprocessing and filtering steps, the total amount of training data is approximately 12 million sentence pairs for the bilingual part, and about 2.5 billion of words for the monolingual part. To built the target language models, the monolingual corpus was first split into several sub-parts 3 http://www.statmt.org/wmt11 based on date and genre information. For each of these sub-corpora, a standard 4-gram LM was then estimated with interpolated Kneser-Ney smoothing (Chen and Goodman, 1996). All models were created without any pruning nor cutoff. The baseline back-off n-gram LM was finally built as a linear combination of several these models, where the interpolation coefficients are chosen so as to minimize the perplexity of a development set. All NNLMs are trained following the prescriptions of Le et al. (2011b), and they all share the same inner structure: the dimension of the projection word space is 500; the size of two hidden layers are respectively 1000 and 500; the short-list contains 2000 words; and the non-linearity is introduced with the sigmoid function. For the recu"
W12-2701,P05-1063,0,0.0247968,"mances as reported in (Mikolov et al., 2011a). To the best of our knowledge, it is the first recurrent NNLM trained on a such large dataset (2.5 billion words) in a reasonable time (about 11 days). 5 Related work There have been many attempts to increase the context beyond a couple of history words (see eg. (Rosenfeld, 2000)), for example: by modeling syn4 Pers. com. with T. Mikolov: on the ”small” WSJ data set, the recurrent model described in (Mikolov et al., 2011b) outperforms the 10-gram NNLM. tactic information, that better reflects the “distance” between words (Chelba and Jelinek, 2000; Collins et al., 2005; Schwartz et al., 2011); with a unigram model of the whole history (Kuhn and Mori, 1990); by using trigger models (Lau et al., 1993); or by trying to model document topics (Seymore and Rosenfeld, 1997). One interesting proposal avoids the ngram assumption by estimating the probability of a sentence (Rosenfeld et al., 2001). This approach relies on a maximum entropy model which incorporates arbitrary features. No significant improvements were however observed with this model, a fact that can be attributed to two main causes: first, the partition function can not be computed exactly as it invol"
W12-2701,H93-1021,0,0.119997,"set (2.5 billion words) in a reasonable time (about 11 days). 5 Related work There have been many attempts to increase the context beyond a couple of history words (see eg. (Rosenfeld, 2000)), for example: by modeling syn4 Pers. com. with T. Mikolov: on the ”small” WSJ data set, the recurrent model described in (Mikolov et al., 2011b) outperforms the 10-gram NNLM. tactic information, that better reflects the “distance” between words (Chelba and Jelinek, 2000; Collins et al., 2005; Schwartz et al., 2011); with a unigram model of the whole history (Kuhn and Mori, 1990); by using trigger models (Lau et al., 1993); or by trying to model document topics (Seymore and Rosenfeld, 1997). One interesting proposal avoids the ngram assumption by estimating the probability of a sentence (Rosenfeld et al., 2001). This approach relies on a maximum entropy model which incorporates arbitrary features. No significant improvements were however observed with this model, a fact that can be attributed to two main causes: first, the partition function can not be computed exactly as it involves a sum over all the possible sentences; second, it seems that data sparsity issues for this model are also adversely affecting the"
W12-2701,P03-1021,0,0.0102337,"uted on newstest2009-2011. On x axis, the number k represents the k th previous word. 1.0 0.8 0.6 0.4 INT > 15 AD J NO M AB R NA M AB K 10 &lt;s O R PR VE N KO V 5 AD 0 PU N DE T SY M PR P NU M 0.0 SE NT 0.2 Figure 3: Average selection rate of max function of the first previous word in terms of word POS-tag information, computed on newstest2009-2011. The green line represents the distribution of occurrences of each tag. of each hypothesis is computed and the k-best list is accordingly reordered. The NNLM weights are optimized as the other feature weights using Minimum Error Rate Training (MERT) (Och, 2003). For all our experiments, we used the value k = 300. To clarify the impact of the language model order in translation performance, we considered three different ways to use NNLMs. In the first setting, the NNLM is used alone and all the scores provided by the MT system are ignored. In the second setting (replace), the NNLM score replaces the score of the standard back-off LM. Finally, the score of the NNLM can be added in the linear combination (add). In the last two settings, the weights used for 7 Table 2: Results for the English to French task obtained with the baseline system and with var"
W12-2701,P11-1063,0,0.0137897,"(Mikolov et al., 2011a). To the best of our knowledge, it is the first recurrent NNLM trained on a such large dataset (2.5 billion words) in a reasonable time (about 11 days). 5 Related work There have been many attempts to increase the context beyond a couple of history words (see eg. (Rosenfeld, 2000)), for example: by modeling syn4 Pers. com. with T. Mikolov: on the ”small” WSJ data set, the recurrent model described in (Mikolov et al., 2011b) outperforms the 10-gram NNLM. tactic information, that better reflects the “distance” between words (Chelba and Jelinek, 2000; Collins et al., 2005; Schwartz et al., 2011); with a unigram model of the whole history (Kuhn and Mori, 1990); by using trigger models (Lau et al., 1993); or by trying to model document topics (Seymore and Rosenfeld, 1997). One interesting proposal avoids the ngram assumption by estimating the probability of a sentence (Rosenfeld et al., 2001). This approach relies on a maximum entropy model which incorporates arbitrary features. No significant improvements were however observed with this model, a fact that can be attributed to two main causes: first, the partition function can not be computed exactly as it involves a sum over all the p"
W12-2701,I08-2089,0,0.022956,"e translation tasks (Allauzen et al., 2011). Following these initial successes, the neural approach has recently been extended in several promising ways (Mikolov et al., 2011a; Kuo et al., 2010; Liu et al., 2011). Another difference between conventional and 1 NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 1–10, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics neural network language models (NNLMs) that has often been overlooked is the ability of the latter to fare with extended contexts (Schwenk and Koehn, 2008; Emami et al., 2008); in comparison, standard n-gram LMs rarely use values of n above n = 4 or 5, mainly because of data sparsity issues and the lack of generalization of the standard estimates, notwithstanding the complexity of the computations incurred by the smoothing procedures (see however (Brants et al., 2007) for an attempt to build very large models with a simple smoothing scheme). The recent attempts of Mikolov et al. (2011b) to resuscitate recurrent neural network architectures goes one step further in that direction, as a recurrent network simulates an unbounded history size, where"
W12-2701,P06-1124,0,0.0358957,"Missing"
W12-2701,W11-2135,1,\N,Missing
W12-3140,J04-2004,0,0.0800912,"nslation model relies on a specific decomposition of the joint probability of a sentence pair P(s, t) using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual units called tuples, defining a joint segmentation of the source and target. In the approach of (Mari˜no et al., 2006), this segmentation is a by-product of source reordering which ultimately derives from initial word and phrase alignments. 2.3.1 An Overview of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model"
W12-3140,J07-2003,0,0.0283293,"322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting"
W12-3140,W08-0310,1,0.935329,"Missing"
W12-3140,2010.iwslt-papers.6,0,0.0806631,"as last year5 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we took advantage of our in-house text processing tools for tokenization and detokenization steps (D´echelotte et al., 2008) and our system was built in ”true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 2010), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 SYSTRAN Software, Inc. Single System The data submitted by SYSTRAN were obtained by a system composed of the standard SYSTRAN MT engine in combination with a statistical post editing (SPE) component. 4 http://geek.kyloo.net/software 5 The fifth edition of the English Gigaword (LDC2011T07) was not used. 325 The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has alwa"
W12-3140,P07-1019,0,0.0343944,"on criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The"
W12-3140,E03-1076,0,0.55884,"ranslation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single Syst"
W12-3140,W07-0732,1,0.793396,"rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k 800k entries per language pair). The SYSTRAN phrase-based SPE component views the output of the rule-based system as the source language, and the (human) reference translation as the target language, see (L. Dugast and Koehn, 2007). It performs corrections and adaptions learned from the 5-gram language model trained on the parallel target-to-target corpus. Moreover, the following measures - limiting unwanted statistical effects - were applied: • Named entities, time and numeric expressions are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by the rule-based engine. • The intersection of both vocabularies (i.e. vocabularies of the rule-based output and the reference translation) is"
W12-3140,N12-1005,1,0.858653,"bilingual pairs, which means that the underlying vocabulary can be quite large. Unfortunately, the parallel data available to train these models are typically smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language 3 Part-of-speech labels for English and German are computed using the TreeTagger (Schmid, 1995). 2 http://ncode.limsi.fr/ 324 model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that can be estimated in a continuous space using the SOUL architecture (Le et al., 2011). The design and integration of a SOUL model for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities. The solution used here was to resort to a two pass approach: the first pass uses a conventional back-off n-gram model to produce a k-best list; in the second pass, t"
W12-3140,J06-4004,1,0.843277,"Missing"
W12-3140,E06-1005,1,0.849031,"glish LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 4 Experiments This year, we tried different sets of single systems for system combination. As RWTH has two different translation systems, we put the output of both systems into system combination. Although both systems have the same preprocessing and language model, their hypotheses differ because of their different decoding approach."
W12-3140,D09-1022,1,0.869178,"done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using t"
W12-3140,2011.iwslt-evaluation.9,1,0.871665,"ocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We us"
W12-3140,P10-2041,0,0.0181873,"ound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single System quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-bas"
W12-3140,W09-0435,1,0.860476,"Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as i"
W12-3140,W08-0303,1,0.84967,"very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters o"
W12-3140,2011.iwslt-papers.6,1,0.847434,"he Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Further"
W12-3140,W11-2124,1,0.868397,"length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known"
W12-3140,J03-1002,1,0.00977827,"RO partner trained their systems on the parallel Europarl and News Commentary corpora. All single systems were tuned on the newstest2009 or newstest2010 development set. The newstest2011 dev set was used to train the system combination parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram langu"
W12-3140,P03-1021,0,0.230828,"n (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out"
W12-3140,P07-2045,0,0.00885526,"to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. The SPE language model was trained on 2M bilingual phrases from the news/Europarl corpora, provided as training data for WMT 2012. An additional language model built from 15M phrases of the English LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W12-3140,W08-1006,0,0.100843,"ained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. For the test sentences, the reordering based on parts-of-speech and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily. In addition, we build reordering lattices for all trainin"
W12-3140,2007.tmi-papers.21,0,0.266844,"the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are"
W12-3140,N04-4026,0,0.0510078,"of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the n"
W12-3140,W05-0836,1,0.92366,"rmed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system a"
W12-3140,W10-1738,1,0.875756,"by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 P"
W12-3140,2008.iwslt-papers.8,1,0.841876,"parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2."
W12-3140,W11-2135,1,\N,Missing
W12-3140,W10-1704,1,\N,Missing
W12-3140,D08-1076,0,\N,Missing
W12-3141,W10-1704,1,0.815478,"eriments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built in “true-case”. Compared to last year, the pre-processing of utf-8 characters was significantly improved. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel"
W12-3141,E09-1010,1,0.834206,"lpful. As the IBM1 model is asymmetric, two models are estimated, one in both directions. Contrary to the reported results, these additional features do not yield significant improvements over the baseline system. We assume that the difficulty is to add information to an already extensively optimized system. Moreover, the IBM1 models are estimated on the same training corpora as the translation system, a fact that may explain the redundancy of these additional features. In a separate series of experiments, we also add WSD features calculated according to a variation of the method proposed in (Apidianaki, 2009). For each word of a subset of the input (source language) vocabulary, a simple WSD classifier produces a probability distribution over a set of translations8 . During reranking, each translation hypothesis is scanned and the word translations that match one of the proposed variant are rewarded using an additional score. While this method had given some Conclusion In this paper, we described our submissions to WMT’12 in the French-English and GermanEnglish shared translation tasks, in both directions. As for our last year’s participation, our main systems are built with n-code, the open source"
W12-3141,J93-2003,0,0.0934316,"ize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will a"
W12-3141,P05-1032,0,0.0155644,"e hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the s"
W12-3141,J04-2004,0,0.03102,"code, an open source in-house Statistical Machine Translation (SMT) system based on bilingual n-grams1 . The main novelty of this year’s participation is the use, in a large scale system, of the continuous space translation models described in (Hai-Son et al., 2012). These models estimate the n-gram probabilities of bilingual translation units using neural networks. We also investigate an alternative approach where the translation probabilities of a phrase based system are estimated “on-the-fly” 1 http://ncode.limsi.fr/ 2 System overview n-code implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006). In this framework, translation is divided in two steps: a source reordering step and a (monotonic) translation step. Source reordering is based on a set of learned rewrite rules that non-deterministically reorder the input words. Applying these rules result in a finite-state graph of possible source reorderings, which is then searched for the best possible candidate translation. 2.1 Features Given a source sentence s of I words, the best translation hypothesis ˆt is defined as the sequence of J words that maximizes a linear combination of fea33"
W12-3141,2010.iwslt-papers.6,0,0.024222,"Missing"
W12-3141,W08-0310,1,0.883725,"Missing"
W12-3141,N12-1005,1,0.885285,"Missing"
W12-3141,P07-2045,0,0.00674765,"lt (31.7 BLEU point) that is slightly worst than the n-code baseline (32.0) and slightly better than the equivalent Moses baseline (31.5), but does it much faster. Model estimation for the test file is reduced to 2 hours and 50 minutes, with an additional overhead for loading and writing files of one and a half hours, compared to roughly 210 hours for our baseline systems under comparable hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that th"
W12-3141,C08-1064,0,0.0749035,"rimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the standard traini"
W12-3141,J06-4004,0,0.217743,"Missing"
W12-3141,P03-1021,0,0.0736745,"x lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, sour"
W12-3141,P02-1040,0,0.0874389,"ation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, source sentences are represented in the form of word lattices containing the most promising reordering hypotheses, s"
W12-3141,C08-1098,0,0.0308747,"ing (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel data was selected. Word alignment models were trained using all the data, whereas the translation models were estimated on a subpart of the parallel data: the UN corpus was discarded for this step and about half of the French-English Giga corpus was filtered based on a perplexity criterion as in (Allauzen et al., 2011)). For French-English, we mainly upgraded the training material from last year by extracting th"
W12-3141,N04-4026,0,0.0279162,"max t,a M X ) λm hm (a, s, t) (1) L Y P (ui |ui−1 , ..., ui−n+1 ) (2) i=1 m=1 where λm is the weight associated with feature function hm and a denotes an alignment between source and target phrases. Among the feature functions, the peculiar form of the translation model constitute one of the main difference between the n-gram approach and standard phrase-based systems. This will be further detailled in section 2.2 and 3. In addition to the translation model, fourteen feature functions are combined: a target-language model (Section 5.3); four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT))"
W12-3141,2002.tmi-tutorials.2,0,0.0391067,"French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will actually allow us to take into account larger cont"
W12-3141,D08-1039,0,\N,Missing
W12-3141,W11-2135,1,\N,Missing
W12-3141,N04-1021,0,\N,Missing
W15-3012,W13-0805,1,0.885747,"SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word p"
W15-3012,P03-1054,0,0.0197594,"he actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et"
W15-3012,N03-1017,0,0.0324773,". 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source"
W15-3012,2005.iwslt-1.8,0,0.122737,"g, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to"
W15-3012,P07-2045,0,0.0106185,"rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in ord"
W15-3012,J04-2004,0,0.0881722,"s Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of"
W15-3012,N12-1005,1,0.956768,"oint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translated candidates. Second, an n-gram translation model from LIMSI, named SOUL (Structured OUtput Layer), helps to rescore the k-best list by utilizing features extracted from translated tuples. In this year participation, we also use a version of the neural network translation models (Le et al., 2012) trained using NCE algorithm (Gutmann and Hyv¨arinen, 2010) as counterpart to SOUL models. A ListNet2.1 Data and Preprocessing The parallel data mainly used are the corpora extracted from Europarl Parliament (EPPS), News Commentary (NC) and the common part of webcrawled data (Common Crawl). The monolingual data are the monolingual part of those corpora. A preprocessing step is applied to the raw data before the actual training. It includes removing excessively long and length-mismatched sentences pairs. Special symbols and nummeric data are normalized, and smartcasing is applied. Sentence pair"
W15-3012,E99-1010,0,0.0623657,"onolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words. All of the language models are trained using the SRILM toolkit (Stolcke, 2002); The word-based 121 org : .... à recevoir le prix nobel de la paix s : .... s8: à s9: recevoir s10: le s11: nobel de la paix s12: prix .... t : .... t8: to t9: receive t10: the t11: nobel peace t12: prize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just abov"
W15-3012,P06-1096,0,0.312547,"Missing"
W15-3012,2007.tmi-papers.21,0,0.269383,"which contain textual elements in different 120 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and Germa"
W15-3012,J06-4004,0,0.194243,"Missing"
W15-3012,D07-1045,0,0.386972,"Missing"
W15-3012,D09-1022,0,0.247402,"Missing"
W15-3012,W05-0836,1,0.875755,"(Vogel, 2003) which finds the best combinations of features in a log-linear framework. The features consist of translation scores, distortion-based and lexicalized reordering scores as well as conventional and non-word language models. In addition, several reordering rules, including short-range, long-range and tree-based reorderings, are applied before decoding step as they are encoded as word lattices. The decoder then generates a list of the best candidates from the lattices. To optimize the factors of individual features on a development dataset, we use minimum error rate training (MERT) (Venugopal et al., 2005). We are going to describe those components in detail as follows. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points. 1 KIT Phrase-based Translation System Introduction In this paper, we present the English→German joint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translat"
W15-3012,W09-0435,1,0.878365,"eedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ to"
W15-3012,W08-0303,1,0.817804,"C. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy class"
W15-3012,W13-2264,1,0.876138,"ose phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to train those classifiers. 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et"
W15-3012,W11-2124,1,0.841127,"sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of a synchronized and segmented sentence pair can be estimated using the n-gram assumption. During training, the segmentation is obtained as a Besides word-based n-gram language models trained on all preprocessed monolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words."
W15-3012,W15-3030,1,0.645912,"(Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs. Therefore, NCE models deliver a positive score, by applying the exponential function to the output layer activities, 122 4 Rescoring 5 After generating translation probabilities using the neural network translation models, we need to combine them with the baseline scores of the phrase-based system in order to select better translations from the k-best lists. As it is done in the baseline decoder, we used a log-linear combination of all features. We trained the model using the ListNet algorithm (Niehues et al., 2015; Cao et al., 2007). This technique defines a probability distribution on the permutations of the list based on the scores of the log-linear model and one based on a reference metric. Therefore, a sentence-based translation quality metric is necessary. In our experiments we used the BLEU+1 score introduced by Liang et al. (2006). Then the model was trained by minimizing the cross entropy between both distributions on the development data. Using this loss function, we can compute the gradient with respect to the weight ωk as follows: System Baseline + ListNet rescoring + NCE + SOUL + NCE + SOUL"
W15-3012,J03-1002,0,0.0188001,"are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice o"
W15-3012,W11-2123,0,\N,Missing
W15-3012,P14-1129,0,\N,Missing
W15-3016,N12-1005,1,0.878657,"plexity differences for both in-domain and out-of-domain LMs. Sentences pairs are ranked according to the MML score and the top N parallel sentences are used to learn the translation table used during decoding. For LM adaptation, we used a log-linear combination of our large LM with a smaller one trained only on the monolingual in-domain corpus.6 SOUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. As in our previous participations (Le et al., 2012b; Allauzen et al., 2013; P´echeux et al., 2014), we take advantage of the proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large output vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4"
W15-3016,D11-1033,0,0.0857942,"Missing"
W15-3016,N12-1047,0,0.227279,"Missing"
W15-3016,P06-2093,0,0.251311,"Missing"
W15-3016,W08-0310,1,0.88318,"Missing"
W15-3016,J07-1003,0,0.0605213,"Missing"
W15-3016,N13-1073,0,0.0478227,"En corpus, no improvement could be observed (Koehn and Haddow, 2012). 2 145 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 145–151, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. pora, we finally removed all sentence pairs that did not match the default criteria of the M OSES script clean-corpus-n.pl or that contained more than 70 tokens. Statistics regarding the parallel corpora used to train SMT systems are reported in Table 1 for the three language pairs under study. Word-level alignments are computed using fast align (Dyer et al., 2013) with options ”-d -o -v”. 2.2 challenge for this task is domain adaptation, for which only monolingual data are distributed. 3.1 Since this is the first time this translation task is considered, only a small development set of newsdiscusssions is available. In order to properly tune and test our systems, we performed a 3-fold crossvalidation, splitting the 1,500 in-domain sentences in two parts. Each random split respects document boundary, and yields roughly 1,000 sentences for tuning and 500 sentences for testing. The source of the documents, the newspapers Le Monde and The Guardian are also"
W15-3016,P13-2121,0,0.0327932,"Missing"
W15-3016,2008.amta-srw.3,0,0.109957,"Missing"
W15-3016,W12-3139,0,0.01708,"on and word alignments Tokenization for French and English text relies on in-house text processing tools (D´echelotte et al., 2008). All bilingual corpora provided by the organizers were used, except for the FrenchEnglish tasks where the UN corpus was not considered.3 We also used a heavily filtered version of the Common Crawl corpus, where we discard all sentences pairs that do not look like proper French/English parallel sentences. For all cor1 http://ncode.limsi.fr http://www.statmt.org/moses/ 3 In fact, when used in combination with the Giga Fr-En corpus, no improvement could be observed (Koehn and Haddow, 2012). 2 145 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 145–151, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. pora, we finally removed all sentence pairs that did not match the default criteria of the M OSES script clean-corpus-n.pl or that contained more than 70 tokens. Statistics regarding the parallel corpora used to train SMT systems are reported in Table 1 for the three language pairs under study. Word-level alignments are computed using fast align (Dyer et al., 2013) with options ”-d -o -v”. 2.2 challenge for this t"
W15-3016,P07-2045,0,0.00467831,"irst into simplified Russian, followed by a conversion into inflected Russian. For French-English, the challenge is domain adaptation, for which only monolingual corpora are available. Finally, for the Finnish-to-English task, we explore unsupervised morphological segmentation to reduce the sparsity of data induced by the rich morphology on the Finnish side. 1 2 Systems Overview Our experiments use N CODE1 , an open source implementation of the n-gram approach, as well as M OSES, which implements a vanilla phrase-based approach.2 For more details about these toolkits, the reader can refer to (Koehn et al., 2007) for M OSES and to (Crego et al., 2011) for N CODE. Introduction This paper documents LIMSI’s participation to the machine translation shared task for three language pairs: French-English and Russian-English in both directions, as well as Finnish-into-English. Each of these tasks poses its own challenges. For French-English, the task differs slightly from previous years as it considers user-generated news discusssions. While the domain remains the same, the texts that need to be translated are of a less formal type. To cope with the style shift, new monolingual corpora have been made available"
W15-3016,P10-1052,1,0.875811,"Missing"
W15-3030,N12-1047,0,0.0900077,"es not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motivated by a pairwise technique, while the work presented in this paper is based on the listwise algorithm ListNet presented in (Cao et al., 2007). Other methods based on more complex models have also been presented, for example (Liu et al., 2013), which uses an additive neural network instead of linear models. 3 exp(sj ) Ps (j) = Pn , k=1 exp(sk ) where sj is a score assigned to the j-th entry of ("
W15-3030,D08-1024,0,0.0298173,"fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community ("
W15-3030,P12-1031,0,0.0157894,"over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motivated by a pairwise technique, while the work presented in this paper is based on the listwise algorithm ListNet"
W15-3030,D11-1125,0,0.115139,"number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motiva"
W15-3030,P07-2045,0,0.00929575,"n defines four different scores to evaluate a hypothesis. In such architecture, the size of the output vocabulary is a bottleneck when normalized distributions are needed. For efficient computation, these models rely on a tree-structured output layer called SOUL (Le et al., 2011). An effective alternative, which however only delivers unnormalized scores, is to train the network using the Noise 5.2 Other optimization techniques For comparison, experimental results include performance obtained with the most widely used algorithms: MERT, KB-MIRA (Cherry and Foster, 2012) as implemented in Moses (Koehn et al., 2007), along with the PRO algorithm. For the latter, we used the MegaM1 version (Daum´e III, 2004). All the results correspond to three random restarts and the weights are chosen according to the best performance on the development data. 5.3 WMT – English to German The results for the English to German news translation task are summarized in Table 1. The translations generated by the phrase-based decoder reach a BLEU score of 20.19. We compared the presented approach with MERT, KB-MIRA and PRO. KB-MIRA and MERT improve the performance by at most 0.3 BLEU points. In contrast, the PRO technique and t"
W15-3030,2014.iwslt-evaluation.1,1,0.736667,"ques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015 for the German– English language pair in both directions. The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign (Cettolo et al., 2014). The systems using the ListNet-based rescoring were submitted to this evaluation campaigns and when evaluated using the BLEU score they were all ranking within the top 3. Before discussing the results, we summarize the translation systems used for experiments along with the additionnal features that rely on continuous space translation models. 5.1 Systems The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system"
W15-3030,P06-1096,0,0.397769,"Missing"
W15-3030,P13-1078,0,0.0290246,"Missing"
W15-3030,2012.iwslt-papers.3,1,0.83718,"e of possible scores. 250 Contrastive Estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012) denoted by NCE in the rest of the paper. In this work, we used these both solutions as well as their combination. For the German to English translation task, we added a source side discriminative word lexicon (Herrmann, 2015). This model used a multi-class maximum entropy classifier for every source word to predict the translation given the context of the word. In addition, we used a neural network translation model using the technique of RBM (Restricted Boltzman Machine)-based language models (Niehues and Waibel, 2012). The baseline system for the TED translation task uses the IWSLT 2015 training data. The system was adapted to the domain by using language model and translation model adaptation techniques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015"
W15-3030,P03-1021,0,0.0402058,"tion for Computational Linguistics. The aim is then to find a function fω that assigns a (i) score to every feature vector xj . This function is fully defined by its set of parameters ω. Using the (i) (i) vector of scores z (i) = {fω (x1 ) . . . , fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and th"
W15-3030,P02-1040,0,0.0926887,"entional scores calculated during decoding, as well as additional models such as neural network translation models. 4.1 Although both methods could be applied together, we did only use one of them, since both methods have similar effects. If not stated differently, we use the feature normalization method in our experiments. 4.2 To estimate the weights, we need to define a probability distribution Py associated to the reference ranking y following Euqation 1. In this work, we propose a distribution based on machine translation evaluation metrics. The most widely used evaluation metric is BLEU (Papineni et al., 2002), which only produces a score at the corpus level. As proposed by Hopkins and May (2011), we will use a smoothed sentence-wise BLEU score to generate the reference ranking. In this work, we use the BLEU+1 score introduced by Liang et al. (2006). When (i) using sj = BLEU(xj ) in Equation 1, whe get the follwing defintion of the probability distribution Py : Score normalization (i) The scores (xj )k are, for example, language model log-probabilities. Since the language model probabilities are calculated as the product of several n-gram probabilities, these values are typically very small. Theref"
W15-3030,W11-2119,0,0.0487127,"Missing"
W15-3030,2014.iwslt-evaluation.17,1,0.77366,"tive word lexicon (Herrmann, 2015). This model used a multi-class maximum entropy classifier for every source word to predict the translation given the context of the word. In addition, we used a neural network translation model using the technique of RBM (Restricted Boltzman Machine)-based language models (Niehues and Waibel, 2012). The baseline system for the TED translation task uses the IWSLT 2015 training data. The system was adapted to the domain by using language model and translation model adaptation techniques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015 for the German– English language pair in both directions. The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign (Cettolo et al., 2014). The systems using the ListNet-based rescoring were submitted to this evaluati"
W15-3030,D07-1080,0,0.0294196,"z (i) = {fω (x1 ) . . . , fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the mac"
W15-3030,2010.iwslt-evaluation.11,1,\N,Missing
W15-3030,N12-1005,1,\N,Missing
W15-3030,W15-3008,1,\N,Missing
W16-2304,D12-1133,0,0.0799569,"Missing"
W16-2304,J04-2004,0,0.202109,"006a). Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved (Mari˜no et al., 2006) and n-gram translation models are then estimated over the training corpus composed of tuple sequences made of surface forms or POS tags. Reordering rules are automatically learned during the unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006a). 2.3 2.4 2.2 Language modelling and domain adaptation N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006b; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate conti"
W16-2304,N12-1047,0,0.110665,"to be too large to be handled by the CRF, so the following experiments were performed on the 300-best output. • words are projected into a 500-dimensional vector space; • the feed-forward architecture includes two hidden layers of size 1000 and 500; • the non-linearity is a sigmoid function; All models are trained for 20 epochs, then the selection relies on the perplexity measured on a validation set. For CTMs, the validation sets are sampled from the parallel training data. 3 Development and test sets Experiments For all our experiments, the MT systems are tuned using the kb-mira algorithm (Cherry and Foster, 2012) implemented in M OSES, including the reranking step. POS tagging is performed using the TreeTagger (Schmid, 1994) for English and Russian (Sharoff and Nivre, 2011), and TTL (Tufis¸ et al., 2008) for Romanian. 4 241 http://pymorphy.readthedocs.io/ In order to train this model, we split the parallel data in two parts. The first (largest) part was used to train the translation system baseline. The second part was used for the training of the hidden CRF. First, the source side was translated with the baseline system, then the resulting output was extended (paradigm generation). References were ob"
W16-2304,W08-0310,1,0.725891,"Missing"
W16-2304,L16-1241,1,0.787062,"ns in Romanian and Russian corpora is a lot higher than in English corpora. In such a situation, surface heuristics are less reliable. In order to address this limitation, we tried to extend the output of the decoder with morphological variations of nouns, pronouns and adjectives. Therefore, for each word in the output baring one of these PoS-tags, we introduced all forms in the paradigm as possible alternatives. The paradigm generation was performed for Russian using pymorphy, a dictionary implemented as a Python module.4 For Romanian, we used the crawled (thus sparse) lexicon introduced in (Aufrant et al., 2016). Once the outputs were extended, we used a CRF model to rescore this new search space. The CRF can use the features of the MT decoder, but can also include morphological or syntactic features in order to estimate output scores, even for words that were not observed in the training data. In the Russian experiment, oracle scores show that a maximum gain of 6.3 BLEU points can be obtained if the extension is performed on the full search space and 2.3 BLEU points on 300-best output of the N CODE decoder. The full search space, while being more promising, proved to be too large to be handled by th"
W16-2304,P14-1129,0,0.110474,"Missing"
W16-2304,W11-2123,0,0.0412066,"s then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompose the joint probability of a sentence pair in a sequence of bilingual units called tuples. ∗ e = arg max e,a K X λk fk (f, e, a) k=1 Various English, Romanian and Russian language models (LM) were trained on the in-domain monolingual corpora, a subset of the commoncrawl corpora and the relevant side of the parallel corpora (for English, the English side of the Czech-English parallel data was used). We trained 4-gram LMs, pruning all singletons with lmplz (Heafield, 2011). In addition to in-domain monolingual data, a considerable amount of out-of-domain data was provided this year, gathered in the common-crawl corpora. Instead of directly training an LM on these corpora, we extracted from them in-domain sentences using the Moore-Lewis (Moore and Lewis, 2010) filtering method, more specifically its implementation in XenC (Rousseau, 2013). As a result, the common-crawl sub-corpora we have used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke,"
W16-2304,P07-2045,0,0.00443495,"translation into Russian and Romanian, we have attempted to extend the output of the decoder with morphological variations and to use a CRF model to rescore this new search space; as for the translation into German, we have been experimenting with source-side pre-ordering based on a dependency structure allowing permutations in order to reproduce the target word order. 1 2 System Overview Our experiments mainly use N CODE,1 an open source implementation of the n-gram approach, as well as M OSES2 for some contrastive experiments. For more details about these toolkits, the reader can refer to (Koehn et al., 2007) for M OSES and to (Crego et al., 2011) for N CODE. 2.1 Data pre-processing and word alignments All the English and Russian data have been cleaned by normalizing character encoding. Tokenization for English text relies on in-house text processing tools (D´echelotte et al., 2008). For the Russian corpora, we used the TreeTagger tokenizer. For Romanian, we developed and used tokro, a rule-based tokenizer. After normalization of diacritics, it repeatedly applies 3 rules: (a) word splitting on slashes, except for url addresses, (b) isolation of punctuation characters from a predefined set (includi"
W16-2304,F13-1033,1,0.854254,"ssian (Sharoff and Nivre, 2011), and TTL (Tufis¸ et al., 2008) for Romanian. 4 241 http://pymorphy.readthedocs.io/ In order to train this model, we split the parallel data in two parts. The first (largest) part was used to train the translation system baseline. The second part was used for the training of the hidden CRF. First, the source side was translated with the baseline system, then the resulting output was extended (paradigm generation). References were obtained by searching for oracle translations in the augmented output. Models were trained using inhouse implementation of hidden CRF (Lavergne et al., 2013) and used features from the decoder as well as additional ones: unigram and bigram of words and POS-tags; number, gender and case of the forms and of the surrounding ones; and information about nearest prepositions and verbs. 3.3 coding it. Reorderings of the source sentence are compactly encoded in a permutation lattice generated by iteratively applying POS-based reordering rules extracted from the parallel data. In this year’s WMT evaluation campaign we investigated ways to improve the re-ordering step by re-implementing the approach proposed by (Lerner and Petrov, 2013). This approach aims"
W16-2304,P06-2093,0,0.0731995,"Missing"
W16-2304,D07-1045,0,0.394133,"Missing"
W16-2304,N12-1005,1,0.933551,"sely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012a; Devlin et al., 2014). 3 Bilingual Sentence Aligner, available at http:// research.microsoft.com/apps/catalog/ 240 3.1 As in our previous participations (Le et al., 2012b; Allauzen et al., 2013; P´echeux et al., 2014; Marie et al., 2015), we take advantage of the proposal of (Le et al., 2012a). Using a specific neural network architecture, the Structured OUtput Layer (SOUL), it becomes possible to estimate n-gram models that use large output vocabulary, thereby making the training of large neural network language models feasible both for target language models (Le et al., 2011) and translati"
W16-2304,D13-1049,0,0.0232522,"ntation of hidden CRF (Lavergne et al., 2013) and used features from the decoder as well as additional ones: unigram and bigram of words and POS-tags; number, gender and case of the forms and of the surrounding ones; and information about nearest prepositions and verbs. 3.3 coding it. Reorderings of the source sentence are compactly encoded in a permutation lattice generated by iteratively applying POS-based reordering rules extracted from the parallel data. In this year’s WMT evaluation campaign we investigated ways to improve the re-ordering step by re-implementing the approach proposed by (Lerner and Petrov, 2013). This approach aims at taking advantage of the dependency structure of the source sentence to predict a permutation of the source words that is as close as possible to a correct syntactic word order in the target language: starting from the root of the dependency tree a classifier is used to recursively predict the order of a node and all its children. More precisely, for a family5 of size n, a multiclass classifier is used to select the best ordering of this family among its n! permutations. A different classifier is trained for each possible family size. Experimental results The experimenta"
W16-2304,N04-4026,0,0.0644058,"e used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match the target word order by unfolding the word alignments (Crego and Mari˜no, 2006a). Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved (Mari˜no et al., 2006) and n-gram translation model"
W16-2304,W15-3016,1,0.856173,"Missing"
W16-2304,tufis-etal-2008-racais,0,0.271893,"Missing"
W16-2304,J06-4004,0,0.296295,"Missing"
W16-2304,2002.tmi-tutorials.2,0,0.147945,"ram translation models are then estimated over the training corpus composed of tuple sequences made of surface forms or POS tags. Reordering rules are automatically learned during the unfolding procedure and are built using partof-speech (POS), rather than surface word forms, to increase their generalization power (Crego and Mari˜no, 2006a). 2.3 2.4 2.2 Language modelling and domain adaptation N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006b; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, Continuous-space models Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve conventional language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012a; Devlin et al., 2014). 3 Bilingual Sentence Aligner,"
W16-2304,P10-2041,0,0.0322506,"ish, Romanian and Russian language models (LM) were trained on the in-domain monolingual corpora, a subset of the commoncrawl corpora and the relevant side of the parallel corpora (for English, the English side of the Czech-English parallel data was used). We trained 4-gram LMs, pruning all singletons with lmplz (Heafield, 2011). In addition to in-domain monolingual data, a considerable amount of out-of-domain data was provided this year, gathered in the common-crawl corpora. Instead of directly training an LM on these corpora, we extracted from them in-domain sentences using the Moore-Lewis (Moore and Lewis, 2010) filtering method, more specifically its implementation in XenC (Rousseau, 2013). As a result, the common-crawl sub-corpora we have used contained about 200M sentences for Romanian and 300M for Russian and English. Finally, we perform a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional fe"
W16-2314,W16-2304,1,0.809831,"ed in this evaluation will be described. In this paper, we present the KIT translation systems as well as the KIT-LIMSI systems for the ACL 2016 First Conference on Machine Translation. We participated in the shared task Machine Translation of News and submitted translation systems for three different directions: English→German, German→English and English→Romanian. 2.1 For training our systems, we used all the data provided by the organizers. In all of our translation systems, the preprocessing step was conducted prior to training. For English→Romanian, we used the preprocessing described in (Allauzen et al., 2016). For the systems involving German and English, it includes removing very long sentences and the sentence pairs which are length-mismatched, normalizing special symbols and smart-casing the first word of each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-ba"
W16-2314,J04-2004,0,0.034554,"ithub.com/lmthang/nmt.matlab Initialization is an important issue when optimizing neural networks. For CTMs, a solution consists in pre-training monolingual n-gram models. Their parameters are then used to initialize bilingual models. 2007) as a potential mean to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014). As in previous submissions, we investigated the integration of n-gram CTMs. Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), an n-gram translation model is constructed based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand. A sentence pair is decomposed into a sequence of bilingual units called tuples defining a joint segmentation. The joint probability of a synchronized and segmented sentence pair can be estimated using the n-gram assumption. During training, the segmentation is obtained as a by-product of source reordering. During the inference step, the SMT dec"
W16-2314,2000.eamt-1.5,0,0.190582,"Missing"
W16-2314,P14-1129,0,0.101445,"Missing"
W16-2314,N12-1005,1,0.872007,"ides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). In this evaluation, we evaluated a combination of both. We used RNN-based language models that use a factored representation. We hoped to improve the modeling of rare words by richer word representations. In the experiments we used up to four different word factors: the word surface form, the POS tags as well as two cluster based wor"
W16-2314,W15-3012,1,0.713624,"Missing"
W16-2314,P06-1096,0,0.260981,"Missing"
W16-2314,W11-2123,0,0.0251631,"Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from Moses toolkit (Koehn et al., 2007). Unless stated otherwise, we used 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). In the decoding phase, the LMs were scored by KenLM toolkit (Heafield, 2011). In We used a phrase-based machine translation system and investigated several models to rescore the system. We used neural network language and translation models. Using these models, we could improve the translation performance in all language pairs we participated. 1 Introduction Following the research we have been conducted over previous years, in this paper, we describe our phase-based translation systems submitted to the First Conference on Machine Translation with the highlights on our new models. In this evaluation, we mainly focused on using neural models in rescoring of a phrase-bas"
W16-2314,D15-1166,0,0.031325,"n of both. We used RNN-based language models that use a factored representation. We hoped to improve the modeling of rare words by richer word representations. In the experiments we used up to four different word factors: the word surface form, the POS tags as well as two cluster based word fac304 Figure 2: The recurrent encoder-decoder architecture for MT proposed by (Cho et al., 2014) Figure 1: Factored RNN Layout of attention mechanism allow us to train the networks to be capable of remembering longer contexts and putting decent word alignments between two sentences (Bahdanau et al., 2015; Luong et al., 2015). Instead of using the architecture in an end-toend fashion, which often called Neural MT (Bahdanau et al., 2015), in order to leverage other translation models that the phrase-based system produces, we opted to use it in our rescoring scheme (see 3.4). We adapted the Neural MT framework1 from (Luong et al., 2015) to be able to compute the conditional probability p(f, ei ) in which f is the source sentence and ei is the ith translation candidate of f produced by our phrase-based decoder. Due to the limited time, this recurrent encoderdecoder-based (ReEnDe) feature was only employed in the dire"
W16-2314,W13-0805,1,0.83729,"nslation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored wor"
W16-2314,J06-4004,0,0.0970116,"Missing"
W16-2314,2015.iwslt-papers.3,1,0.710126,"2009), is a lexical translation model which calculates the probability of a target word given the words of the source sentence. (Niehues and Waibel, 2013) proposed an extension of DWL where they use n consecutive source words as one feature, thus they could incorporate better the order information of the source sentences into classification. In addition to this DWL, we integrated a DWL in the reverse direction in rescoring. We will refer to this model as source DWL (SDWL). This model predicts the target word for a given source word using numbers of context features as described in details in (Herrmann et al., 2015). To deal with the differences in word order between source and target languages, our systems employed various reordering strategies, which are described in the next section. 2.2 3 N -best list rescoring In order to easily integrate more complex models, we used n-best list rescoring in our submission. We evaluated a neural network language model using a factored representation of the words. Using this framework, we were also able to easily extend the model to a bilingual model. Furthermore, we investigated the use of an encoder-decoder model in rescoring. Finally, in cooperation with LIMSI, we"
W16-2314,D09-1022,0,0.0653791,"Missing"
W16-2314,D07-1091,0,0.0617416,"to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). In this evaluation, we evaluated a combination of both. We used RNN-based language models that use a factored representation. We hoped to improve the modeling of rare words by richer word representations. In the experiments we used up to four different word factors: the word surface form, the POS tags as well as two cluster based word fac304 Figure 2: The recurrent encoder-decoder architecture for MT proposed by (Cho et al., 2014) Figure 1: Factored RNN L"
W16-2314,P03-1054,0,0.0137533,"odels, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). In this evaluation, we eval"
W16-2314,2011.iwslt-evaluation.9,1,0.890671,"prior to training. For English→Romanian, we used the preprocessing described in (Allauzen et al., 2016). For the systems involving German and English, it includes removing very long sentences and the sentence pairs which are length-mismatched, normalizing special symbols and smart-casing the first word of each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from"
W16-2314,E03-1076,0,0.154227,"erent directions: English→German, German→English and English→Romanian. 2.1 For training our systems, we used all the data provided by the organizers. In all of our translation systems, the preprocessing step was conducted prior to training. For English→Romanian, we used the preprocessing described in (Allauzen et al., 2016). For the systems involving German and English, it includes removing very long sentences and the sentence pairs which are length-mismatched, normalizing special symbols and smart-casing the first word of each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the"
W16-2314,W15-4917,1,0.842151,"Missing"
W16-2314,W09-0435,1,0.830188,"uous space translation models in rescoring. We used the ListNet approach as described in Section 3.4 to estimate the weights of different models in our systems. Reordering Models In all translation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of p"
W16-2314,W13-2264,1,0.886114,"nslation systems. They included bilingual LMs, cluster LMs and the LMs based on POS sequences. For cluster and POS-based LMs, we used an n-gram size of nine tokens. During decoding, these language models were used as additional models in the log-linear combination. A family of lexical translation models, which we called discriminative word lexicon (DWL), were also utilized in our translation systems. A discriminative word lexicon, first introduced by (Mauser et al., 2009), is a lexical translation model which calculates the probability of a target word given the words of the source sentence. (Niehues and Waibel, 2013) proposed an extension of DWL where they use n consecutive source words as one feature, thus they could incorporate better the order information of the source sentences into classification. In addition to this DWL, we integrated a DWL in the reverse direction in rescoring. We will refer to this model as source DWL (SDWL). This model predicts the target word for a given source word using numbers of context features as described in details in (Herrmann et al., 2015). To deal with the differences in word order between source and target languages, our systems employed various reordering strategies"
W16-2314,W05-0836,0,0.102374,"each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from Moses toolkit (Koehn et al., 2007). Unless stated otherwise, we used 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). In the decoding phase, the LMs were scored by KenLM toolkit (Heafield, 2011). In We used a phrase-based machine translation sys"
W16-2314,J03-1002,0,0.0288076,"he German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from Moses toolkit (Koehn et al., 2007). Unless stated otherwise, we used 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). In the decoding phase, the LMs were scored by KenLM toolkit (Heafield, 2011). In We used a phrase-based machine translation system and investigated several models to rescore the system. We used neural network language and translation"
W16-2314,P02-1040,0,0.101538,"any of these values are negative numbers with high absolute value since they are computed as the logarithm of relatively small probabilities. Therefore, we rescaled all scores observed on the development data to the range of [−1, 1] prior to rescoring. 306 4 Results recurrent encoder-decoder scores. It was submitted as the joint KIT-LIMSI submission system. In this section, we present a summary of our experiments in the evaluation campaign. Individual components that lead to improvements in the translation performance are described step by step. The scores are reported in case-sensitive BLEU (Papineni et al., 2002). In the rescoring scheme of our systems, the BLEU scores on the development set are normally smaller than those in the decoding phase because they are tuned by different optimization algorithms (ListNet and MERT). The rescoring configurations are mentioned in the tables in italic texts. The test scores from which we choose to be the submitted systems are mentioned in the tables in bold numbers. 4.1 System Baseline + DWL + Lex. Reorderings + ReEnDe + SDWL + ReEnDe + Factored + ReEnDe + CTMs Dev 21.81 22.44 20.76 20.79 20.78 Test 22.91 23.34 24.08 24.21 24.24 Table 1: Experiments for English→Ge"
W16-2314,W08-1006,0,0.0257277,"r to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). I"
W16-2314,2007.tmi-papers.21,0,0.529618,"oring. Finally, in cooperation with LIMSI, we used the continuous space translation models in rescoring. We used the ListNet approach as described in Section 3.4 to estimate the weights of different models in our systems. Reordering Models In all translation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models"
W16-2314,C08-1098,0,0.0960257,"described in Section 3.4 to estimate the weights of different models in our systems. Reordering Models In all translation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improveme"
W16-2314,D07-1045,0,0.0833493,"Missing"
W16-2314,P07-2045,0,\N,Missing
W16-2314,W15-3030,1,\N,Missing
W16-2314,2005.iwslt-1.8,0,\N,Missing
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W17-4101,N06-2001,0,0.0123151,"units, whether they are built via a different supervised method with embedded language knowledge, or from the training data, has been attempted many times, especially for speech recognition. The main goal is to reduce the OOV rate. While most of them were focused on a specific language, (Creutz et al., 2007) is a representative example of such a model applied to several morphologically-rich languages. One of the first occurrences of general language models integrating morphological features to represent words are the factored language model (Bilmes and Kirchhoff, 2003) and its neural version (Alexandrescu and Kirchhoff, 2006). Input words are represented by their embedding, plus several other features, some of which include morphemes. To alleviate the impact of OOVs, (Mueller and Schuetze, 2011) used morphological features for class-based predictions when input words are unknown, obtaining state-of-the-art Most of neural language models use different kinds of embeddings for word prediction. While word embeddings can be associated to each word in the vocabulary or derived from characters as well as factored morphological decomposition, these word representations are mainly used to parametrize the input, i.e. the co"
W17-4101,P16-1186,0,0.0680412,"Missing"
W17-4101,J81-4005,0,0.734133,"Missing"
W17-4101,P16-2058,0,0.0386919,"Missing"
W17-4101,D15-1041,0,0.0291475,"r they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in (Vania and Lopez, 2017). This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies. • When evaluating perplexity on the full training vocabulary, using an augmented output representation improves the model performance. • Not using the look-up table for rare words also improves the model performance. Finally, we describe a short experiment with factorin"
W17-4101,P16-1156,0,0.0506405,"Missing"
W17-4101,N03-2002,0,0.0258231,"rd decomposition into subwords units. Using subword units, whether they are built via a different supervised method with embedded language knowledge, or from the training data, has been attempted many times, especially for speech recognition. The main goal is to reduce the OOV rate. While most of them were focused on a specific language, (Creutz et al., 2007) is a representative example of such a model applied to several morphologically-rich languages. One of the first occurrences of general language models integrating morphological features to represent words are the factored language model (Bilmes and Kirchhoff, 2003) and its neural version (Alexandrescu and Kirchhoff, 2006). Input words are represented by their embedding, plus several other features, some of which include morphemes. To alleviate the impact of OOVs, (Mueller and Schuetze, 2011) used morphological features for class-based predictions when input words are unknown, obtaining state-of-the-art Most of neural language models use different kinds of embeddings for word prediction. While word embeddings can be associated to each word in the vocabulary or derived from characters as well as factored morphological decomposition, these word representat"
W17-4101,P14-1129,0,0.0824454,"Missing"
W17-4101,P15-1001,0,0.0975247,"Missing"
W17-4101,N16-1077,0,0.0239541,"er improvement. It is worth noticing that this also opens the vocabulary, since our model can be used to rescore unknown words. Additional experiments suggest that factoring the output of the model with a lemma+tags decomposition, then re-inflecting these into words, could make generation easier: this is a direction we plan to investigate. Lemmas + Char CNN Words Yvon, 2017), these objectives are individually easier when working with morphologically-rich languages, and fully inflected words can be obtained by using morphological inflection models, which have been shown to be quite successful (Faruqui et al., 2016; Kann et al., 2017). Table 7 shows the test perplexities on lemmas for various input and output representations. We can observe that in all cases training is far more stable, with generally lower standard deviations. In this case, using a lemma+tags with a BiLSTM or a Words+CharCNN input representation both give the best results, while augmenting the output representation of the lemma with a character-build embedding also improves results. This makes the joint learning of a factored prediction and reinflection language model a very interesting direction for future work. Acknowledgements We wi"
W17-4101,N16-1155,0,0.0275799,"m the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in (Vania and Lopez, 2017). This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies. • When evaluating perplexity on the full training vocabulary, using an augmented output representation improves the model performance. • Not using the look-up table for rare words also improves the model performance. Finally, we descr"
W17-4101,E17-1049,0,0.048872,"Missing"
W17-4101,E17-1048,0,0.0291127,"Missing"
W17-4101,N12-1005,1,0.783398,"ntly improve the performance of the model. Moreover, reducing the size of the output look-up table, to let the character-based embeddings represent rare words, brings further improvement. 1 Introduction Most of neural language models, such as n-gram models (Bengio et al., 2003) are word based and rely on the definition of a finite vocabulary V. Therefore, a look-up table maps each wordw ∈ V to a vector of real features, and is stored in a matrix. While this approach yields significant improvement for a variety of tasks and languages, see for instance (Schwenk, 2007) in speech recognition and (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2014) in machine translation, it induces several limitations. 1 Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 1–13, c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics. the effect of different architectures, as well as the effect of different input representations. Our results show that: results on English. More recently, several types of language models represent words as function of subwords units: using a recursive structure (Luong et al., 2013), or an additive one (Botha and"
W17-4101,D15-1176,0,0.238443,"different architectures, as well as the effect of different input representations. Our results show that: results on English. More recently, several types of language models represent words as function of subwords units: using a recursive structure (Luong et al., 2013), or an additive one (Botha and Blunsom, 2014). Quite a lot of work has been made on language models that extract features directly from the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword uni"
W17-4101,P11-2092,0,0.015195,"ecognition. The main goal is to reduce the OOV rate. While most of them were focused on a specific language, (Creutz et al., 2007) is a representative example of such a model applied to several morphologically-rich languages. One of the first occurrences of general language models integrating morphological features to represent words are the factored language model (Bilmes and Kirchhoff, 2003) and its neural version (Alexandrescu and Kirchhoff, 2006). Input words are represented by their embedding, plus several other features, some of which include morphemes. To alleviate the impact of OOVs, (Mueller and Schuetze, 2011) used morphological features for class-based predictions when input words are unknown, obtaining state-of-the-art Most of neural language models use different kinds of embeddings for word prediction. While word embeddings can be associated to each word in the vocabulary or derived from characters as well as factored morphological decomposition, these word representations are mainly used to parametrize the input, i.e. the context of prediction. This work investigates the effect of using subword units (character and factored morphological decomposition) to build output representations for neural"
W17-4101,W13-3512,0,0.0386074,", 2007) in speech recognition and (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2014) in machine translation, it induces several limitations. 1 Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 1–13, c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics. the effect of different architectures, as well as the effect of different input representations. Our results show that: results on English. More recently, several types of language models represent words as function of subwords units: using a recursive structure (Luong et al., 2013), or an additive one (Botha and Blunsom, 2014). Quite a lot of work has been made on language models that extract features directly from the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et"
W17-4101,P16-2067,0,0.0176731,"Quite a lot of work has been made on language models that extract features directly from the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in (Vania and Lopez, 2017). This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies. • When evaluating perplexity on the full training vocabulary, using an augmented output representation improves the model performance. • Not us"
W17-4101,P16-1101,0,0.0198423,"has been made on language models that extract features directly from the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in (Vania and Lopez, 2017). This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies. • When evaluating perplexity on the full training vocabulary, using an augmented output representation improves the model performance. • Not using the look-up tab"
W17-4101,C14-1015,0,0.058457,"units: using a recursive structure (Luong et al., 2013), or an additive one (Botha and Blunsom, 2014). Quite a lot of work has been made on language models that extract features directly from the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in (Vania and Lopez, 2017). This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies. • When evaluating perplexity on the fu"
W17-4101,W13-3204,0,0.025001,"el Models in NLP, pages 1–13, c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics. the effect of different architectures, as well as the effect of different input representations. Our results show that: results on English. More recently, several types of language models represent words as function of subwords units: using a recursive structure (Luong et al., 2013), or an additive one (Botha and Blunsom, 2014). Quite a lot of work has been made on language models that extract features directly from the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation"
W17-4101,P14-5003,0,0.0547183,"Missing"
W17-4101,P17-1184,0,0.0386719,"n external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in (Vania and Lopez, 2017). This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies. • When evaluating perplexity on the full training vocabulary, using an augmented output representation improves the model performance. • Not using the look-up table for rare words also improves the model performance. Finally, we describe a short experiment with factoring the output predictions using a morphological analysis, which we believe could lead to a facilitated word generation when combined with reinflexion models. Our paper is organized as follows: Section"
W17-4101,D16-1157,0,0.0312018,"itive one (Botha and Blunsom, 2014). Quite a lot of work has been made on language models that extract features directly from the character sequence, whether they use character n-grams (Sperr et al., 2013), or characters composed by a convolutional layer (Santos and Zadrozny, 2014; Kim et al., 2015) or a BiLSTM layer (Ling et al., 2015). This avoids using an external morphological analyser. We can note that these types of models have also been applied with success to several other task, including learning word representations (Qiu et al., 2014; Cotterell et al., 2016; Bojanowski et al., 2016; Wieting et al., 2016), POS tagging (Plank et al., 2016; Ma and Hovy, 2016; Heigold et al., 2017), Named entity recognition (Gillick et al., 2016), Parsing (Ballesteros et al., 2015) and Machine translation (Costa-juss`a and Fonollosa, 2016). Recently, an exhaustive summary of previous work on word representation by composing subword units was presented in (Vania and Lopez, 2017). This work also compares the types of subword unit, how they are composed, and their impact on various morphological typologies. • When evaluating perplexity on the full training vocabulary, using an augmented output representation improve"
W17-4721,P07-2045,0,0.00542119,"Missing"
W17-4721,W16-2302,0,0.0393102,"Missing"
W17-4721,W13-5624,0,0.0674526,"Missing"
W17-4721,W17-4703,1,0.801097,"Missing"
W17-4721,E17-3017,0,0.0686027,"Missing"
W17-4721,W17-4705,1,0.865179,"Missing"
W17-4721,P16-1009,0,0.0612462,"Missing"
W17-4721,W08-0310,1,0.831832,"Missing"
W17-4721,P16-1162,0,0.0630628,"Missing"
W17-4721,D14-1025,0,0.0589701,"Missing"
W17-4721,P14-5003,0,0.0639461,"Missing"
W17-4721,W16-2342,0,0.056585,"Missing"
W19-5802,D18-2029,0,0.0701647,"Missing"
W19-5802,P09-1002,0,0.175208,"learning, USE improves performance on different NLP tasks at the sentence and phrase level (e.g. sentiment analysis). We use the Deep Averaging Network (DAN) encoder,5 where input word and bigram embeddings are averaged and fed through a feedforward neural network, to create embeddings for WiC sentences. 3 http://u.cs.biu.ac.il/ ˜nlp/resources/ downloads/context2vec/ 4 The medium-sized model at https://allennlp. org/elmo. 5 https://tfhub.dev/google/ universal-sentence-encoder/2 4 Automatic Substitution Manual substitute annotations have been useful for in-context usage similarity estimation (Erk et al., 2009; McCarthy et al., 2016). The idea is that a high proportion of shared substitutes between two word instances reflects their semantic similarity.6 Extending previous work where manual substitute annotations were used to estimate usage similarity (Erk et al., 2009), we automatically annotate WiC instances with substitutes, and use features based on their overlap for our classifier. We use the context2vec method for automatic lexical substitution (Melamud et al., 2016). Given a sentence with a new instance of a target word t, and a set of candidate substitutes for the word (S = s1 , s2 , ..., sn"
W19-5802,ide-etal-2008-masc,0,0.0175976,"the verb drink (gold label: F) with automatic substitute annotations assigned by context2vec. Substitutes in italics were discarded after filtering. • GAP score: GAP (Generalized Average Precision) considers the order of ranked elements and their weights (Kishida, 2005). GAP score ranges from 0 to 1 (for perfect disagreement/agreement). We take the average score between the rankings produced for a sentence pair in both directions (GAP(R1 , R2 ) and GAP(R2 , R1 )). Weights are the scores assigned to the substitutes by context2vec. We use the GAP implementation shared by Melamud et al. (2015). (Ide et al., 2008) which contains manual substitute annotations for all content words in a sentence. We use a balanced collection of similar (T ) and dissimilar (F) sentence pairs from CoInCo, with labels automatically assigned based on substitute overlap (Gar´ı Soler et al., 2019).10 We apply the automatic substitution method described in Section 4, and extract substitute- and embeddingbased features to be used by our models. • Substitute cosine similarity. We form pairs of substitutes from R1 and R2 , and calculate the average of their GloVe cosine similarities. This feature accounts for the semantic similari"
W19-5802,E14-1057,0,0.0569321,"Missing"
W19-5802,J16-2003,1,0.936362,"roves performance on different NLP tasks at the sentence and phrase level (e.g. sentiment analysis). We use the Deep Averaging Network (DAN) encoder,5 where input word and bigram embeddings are averaged and fed through a feedforward neural network, to create embeddings for WiC sentences. 3 http://u.cs.biu.ac.il/ ˜nlp/resources/ downloads/context2vec/ 4 The medium-sized model at https://allennlp. org/elmo. 5 https://tfhub.dev/google/ universal-sentence-encoder/2 4 Automatic Substitution Manual substitute annotations have been useful for in-context usage similarity estimation (Erk et al., 2009; McCarthy et al., 2016). The idea is that a high proportion of shared substitutes between two word instances reflects their semantic similarity.6 Extending previous work where manual substitute annotations were used to estimate usage similarity (Erk et al., 2009), we automatically annotate WiC instances with substitutes, and use features based on their overlap for our classifier. We use the context2vec method for automatic lexical substitution (Melamud et al., 2016). Given a sentence with a new instance of a target word t, and a set of candidate substitutes for the word (S = s1 , s2 , ..., sn ), context2vec ranks al"
W19-5802,K16-1006,0,0.234407,"btained vector representations is used as a feature for our classifier. We use the following types of embeddings: SIF (Smooth Inverse Frequency): Simple method for deriving sentence representations from uncontextualized embeddings (Arora et al., 2017). Dimensionality reduction is applied to a weighted average of the vectors of words in a sentence. Weighting is based on word frequency in Common Crawl. We use SIF in combination with 300d GloVe vectors trained on Common Crawl (Pennington et al., 2014).2 Context2vec: Neural model that learns embeddings for words and their contexts simultaneously (Melamud et al., 2016). It is based on word2vec’s 1 http://www.wiktionary.org/ 2 https://nlp.stanford.edu/projects/ glove/ CBOW (Mikolov et al., 2013), but replaces the averaging of context word embeddings with a biLSTM that learns a representation of a sentence excluding the target word. We use a 600-d model pre-trained on the UkWac corpus (Baroni et al., 2009).3 ELMo (Embeddings from Language Models): Contextualized word representations obtained from the internal states of a deep bidirectional LSTM trained with a language model objective (Peters et al., 2018). Instead of learning the best linear combination of la"
W19-5802,W15-1501,0,0.0250447,"way (gold label: T) and the verb drink (gold label: F) with automatic substitute annotations assigned by context2vec. Substitutes in italics were discarded after filtering. • GAP score: GAP (Generalized Average Precision) considers the order of ranked elements and their weights (Kishida, 2005). GAP score ranges from 0 to 1 (for perfect disagreement/agreement). We take the average score between the rankings produced for a sentence pair in both directions (GAP(R1 , R2 ) and GAP(R2 , R1 )). Weights are the scores assigned to the substitutes by context2vec. We use the GAP implementation shared by Melamud et al. (2015). (Ide et al., 2008) which contains manual substitute annotations for all content words in a sentence. We use a balanced collection of similar (T ) and dissimilar (F) sentence pairs from CoInCo, with labels automatically assigned based on substitute overlap (Gar´ı Soler et al., 2019).10 We apply the automatic substitution method described in Section 4, and extract substitute- and embeddingbased features to be used by our models. • Substitute cosine similarity. We form pairs of substitutes from R1 and R2 , and calculate the average of their GloVe cosine similarities. This feature accounts for t"
W19-5802,N13-1092,0,0.088872,"Missing"
W19-5802,P15-2070,0,0.0653881,"Missing"
W19-5802,N18-1202,0,0.0379724,"beddings for words and their contexts simultaneously (Melamud et al., 2016). It is based on word2vec’s 1 http://www.wiktionary.org/ 2 https://nlp.stanford.edu/projects/ glove/ CBOW (Mikolov et al., 2013), but replaces the averaging of context word embeddings with a biLSTM that learns a representation of a sentence excluding the target word. We use a 600-d model pre-trained on the UkWac corpus (Baroni et al., 2009).3 ELMo (Embeddings from Language Models): Contextualized word representations obtained from the internal states of a deep bidirectional LSTM trained with a language model objective (Peters et al., 2018). Instead of learning the best linear combination of layer representations for a task – a common way of using ELMo – we use out-of-the-box 512-d embeddings.4 We experiment with the top layer, and the average of the three hidden layers. We represent each WiC sentence in two ways: a) with the ELMo embedding corresponding to the target word, and b) with the average of ELMo embeddings of all words in the sentence. We also average the embeddings at a context window of size two, as this was shown to work better for word usage similarity with ELMo (Gar´ı Soler et al., 2019). BERT (Bidirectional Encod"
W19-5802,N19-1128,0,0.0454543,"ent types, and with features based on in-context substitute annotations automatically assigned to WiC sentence pairs. The model with the highest performance on the WiC development set uses a combination of cosine similarities from different embedding types. It obtains an accuracy of 66.7 on the shared task test set and is ranked third among the participating systems. 1 Introduction The SemDeep-5 WiC shared task proposes to identify the intended meaning of words in context. It is framed as a binary classification task that addresses whether two instances of a target word have the same meaning (Pilehvar and CamachoCollados, 2019). The WiC dataset contains 7,466 sentence pairs and is proposed as a new evaluation benchmark for context-sensitive word representations. We apply to this task the method from Gar´ı Soler et al. (2019) which addresses the usage similarity of contextualized instances of words. The method integrates cosine similarities from different types of context-sensitive embeddings and in-context automatic substitutes. Our best system combines cosine similarities from three embedding types. It obtains an accuracy of 66.7 on the WiC test set, and is ranked third among all systems that participated in the ta"
