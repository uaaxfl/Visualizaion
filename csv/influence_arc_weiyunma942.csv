2011.mtsummit-papers.62,P05-1074,0,0.0230514,"Missing"
2011.mtsummit-papers.62,2000.iwpt-1.9,0,0.776082,"Missing"
2011.mtsummit-papers.62,W10-1742,0,0.0570337,"Missing"
2011.mtsummit-papers.62,W10-1747,0,0.147274,"ce. We then use a text-to-text generator operating on those lattices to generate those hypotheses. We filter ungrammatical combinations using a feature-based lexicalized tree adjoining grammars (FB-LTAG) and then use a TER-based metric to compute a consensus score function to select the best translation among grammatical hypotheses. The system combination gains 1.38 BLEU points over the best individual system. 1 Introduction Recently many MT combination approaches have been presented. Consensus network (CN) decoding (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Rosti et al. 2010; Leusch and Ney, 2010) is one of the most successful approaches, in which the words in all hypotheses are aligned with a backbone hypothesis. A word-based lattice is then formed with word alternatives, including nulls, each with associated scores from voting or other confidence scores. Then, the combined translation sentence(s) can be produced with the same word order as the backbone by selecting the path with the highest score(s) along the lattice. 546 In this paper, rather than use the CN decoding framework, we borrow the idea of text-to-text generation, which has been used successfully for sentence fusion as par"
2011.mtsummit-papers.62,W10-1746,0,0.0373586,"Missing"
2011.mtsummit-papers.62,N07-1029,0,0.463074,"ting all possible hypotheses for the same source sentence. We then use a text-to-text generator operating on those lattices to generate those hypotheses. We filter ungrammatical combinations using a feature-based lexicalized tree adjoining grammars (FB-LTAG) and then use a TER-based metric to compute a consensus score function to select the best translation among grammatical hypotheses. The system combination gains 1.38 BLEU points over the best individual system. 1 Introduction Recently many MT combination approaches have been presented. Consensus network (CN) decoding (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Rosti et al. 2010; Leusch and Ney, 2010) is one of the most successful approaches, in which the words in all hypotheses are aligned with a backbone hypothesis. A word-based lattice is then formed with word alternatives, including nulls, each with associated scores from voting or other confidence scores. Then, the combined translation sentence(s) can be produced with the same word order as the backbone by selecting the path with the highest score(s) along the lattice. 546 In this paper, rather than use the CN decoding framework, we borrow the idea of text-to-text generation, w"
2012.amta-papers.11,W07-0726,0,0.116305,"escribed above are based on the estimation of the degree of agreement between a phrase with another phrase. Now our question is: how can we estimate and utilize the agreement degree between a set of consecutive phrases with another set of consecutive phrases during decoding? In lattice-based combination, this issue has not been addressed before. Our solution is simple; –we consider N-gram consensus in addition to the confidence estimations for paraphrases during decoding. This idea of considering N-gram consensus was widely used in N-best list reranking (Chen et al., 2005; Zens and Ney, 2006; Chen et al., 2007). These years the technique has also been presented in some wordbased combination schemes and proven effectives. The approaches can be divided into two categories: one is based on a sentence-specific LM, built on translation hypotheses of multiple systems (Zhao and He 2009; Heafield and Lavie 2010); the other one is based on a corpus-based LM, built on the whole tuning/test corpus of all translation hypotheses of multiple systems (Matusov et al, 2008; Leusch et al, 2011). The strength of sentence-specific LM is that it considers the most specific data available while the corpus-based LM has th"
2012.amta-papers.11,W10-1746,0,0.044251,"Missing"
2012.amta-papers.11,P07-1040,0,0.248842,"based MT techniques in the combination framework. We show how phrase extraction rules and confidence estimations inspired from machine translation improve results. We also propose system-specific LMs for estimating N-gram consensus. Our results show that our approach yields a strong improvement over the best single MT system and competes with other stateof-the-art combination systems. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Confusion Network (CN) decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). A CN is a linear word lattice structure, in which the words in all translation hypotheses are aligned against the corresponding words of a selected backbone hypothesis. Each word in the CN is assigned a confidence score and the decoder Kathleen McKeown simply finds the path with the highest sum of these scores. In addition to word-level combination approaches, such as CN decoding, some phraselevel combination techniques have also recently been presented; their goal is to retain coherence and consistency between the words"
2012.amta-papers.11,2006.amta-papers.25,0,0.0432916,"d be produced as follows: Eb : w1 w2 w3 w4 w5 S P T D Eh : w 2 w 1 w 3 ε Y ε ε I [ w6 w7 I w4 w5 w6 w8] w9 P [ [ w10 w11] M w 8 w 7] w 10 P [ w9 ] Fig 2. The alignment between Eb and reordered Eh 3 Monolingual Word Alignment Our paraphrases are deduced from monolingual word alignment. Any monolingual word aligner can serve the purpose. Since in our implementation, we adopt TERp as our alignment tool, we briefly review it and use a virtual example to illustrate its alignment output format and how we slightly adjust the format to meet our needs. TERp (Snover et al. 2009) is an extension of TER (Snover et al. 2006). Both TERp and TER are automatic evaluation metrics for MT, based on measuring the ratio of the number of edit operations between the reference sentence and the MT system hypothesis. TERp uses all the edit operations of TER—Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and Paraphrases. TERp identifies the Stem Matches and Synonym Matches using the Porter stemming algorithm (Porter, 1980) and WordNet (Fellbaum, 1998) respectively. Sequences of words in the reference are considered to be paraphrases of a sequence of"
2012.amta-papers.11,W11-2121,0,0.0389505,"Missing"
2012.amta-papers.11,D07-1029,0,\N,Missing
2012.amta-papers.11,E06-1005,0,\N,Missing
2012.amta-papers.11,W09-0441,0,\N,Missing
2012.amta-papers.11,W06-3110,0,\N,Missing
2012.amta-papers.11,P08-2021,0,\N,Missing
2012.amta-papers.11,W08-0329,0,\N,Missing
2012.amta-papers.11,D09-1115,0,\N,Missing
2012.amta-papers.11,P11-1125,0,\N,Missing
2012.amta-papers.11,N03-1017,0,\N,Missing
2012.amta-papers.11,N07-1029,0,\N,Missing
2012.amta-papers.11,N09-2052,0,\N,Missing
2012.amta-papers.11,D08-1011,0,\N,Missing
2012.amta-papers.11,W11-2118,0,\N,Missing
2012.amta-papers.11,2005.iwslt-1.11,0,\N,Missing
2012.amta-papers.11,W09-0405,0,\N,Missing
2012.amta-papers.11,W09-0407,0,\N,Missing
2012.amta-papers.11,P03-1021,0,\N,Missing
2012.amta-papers.11,2010.amta-papers.34,0,\N,Missing
2012.amta-papers.11,2010.amta-papers.9,0,\N,Missing
2020.lrec-1.235,E06-1002,0,0.03722,"Missing"
2020.lrec-1.235,D11-1071,0,0.0267012,"data or knowledge base IDs. One can also use that pipeline to tackle this problem, but in our design, we adopt a different strategy — we proposed a knowledge base driven approach in order to directly tackle the task without explicit coreference resolution. Related Work The two main strategies for entity linking are discrete feature-based and embedding-based systems. Shen et al. (2015) systematically organize discrete feature-based entity linking systems. Discrete features include name string comparisons (Liu et al., 2013) and similarities of bags of words (Lin et al., 2012), concept vectors (Chen and Ji, 2011), and other manually designed features (McNamee et al., 2009). With these discrete features, supervised learning models such as binary classifiers (Zhang et al., 2010) and learning-to-rank methods (Kulkarni et al., 2009) have been implemented for entity ranking. However, featured-based entity linking systems are highly data-dependent, which requires extensive effort to design domain-specific features. Moreover, as the discrete features are often too sparse to train the model, they are unlikely to apply to different domains. Recently, many approaches have devised to learn representations of the"
2020.lrec-1.235,C10-1032,0,0.0393054,"the words it contains), the context embedding by a convolutional neural network (CNN), and entity embedding through entity surface words and entity class from the knowledge base. Conventionally, supervised models have been used for entity linking. However, one significant problem with supervised approaches is their heavy reliance on large amounts of annotated training data. Moreover, entity linking annotation is expensive and time-consuming. Some supervised approaches train their models on a small manuallycreated data set consisting of thousands of labeled entity mentions (Shen et al., 2012; Dredze et al., 2010; McNamee, 2010; Li et al., 2009). Some systems (Bunescu and Pas¸ca, 2006; Agirre et al., 2009) use hyperlinks in Wikipedia articles to construct training data. For all these entity linking researches, mention scopes need to be identified by named entity recognition, but the scope identification is not always indispensable in terms of practical needs for applications, such as product recommendation or product analysis. That motivates us to design this special task based on headword-oriented entity linking in this paper. Our special task can be regarded as nominal coreference (Fonseca et al., 2"
2020.lrec-1.235,N16-1150,0,0.0405205,"Missing"
2020.lrec-1.235,D17-1284,0,0.103507,"(McNamee et al., 2009). With these discrete features, supervised learning models such as binary classifiers (Zhang et al., 2010) and learning-to-rank methods (Kulkarni et al., 2009) have been implemented for entity ranking. However, featured-based entity linking systems are highly data-dependent, which requires extensive effort to design domain-specific features. Moreover, as the discrete features are often too sparse to train the model, they are unlikely to apply to different domains. Recently, many approaches have devised to learn representations of the entity and use it for entity linking. Gupta et al. (2017) build 3. Corpus Construction We collected over 5,000 products and about 50,000 blog articles from the PIXstyleMe3 web service. These articles contain a total of over 5 million sentences and over 41 million words. We standardized the text in the database and applied word segmentation. We also created repositories of products, brands, and product types as knowledge bases. 3.1. Product Database Product We created a product database from PIXstyleMe’s database consisting of product names, brands, descriptions, etc. After manually removing duplicate products and fixing typos, we collected 5,060 pro"
2020.lrec-1.235,O07-4005,0,0.00945075,"1. Baselines Due to the lack of existing models that perfectly fit out special task HEL, we use two simple baselines — the heuristic rule (§3.4.) and the similarity classifier (described below). The similarity classifier computes the similarity of mentions with their candidate products. Note that the similarity classifier can be used only for product ID linking; that is, it can be used only as a baseline of product classification. Similarity Classifier Given a mention m ∈ M, we select a candidate product p ∈ P with the same headword as the mention. We find a noun phrase n by sentence parsing (Hsieh et al., 2007; Hsieh et al., 2012; Yang et al., 2008) the mention with the most significant Jaccard similarity coefficient5 to the candidate, and compute the similarity of p and n. Precisely, we propose two similarity methods — embeddings (denoted as Emb) and bag-of-words (denoted as Bag). First, we use the product name encoder (§4.3.3.) on both the product name p and the noun phrase n, and comp,n pute the cosine similarity semb of those embeddings. We assign the ID of the candidate with the highest similarity to the mention. Another way is to replace the above emp,n beddings by bags of words to compute sb"
2020.lrec-1.235,W12-6338,0,0.0189795,"the lack of existing models that perfectly fit out special task HEL, we use two simple baselines — the heuristic rule (§3.4.) and the similarity classifier (described below). The similarity classifier computes the similarity of mentions with their candidate products. Note that the similarity classifier can be used only for product ID linking; that is, it can be used only as a baseline of product classification. Similarity Classifier Given a mention m ∈ M, we select a candidate product p ∈ P with the same headword as the mention. We find a noun phrase n by sentence parsing (Hsieh et al., 2007; Hsieh et al., 2012; Yang et al., 2008) the mention with the most significant Jaccard similarity coefficient5 to the candidate, and compute the similarity of p and n. Precisely, we propose two similarity methods — embeddings (denoted as Emb) and bag-of-words (denoted as Bag). First, we use the product name encoder (§4.3.3.) on both the product name p and the noun phrase n, and comp,n pute the cosine similarity semb of those embeddings. We assign the ID of the candidate with the highest similarity to the mention. Another way is to replace the above emp,n beddings by bags of words to compute sbag . Besides, we als"
2020.lrec-1.235,Q15-1036,0,0.0147659,"e document-wise information to obtain document context embeddings. The first is the exact match. An exact match is a scope of characters in the article which uses exactly the same characters as 4 The postamble context is reversed so that the LSTM starts at the last word wL and ends at the mention m. one of the cosmetic product. We assume that the mention m is related to one or more products mentioned in the article. Since we have high confidence in the exact matches, such information is valuable to producing embeddings. We use m a bag-of-products representation vexact ∈ {0, 1}|P |, similar to Lazic et al. (2015), when collecting the exact-matched products that appeared in the article. Also, we assume that the mention m is related to one of the brands mentioned in the article. Similarly, we use a bag-ofm brands representation vbrand ∈ {0, 1}|B |, collecting all the brands that appear in the article. m To produce the document context embeddings vdocu ∈ RD , m m we concatenate vexact and vbrand and pass them through a single-layer feed-forward network. Mention-Context Encoder Finally, we combine the m m m above local vlocal , title vtitle , and document vdocu context embeddings by concatenating and pass"
2020.lrec-1.235,W12-3016,0,0.0434071,"Missing"
2020.lrec-1.235,P13-1128,0,0.0322193,"yntactic patterns are linked together as coreference chains before being linked to unique data or knowledge base IDs. One can also use that pipeline to tackle this problem, but in our design, we adopt a different strategy — we proposed a knowledge base driven approach in order to directly tackle the task without explicit coreference resolution. Related Work The two main strategies for entity linking are discrete feature-based and embedding-based systems. Shen et al. (2015) systematically organize discrete feature-based entity linking systems. Discrete features include name string comparisons (Liu et al., 2013) and similarities of bags of words (Lin et al., 2012), concept vectors (Chen and Ji, 2011), and other manually designed features (McNamee et al., 2009). With these discrete features, supervised learning models such as binary classifiers (Zhang et al., 2010) and learning-to-rank methods (Kulkarni et al., 2009) have been implemented for entity ranking. However, featured-based entity linking systems are highly data-dependent, which requires extensive effort to design domain-specific features. Moreover, as the discrete features are often too sparse to train the model, they are unlikely to apply to"
2020.lrec-1.235,W03-1705,1,0.458099,"SP (Specific Product), PID (Product with ID), OSP (Other Specific Product), and GP (General Product) Brand We also collected the aliases (including English and Chinese names) of each brand (364 in total) from the database. For example, both “sk2” and “skii” refer to the brand “SK-II”; “shuuemura”, “shu uemura”, and “植村秀” represent the brand “Shu Uemura”. Headword We collected 926 headwords, each of which represents a type of product. Examples are “面膜” (facial mask), “唇膏” (lipstick), and “香水” (perfume). Word Segmentation After the above preprocessing, we applied word segmentation using CKIPWS (Ma and Chen, 2003a; Ma and Chen, 2003b) on product names and their descriptions. In order to get better word segmentation, we add many cosmetic specific words into our lexicon. We also make some necessary modifications to the ambiguous terms after word segmentation. For example, “修容蜜 粉餅” can be segmented as “修容” (contouring) + “蜜粉 餅” (pressed powder cake) or “修容蜜” (liquid blush) + “粉餅” (powder cake). Though the previous one is more semantically reasonable, the word segmentation tool is not able to determine it since all the above four words are collected in the lexicon. Therefore, we add “修容蜜粉餅” to the lexicon"
2020.lrec-1.235,W03-1726,1,0.582927,"SP (Specific Product), PID (Product with ID), OSP (Other Specific Product), and GP (General Product) Brand We also collected the aliases (including English and Chinese names) of each brand (364 in total) from the database. For example, both “sk2” and “skii” refer to the brand “SK-II”; “shuuemura”, “shu uemura”, and “植村秀” represent the brand “Shu Uemura”. Headword We collected 926 headwords, each of which represents a type of product. Examples are “面膜” (facial mask), “唇膏” (lipstick), and “香水” (perfume). Word Segmentation After the above preprocessing, we applied word segmentation using CKIPWS (Ma and Chen, 2003a; Ma and Chen, 2003b) on product names and their descriptions. In order to get better word segmentation, we add many cosmetic specific words into our lexicon. We also make some necessary modifications to the ambiguous terms after word segmentation. For example, “修容蜜 粉餅” can be segmented as “修容” (contouring) + “蜜粉 餅” (pressed powder cake) or “修容蜜” (liquid blush) + “粉餅” (powder cake). Though the previous one is more semantically reasonable, the word segmentation tool is not able to determine it since all the above four words are collected in the lexicon. Therefore, we add “修容蜜粉餅” to the lexicon"
2020.lrec-1.235,P09-1113,0,0.0646346,"Missing"
2020.lrec-1.235,K16-1025,0,0.0236723,"dding model to address the problem as a strong baseline, where diverse information about products is jointly encoded using dense representations. • We present a special transfer learning framework, involving distant supervision of the model with a large number of noisy labels, and then supervised learning using a small number of manually labeled data. • To the best of our knowledge, we are the first to study entity analysis in the cosmetic domain. 2. an embedding-based linking system that learns representations for each entity without domain-specific training data or hand-engineered features. Yamada et al. (2016) learn word and entity embeddings for named entity disambiguation based on the skip-gram model. Francis-Landau et al. (2016) utilize convolutional neural networks to capture semantic correspondence between a mention’s context and a proposed target entity. Sun et al. (2015) disambiguate entities using the mention embedding (the average of the embeddings of the words it contains), the context embedding by a convolutional neural network (CNN), and entity embedding through entity surface words and entity class from the knowledge base. Conventionally, supervised models have been used for entity lin"
2020.lrec-1.235,I08-2098,0,0.0161387,"models that perfectly fit out special task HEL, we use two simple baselines — the heuristic rule (§3.4.) and the similarity classifier (described below). The similarity classifier computes the similarity of mentions with their candidate products. Note that the similarity classifier can be used only for product ID linking; that is, it can be used only as a baseline of product classification. Similarity Classifier Given a mention m ∈ M, we select a candidate product p ∈ P with the same headword as the mention. We find a noun phrase n by sentence parsing (Hsieh et al., 2007; Hsieh et al., 2012; Yang et al., 2008) the mention with the most significant Jaccard similarity coefficient5 to the candidate, and compute the similarity of p and n. Precisely, we propose two similarity methods — embeddings (denoted as Emb) and bag-of-words (denoted as Bag). First, we use the product name encoder (§4.3.3.) on both the product name p and the noun phrase n, and comp,n pute the cosine similarity semb of those embeddings. We assign the ID of the candidate with the highest similarity to the mention. Another way is to replace the above emp,n beddings by bags of words to compute sbag . Besides, we also evaluate the accur"
2020.lrec-1.235,C10-1145,0,0.0762375,"Missing"
2020.lrec-1.365,N15-1184,0,0.0223603,"deaf person). (b) 瞎子 (blind person). (c) 瘸子 (lame person). Figure 5: Some definition graphs that leads to the organ-disabled relation. (a) 畫具 (painting tools). (b) 畫筆 (painting brush). (c) 繪圖機 (plotter). Figure 6: Some definition graphs that leads to the painter-instrument relation. (a) 牙醫 (dentist). (b) 獸醫 (veterinarian). (c) 醫官 (medical officer). Figure 7: Some definition graphs that leads to the doctor-patient relation. We infused distributed word representations with the hypohyper and same-taxon knowledge in the E-HowNet taxonomy (Section 3.2.) and the HIT-Thesaurus2 through retrofitting (Faruqui et al., 2015). For example, in Figure 1, the word vector of 物體 was optimized to be close to both its distributed representation and the word vectors of 物質 (same-taxon) and 東西 (hypo-hyper). Table 5, 6 shows the results of different combinations of 2 同義詞詞林擴展版(https://github.com/ taozhijiang/chinese_correct_wsd) retrofitted embeddings and benchmarks. Firstly, retrofitted embeddings achieve better performance on most existing datasets, suggesting the benefits of embedding more commonsense knowledge. Secondly, on CA-EHN, each retrofitted embedding significantly outperforms its pure distributed counterpart in Ta"
2020.lrec-1.365,P18-2023,0,0.0676244,"guage inference (NLI) task (Bowman et al., 2015; Williams et al., 2018) has proved a good pre-training objective for sentence representations (Conneau et al., 2017), commonsense coverage is implicit and limited by the amount of annotated sentence pairs. Furthermore, most models are still end-to-end, relying heavily on word representations to provide background world knowledge. Therefore, it is desirable to model commonsense knowledge down to word-level analogical reasoning. In this sense, existing analogy benchmarks are lackluster. For Chinese analogy (CA), the simplified Chinese dataset CA8 (Li et al., 2018) and the traditional Chinese dataset CA-Google (Chen and Ma, 2018), translated from English (Mikolov et al., 2013), contain only a few dozen relations, most of which are either morphological, e.g., a shared prefix, or about named entities, e.g., capital-country. However, commonsense knowledge bases such as WordNet (Miller, 1995) and ConceptNet (Speer and Havasi, 2012) have long annotated relations in our lexicon. Among them, E-HowNet (Chen et al., 2005; Ma and Shih, 2018), extended from HowNet (Dong and Dong, 2003), currently annotates 88K traditional Chinese words with their structured defini"
2020.lrec-1.365,L18-1724,1,0.923396,"ing. In this sense, existing analogy benchmarks are lackluster. For Chinese analogy (CA), the simplified Chinese dataset CA8 (Li et al., 2018) and the traditional Chinese dataset CA-Google (Chen and Ma, 2018), translated from English (Mikolov et al., 2013), contain only a few dozen relations, most of which are either morphological, e.g., a shared prefix, or about named entities, e.g., capital-country. However, commonsense knowledge bases such as WordNet (Miller, 1995) and ConceptNet (Speer and Havasi, 2012) have long annotated relations in our lexicon. Among them, E-HowNet (Chen et al., 2005; Ma and Shih, 2018), extended from HowNet (Dong and Dong, 2003), currently annotates 88K traditional Chinese words with their structured definitions and English translations. In this paper, we propose an algorithm to extract accurate analogies from E-HowNet with refinements from linguists. We present CA-EHN, the first commonsense analogy dataset containing 90,505 analogies covering 5,656 words and 763 relations. In the experiments, we show that it is useful to embed more commonsense knowledge and that CA-EHN tests this aspect of word embedding. 2. Related Work In this work, we use word sense definitions from the"
2020.lrec-1.365,O01-1009,1,0.311647,"asses, we find that CA-EHN have an unprecedented coverage of 763 relations. Figures 4, 5, 6, 7 show some of the relations. 5.3. Embedding Benchmarking To evaluate the robustness of using CA-EHN for the classic intrinsic embedding evaluation task (Section 4.2.), we trained and tested different word embeddings across different benchmark datasets. We trained each word embedding using either GloVe (Pennington et al., 2014) or SGNS (Mikolov et al., 2013) on a small or a large corpus. The small corpus consisted of the traditional Chinese part of Chinese Gigaword (Graff and Chen, 2003) and ASBC 4.0 (Ma et al., 2001). The large corpus additionally included the Chinese part of Wikipedia. When calculating accuracy, only those analogy questions of which all words were in an embedding were considered. So a smaller dictionary was not penalized by lower analogy question coverage. Table 4 shows the results of different combinations of embeddings and benchmarks. It can be seen that CA-EHN is a robust benchmark for the analogy task. On all existing benchmarks and CA-EHN, it is consistent that GloVeSmall is the worst-performing and SGNS-Large is the best. Furthermore, the new dedicated commonsense analogy corpus ap"
2020.lrec-1.365,speer-havasi-2012-representing,0,0.0423372,"world knowledge. Therefore, it is desirable to model commonsense knowledge down to word-level analogical reasoning. In this sense, existing analogy benchmarks are lackluster. For Chinese analogy (CA), the simplified Chinese dataset CA8 (Li et al., 2018) and the traditional Chinese dataset CA-Google (Chen and Ma, 2018), translated from English (Mikolov et al., 2013), contain only a few dozen relations, most of which are either morphological, e.g., a shared prefix, or about named entities, e.g., capital-country. However, commonsense knowledge bases such as WordNet (Miller, 1995) and ConceptNet (Speer and Havasi, 2012) have long annotated relations in our lexicon. Among them, E-HowNet (Chen et al., 2005; Ma and Shih, 2018), extended from HowNet (Dong and Dong, 2003), currently annotates 88K traditional Chinese words with their structured definitions and English translations. In this paper, we propose an algorithm to extract accurate analogies from E-HowNet with refinements from linguists. We present CA-EHN, the first commonsense analogy dataset containing 90,505 analogies covering 5,656 words and 763 relations. In the experiments, we show that it is useful to embed more commonsense knowledge and that CA-EHN"
2020.lrec-1.365,N18-1101,0,0.0244473,"h translations. We present CA-EHN, the first commonsense word analogy dataset containing 90,505 analogies covering 5,656 words and 763 relations. Experiments show that CA-EHN stands out as a great indicator of how well word representations embed commonsense knowledge. The dataset is publicly available at https://github.com/ckiplab/CA-EHN. Keywords: Corpus, Lexicon, Lexical Database, Ontologies 1. Introduction Commonsense reasoning is fundamental for natural language agents to generalize inference beyond training corpora. Although the natural language inference (NLI) task (Bowman et al., 2015; Williams et al., 2018) has proved a good pre-training objective for sentence representations (Conneau et al., 2017), commonsense coverage is implicit and limited by the amount of annotated sentence pairs. Furthermore, most models are still end-to-end, relying heavily on word representations to provide background world knowledge. Therefore, it is desirable to model commonsense knowledge down to word-level analogical reasoning. In this sense, existing analogy benchmarks are lackluster. For Chinese analogy (CA), the simplified Chinese dataset CA8 (Li et al., 2018) and the traditional Chinese dataset CA-Google (Chen an"
2020.lrec-1.365,D15-1075,0,0.114069,"Missing"
2020.lrec-1.365,L18-1132,1,0.932218,", 2018) has proved a good pre-training objective for sentence representations (Conneau et al., 2017), commonsense coverage is implicit and limited by the amount of annotated sentence pairs. Furthermore, most models are still end-to-end, relying heavily on word representations to provide background world knowledge. Therefore, it is desirable to model commonsense knowledge down to word-level analogical reasoning. In this sense, existing analogy benchmarks are lackluster. For Chinese analogy (CA), the simplified Chinese dataset CA8 (Li et al., 2018) and the traditional Chinese dataset CA-Google (Chen and Ma, 2018), translated from English (Mikolov et al., 2013), contain only a few dozen relations, most of which are either morphological, e.g., a shared prefix, or about named entities, e.g., capital-country. However, commonsense knowledge bases such as WordNet (Miller, 1995) and ConceptNet (Speer and Havasi, 2012) have long annotated relations in our lexicon. Among them, E-HowNet (Chen et al., 2005; Ma and Shih, 2018), extended from HowNet (Dong and Dong, 2003), currently annotates 88K traditional Chinese words with their structured definitions and English translations. In this paper, we propose an algor"
2020.lrec-1.365,I05-7001,0,0.0638956,"l analogical reasoning. In this sense, existing analogy benchmarks are lackluster. For Chinese analogy (CA), the simplified Chinese dataset CA8 (Li et al., 2018) and the traditional Chinese dataset CA-Google (Chen and Ma, 2018), translated from English (Mikolov et al., 2013), contain only a few dozen relations, most of which are either morphological, e.g., a shared prefix, or about named entities, e.g., capital-country. However, commonsense knowledge bases such as WordNet (Miller, 1995) and ConceptNet (Speer and Havasi, 2012) have long annotated relations in our lexicon. Among them, E-HowNet (Chen et al., 2005; Ma and Shih, 2018), extended from HowNet (Dong and Dong, 2003), currently annotates 88K traditional Chinese words with their structured definitions and English translations. In this paper, we propose an algorithm to extract accurate analogies from E-HowNet with refinements from linguists. We present CA-EHN, the first commonsense analogy dataset containing 90,505 analogies covering 5,656 words and 763 relations. In the experiments, we show that it is useful to embed more commonsense knowledge and that CA-EHN tests this aspect of word embedding. 2. Related Work In this work, we use word sense"
2020.sigdial-1.1,L18-1724,1,0.862453,"Missing"
2020.sigdial-1.1,D15-1199,0,0.407681,"e Y1:t = [y1 , y2 , y3 , . . . , yt ], where t is the current time step of generation and yt is the token generated at the current step, a reward is to be given to the current token yt . However, these rewards can be estimated only once the entire sequence has been generated. To account for this, the generator must “roll out” the complete responses at every current step. For example, if we roll out starting from time step t, the complete utterance can be generated using Monte Carlo search as Background Semantically conditioned LSTM To incorporate given dialogue acts into utterance generation, Wen et al. (2015b) propose the semantic controlled LSTM (SC LSTM) cell, a special neural cell. The assigned dialogue acts are represented in one-hot form, and are fed into dialogue acts cells, which rely on a decreasing mechanism on dialogue acts information to avoid repetition. The formula for this semantically conditioned LSTM is as following: it = σ(Wwi wt + Whi ht−1 ) (1) ft = σ(Wwf wt + Whf ht−1 ) (2) ot = σ(Wwo wt + Who ht−1 ) (3) cˆt = tanh(Wwc wt + Whc ht−1 ) (4) ct = ft ⊗ ct−1 + it ⊗ cˆt + tanh(Wdc dt ) (5) ht = ot ⊗ tanh(ct ) (6) n Y1:T ∈ M C G (Y1:t ; N ) X l l αl Whr ht−1 ) (7) dt = rt ⊗ dt−1 (8)"
2020.sigdial-1.1,D14-1179,0,0.0101015,"Missing"
2020.sigdial-1.1,W15-4639,0,0.302491,"e Y1:t = [y1 , y2 , y3 , . . . , yt ], where t is the current time step of generation and yt is the token generated at the current step, a reward is to be given to the current token yt . However, these rewards can be estimated only once the entire sequence has been generated. To account for this, the generator must “roll out” the complete responses at every current step. For example, if we roll out starting from time step t, the complete utterance can be generated using Monte Carlo search as Background Semantically conditioned LSTM To incorporate given dialogue acts into utterance generation, Wen et al. (2015b) propose the semantic controlled LSTM (SC LSTM) cell, a special neural cell. The assigned dialogue acts are represented in one-hot form, and are fed into dialogue acts cells, which rely on a decreasing mechanism on dialogue acts information to avoid repetition. The formula for this semantically conditioned LSTM is as following: it = σ(Wwi wt + Whi ht−1 ) (1) ft = σ(Wwf wt + Whf ht−1 ) (2) ot = σ(Wwo wt + Who ht−1 ) (3) cˆt = tanh(Wwc wt + Whc ht−1 ) (4) ct = ft ⊗ ct−1 + it ⊗ cˆt + tanh(Wdc dt ) (5) ht = ot ⊗ tanh(ct ) (6) n Y1:T ∈ M C G (Y1:t ; N ) X l l αl Whr ht−1 ) (7) dt = rt ⊗ dt−1 (8)"
2020.sigdial-1.1,I05-7001,0,0.0943271,"Missing"
2020.sigdial-1.1,D17-1233,0,0.013522,"ntic controls. 1 Introduction Dialogue generation systems with adequate artificial intelligence responses hold great potential for practical use. A decent human-computer dialogue system should generate coherent and informative responses based on human-provided posts (Li et al., 2017). Sequence-to-sequence models (Sutskever et al., 2014) with long-short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent networks (Cho et al., 2014) have demonstrated profound improvements in open-domain dialogue systems (Shang et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Xu et al., 2016; Yao et al., 2017). However, these models often generate overly generic responses (Sordoni et al., 2015; Li et al., 2016a) that are independent of the given posts 1 Proceedings of the SIGdial 2020 Conference, pages 1–9 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics semantics to avoid dull or repetitive responses. As with conventional GAN (Goodfellow et al., 2014), our conditional SeqGAN comprises a generator and a discriminator; however, with the proposed discriminator we seek to not only distinguish machine-generated utterances from humangenerated utterances but also dis"
2020.sigdial-1.1,N16-1014,0,0.0295102,"s hold great potential for practical use. A decent human-computer dialogue system should generate coherent and informative responses based on human-provided posts (Li et al., 2017). Sequence-to-sequence models (Sutskever et al., 2014) with long-short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent networks (Cho et al., 2014) have demonstrated profound improvements in open-domain dialogue systems (Shang et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Xu et al., 2016; Yao et al., 2017). However, these models often generate overly generic responses (Sordoni et al., 2015; Li et al., 2016a) that are independent of the given posts 1 Proceedings of the SIGdial 2020 Conference, pages 1–9 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics semantics to avoid dull or repetitive responses. As with conventional GAN (Goodfellow et al., 2014), our conditional SeqGAN comprises a generator and a discriminator; however, with the proposed discriminator we seek to not only distinguish machine-generated utterances from humangenerated utterances but also distinguish postindependent from post-dependent utterances. The resulting additional SeqGAN architecture"
2020.sigdial-1.1,D16-1127,0,0.0341378,"s hold great potential for practical use. A decent human-computer dialogue system should generate coherent and informative responses based on human-provided posts (Li et al., 2017). Sequence-to-sequence models (Sutskever et al., 2014) with long-short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent networks (Cho et al., 2014) have demonstrated profound improvements in open-domain dialogue systems (Shang et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Xu et al., 2016; Yao et al., 2017). However, these models often generate overly generic responses (Sordoni et al., 2015; Li et al., 2016a) that are independent of the given posts 1 Proceedings of the SIGdial 2020 Conference, pages 1–9 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics semantics to avoid dull or repetitive responses. As with conventional GAN (Goodfellow et al., 2014), our conditional SeqGAN comprises a generator and a discriminator; however, with the proposed discriminator we seek to not only distinguish machine-generated utterances from humangenerated utterances but also distinguish postindependent from post-dependent utterances. The resulting additional SeqGAN architecture"
2020.sigdial-1.1,D17-1230,0,0.0223418,"or a given/assigned/predicted synset, only one of its synonyms should appear in the generated response; this constitutes a simple but effective semantic-control mechanism. We conduct both quantitative and qualitative evaluations, which show that the generated responses are not only higher-quality but also reflect the assigned semantic controls. 1 Introduction Dialogue generation systems with adequate artificial intelligence responses hold great potential for practical use. A decent human-computer dialogue system should generate coherent and informative responses based on human-provided posts (Li et al., 2017). Sequence-to-sequence models (Sutskever et al., 2014) with long-short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent networks (Cho et al., 2014) have demonstrated profound improvements in open-domain dialogue systems (Shang et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Xu et al., 2016; Yao et al., 2017). However, these models often generate overly generic responses (Sordoni et al., 2015; Li et al., 2016a) that are independent of the given posts 1 Proceedings of the SIGdial 2020 Conference, pages 1–9 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Compu"
2020.sigdial-1.1,P15-1152,0,0.216168,"rated responses are not only higher-quality but also reflect the assigned semantic controls. 1 Introduction Dialogue generation systems with adequate artificial intelligence responses hold great potential for practical use. A decent human-computer dialogue system should generate coherent and informative responses based on human-provided posts (Li et al., 2017). Sequence-to-sequence models (Sutskever et al., 2014) with long-short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent networks (Cho et al., 2014) have demonstrated profound improvements in open-domain dialogue systems (Shang et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Xu et al., 2016; Yao et al., 2017). However, these models often generate overly generic responses (Sordoni et al., 2015; Li et al., 2016a) that are independent of the given posts 1 Proceedings of the SIGdial 2020 Conference, pages 1–9 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics semantics to avoid dull or repetitive responses. As with conventional GAN (Goodfellow et al., 2014), our conditional SeqGAN comprises a generator and a discriminator; however, with the proposed discriminator we seek to not only disting"
2020.sigdial-1.1,N15-1020,0,0.0196063,"intelligence responses hold great potential for practical use. A decent human-computer dialogue system should generate coherent and informative responses based on human-provided posts (Li et al., 2017). Sequence-to-sequence models (Sutskever et al., 2014) with long-short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent networks (Cho et al., 2014) have demonstrated profound improvements in open-domain dialogue systems (Shang et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Xu et al., 2016; Yao et al., 2017). However, these models often generate overly generic responses (Sordoni et al., 2015; Li et al., 2016a) that are independent of the given posts 1 Proceedings of the SIGdial 2020 Conference, pages 1–9 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics semantics to avoid dull or repetitive responses. As with conventional GAN (Goodfellow et al., 2014), our conditional SeqGAN comprises a generator and a discriminator; however, with the proposed discriminator we seek to not only distinguish machine-generated utterances from humangenerated utterances but also distinguish postindependent from post-dependent utterances. The resulting additional Seq"
2021.findings-acl.228,D14-1181,0,0.00259288,"work We propose H-FND, a hierarchical false-negative denoising framework that determines whether to keep, discard, or revise negative instances. As illustrated in Fig. 1, H-FND is composed of the denoising agent and relation classifier modules. The denoising agent makes a ternary decision on the action to take on each negative instance, and after discarding, the relation classifier predicts a new relation for each to-be-revised instance to produce a cleaned dataset. 3.1 Convolutional Neural Network Convolutional neural networks (CNN) are commonly adopted for sentence-level feature extraction (Kim, 2014) in language understanding tasks, such as relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). PCNNs (Zeng et al., 2015), a variation of CNN that applies piecewise max pooling, are also widely used for extracting sentence features (Lin et al., 2016; Qin et al., 2018a). We included both as the base model in our experiments to show that our framework is base model agnostic. In our implementation, the extracted features of a learning instance s are fed into a fully connected softmax classifier to compute the final logits: O(r) = softmax(FC(CNN(s))). For detailed mathematical descri"
2021.findings-acl.228,P18-1046,0,0.0276529,"Missing"
2021.findings-acl.228,P18-1199,0,0.096579,"FN degrade model performance if they are treated as correct labels at training time. FPs harm prediction precision, while excessive FNs lead to low recall rates. In addition to denoising methods for learning robustly with noisy data (Han et al., 2018; Northcutt et al., 2019), many works focus on alleviating the FP problem in DS datasets, including those on pattern-based extraction (Alfonseca et al., 2012; Jia et al., 2019), multiple-instance learning (Surdeanu et al., 2012; Lin et al., 2016; Zeng et al., 2018), and sentence-level denoising with adversarial training or reinforcement learning (Qin et al., 2018a,b; Feng et al., 2018). However, few investigate the FN problem for distant supervision (Xu et al., 2013; Roller et al., 2015). To the best of our knowledge, there is no previous study on this problem for deep neural networks. In this paper, we investigate the impact of FNs on neural-based models and propose H-FND, a hierarchical false-negative denoising framework for robust distant supervision. Specifically, this framework integrates a deep reinforcement learning agent which keeps, discards, or revises probable FN instances with a relation classifier to generate revised relations. In additio"
2021.findings-acl.228,P15-2045,0,0.115011,"excessive FNs lead to low recall rates. In addition to denoising methods for learning robustly with noisy data (Han et al., 2018; Northcutt et al., 2019), many works focus on alleviating the FP problem in DS datasets, including those on pattern-based extraction (Alfonseca et al., 2012; Jia et al., 2019), multiple-instance learning (Surdeanu et al., 2012; Lin et al., 2016; Zeng et al., 2018), and sentence-level denoising with adversarial training or reinforcement learning (Qin et al., 2018a,b; Feng et al., 2018). However, few investigate the FN problem for distant supervision (Xu et al., 2013; Roller et al., 2015). To the best of our knowledge, there is no previous study on this problem for deep neural networks. In this paper, we investigate the impact of FNs on neural-based models and propose H-FND, a hierarchical false-negative denoising framework for robust distant supervision. Specifically, this framework integrates a deep reinforcement learning agent which keeps, discards, or revises probable FN instances with a relation classifier to generate revised relations. In addition, to constrain the study to the FN problem and to construct groundtruth relations to further analyze model behavior, we conduc"
2021.findings-acl.228,D12-1042,0,0.252607,"be applied for many applications, such Equal contribution. The code can be found at https://github.com/ ckiplab/hfnd 1 Type TP FP FN Table 1: Distant supervision and different types of incorrectly labeled relations. The head and tail entities are shown in boldface, and “PoB” stands for the relation “Place of Birth”. Introduction * Relation PoB Relation PoB (3) PoB (7) NA (7) as question answering and knowledge graph completion. A major difficulty with supervising relation extraction models is the cost of collecting training data, against which distant supervision (DS) (Hoffmann et al., 2011; Surdeanu et al., 2012) is proposed. DS obtains the relational facts from a knowledge base and aligns these facts to all sentences in the corpus to generate learning instances. In specific, if a relation triple r(h, t) exists in a knowledge base, then for a sentence s which mentions both the head entity h and the tail entity t, it is tagged with relation r to form a learning instance (r, h, t, s). Since an effective classifier is expected not only to extract relation triples from a given text but also have to identify those unrelated entity pairs, the negative samples from texts are also needed for the training. In"
2021.findings-acl.228,P13-2117,0,0.0579649,"Missing"
2021.findings-acl.228,D15-1203,0,0.0214392,"gative instances. As illustrated in Fig. 1, H-FND is composed of the denoising agent and relation classifier modules. The denoising agent makes a ternary decision on the action to take on each negative instance, and after discarding, the relation classifier predicts a new relation for each to-be-revised instance to produce a cleaned dataset. 3.1 Convolutional Neural Network Convolutional neural networks (CNN) are commonly adopted for sentence-level feature extraction (Kim, 2014) in language understanding tasks, such as relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). PCNNs (Zeng et al., 2015), a variation of CNN that applies piecewise max pooling, are also widely used for extracting sentence features (Lin et al., 2016; Qin et al., 2018a). We included both as the base model in our experiments to show that our framework is base model agnostic. In our implementation, the extracted features of a learning instance s are fed into a fully connected softmax classifier to compute the final logits: O(r) = softmax(FC(CNN(s))). For detailed mathematical descriptions of CNN and PCNN, please refer to the Appendix. 3.2 instance, and the second decides whether to revise the input instance to be n"
C02-1049,O97-4005,0,0.606936,"Missing"
C02-1049,C92-1019,1,0.940069,"Missing"
C02-1049,Y96-1018,1,0.868122,"Missing"
C02-1049,O98-3002,1,0.876146,"Missing"
C02-1049,O92-1003,0,0.240942,"Missing"
C02-1049,J93-1001,0,0.0459521,"Missing"
C02-1049,J96-1001,0,\N,Missing
C02-1049,J96-3004,0,\N,Missing
C02-1049,J93-1007,0,\N,Missing
C02-1049,C00-1027,0,\N,Missing
C02-1049,O97-4003,1,\N,Missing
C02-1049,O91-1003,1,\N,Missing
C02-1049,O93-1004,0,\N,Missing
D15-1122,W07-0726,0,0.0193598,"et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 200"
D15-1122,N12-1059,0,0.0671925,"le MT engines through its hierarchical paraphrases, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further sentence-level selection, en"
D15-1122,2010.amta-papers.9,0,0.316637,"hen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 2009; Du and Way 2010; Ma and McKeown 2012). In a lattice, each edge is associated with a phrase (a single word or a sequence of words) rather than a single word. The construction of the lattice is based on the extraction of phrase pairs from word alignments between a selected best MT system hypothesis (the backbone) and the other translation hypotheses. One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines, i.e, the consensus among occurrences of discontinuous words. In this paper, we propose another phrase-level combi"
D15-1122,D13-1111,0,0.0207575,", which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further sentence-level selection, enabling us to exploit more sophisticated info"
D15-1122,W11-2118,0,0.0360959,"Missing"
D15-1122,W10-1746,0,0.0204563,"target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these app"
D15-1122,2006.amta-papers.25,0,0.232378,"Missing"
D15-1122,D07-1029,0,\N,Missing
D15-1122,E06-1005,0,\N,Missing
D15-1122,P08-2021,0,\N,Missing
D15-1122,P07-1040,0,\N,Missing
D15-1122,D09-1115,0,\N,Missing
D15-1122,N04-1022,0,\N,Missing
D15-1122,E14-2008,0,\N,Missing
D15-1122,N07-1029,0,\N,Missing
D15-1122,P09-1106,0,\N,Missing
D15-1122,D08-1011,0,\N,Missing
D15-1122,J07-2003,0,\N,Missing
D15-1122,W09-0405,0,\N,Missing
D15-1122,W09-0407,0,\N,Missing
D15-1122,P03-1021,0,\N,Missing
D17-1282,Q16-1026,0,0.409321,"oosevelt Rd. Taipei 10617, Taiwan jacobvsdanniel@gmail.com Ruo-Ping Dong National Tsing Hua University No. 101, Sec. 2, Kuang-Fu Rd. Hsinchu 30013, Taiwan dongruoping@gmail.com Yu-Siang Wang National Taiwan University No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan b03202047@ntu.edu.tw Ju-Chieh Chou National Taiwan University No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan jjery2243542@gmail.com Wei-Yun Ma Academia Sinica No. 128, Sec. 2, Academia Rd. Taipei 11529, Taiwan ma@iis.sinica.edu.tw Abstract performances in several benchmark datasets (Ratinov and Roth, 2009; Passos et al., 2014; Chiu and Nichols, 2016). In this paper, we utilize the linguistic structures of texts to improve named entity recognition by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that named entities are highly related to linguistic constituents, we propose a constituent-based BRNN-CNN for named entity recognition. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are linguistic constituents. Then it classifies these chunks with a constituency tree structure b"
D17-1282,Q14-1037,0,0.0279969,"Missing"
D17-1282,N09-1037,0,0.041589,"Missing"
D17-1282,N06-2015,0,0.186336,"taking, for each token, the hidden features of its previous token as well as its raw features to compute its own hidden features. Then they classify each token by these hidden features. With both forward and backward directions, networks learn how to propagate the information of a token sequence to each token. Chiu and Nichols (2016) utilize a variation of recurrent networks, bidirectional LSTM, attached with a CNN, which learns character-level features instead of handcrafting. They accomplish state-of-the-art results on both CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) and OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2013) datasets. Introduction Named Entity Recognition (NER) can be seen as a combined task of locating named entity chunks of texts and classifying which named entity category a chunk falls into. Traditional approaches label each token in texts as a part of a named entity chunk, e.g. “person begin”, and achieve high Classical sequential labeling approaches take little information about phrase structures of sentences. However, according to our analysis, most named entity chunks are actually linguistic constituents, e.g. noun phrases. This motivates us to focus on a constituent"
D17-1282,P13-1045,0,0.09615,"Missing"
D17-1282,D13-1170,0,0.00650794,"Missing"
D17-1282,P15-1150,0,0.0873896,"Missing"
D17-1282,W03-0419,0,0.0843402,"Missing"
D17-1282,W14-1609,0,0.0283419,"sity No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan jacobvsdanniel@gmail.com Ruo-Ping Dong National Tsing Hua University No. 101, Sec. 2, Kuang-Fu Rd. Hsinchu 30013, Taiwan dongruoping@gmail.com Yu-Siang Wang National Taiwan University No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan b03202047@ntu.edu.tw Ju-Chieh Chou National Taiwan University No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan jjery2243542@gmail.com Wei-Yun Ma Academia Sinica No. 128, Sec. 2, Academia Rd. Taipei 11529, Taiwan ma@iis.sinica.edu.tw Abstract performances in several benchmark datasets (Ratinov and Roth, 2009; Passos et al., 2014; Chiu and Nichols, 2016). In this paper, we utilize the linguistic structures of texts to improve named entity recognition by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that named entities are highly related to linguistic constituents, we propose a constituent-based BRNN-CNN for named entity recognition. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are linguistic constituents. Then it classifies these chunks with a cons"
D17-1282,D14-1162,0,0.0761586,"the head children. The heuristic is that a head constituent is usually modified by its siblings in a near to far fashion. Algorithm 1 shows the recursive procedure called for the root node of a parse. Figure 1 shows the application of the algorithm to the parse of senator Edward Kennedy. With the heuristic that Edward modifies the head node Kennedy before senator. The binarization process successfully adds a new node Edward Kennedy that corresponds to a person name. Computing Word Embeddings For each word, our network retrieves one embedding from a trainable lookup table initialized by GloVe (Pennington et al., 2014). However, to capture the morphology information of a word and help dealing with unseen words, the network computes another character-level embedding. Inspired by Kim et al. (2016), the network passes onehot character vectors through a series of convolutional and highway layers to generate the embedding. These two embeddings are concatenated as the final embedding of a word. 3.3 Computing Hidden Features Given a constituency parse tree, where every node represents a constituent, our network recursively computes two hidden state features for every node. First, for each node i with left sibling"
D17-1282,W13-3516,0,0.140817,"Missing"
D17-1282,W09-1119,0,0.181856,"i National Taiwan University No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan jacobvsdanniel@gmail.com Ruo-Ping Dong National Tsing Hua University No. 101, Sec. 2, Kuang-Fu Rd. Hsinchu 30013, Taiwan dongruoping@gmail.com Yu-Siang Wang National Taiwan University No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan b03202047@ntu.edu.tw Ju-Chieh Chou National Taiwan University No. 1, Sec. 4, Roosevelt Rd. Taipei 10617, Taiwan jjery2243542@gmail.com Wei-Yun Ma Academia Sinica No. 128, Sec. 2, Academia Rd. Taipei 11529, Taiwan ma@iis.sinica.edu.tw Abstract performances in several benchmark datasets (Ratinov and Roth, 2009; Passos et al., 2014; Chiu and Nichols, 2016). In this paper, we utilize the linguistic structures of texts to improve named entity recognition by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that named entities are highly related to linguistic constituents, we propose a constituent-based BRNN-CNN for named entity recognition. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are linguistic constituents. Then it classifies the"
D17-1282,J03-4003,0,\N,Missing
D18-1474,P16-1223,0,0.0798313,"Missing"
D18-1474,P17-1171,0,0.0201481,") cells and gate recurrent unit (GRU) cells have achieved great success and are increasingly being applied in nature language processing tasks, e.g., part-of-speech (POS) tagging (Wang et al., 2015), named-entity recognition (Chiu and Nichols, 2015), sentiment analysis (Zhang et al., 2018), document classification (Kim, 2014; Le and Mikolov, 2014a), cloze (Srinivasan et al., 2018), machine translation (Bahdanau et al., 2015), dialogue modeling (Mei et al., 2017), document summarization (Allahyari et al., 2017), automatic knowledge extraction (Durme and Schubert, 2008), and question answering (Chen et al., 2017). Those tasks all call for text comprehension techniques. To solve these tasks, the proposed models read all the text available. That is, models read every token or word of the text from beginning to end. However, for some classification tasks, it is not necessary to treat each individual word equally. Take, for example, sentiment analysis: sentences such as “this movie is amazing” or “too boring” are sufficient to judge a sentiment without reading the entire comment. In addition, the fact that texts are often written redundantly also motivates reading selectively, especially for certain NLP t"
D18-1474,W08-2219,0,0.060037,"Missing"
D18-1474,P11-1015,0,0.15237,"Missing"
D18-1474,D17-1259,0,0.0164416,"i ; θU )R] U i=1 ≈ M T 1 XX m m [∇θU log p(sm 1:i |hi ; θU )R ], M m=1 i=1 where the superscript m denotes that it belongs to the m-th example. Eventually, the term ∇θU log p(s1:i |hi ; θU ) is computed by backpropagation as usual. Though the approximation of ∇θU J2 (θU ) is unbiased, it may have very high variance (Williams, 1992). One common way to reduce this variance is to subtract a baseline value b from the reward function R, transforming the approximated gradient into ∇θU J2 (θU ) ≈ 4441 M T 1 XX m m m [∇θU log p(sm 1:i |hi ; θU )R − bi ]. M m=1 i=1 Here we apply same bias strategy as (Lewis et al., 2017), treating the bias value b as the average reward from then until now. The final objective function for LSTM-Shuttle to minimize is J(θR , θU ) = J1 (θR ) − J2 (θU ), which is entirely differentiable and can be computed by standard backpropagation. 3.3 Implementation detail and Inference To simulate negative step selection, which corresponds to reading backward in the shuttle action, we set the shuttle output dimension to [0, 2K], where 0 maps to −K, 1 maps to −(K − 1), . . . , K maps to 0, . . . , 2K − 1 maps to +(K − 1), and 2K maps to +K. We used the Adam optimizer (Kingma and Ba, 2014) wit"
D18-1474,P05-1015,0,0.765429,"to correct 4439 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4439–4448 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics misunderstandings, we skim more words and thus read faster without reducing our comprehension. In this paper, we propose LSTM-Shuttle, which teaches the RNN model to speed read by modeling both forward and backward reading behavior. We evaluate the proposed method on sentiment analysis, document classification, and cloze tasks. We use IMDB (L. et al., 2011) and Rotten Tomatoes (Pang and Lee, 2005) as sentiment analysis datasets, AG New (Shang et al., 2015) as a document classification dataset, and Facebook Children’s Book Test (Hill et al., 2015) as a cloze dataset. The experiments show that the proposed method achieves better prediction performance and reads faster at the same time, in comparison with the LSTM baseline (Hochreiter and Schmidhuber, 1997) and LSTM-Jump. We also analyze the shuttling behavior under different settings, proving that reading forward and backward does help in reading. 2 Related Work The proposed method is inspired mainly by LSTM-Jump (Yu et al., 2017), which"
D18-1474,P15-1152,0,0.0539064,"cal Methods in Natural Language Processing, pages 4439–4448 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics misunderstandings, we skim more words and thus read faster without reducing our comprehension. In this paper, we propose LSTM-Shuttle, which teaches the RNN model to speed read by modeling both forward and backward reading behavior. We evaluate the proposed method on sentiment analysis, document classification, and cloze tasks. We use IMDB (L. et al., 2011) and Rotten Tomatoes (Pang and Lee, 2005) as sentiment analysis datasets, AG New (Shang et al., 2015) as a document classification dataset, and Facebook Children’s Book Test (Hill et al., 2015) as a cloze dataset. The experiments show that the proposed method achieves better prediction performance and reads faster at the same time, in comparison with the LSTM baseline (Hochreiter and Schmidhuber, 1997) and LSTM-Jump. We also analyze the shuttling behavior under different settings, proving that reading forward and backward does help in reading. 2 Related Work The proposed method is inspired mainly by LSTM-Jump (Yu et al., 2017), which predicts how many words should be neglected, accelerating t"
D18-1474,N18-2015,0,0.031316,"icts both better and more quickly. To demonstrate how LSTM-Shuttle actually behaves, we also analyze the shuttling operation and present a case study. 1 Introduction Recently, recurrent neural networks (RNNs) and long short-term memory (LSTM) cells and gate recurrent unit (GRU) cells have achieved great success and are increasingly being applied in nature language processing tasks, e.g., part-of-speech (POS) tagging (Wang et al., 2015), named-entity recognition (Chiu and Nichols, 2015), sentiment analysis (Zhang et al., 2018), document classification (Kim, 2014; Le and Mikolov, 2014a), cloze (Srinivasan et al., 2018), machine translation (Bahdanau et al., 2015), dialogue modeling (Mei et al., 2017), document summarization (Allahyari et al., 2017), automatic knowledge extraction (Durme and Schubert, 2008), and question answering (Chen et al., 2017). Those tasks all call for text comprehension techniques. To solve these tasks, the proposed models read all the text available. That is, models read every token or word of the text from beginning to end. However, for some classification tasks, it is not necessary to treat each individual word equally. Take, for example, sentiment analysis: sentences such as “thi"
D18-1474,P17-1172,0,0.473348,"uch as “this movie is amazing” or “too boring” are sufficient to judge a sentiment without reading the entire comment. In addition, the fact that texts are often written redundantly also motivates reading selectively, especially for certain NLP tasks. In terms of human reading habits, although people tend to skim text when reading a newspaper or a novel, this does not significantly impair comprehension. Speed reading, a reading technique, is used to improve one’s ability to read quickly. Work has been done on modeling skimming behavior along with the original sequence modeling RNN. LSTM-Jump (Yu et al., 2017) predicts how many words to skim based on the RNN hidden state. They show that neglecting some words in a document does not greatly harm prediction accuracy but does significantly accelerate the process. They also show that for certain tasks such as cloze, skimming even outperforms traditional methods. In addition, (Yu et al., 2018) use RNN hidden states to decide when to stop. If the RNN judges it has achieved sufficient comprehension of the context, it stops early and produce the answer directly. However, strictly speakly, simply skimming and stopping early is not speed reading. For example,"
E17-2082,N09-1003,0,0.0403696,"6 word pairs. 3.1.2 3.2 Word Similarity Task The aim of word similarity task is to check whether a given word would have the similarity score which closely corresponds to human judges. These datasets contain relatedness scores for pairs of words; the cosine similarity of the embedding for two words should have high correlation. We use five datasets to evaluate: MEN-3k (Bruni et al., 2014), RW (Luong et al., 2013), WordSim353 (Finkelstein et al., 2002), also the partitioned dataset from WordSim-353, separated into the dataset into two different relations, WS353Similarity and WS353-Relatedness (Agirre et al., 2009; Zesch et al., 2008). Table 1 shows that comparing to the baseline, all of our proposed three methods get significant improvement. The results support our argument that incorporating prior knowledge into context-based embeddings can complement the embedding quality of those words which lack enough statistics of word occurrences. Parameter Setting We set all our embedding size to 300, which is a suitable embedding size mentioned in (Melamud et al., 2016). The training iteration for RCM is 100. Learning rate for CBOW is 0.025. We experiment on an array of learning rates for the Baseline(Joint)"
E17-2082,N13-1092,0,0.0407491,"Missing"
E17-2082,P12-1092,0,0.0383931,"this idea, we argue that while incorporating prior knowledge into context-based embeddings, words with different statistics of word occurrences should be treated differently. With this idea, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings. Distributed word representation maps each word into a real-valued vector. The produced vector has implied the abstract meaning of the word for their syntactic (Collobert and Weston, 2008; Luong et al., 2013; Mnih and Hinton, 2007; Turian et al., 2010) and semantic (Huang et al., 2012; Socher et al., 2013b) information. These vectors have been used as features in a variety of applications, such as information retrieval (Salton and McGill, 1984), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), name entity recognition (Turian et al., 2010), and syntactic parsing (Socher et al., 2013a). 509 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 509–515, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Learning Embeddings"
E17-2082,E14-1051,0,0.0255567,"e word w. The objective function is shown as following: In this section, we will first review word2vec, a popular context-based embedding approach, and then introduce Relation Constrained Model (RCM) to incorporate prior knowledge. Finally we propose our approach to utilize the both two models, making words with different statistics of word occurrences be treated differently while incorporating prior knowledge. 2.1 Context-based Embedding Context-based embedding has two main model families: global matrix factorization methods, such as latent semantic analysis (LSA) (Bullinaria and Levy, 2007; Lebret and Collobert, 2014; Pennington et al., 2014; Rohde et al., 2006) and local context window methods (Bengio, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a). Both training models learn the embedding by using the statistical information of the word context from a large corpus. In this paper, we adopt continuous bag-of-word (CBOW) in word2vec (Mikolov et al., 2013a) as our context-based embedding model. CBOW is an unsupervised learning algorithm using a neural language models, given a target word wt and its c neighboring words, the model is aimed at maximizing the log-likelihood of each word given its cont"
E17-2082,P14-2050,0,0.0303383,"23.4 23.3 21.9 WS353s 37.3 41.6 42.5 41.3 41.9 40.5 41.5 42.5 41.4 Average 21.1 25.8 26.2 26.0 26.4 25.5 26.3 26.5 25.7 Table 3: Spearman rank correlation on word similarity task. All embeddings are 300 dimensions. The corpus is the same as Table 1, but the size is 1/100. The best result for each dataset is highlighted in bold. Resource of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. The goal of this task is to find a term c for a given term d so that c:d best resembles a sample relationship a:b. We use the vector offset method (Levy and Goldberg, 2014; Mikolov et al., 2013b), computing ed = ea − eb + ec and returning the vector which has the highest cosine similarity to ed . We use two datasets, Googles analogy dataset (Mikolov et al., 2013b), which contains 19,544 questions, about half of the questions are syntactic analogies and another half of a more semantic nature, and MSR analogy dataset (Mikolov et al., 2013b), which contains 8,000 syntactic analogy questions. Table 2 shows the similar result as Word Similarity and demonstrates our proposed methods are stable and can be applied to different tasks. 3.4 PPDB WordNet Method CBOW Baseli"
E17-2082,D13-1167,0,0.0146739,"ovements on two different tasks: Word Similarity and Analogical Reasoning. 1 Knowledge bases provide rich semantic relatedness between words, which are more likely to capture the desired semantics on certain NLP tasks. To improve the quality of context-based embeddings, some researchers attempted to incorporate knowledge base, such as WordNet (Miller, 1995) and Paraphrase Database (Ganitkevitch et al., 2013) into the learning process. Recent work has shown that aggregating the knowledge base information into context-based embeddings can significantly improve the embeddings (Bian et al., 2014; Chang et al., 2013; Faruqui et al., 2015; Xu et al., 2014; Yih et al., 2012; Yu and Dredze, 2014). Introduction One implicit but critical reason of the success on using knowledge bases, based on our insight, is that knowledge bases can complement the embedding quality of those words which lack enough statistics of word occurrences, such as enough occurrences or diversity of their context. These words may suffer the difficulty obtaining meaningful information from the given corpus. Following this idea, we argue that while incorporating prior knowledge into context-based embeddings, words with different statistic"
E17-2082,W13-3512,0,0.0810137,"Missing"
E17-2082,N16-1118,0,0.0223266,"also the partitioned dataset from WordSim-353, separated into the dataset into two different relations, WS353Similarity and WS353-Relatedness (Agirre et al., 2009; Zesch et al., 2008). Table 1 shows that comparing to the baseline, all of our proposed three methods get significant improvement. The results support our argument that incorporating prior knowledge into context-based embeddings can complement the embedding quality of those words which lack enough statistics of word occurrences. Parameter Setting We set all our embedding size to 300, which is a suitable embedding size mentioned in (Melamud et al., 2016). The training iteration for RCM is 100. Learning rate for CBOW is 0.025. We experiment on an array of learning rates for the Baseline(Joint) and the best one is 0.0001. While the learning rate for Threshold remains 0.0001, we attempt various learning rates for Function(Freq.) and Function(Ent.) and the best one is 0.001, which is larger than 0.0001. This setting can be actually explained by that the output values of the two functions are between 0 to 1, which is used to decrease the learning rate. In other words, the learning rate of the two functions needs to be set a larger value than the b"
E17-2082,N15-1184,0,0.0170103,"erent tasks: Word Similarity and Analogical Reasoning. 1 Knowledge bases provide rich semantic relatedness between words, which are more likely to capture the desired semantics on certain NLP tasks. To improve the quality of context-based embeddings, some researchers attempted to incorporate knowledge base, such as WordNet (Miller, 1995) and Paraphrase Database (Ganitkevitch et al., 2013) into the learning process. Recent work has shown that aggregating the knowledge base information into context-based embeddings can significantly improve the embeddings (Bian et al., 2014; Chang et al., 2013; Faruqui et al., 2015; Xu et al., 2014; Yih et al., 2012; Yu and Dredze, 2014). Introduction One implicit but critical reason of the success on using knowledge bases, based on our insight, is that knowledge bases can complement the embedding quality of those words which lack enough statistics of word occurrences, such as enough occurrences or diversity of their context. These words may suffer the difficulty obtaining meaningful information from the given corpus. Following this idea, we argue that while incorporating prior knowledge into context-based embeddings, words with different statistics of word occurrences"
E17-2082,N13-1090,0,0.708967,"nd then introduce Relation Constrained Model (RCM) to incorporate prior knowledge. Finally we propose our approach to utilize the both two models, making words with different statistics of word occurrences be treated differently while incorporating prior knowledge. 2.1 Context-based Embedding Context-based embedding has two main model families: global matrix factorization methods, such as latent semantic analysis (LSA) (Bullinaria and Levy, 2007; Lebret and Collobert, 2014; Pennington et al., 2014; Rohde et al., 2006) and local context window methods (Bengio, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a). Both training models learn the embedding by using the statistical information of the word context from a large corpus. In this paper, we adopt continuous bag-of-word (CBOW) in word2vec (Mikolov et al., 2013a) as our context-based embedding model. CBOW is an unsupervised learning algorithm using a neural language models, given a target word wt and its c neighboring words, the model is aimed at maximizing the log-likelihood of each word given its context. The objective function is shown as following: J= 1 T T X t=1  t+c log p wt |wt−c N 1 X X J= log p (w|wi ) N where &gt; p(w|wi ) = exp(e0w ew"
E17-2082,D12-1111,0,0.0569769,"Missing"
E17-2082,P14-2089,0,0.446668,"Knowledge bases provide rich semantic relatedness between words, which are more likely to capture the desired semantics on certain NLP tasks. To improve the quality of context-based embeddings, some researchers attempted to incorporate knowledge base, such as WordNet (Miller, 1995) and Paraphrase Database (Ganitkevitch et al., 2013) into the learning process. Recent work has shown that aggregating the knowledge base information into context-based embeddings can significantly improve the embeddings (Bian et al., 2014; Chang et al., 2013; Faruqui et al., 2015; Xu et al., 2014; Yih et al., 2012; Yu and Dredze, 2014). Introduction One implicit but critical reason of the success on using knowledge bases, based on our insight, is that knowledge bases can complement the embedding quality of those words which lack enough statistics of word occurrences, such as enough occurrences or diversity of their context. These words may suffer the difficulty obtaining meaningful information from the given corpus. Following this idea, we argue that while incorporating prior knowledge into context-based embeddings, words with different statistics of word occurrences should be treated differently. With this idea, we propose"
E17-2082,D14-1162,0,0.104164,"tion is shown as following: In this section, we will first review word2vec, a popular context-based embedding approach, and then introduce Relation Constrained Model (RCM) to incorporate prior knowledge. Finally we propose our approach to utilize the both two models, making words with different statistics of word occurrences be treated differently while incorporating prior knowledge. 2.1 Context-based Embedding Context-based embedding has two main model families: global matrix factorization methods, such as latent semantic analysis (LSA) (Bullinaria and Levy, 2007; Lebret and Collobert, 2014; Pennington et al., 2014; Rohde et al., 2006) and local context window methods (Bengio, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a). Both training models learn the embedding by using the statistical information of the word context from a large corpus. In this paper, we adopt continuous bag-of-word (CBOW) in word2vec (Mikolov et al., 2013a) as our context-based embedding model. CBOW is an unsupervised learning algorithm using a neural language models, given a target word wt and its c neighboring words, the model is aimed at maximizing the log-likelihood of each word given its context. The objective functi"
E17-2082,P13-1045,0,0.0154807,"that while incorporating prior knowledge into context-based embeddings, words with different statistics of word occurrences should be treated differently. With this idea, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings. Distributed word representation maps each word into a real-valued vector. The produced vector has implied the abstract meaning of the word for their syntactic (Collobert and Weston, 2008; Luong et al., 2013; Mnih and Hinton, 2007; Turian et al., 2010) and semantic (Huang et al., 2012; Socher et al., 2013b) information. These vectors have been used as features in a variety of applications, such as information retrieval (Salton and McGill, 1984), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), name entity recognition (Turian et al., 2010), and syntactic parsing (Socher et al., 2013a). 509 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 509–515, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Learning Embeddings 2.2 RCM (Yu and Dred"
E17-2082,D13-1170,0,0.00490736,"that while incorporating prior knowledge into context-based embeddings, words with different statistics of word occurrences should be treated differently. With this idea, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings. Distributed word representation maps each word into a real-valued vector. The produced vector has implied the abstract meaning of the word for their syntactic (Collobert and Weston, 2008; Luong et al., 2013; Mnih and Hinton, 2007; Turian et al., 2010) and semantic (Huang et al., 2012; Socher et al., 2013b) information. These vectors have been used as features in a variety of applications, such as information retrieval (Salton and McGill, 1984), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), name entity recognition (Turian et al., 2010), and syntactic parsing (Socher et al., 2013a). 509 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 509–515, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Learning Embeddings 2.2 RCM (Yu and Dred"
E17-2082,P10-1040,0,0.0688578,"on from the given corpus. Following this idea, we argue that while incorporating prior knowledge into context-based embeddings, words with different statistics of word occurrences should be treated differently. With this idea, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings. Distributed word representation maps each word into a real-valued vector. The produced vector has implied the abstract meaning of the word for their syntactic (Collobert and Weston, 2008; Luong et al., 2013; Mnih and Hinton, 2007; Turian et al., 2010) and semantic (Huang et al., 2012; Socher et al., 2013b) information. These vectors have been used as features in a variety of applications, such as information retrieval (Salton and McGill, 1984), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), name entity recognition (Turian et al., 2010), and syntactic parsing (Socher et al., 2013a). 509 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 509–515, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational"
I17-3015,D13-1160,0,0.0196502,"ns to narrow down the wide range of the categories to which the object belongs. The question can be: “Is it animal?” or “Can it fly?”, etc. The game terminates when the correct object is guessed by the questioners. Guess What is a kind of Chinese-based Twenty Questions game, where the system serves as the answerer and users as questioners. The answer set of the system currently contains 200 terms, which are general concepts such as dog, cat, boat, computer, etc. Figure 2 shows a running example of Guess What system. The framework involves different research topics, such as question answering (Berant et al., 2013; Kwok et al., 2001) and relation prediction (Xu et al., 2016). The techniques include understanding the questions and identifying whether the object fits the description of the users’ questions. Since most descriptions are based on the existence of a relationship between two entities, such as “Is it an animal?” or “Can it fly?”, the latter mission turns out to be identifying whether a certain relationship between entities holds or not, which is a kind of on-demand knowledge validation. Guess What goes through the following procedures: Parsing the user’s question, followed by extracting knowle"
I17-3015,I05-7001,0,0.0274386,"ying whether the object fits the description of the users’ questions. Since most descriptions are based on the existence of a relationship between two entities, such as “Is it an animal?” or “Can it fly?”, the latter mission turns out to be identifying whether a certain relationship between entities holds or not, which is a kind of on-demand knowledge validation. Guess What goes through the following procedures: Parsing the user’s question, followed by extracting knowledge and reasoning from metadata of Wikipedia2 and a lexical semantic representation model named E-HowNet3 (Ma and Chen, 2009; Chen et al., 2005). If the related knowledge In this demo, we propose an idea of ondemand knowledge validation and fulfill the idea through an interactive QuestionAnswering (QA) game system, which is named Guess What. An object (e.g. dog) is first randomly chosen by the system, and then a user can repeatedly ask the system questions in natural language to guess what the object is. The system would respond with yes/no along with a confidence score. Some useful hints can also be given if needed. The proposed framework provides a pioneering example of on-demand knowledge validation in dialog environment to address"
I17-3015,C16-1138,0,0.0144648,"bject belongs. The question can be: “Is it animal?” or “Can it fly?”, etc. The game terminates when the correct object is guessed by the questioners. Guess What is a kind of Chinese-based Twenty Questions game, where the system serves as the answerer and users as questioners. The answer set of the system currently contains 200 terms, which are general concepts such as dog, cat, boat, computer, etc. Figure 2 shows a running example of Guess What system. The framework involves different research topics, such as question answering (Berant et al., 2013; Kwok et al., 2001) and relation prediction (Xu et al., 2016). The techniques include understanding the questions and identifying whether the object fits the description of the users’ questions. Since most descriptions are based on the existence of a relationship between two entities, such as “Is it an animal?” or “Can it fly?”, the latter mission turns out to be identifying whether a certain relationship between entities holds or not, which is a kind of on-demand knowledge validation. Guess What goes through the following procedures: Parsing the user’s question, followed by extracting knowledge and reasoning from metadata of Wikipedia2 and a lexical se"
I17-4014,D14-1162,0,0.0950517,"e problem should be concatenated to these embeddings. Due to the sparsity of labeled Chinese phrases and words for sentiment analysis, we use unsupervised word embeddings without further tuning. The corpus on which we compute the distributional semantics of Chinese words comes from both the Chinese Gigaword corpus (Graff and Chen, 2003) and the Sinica Corpus (Chen et al., 1996). The former contains over 735 million Chinese characters from the Central News Agency of Taiwan, and the latter contains over 17 million Chinese characters from documents of balanced topics. We use the GloVe algorithm (Pennington et al., 2014) to obtain 300-dimensional word embeddings from a union of these corpora. The resulting 517,015 embeddings cover all words in the training phrases. To combine word embeddings of phrases to phrase embeddings, we notice the sparsity of available phrase and hence take a simple approach. Observing all given phrases are a compound of one to two modifier words and one head word, we append the word embeddings of the modifier words of a phrase to the word embedding of its head word. With zero paddings to the phrases with only one modifier word, 900-dimensional phrase embeddings are acquired for all ph"
I17-4014,N16-1066,0,0.0176016,"for DSAP. arousal, which represents the degree of excitement. The values of both dimensions are limited to a closed interval between 1 and 9, where 1 represents most negative for valence and calmest for arousal. The DSAP shared task calls for systems that automatically predict VA for Chinese phrases to overcome the scarcity of labeled Chinese phrases and words. Lists of words for different types of modifiers are provided. This includes negation words like 不 and 沒有, modal words like 本來 and 應該, and degree words like 有點 and 更加. In addition, some head words with VA annotations are also provided (Yu et al., 2016). Finally, a training data of VA-annotated phrases with their modifier types, e.g. (deg neg, 稍微不小心), are provided. Table 1 shows the statistics of the training data. Introduction Sentiment analysis can be a useful tool in understanding public opinions for items of various subjects, such as movies, hotels, and political figures, from unstructured texts. The problem is often defined in two different ways: one that assigns texts to discrete categories, and the other that seeks to get every sample a real value for each dimension (Calvo and Mac Kim, 2013). For the Dimensional Sentiment Analysis for"
L18-1132,N09-1003,0,0.17328,"Missing"
L18-1132,P12-1015,0,0.0689618,"edia title embeddings are also verified and analyzed in detail. Keywords: word embedding, word embedding benchmark, Wikipedia title embedding, Wikipedia category, knowledge base 1. Introduction Word embeddings are widely used in various natural language processing (NLP) tasks. Researches evaluates word embeddings on some extrinsic, practical NLP tasks, and also some intrinsic tasks. The two most popular kinds of intrinsic tasks are word similarity and analogical reasoning, and there are an array of benchmarks for the two kind of tasks in English. Take word similarity for instance, the MEN-3k (Bruni et al., 2012) dataset consists of 3,000 word pairs. The MTurk-287 (Radinsky et al., 2011) dataset consists of 287 word pairs. The SimLex-999 (Hill et al., 2016) consists of 999 word pairs. For analogical reasoning tasks, the dataset is composed of analogous word pairs. Each word pair is a tuple of word relations that follow a common syntactic relation. For instance, the Google analogy (Mikolov et al., 2013) dataset consists of 19,544 question pairs. Though there are lots of benchmarks to evaluate word embeddings in English, there are barely evaluation sets with large enough amount of data for Chinese word"
L18-1132,I05-7001,0,0.549598,"considerations, the size of Google analogy dataset is reduced from 19544 words to 11126 words, shown in Table 1. Even though we follow the translation policy, we still encounter some obstacles while translating the datasets. The difficulties will be elaborated in Section 2.2.. 2.2. 2.1. Translation Process In this section, we will illustrate the translation process and policy we apply. First, to get appropriate translated Chinese words, we use authoritative online dictionaries as resources, including Cambridge Dictionary (EnglishChinese), E-Hownet (Group and others, 2009; Huang et al., 2008; Chen et al., 2005), Merriam-Webster Dictionary, Oxford Dictionary, and Wiktionary3 . To maintain the original semantic meaning in English, we disambiguate the appropriate Chinese words by using Cambridge Dictionary (English-Chinese) and E-Hownet. Some words in English have different meaning in different situations. Take some word pairs from MEN-3k dataset, which consists of word pairs with similarity score in a 50 scale, for example, (palm, tree) 37.0 and (hand, palm) 44.0 both have relatively high similarity scores. According to the dictionaries, palm can either mean “手 掌(the under part of the hand between the"
L18-1132,O07-4005,0,0.0348933,"ipedia dump file in September 2016, which compromised 1,243,319 articles at a time. There are 244,430,247 words in the Chinese Wikipedia corpus and each title has 2.16 categories in average. Chen and Ma (2017) obtained Wikipedia title embedding by linear combining context embedding and categories embedding. They generated context embedding by skipgram model and proposed several methods to generate categories embedding. In this paper, we use the same context embedding but purpose improved method to acquire categories embedding. We use CKIP Chinese parser (Hsieh et al., 2012; Yang et al., 2008; Hsieh et al., 2007) to parse each category and get the rigorously parsed noun-phrase (NP) headword. 3.2. Categories Embedding Wikipedia categories can partially represent the corresponding title. Chen and Ma (2017) proposed several approaches of acquiring Wikipedia categories embedding, categories embedding for short. In this section, we first briefly introduce (Chen and Ma 2017)’s work, followed by description of our extension on the new approach to extract the headword of a category, elaborated in Section 3.3.2.. Formally, given a title t and its corresponding categories from C 1 to C n , each category C i has"
L18-1132,W12-6338,0,0.0988176,"and download the Chinese version of Wikipedia dump file in September 2016, which compromised 1,243,319 articles at a time. There are 244,430,247 words in the Chinese Wikipedia corpus and each title has 2.16 categories in average. Chen and Ma (2017) obtained Wikipedia title embedding by linear combining context embedding and categories embedding. They generated context embedding by skipgram model and proposed several methods to generate categories embedding. In this paper, we use the same context embedding but purpose improved method to acquire categories embedding. We use CKIP Chinese parser (Hsieh et al., 2012; Yang et al., 2008; Hsieh et al., 2007) to parse each category and get the rigorously parsed noun-phrase (NP) headword. 3.2. Categories Embedding Wikipedia categories can partially represent the corresponding title. Chen and Ma (2017) proposed several approaches of acquiring Wikipedia categories embedding, categories embedding for short. In this section, we first briefly introduce (Chen and Ma 2017)’s work, followed by description of our extension on the new approach to extract the headword of a category, elaborated in Section 3.3.2.. Formally, given a title t and its corresponding categories"
L18-1132,I08-2098,0,0.0923696,"nese version of Wikipedia dump file in September 2016, which compromised 1,243,319 articles at a time. There are 244,430,247 words in the Chinese Wikipedia corpus and each title has 2.16 categories in average. Chen and Ma (2017) obtained Wikipedia title embedding by linear combining context embedding and categories embedding. They generated context embedding by skipgram model and proposed several methods to generate categories embedding. In this paper, we use the same context embedding but purpose improved method to acquire categories embedding. We use CKIP Chinese parser (Hsieh et al., 2012; Yang et al., 2008; Hsieh et al., 2007) to parse each category and get the rigorously parsed noun-phrase (NP) headword. 3.2. Categories Embedding Wikipedia categories can partially represent the corresponding title. Chen and Ma (2017) proposed several approaches of acquiring Wikipedia categories embedding, categories embedding for short. In this section, we first briefly introduce (Chen and Ma 2017)’s work, followed by description of our extension on the new approach to extract the headword of a category, elaborated in Section 3.3.2.. Formally, given a title t and its corresponding categories from C 1 to C n ,"
L18-1724,C10-3012,0,0.0306116,"ical entries in E-HowNet 2.0. 4. On-line Systems of E-HowNet 2.0 The current E-HowNet ontology shown on the web is the result of automatic constructed by a computer program according to the pre-defined hierarchical structure of primitive and basic concepts as well as E-HowNet expressions, which contain more than 88,000 lexical senses. Based on this system, the E-HowNet Relation Database is also constructed to provide a new direction of clustering concepts. 4.1 Automatic Ontology Reconstruction To construct a complete lexical taxonomy, we use a strategy that categorizes concepts automatically (Chen et al, 2010). Step 1. Attach lexical senses. Words and associated sense expressions are first attached to the top level ontology nodes according to their head concepts. For instance, the head concept of the expression ‘{choose |選 擇:manner={cautious|慎}}’ is ‘choose|選擇’. Step 2. Sub-categorization by attribute-values. Lexical concepts with the same semantic head are further subcategorized according to their attribute values. Lexicons that have the same attribute values share specific characteristics; therefore further sub-categorization is performed based on the distinct attribute-values of the lexicons. St"
L18-1724,Y07-1013,0,0.112642,"Missing"
ma-huang-2006-uniform,W02-1811,0,\N,Missing
ma-huang-2006-uniform,C02-1145,0,\N,Missing
ma-huang-2006-uniform,W00-1205,1,\N,Missing
ma-huang-2006-uniform,O92-1003,0,\N,Missing
N13-1045,J99-2004,0,0.668943,"he other is translation selection or reranking (Hildebrand and Vogel 2008; Callison-Burch et al., 2012), where candidate translations are generated by different decoding processes or different decoders. This paper belongs to the latter; the goal is to identify ungrammatical hypotheses from given candidate translations using grammatical knowledge in the target language that expresses syntactic dependencies between words. To achieve that, we propose a novel Structured Language Model (SLM) - Supertagged Dependency Language Model (SDLM) to model the syntactic dependencies between words. Supertag (Bangalore and Joshi, 1999) is an elementary syntactic structure based on Lexicalized Tree Adjoining Grammar (LTAG). Traditional supertagged n-gram LM predicts the next supertag based on the immediate words to the left with supertags, so it can not explicitly model long-distance dependency relations. In contrast, SDLM predicts the next supertag using the words with supertags on which it syntactically depend, and these words could be anywhere and arbitrarily far apart in a sentence. A candidate translation’s grammatical degree or “fluency” can be measured by simply calculating the SDLM likelihood of the supertagged depen"
N13-1045,2000.iwpt-1.9,0,0.0405998,"ity to extend to the remaining directions in the dependency list. 435 In contrast to the LTAG parsing and supertagging-based approaches, we propose an alternative mechanism: first we use a state-of-theart constituent parser to obtain the parse of a sentence, and then we extract elementary trees with dependencies from the parse to assign each word with an elementary tree. The second step is similar to the approach used in extracting elementary trees from the TreeBank (Xia, 1999; Chen and VijayShanker, 2000). 4.1 Elementary Tree Extraction We use an elementary tree extractor, a modification of (Chen and Vijay-Shanker, 2000), to serve our purpose. Heuristic rules were used to distinguish arguments from adjuncts, and the extraction process can be regarded as a process that gradually decomposes a constituent parse to multiple elementary trees and records substitutions and adjunctions. From elementary trees, we can obtain supertags by only considering syntactic structure and ignoring anchor words. Take the sentence – “The hungry boys ate dinner” as an example; the constituent parse and extracted supertags are shown in Figure 1. In Figure 1 (b), dotted lines represent the operations of substitution and adjunction. No"
N13-1045,D09-1076,0,0.0142569,"ns in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination. 1 Introduction In recent years, there has been a burgeoning interest in incorporating syntactic structure into Statistical machine translation (SMT) models (e..g, Galley et al., 2006; DeNeefe and Knight 2009; Quirk et al., 2005). In addition to modeling syntactic structure in the decoding process, a methodology for candidate translation selection has also emerged. This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best reranking (Och et al., 20"
N13-1045,P08-1009,0,0.0327969,"Missing"
N13-1045,E06-1005,0,0.0386225,"at integrates supertags into the target side of the translation model and the target n-gram LM. Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion n"
N13-1045,W11-2121,0,0.25327,"Missing"
N13-1045,C94-1024,0,\N,Missing
N13-1045,W12-3102,0,\N,Missing
N13-1045,P08-2021,0,\N,Missing
N13-1045,P07-1040,0,\N,Missing
N13-1045,C88-2121,0,\N,Missing
N13-1045,W12-3112,0,\N,Missing
N13-1045,P05-1034,0,\N,Missing
N13-1045,P06-1121,0,\N,Missing
N13-1045,2008.amta-srw.3,0,\N,Missing
N13-1045,P07-1037,0,\N,Missing
N13-1045,N04-1021,0,\N,Missing
N13-1045,W06-2606,0,\N,Missing
N19-4015,P82-1020,0,0.782451,"Missing"
N19-4015,D15-1221,0,0.0577385,"Missing"
N19-4015,N18-1015,0,0.199153,"Missing"
O06-1002,O03-1010,0,0.055697,"Missing"
O06-1002,Y03-1016,0,0.0450925,"Missing"
O06-1002,O04-2005,0,0.0615608,"Missing"
O12-1014,O06-1023,0,0.0201501,"propagate the failure information to relevant words. We call the modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through loo"
O12-1014,O10-2001,0,0.0283016,"pagate the failure information to relevant words. We call the modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through look-u"
O12-1014,stymne-ahrenberg-2010-using,0,0.0243733,"to relevant words. We call the modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through look-up in the XTAG grammar, and fina"
O12-1014,J93-4006,0,0.295024,"modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through look-up in the XTAG grammar, and finally reconstruct the original pa"
O12-1014,C88-2147,0,\N,Missing
O12-1014,P03-1054,0,\N,Missing
O12-1014,C88-2121,0,\N,Missing
O12-5001,2000.iwpt-1.9,0,0.133896,"Missing"
O12-5001,O10-2001,0,0.0639591,"Missing"
O12-5001,J93-4006,0,0.352581,"Missing"
O12-5001,P03-1054,0,0.0120037,"Missing"
O12-5001,2011.mtsummit-papers.62,1,0.832835,"Missing"
O12-5001,C88-2121,0,0.280262,"Missing"
O12-5001,stymne-ahrenberg-2010-using,0,0.056021,"Missing"
O12-5001,O06-1023,0,0.0543074,"Missing"
O12-5001,C88-2147,0,\N,Missing
O15-1006,D14-1058,0,0.0383711,"Missing"
O15-1006,P14-1026,0,0.0420504,"Missing"
O15-1006,Q13-1005,0,0.0382782,"Missing"
O15-1006,strassel-etal-2010-darpa,0,\N,Missing
O15-1006,Q15-1001,0,\N,Missing
O15-1006,D09-1082,0,\N,Missing
O15-1006,A83-1031,0,\N,Missing
O15-1006,D09-1122,0,\N,Missing
O15-1006,D08-1097,0,\N,Missing
O15-1006,W02-1811,0,\N,Missing
O15-1006,C02-1049,1,\N,Missing
O15-1006,O07-4005,0,\N,Missing
O15-1006,W14-2401,0,\N,Missing
O15-1006,J14-1002,0,\N,Missing
O15-1006,P01-1052,0,\N,Missing
O15-1006,J80-2001,0,\N,Missing
O15-1006,O04-2005,0,\N,Missing
O15-1006,D14-1100,0,\N,Missing
O15-1006,D13-1160,0,\N,Missing
O15-1006,O14-3003,0,\N,Missing
O15-1006,O15-3002,1,\N,Missing
O15-1006,O15-1007,1,\N,Missing
O15-1006,I05-7001,0,\N,Missing
O15-3001,Q13-1005,0,0.0366724,"Missing"
O15-3001,J80-2001,0,0.282248,"Missing"
O15-3001,I05-7001,0,0.0671535,"Missing"
O15-3001,C02-1049,1,0.531032,"Missing"
O15-3001,J14-1002,0,0.0592436,"Missing"
O15-3001,D09-1122,0,0.0349179,"Missing"
O15-3001,D14-1058,0,0.0703962,"Missing"
O15-3001,D14-1100,0,0.0359364,"Missing"
O15-3001,O07-4005,0,0.0225363,"Missing"
O15-3001,O15-3002,1,0.871096,"Missing"
O15-3001,P14-1026,0,0.0768712,"Missing"
O15-3001,W03-1726,1,0.635445,"Missing"
O15-3001,P01-1052,0,0.14657,"Missing"
O15-3001,strassel-etal-2010-darpa,0,0.425543,"Missing"
O15-3001,O04-2005,0,0.102625,"Missing"
O15-3001,W02-1811,0,0.0479075,"Missing"
O15-3001,D09-1082,0,0.0271275,"Missing"
O15-3001,Q15-1001,0,\N,Missing
O15-3001,A83-1031,0,\N,Missing
O15-3001,D08-1097,0,\N,Missing
O15-3001,W14-2401,0,\N,Missing
O15-3001,D13-1160,0,\N,Missing
O15-3001,O14-3003,0,\N,Missing
O15-3001,O15-1007,1,\N,Missing
O16-1010,Q13-1012,0,0.0488759,"Missing"
O16-1010,D14-1100,1,0.890841,"Missing"
O16-1010,W15-1501,0,0.0438441,"Missing"
O16-1010,P14-2050,0,0.0463358,"Missing"
O16-1010,I05-1016,1,\N,Missing
O16-1010,N10-1095,0,\N,Missing
O16-1010,W04-1116,0,\N,Missing
O16-1010,W03-1706,0,\N,Missing
O16-1010,W03-1717,0,\N,Missing
O16-1010,W03-1012,0,\N,Missing
O16-1010,C08-1132,0,\N,Missing
O16-1010,P03-1054,0,\N,Missing
O16-1010,P06-1054,0,\N,Missing
O16-1010,O07-4005,1,\N,Missing
O16-1010,P05-1022,0,\N,Missing
O16-1010,W12-6338,1,\N,Missing
O16-1010,P15-1112,0,\N,Missing
O16-1010,P10-1040,0,\N,Missing
O16-1010,W13-3202,0,\N,Missing
O16-1010,D14-1082,0,\N,Missing
O16-1010,W12-6335,0,\N,Missing
O16-1010,P14-2131,0,\N,Missing
O16-1010,O04-2005,0,\N,Missing
O16-1031,D14-1058,0,0.0450783,"Missing"
O16-1031,O15-3001,1,0.892213,"Missing"
O16-1031,N16-3014,1,0.881816,"Missing"
O16-1031,P14-1026,0,0.0651902,"Missing"
O16-1031,Q15-1001,0,0.0271679,"Missing"
O16-1031,D15-1202,0,0.0211141,"Missing"
O16-1031,2007.mtsummit-papers.71,0,0.152827,"Missing"
O16-1031,J05-1004,0,0.0759298,"Missing"
O16-1031,J07-3004,0,0.123106,"Missing"
O16-1031,W07-1522,0,0.0869205,"Missing"
O16-1031,D07-1078,0,0.025265,"Missing"
O16-1031,N10-1016,0,0.0233187,"Missing"
O16-1031,burchardt-etal-2006-salsa,0,0.110692,"Missing"
O16-1031,basile-etal-2012-developing,0,0.0541651,"Missing"
O16-1031,mille-wanner-2010-syntactic,0,0.0530564,"Missing"
O16-1031,D15-1135,0,0.0352356,"Missing"
O16-1031,Q15-1042,0,0.0229697,"Missing"
O16-1031,W00-1205,0,0.244554,"Missing"
O16-1031,I05-7001,0,0.0691437,"Missing"
O16-1031,O14-3003,1,0.845921,"Missing"
O16-1031,O09-3002,0,0.105151,"Missing"
O16-1031,N16-1136,0,0.0215744,"Missing"
O16-1031,P16-1084,0,0.0251462,"Missing"
O16-3002,P05-1022,0,0.481483,"Missing"
O16-3002,D14-1082,0,0.241463,"Missing"
O16-3002,Q13-1012,0,0.28572,"Missing"
O16-3002,W12-6338,1,0.854926,"Missing"
O16-3002,D14-1100,1,0.757959,"Missing"
O16-3002,O07-4005,1,0.591713,"Missing"
O16-3002,N10-1095,0,0.577933,"Missing"
O16-3002,P03-1054,0,0.039522,"Missing"
O16-3002,W13-3202,0,0.296274,"Missing"
O16-3002,P14-2050,0,0.0766705,"Missing"
O16-3002,W15-1501,0,0.115033,"Missing"
O16-3002,O16-3002,1,0.0513221,"Missing"
O16-3002,W03-1012,0,0.844752,"Missing"
O16-3002,D13-1170,0,0.0289834,"Missing"
O16-3002,W03-1706,0,0.328703,"Missing"
O16-3002,W12-6335,0,0.46829,"Missing"
O16-3002,P10-1040,0,0.195874,"Missing"
O16-3002,P06-1054,0,0.779828,"Missing"
O16-3002,W03-1717,0,0.402441,"Missing"
O16-3002,W04-1116,0,0.483184,"Missing"
O16-3002,C08-1132,0,0.600703,"Missing"
O16-3002,P15-1112,0,0.35817,"Missing"
O16-3002,I05-1016,1,\N,Missing
O16-3002,P14-2131,0,\N,Missing
O16-3002,O04-2005,0,\N,Missing
P09-2084,P01-1067,0,0.202293,"Missing"
P09-2084,C08-1145,0,\N,Missing
P09-2084,W09-0410,0,\N,Missing
P09-2084,D07-1077,0,\N,Missing
P09-2084,2007.mtsummit-papers.11,0,\N,Missing
P19-1136,P11-1056,0,0.521592,"structured text. Though important and well-studied, three key aspects are yet to be fully handled in an unified framework: • End-to-end joint modeling of entity recognition and relation extraction; • Prediction of overlapping relations, i.e., relations that share a common mention; • Consideration of the interaction between relations, especially overlapping relations. Traditionally, a pipelined approach is used to first extract entity mentions using a named entity recognizer and then predict the relations between every pair of extracted entity mentions (Zelenko et al., 2003; Zhou et al., 2005; Chan and Roth, 2011). Joint entity recognition and relation extraction models (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mention"
P19-1136,P17-1017,0,0.175575,"Missing"
P19-1136,D15-1162,0,0.0632516,"Missing"
P19-1136,D15-1278,0,0.0199947,"l., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency paths between two entity mentions (Yan et al., 2015; Li et al., 2015), or the minimal constituency sub-tree spanning two entity mentions (Socher et al., 2012) to encode relevant information for each pair of entity mentions. However, these methods are not end-to-end joint modeling of entities and relations. They assume entity mentions are given and are expected to degrade significantly in performance when a named entity recognizer is needed in the pipeline for real world usage. Another challenge for relation extraction is how to take into account the interaction between relations, which is especially important for overlapping relations, i.e., relations sharing c"
P19-1136,N19-1308,0,0.0644586,"een used in many natural language processing (NLP) tasks. Marcheggiani and Titov (2017) applies GCN on word sequences for semantic role labeling. Liu et al. (2018) encode long documents via GCN to perform text matching. Cetoli et al. (2016) combine RNN and GCN to recognize named entities. There are also some works (Peng et al., 2017; Zhang et al., 2018; Qian et al., 1410 graphs for each relation, to which we apply GCN on each graph to integrate each relation’s information and further consider the interaction between entities and relations. 4.1 Figure 1: Graph Convolutional Network (GCN) 2019; Luan et al., 2019) about considering dependency structure of word sequence for relation extraction. In our proposed GrpahRel, we not only stack Bi-LSTM and GCN to consider both linear and dependency structures but also adopt a 2ndphase relation-weighted GCN to further model the interaction between entities and relations. 3 Review of GCN As convolutional neural network (CNN), Graph Convolutional Network (GCN) (Kipf and Welling, 2017) convolves the features of neighboring nodes and also propagates the information of a node to its nearest neighbors. Shown in Fig. 1, by stacking GCN layers, GCN can extract regional"
P19-1136,D17-1159,0,0.511522,"ting a relation and copying two words from the sentence. The seq2seq setup partially handles interaction between triplets. However, interactions between relations are only unidirectionally captured by considering previous generated triplets with a compulsory linear order when generating a new one. Instead, in this paper, we propose propagating entity and relation information on a word graph with automatically learned linkage by applying 2nd-phase GCN on top of the LSTM-GCN encoder. Recently, considering dependency structure by GCN has been used in many natural language processing (NLP) tasks. Marcheggiani and Titov (2017) applies GCN on word sequences for semantic role labeling. Liu et al. (2018) encode long documents via GCN to perform text matching. Cetoli et al. (2016) combine RNN and GCN to recognize named entities. There are also some works (Peng et al., 2017; Zhang et al., 2018; Qian et al., 1410 graphs for each relation, to which we apply GCN on each graph to integrate each relation’s information and further consider the interaction between entities and relations. 4.1 Figure 1: Graph Convolutional Network (GCN) 2019; Luan et al., 2019) about considering dependency structure of word sequence for relation"
P19-1136,P16-1105,0,0.163027,"ll word pairs of the text, are considered by our method; • We perform end-to-end, joint modeling of entities and relations while considering all word pairs for prediction; • The interaction between entities and relations is carefully considered. We evaluate the method on two public relation extraction datasets: NYT and WebNLG. The experimental result shows that GraphRel substantially improves the overlapping relations over previous work, and achieves a new state-of-the-art on both datasets. 2 Related Work The BiLSTM-GCN encoder part of our model resembles the BiLSTM-TreeLSTM model proposed by Miwa and Bansal (2016), as they also stack a dependency tree on top of sequences to jointly model entities and relations. They use Bi-LSTM on each sentence for automatic feature learning, and the extracted hidden features are shared by a sequential entity tagger and a shortest dependency path relation classifier. However, while introducing shared parameters for joint entity recognition and relation extraction, they must still pipeline the entity mentions predicted by the tagger to form mention pairs for the relation classifier. Instead of trying to classify each mention pair as in previous work, Zheng et al. (2017)"
P19-1136,D14-1200,0,0.375495,"amework: • End-to-end joint modeling of entity recognition and relation extraction; • Prediction of overlapping relations, i.e., relations that share a common mention; • Consideration of the interaction between relations, especially overlapping relations. Traditionally, a pipelined approach is used to first extract entity mentions using a named entity recognizer and then predict the relations between every pair of extracted entity mentions (Zelenko et al., 2003; Zhou et al., 2005; Chan and Roth, 2011). Joint entity recognition and relation extraction models (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency paths between two entity mentions (Yan et a"
P19-1136,Q17-1008,0,0.0203955,"linear order when generating a new one. Instead, in this paper, we propose propagating entity and relation information on a word graph with automatically learned linkage by applying 2nd-phase GCN on top of the LSTM-GCN encoder. Recently, considering dependency structure by GCN has been used in many natural language processing (NLP) tasks. Marcheggiani and Titov (2017) applies GCN on word sequences for semantic role labeling. Liu et al. (2018) encode long documents via GCN to perform text matching. Cetoli et al. (2016) combine RNN and GCN to recognize named entities. There are also some works (Peng et al., 2017; Zhang et al., 2018; Qian et al., 1410 graphs for each relation, to which we apply GCN on each graph to integrate each relation’s information and further consider the interaction between entities and relations. 4.1 Figure 1: Graph Convolutional Network (GCN) 2019; Luan et al., 2019) about considering dependency structure of word sequence for relation extraction. In our proposed GrpahRel, we not only stack Bi-LSTM and GCN to consider both linear and dependency structures but also adopt a 2ndphase relation-weighted GCN to further model the interaction between entities and relations. 3 Review of"
P19-1136,D14-1162,0,0.0865206,"l GCN to further extract regional dependency features. Then, based on the extracted word features, we predict the relation for each word pair along with the word entity. 4.1.1 Bi-LSTM We use the well-known long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) as our bi-RNN cell. For each word, we combine the word embedding and part-of-speech (POS) embedding as the initial feature: h0u = W ord(u) ⊕ P OS(u), where h0u represents the initial feature of word u, and W ord(u) and P OS(u) are the word and POS embedding of word u, respectively. We use pretrained word embeddings from GloVe (Pennington et al., 2014), and the POS embedding is randomly initialized for training with the whole GraphRel. 4.1.2 Bi-GCN Since the original input sentence is a sequence and has no inherent graph structure to speak of, as Cetoli et al. (2016), we use dependency parser to create a dependency tree for the input sentence. We use the dependency tree as the input sentence’s adjacency matrix and use GCN to extract regional dependency features. The original GCN was designed for undirected graphs. To consider both incoming and outgoing word features, we follow Marcheggiani and Titov (2017) and implement bi-GCN as     →"
P19-1136,N19-1082,0,0.0959754,"Missing"
P19-1136,P15-1061,0,0.014353,"and relation extraction models (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency paths between two entity mentions (Yan et al., 2015; Li et al., 2015), or the minimal constituency sub-tree spanning two entity mentions (Socher et al., 2012) to encode relevant information for each pair of entity mentions. However, these methods are not end-to-end joint modeling of entities and relations. They assume entity mentions are given and are expected to degrade significantly in performance when a named entity recognizer is needed in the pipeline for real world usage. Another challenge for relation extraction is how to take into account the interaction betwee"
P19-1136,D12-1110,0,0.0436564,"o tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency paths between two entity mentions (Yan et al., 2015; Li et al., 2015), or the minimal constituency sub-tree spanning two entity mentions (Socher et al., 2012) to encode relevant information for each pair of entity mentions. However, these methods are not end-to-end joint modeling of entities and relations. They assume entity mentions are given and are expected to degrade significantly in performance when a named entity recognizer is needed in the pipeline for real world usage. Another challenge for relation extraction is how to take into account the interaction between relations, which is especially important for overlapping relations, i.e., relations sharing common entity mentions. For example, (BarackObama, PresidentOf, UnitedStates) can be infer"
P19-1136,D15-1206,0,0.0453721,"ki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency paths between two entity mentions (Yan et al., 2015; Li et al., 2015), or the minimal constituency sub-tree spanning two entity mentions (Socher et al., 2012) to encode relevant information for each pair of entity mentions. However, these methods are not end-to-end joint modeling of entities and relations. They assume entity mentions are given and are expected to degrade significantly in performance when a named entity recognizer is needed in the pipeline for real world usage. Another challenge for relation extraction is how to take into account the interaction between relations, which is especially important for overlapping relations, i.e., r"
P19-1136,P14-1038,0,0.194564,"in an unified framework: • End-to-end joint modeling of entity recognition and relation extraction; • Prediction of overlapping relations, i.e., relations that share a common mention; • Consideration of the interaction between relations, especially overlapping relations. Traditionally, a pipelined approach is used to first extract entity mentions using a named entity recognizer and then predict the relations between every pair of extracted entity mentions (Zelenko et al., 2003; Zhou et al., 2005; Chan and Roth, 2011). Joint entity recognition and relation extraction models (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency paths between two en"
P19-1136,C10-2160,0,0.181361,"o be fully handled in an unified framework: • End-to-end joint modeling of entity recognition and relation extraction; • Prediction of overlapping relations, i.e., relations that share a common mention; • Consideration of the interaction between relations, especially overlapping relations. Traditionally, a pipelined approach is used to first extract entity mentions using a named entity recognizer and then predict the relations between every pair of extracted entity mentions (Zelenko et al., 2003; Zhou et al., 2005; Chan and Roth, 2011). Joint entity recognition and relation extraction models (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency pat"
P19-1136,C14-1220,0,0.075989,"oint entity recognition and relation extraction models (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence between two entity mentions (Zeng et al., 2014; dos Santos et al., 2015), the shortest dependency paths between two entity mentions (Yan et al., 2015; Li et al., 2015), or the minimal constituency sub-tree spanning two entity mentions (Socher et al., 2012) to encode relevant information for each pair of entity mentions. However, these methods are not end-to-end joint modeling of entities and relations. They assume entity mentions are given and are expected to degrade significantly in performance when a named entity recognizer is needed in the pipeline for real world usage. Another challenge for relation extraction is how to take into acco"
P19-1136,P18-1047,0,0.657491,"ion pair as in previous work, Zheng et al. (2017) formulate relation extraction as a sequential tagging problem (NovelTagging) as with entity recognition. This allows them to model relation extraction by an LSTM decoder on top of a Bi-LSTM encoder. However, while showing promising results on the NYT dataset, their strength comes from focusing on isolated relations and completely giving up overlapping relations, which are relatively rare in the dataset. In comparison, the proposed GraphRel gracefully handles all types of relations while being end-to-end and jointly modeling entity recognition. Zeng et al. (2018) then propose an end-to-end sequence-to-sequence model for relation extraction. They encode each sentence by a Bi-LSTM, and use the last encoder hidden state to initialize one (OneDecoder) or multiple (MultiDecoder) LSTMs for dynamic decoding relation triplets. When decoding, triplets are generated by selecting a relation and copying two words from the sentence. The seq2seq setup partially handles interaction between triplets. However, interactions between relations are only unidirectionally captured by considering previous generated triplets with a compulsory linear order when generating a ne"
P19-1136,D18-1244,0,0.0321604,"enerating a new one. Instead, in this paper, we propose propagating entity and relation information on a word graph with automatically learned linkage by applying 2nd-phase GCN on top of the LSTM-GCN encoder. Recently, considering dependency structure by GCN has been used in many natural language processing (NLP) tasks. Marcheggiani and Titov (2017) applies GCN on word sequences for semantic role labeling. Liu et al. (2018) encode long documents via GCN to perform text matching. Cetoli et al. (2016) combine RNN and GCN to recognize named entities. There are also some works (Peng et al., 2017; Zhang et al., 2018; Qian et al., 1410 graphs for each relation, to which we apply GCN on each graph to integrate each relation’s information and further consider the interaction between entities and relations. 4.1 Figure 1: Graph Convolutional Network (GCN) 2019; Luan et al., 2019) about considering dependency structure of word sequence for relation extraction. In our proposed GrpahRel, we not only stack Bi-LSTM and GCN to consider both linear and dependency structures but also adopt a 2ndphase relation-weighted GCN to further model the interaction between entities and relations. 3 Review of GCN As convolutiona"
P19-1136,P17-1113,0,0.678009,"LiveIn, WhiteHouse) and (WhiteHouse, PresidentialPalace, UnitedStates), where the latter two are said to exhibit SingleEntityOverlap. Although common in knowledge base completion, such interaction, whether via direct deduction or indirect 1409 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1409–1418 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics evidence, is particularly difficult for joint entity recognition and relation extraction models, where entities are not present in the input. Indeed, although Zheng et al. (2017) propose a strong neural end-to-end joint model of entities and relations based on an LSTM sequence tagger, they have to completely give up overlapping relations. In this paper, we propose GraphRel, a neural end-to-end joint model for entity recognition and relation extraction that is the first to handle all three key aspects in relation extraction. GraphRel learns to automatically extract hidden features for each word by stacking a Bi-LSTM sentence encoder and a GCN (Kipf and Welling, 2017) dependency tree encoder. Then GraphRel tags entity mention words and predicts relation triplets that co"
P19-1136,P05-1053,0,0.258644,"onstruction from unstructured text. Though important and well-studied, three key aspects are yet to be fully handled in an unified framework: • End-to-end joint modeling of entity recognition and relation extraction; • Prediction of overlapping relations, i.e., relations that share a common mention; • Consideration of the interaction between relations, especially overlapping relations. Traditionally, a pipelined approach is used to first extract entity mentions using a named entity recognizer and then predict the relations between every pair of extracted entity mentions (Zelenko et al., 2003; Zhou et al., 2005; Chan and Roth, 2011). Joint entity recognition and relation extraction models (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Ren et al., 2017) have been built to take advantage of the close interaction between these two tasks. While showing the benefits of joint modeling, these complicated methods are feature-based structured learning systems and hence rely heavily on feature engineering. With the success of deep neural networks, NNbased automatic feature learning methods have been applied to relation extraction. These methods use CNN, LSTM, or Tree-LSTM on the word sequence betw"
W03-1705,P98-2206,0,0.108378,"Missing"
W03-1705,P97-1041,0,0.0806142,"Missing"
W03-1705,O92-1003,0,0.102765,"Missing"
W03-1705,O97-4005,0,0.360836,"Missing"
W03-1705,C02-1049,1,0.834408,"Missing"
W03-1705,O98-3002,1,0.926275,"Missing"
W03-1705,C00-1027,0,0.0346846,"Missing"
W03-1705,C92-1019,1,0.879861,"Missing"
W03-1705,O97-4003,1,0.882541,"Missing"
W03-1705,J93-1007,0,0.0856207,"Missing"
W03-1705,J96-1001,0,\N,Missing
W03-1705,J90-1003,0,\N,Missing
W03-1705,C96-2184,1,\N,Missing
W03-1705,C98-2201,0,\N,Missing
W03-1705,O93-1004,0,\N,Missing
W03-1726,C92-1019,1,0.5056,"Missing"
W03-1726,O98-3002,1,0.84534,"Missing"
W03-1726,C02-1049,1,0.845325,"Missing"
W03-1726,W02-1811,1,0.806482,"Missing"
W03-1726,W03-1705,1,0.798817,"Missing"
W03-1726,O03-4001,0,\N,Missing
W03-1726,O97-4003,1,\N,Missing
Y06-1027,C90-2010,1,0.576118,"Missing"
Y06-1027,P89-1010,0,0.388147,"Missing"
Y06-1027,I05-3007,1,0.853573,"ations and only one pattern for the simplest verb-object relation as shown as (2) (2) Collocating Pattern for Object from CSE I 1:""V[BCJ]"" ""Di""? ""N[abc]""? ""DE""? ""N[abc]""? 2: ""Na"" [tag!= ""Na""] (""XXX"" represents XXX is a regular expression, ""XXX""? represents XXX appears zero or one time, ""XXX""{a,b} represents XXX appears a~b times.) In (2), the 1: and 2: identify the two collocated components. Between the components, zero or one particle may appear (denoted by ""Di""?), zero or one processor may appears (denoted by any_noun? ""DE""?), and zero or one noun-modifier may appears (denoted by ""N[abc]""?) Huang et al. (2005) pointed out that the prototype version of CSE I did not deal with the prevalent non-canonical word orders in Chinese (3). In addition, we also noticed that it fails to identify grammatical relations when an argument lies some distance away from a verb because of internal modification (4). Chinese objects often occur in pre-verbal positions in various pre-posing constructions, such as topicalization. (3) a. 全穀麵包，吃了很健康。 quan.gu mian.bao, chi le hen jian.kang whole-grain bread, eat LE very healthy ‘Eating whole-grain bread is very healthy.’ b. 有人嘗試要將這荷花分類，卻越分越累。 you ren chang.shi yao jiang zhe h"
Y06-1027,P98-2127,0,0.0421635,"Missing"
Y06-1027,ma-huang-2006-uniform,1,0.7926,"OURCE[PP{自、於}] THEME< SOURCE[PP{歸、為}]<VI (A<B represents B appears behind A. A<<B represents B appears immediately behind A) 3.3 Implementation: Preparation of Corpus and Grammar There are two steps in the implementation of CSE II: the first step is corpus preparation, and the second is grammar adaptation. For corpus, we follow CSE I and use the LDC Chinese Gigaword Corpus because of its size (over 11 billion characters) and its coverage of both traditional and simplified characters. The Gigaword Corpus was fully automatically segmented and tagged using the Academia Sinica tagset and tagtool (Ma and Huang 2006). Our work in adaptation for CSE II includes resolution of categorical ambiguity for nominalization and improvement of unknown word resolution. 208 For grammar adaptation, we concentrate on exploiting lexically encoded ICG grammatical knowledge. Since the corpus was tagged with Academia Sinica tagset, the verb-subclass information for each verb is specified. Hence we can utilize the structural information from ICG BP. Since the tagged corpus has identified the verb-subclasses, we are able to correctly identify different grammatical relations, even though two verbs may share the same local stru"
Y06-1027,J90-1003,0,\N,Missing
Y06-1027,C98-2122,0,\N,Missing
