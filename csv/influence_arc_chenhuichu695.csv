2011.mtsummit-papers.53,I08-1012,0,0.0164577,". We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental results are shown in Table 8. Common Chinese characters information is detected by our proposed detecting method and Kanconvit. The alignment accuracy of the alignment model we used with"
2011.mtsummit-papers.53,P07-1003,0,0.022513,"JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental results are shown in Table 8. Common Chinese characters information is detected by our proposed detecting method and Kanconvit. The alignment accuracy of the alignment model we used without incorporating the information is indicated as ”Baseline”, the alignment accuracy after adjusting the base distribution to reﬂect the information is indicated as ”BD”, and the alignment accuracy after exploiting the information directly into the 7 469 Settings http://code.google.com/p/berkeleyaligner/ grow-diag-ﬁnal-and BerkelyAligner Baselin"
2011.mtsummit-papers.53,D08-1033,0,0.0424857,"Missing"
2011.mtsummit-papers.53,I05-1059,0,0.254738,"rk Common Chinese characters information have been employed for a number of Japanese-Chinese related tasks. Tan et al. (1995) availed the occurrence of common Chinese characters as a feature of Japanese-Traditional Chinese sentence pair to ﬁnd a Meaning Kanji TC SC snow    country    love    begin hair      Table 1: Examples of Chinese characters (TC denotes Traditional Chinese and SC denotes Simpliﬁed Chinese). SC TC      , ,     , ,  ... ... Table 2: Hanzi converter version 3.0 standard conversion table. direct correspondence in automatic sentence alignment task. Goh et al. (2005) built a JapaneseSimpliﬁed Chinese dictionary partly using direct conversion of Japanese into Chinese for the Japanese words that all the characters in the word are made up of Kanji only, namely Kanji words. They did the conversion using a Chinese encoding converter1 which can convert Traditional Chinese into Simpliﬁed Chinese. It works because most Kanji are identical to Traditional Chinese. And for the Kanji with visual variations that cannot be automatically converted using the converter, they manually converted them by hand. In the context of machine translation, Kondrak et al. (2003) inco"
2011.mtsummit-papers.53,N06-1023,1,0.781188,"ts (Och and Ney, 2003). The unit of evaluation was word. We used precision, recall and alignment error rate (AER) as evaluation criteria. All the experiments were run on original forms of words. We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental resu"
2011.mtsummit-papers.53,N03-1017,0,0.0328458,"Missing"
2011.mtsummit-papers.53,N03-2016,0,0.187254,"nt task. Goh et al. (2005) built a JapaneseSimpliﬁed Chinese dictionary partly using direct conversion of Japanese into Chinese for the Japanese words that all the characters in the word are made up of Kanji only, namely Kanji words. They did the conversion using a Chinese encoding converter1 which can convert Traditional Chinese into Simpliﬁed Chinese. It works because most Kanji are identical to Traditional Chinese. And for the Kanji with visual variations that cannot be automatically converted using the converter, they manually converted them by hand. In the context of machine translation, Kondrak et al. (2003) incorporated cognates (words or languages which have the same origin) information in European languages into the translation models of Brown et al. (1993). They arbitrarily selected a subset from the Europarl corpus as training data and extracted a list of likely cognate word pairs from the training corpus on the basis of orthographic similarity, and appended to the corpus itself in order to reinforce the co-occurrence count between cognates. The results of experiments conducted on a variety of bitexts showed that cognate identiﬁcation can improve word alignments without modifying the statist"
2011.mtsummit-papers.53,P09-1058,0,0.0307693,"Missing"
2011.mtsummit-papers.53,W02-1018,0,0.0346189,"y of JUMAN (Kurohashi et al., 1994). Table 5 gives some examples of Kana-Kanji conversion results. We only do Kana-Kanji conversion for content words because it is proved that do Kana-Kanji conversion for function words may lead to wrong alignment in the alignment experiments we did. 4 Alignment Model We used an alignment model proposed by Nakazawa and Kurohashi (2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazawa and Kurohashi, 2009) and deﬁnes the reorderings on the word dependency trees. 4.1 Generative Story Description Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), the model we used ﬁrst describes the generative story for the"
2011.mtsummit-papers.53,W09-2302,1,0.84403,"(2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazawa and Kurohashi, 2009) and deﬁnes the reorderings on the word dependency trees. 4.1 Generative Story Description Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), the model we used ﬁrst describes the generative story for the joint alignment model. 1. Generate  concepts from which subtree pairs are generated independently. 2. Combine the subtrees in each language so as to create parallel sentences. Here, subtrees are equivalent to phrases in the previous works. One subtree in a concept can be NULL, which represents an unaligned subtree. The model restricts the unaligned subtrees to be compo"
2011.mtsummit-papers.53,I11-1089,1,0.725191,"to be used. The Chinese characters in Kanji expressions are again useful as clues to ﬁnd word-to-word matchings. We can use Kana-Kanji conversion techniques to get the Kanji expressions from Kana expressions, but here, we simply consult a Japanese dictionary of JUMAN (Kurohashi et al., 1994). Table 5 gives some examples of Kana-Kanji conversion results. We only do Kana-Kanji conversion for content words because it is proved that do Kana-Kanji conversion for function words may lead to wrong alignment in the alignment experiments we did. 4 Alignment Model We used an alignment model proposed by Nakazawa and Kurohashi (2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazaw"
2011.mtsummit-papers.53,J03-1002,0,0.00494741,"shown in Table 6) • Kanconvit: Kanconvit Kanji to Hanzi conversion and Kana-Kanji conversion The results shown in Table 7 veriﬁed the effectiveness of our proposed detecting method. Also, there is complementation between Kanconvit and our proposed detecting method. 4 http://www.jst.go.jp http://www.nict.go.jp/ 6 http://kanconvit.ta2o.net/ 5 The training corpus we used is the same one we used in Subsection 5.1. As gold-standard data, we used 510 sentence pairs for Japanese-Chinese which were annotated by hand. There are two types of annotations, sure (S) alignments and possible (P) alignments (Och and Ney, 2003). The unit of evaluation was word. We used precision, recall and alignment error rate (AER) as evaluation criteria. All the experiments were run on original forms of words. We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi"
2012.eamt-1.7,I05-3025,0,0.0307599,"nd “» ‰↔» T(anesthesia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Ja"
2012.eamt-1.7,E09-1063,0,0.316822,"Missing"
2012.eamt-1.7,I08-1033,0,0.17868,"ctive for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the tr"
2012.eamt-1.7,C04-1081,0,0.0241624,"Missing"
2012.eamt-1.7,W08-0336,0,0.0350343,"orter unit standard. Therefore, the segmentation unit in Chinese may be longer than Japanese even for the same concept. This can increase the number of 1-to-n alignments which makes the word alignment task more difficult. Taking “founder” Introduction As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important first step in MT. Studies showed that a MT system with Chinese word segmentation outperforms the one treating each Chinese character as a single word, and the quality of Chinese word segmentation affects the MT performance (Xu et al., 2004; Chang et al., 2008). It has been found that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence"
2012.eamt-1.7,2011.mtsummit-papers.53,1,0.899763,"iation for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Japanese MT and exploit common Chinese characters in Chinese word segmentation optimization. Table 1: Examples of common Chinese characters (TC denotes Traditional Chinese and SC denotes Simplified Chinese). in Figure 1 as an example, the Chinese segmenter recognizes it as one token, while the Japanese segmenter splits"
2012.eamt-1.7,wang-etal-2010-adapting,0,0.0166521,"s mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the transformed data for training the Chinese segmenter. Because the extracted lexicons are derived from Japanese word segmentation results, they follow Japanese 37 从_P/ 有效性_NN /高_VA/的_DEC/ 格要素_NN /… CTB: Lexicon: Lexicon"
2012.eamt-1.7,W02-1001,0,0.0137644,"e same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based word segmentation and POS tagging tool with a system dictionary, weights for the lexicons in the system dictionary are automatically learned from the training data using averaged structured perceptron (Collins, 2002). For Japanese, we used JUMAN (Kurohashi et al., 1994). We conducted Chinese-Japanese translation experiments to show the effectiveness of exploiting common Chinese characters in Chinese word segmentation optimization. 4.1.1 Chinese Annotated Corpus We used two types of manually annotated Chinese corpus for training the Chinese segmenter. One is NICT Chinese Treebank, which is from the same domain as the parallel training corpus and contains 9,792 sentences. Note that the annotated sentences in this corpus are not included in the parallel training corpus. The other corpus is CTB 7 (LDC2010T07)"
2012.eamt-1.7,I11-1035,0,0.118655,"esia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the P"
2012.eamt-1.7,I05-1059,0,0.173956,"und that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Ja"
2012.eamt-1.7,P07-2045,0,0.00948082,"racters, such as “L‚(praise)”, “×L(poem)” etc. Obviously, splitting “L ‚(praise)” into “L(song)” and “‚(eulogy)”, or splitting “× L(poem)” into “×(poem)” and “L(song)” is undesirable. Also, there are few consecutive tokens in the training data that can be combined to one extracted lexicon, we do not consider this pattern. 4 corpus was created by the Japanese project “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of this corpora are shown in Table 3. 4.1.2 4.1.3 4.1.4 5 SMT Model We used the state-of-the-art phrase-based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (6→20). It was tuned by MERT using another 500 development sentence pairs. 4.1.5 Test Sets We translated 5 test sets of Chinese sentences from the same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based wo"
2012.eamt-1.7,xia-etal-2000-developing,0,0.106997,"Missing"
2012.eamt-1.7,W04-1118,0,0.085468,"Missing"
2012.eamt-1.7,W04-3230,0,0.0697192,"Missing"
2012.iwslt-evaluation.12,D11-1047,1,0.920432,"Figure 1 shows the overview of our EBMT system on Chinese-English translation. The translation example database is automatically constructed from a parallel training corpus by means of a Bayesian subtree alignment model. Note that both source and target sides of all the examples are stored in dependency tree structures. An input sentence is also parsed and transformed into a dependency structure. For all the sub-trees in the input dependency structure, matching examples are searched in the example database. This step is the most time consuming part, and we exploit a fast tree retrieval method [2]. There are many available examples for one sub-tree, and also, there are many possible sub-tree combinations. The best combination is detected by a log-linear decoding model with features described in Section 3. In the example in Figure 1, ﬁve examples are used. They are combined and produce an output dependency tree. We call nodes surrounding those of the example, “bond” nodes. The bond nodes of one example are replaced by other examples, and thus examples can be combined. We attended the IWSLT 2012 OLYMPICS task which is a Chinese-to-English text translation task. Based on the characteristi"
2012.iwslt-evaluation.12,I11-1089,1,0.840723,"equential model is prone to many such errors even for short simple sentences of a distant language pair. Even if the word order differs greatly between languages, phrase dependencies tend to hold between languages. This can be seen in Figure 2. Therefore, incorporating dependency analysis into the alignment model is useful for distant lanFigure 3: Alignment results from bi-directional GIZA++. Black boxes depict the system output, while dark (Sure) and light (Possible) gray cells denote gold-standard alignments. guage pairs. We exploit Bayesian subtree alignment model based on dependency trees [3]. This model incorporates dependency relations of words into the alignment model and deﬁne the reorderings on the word dependency trees. Figure 2 shows an example of the dependency trees for Japanese and English. 3. Tree-based Translation As a tree-based translation method, we adopt an examplebased machine translation system [1]. In this section, we brieﬂy introduce the translation procedure in our EBMT system. 3.1. Retrieval of Translation Examples The input sentence is converted into the dependency structure as in the alignment step. Then, for each sub-tree, avail97 The 9th International Wor"
2012.iwslt-evaluation.12,J05-4003,0,0.118733,"Missing"
2012.iwslt-evaluation.12,E09-1063,0,0.0208048,"ﬁer trained on the BTEC corpus does not work well on the HIT corpus because of the difference between these two corpora, thus some parallel sentences are also ﬁltered in this process. BLEU 0.1162 0.1209 0.1271 Table 1: Results of preliminary translation experiments. 4.4. Optimized Chinese Segmenter As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important ﬁrst step in machine translation. Research shows that optimal Chinese word segmentation for machine translation is dependent on the other language, therefore, a bilingual approach is necessary [6]. In this task, we adopted a Chinese segmenter optimized based on a bilingual perspective, which exploits common Chinese characters shared between Chinese and Japanese for Chinese word segmentation optimization [7]. The BLEU scores with and without Chinese segmenter optimization are given in Table 1, indicated as “Optimized” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation"
2012.iwslt-evaluation.12,2012.eamt-1.7,1,0.576691,"e 1: Results of preliminary translation experiments. 4.4. Optimized Chinese Segmenter As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important ﬁrst step in machine translation. Research shows that optimal Chinese word segmentation for machine translation is dependent on the other language, therefore, a bilingual approach is necessary [6]. In this task, we adopted a Chinese segmenter optimized based on a bilingual perspective, which exploits common Chinese characters shared between Chinese and Japanese for Chinese word segmentation optimization [7]. The BLEU scores with and without Chinese segmenter optimization are given in Table 1, indicated as “Optimized” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation results are much more similar to English in number, which can reduce the number of 1-to-n alignments and improve the alignment accuracy. 4.5. Rule-based Decoding Constraints Translating long and complex sentences"
2012.iwslt-evaluation.12,2011.iwslt-evaluation.5,0,0.0312388,"d” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation results are much more similar to English in number, which can reduce the number of 1-to-n alignments and improve the alignment accuracy. 4.5. Rule-based Decoding Constraints Translating long and complex sentences is a critical problem in machine translation, because it increases the computational complexity. Finch et al. [8] presented a simple yet efﬁcient method to solve this problem. They split a sentence into smaller units based on part-of-speech (POS) tags and commas, and translate the split units separately. Following their method, we also split a sentence into smaller units during decoding. Our EBMT system tends to choose large examples. Since the development data of this task also has the sub-sentence problem (described in Section 4.3), our system may use examples across punctuation boundaries which can generate translations with unnatural word order. Therefore, we split a source sentence based on comma, p"
2012.iwslt-evaluation.12,2011.iwslt-papers.6,0,0.0390629,"Missing"
2012.iwslt-evaluation.12,I08-1012,0,0.0274568,"Missing"
2012.iwslt-evaluation.12,N10-1015,0,0.069588,"Missing"
2012.iwslt-evaluation.12,federico-etal-2012-iwslt,0,\N,Missing
2012.iwslt-evaluation.12,E09-1000,0,\N,Missing
2020.acl-main.33,E17-2068,0,0.0319338,"! I caught a cold! – Cold Cold Cold Table 1: Examples of BERT classification for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; henc"
2020.acl-main.33,P14-1062,0,0.0613231,"tion for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; hence, semantically similar texts tend to have closer representations. Meanwhile,"
2020.acl-main.33,D19-1542,1,0.827149,"N treat all negative examples equally, disregarding variables, such as relations between class labels. Future work should focus on the semantic relations among class labels in the auxiliary task. 354 4 Related Work Multitask learning has been employed to improve the performance of text classification (Liu et al., 2019; Xiao et al., 2018). Previous studies aimed to improve multiple tasks; hence, they required multiple sets of annotated datasets. In contrast, our method does not require any extra labelled datasets and is easily applicable to various classification tasks. The methods proposed in Arase and Tsujii (2019) and Phang et al. (2018) improved the BERT classification performance by further training the pre-trained model using natural language inference and paraphrase recognition. Similar to multitask learning, both methods require an additional largescale labelled dataset. Furthermore, these previous studies revealed that the similarity of tasks in training affects the models’ final performance (Xiao et al., 2018; Arase and Tsujii, 2019). Our method achieved consistent improvements across tasks, indicating its wider applicability. 5 Conclusion In this paper, we proposed a simple multitask learning m"
2020.acl-main.33,P18-1216,0,0.0185967,"Cold Cold Cold Table 1: Examples of BERT classification for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; hence, semantically simi"
2020.acl-main.33,P12-2018,0,0.058188,"BERT A cold is a legit disease. Oh my god! I caught a cold! – Cold Cold Cold Table 1: Examples of BERT classification for labelling a disease contracted by a writer. Both sentences are about the common cold. Only the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to"
2020.acl-main.33,C18-1175,0,0.0208288,"using negative supervision is crucial. SST-5 is an exception wherein our models degraded the performance of the Baseline. We hypothesise that this is because its class labels are gradational, e.g. Somewhat Negative is closer to Negative rather than Positive sentences. AM and AAN treat all negative examples equally, disregarding variables, such as relations between class labels. Future work should focus on the semantic relations among class labels in the auxiliary task. 354 4 Related Work Multitask learning has been employed to improve the performance of text classification (Liu et al., 2019; Xiao et al., 2018). Previous studies aimed to improve multiple tasks; hence, they required multiple sets of annotated datasets. In contrast, our method does not require any extra labelled datasets and is easily applicable to various classification tasks. The methods proposed in Arase and Tsujii (2019) and Phang et al. (2018) improved the BERT classification performance by further training the pre-trained model using natural language inference and paraphrase recognition. Similar to multitask learning, both methods require an additional largescale labelled dataset. Furthermore, these previous studies revealed tha"
2020.acl-main.33,C18-1330,0,0.0310524,"Missing"
2020.acl-main.33,N16-1174,0,0.38251,"y the second example indicates that the writer had a cold. BERT misclassified the first sentence. Introduction Text classification generally consists of two processes: an encoder that converts texts to numerical representations and a classifier that estimates hidden relations between the representations and class labels. The text representations are generated using N -gram statistics (Wang and Manning, 2012), word embeddings (Joulin et al., 2017; Wang et al., 2018), convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015; Shen et al., 2018), and recurrent neural networks (Yang et al., 2016, 2018). Recently, powerful pre-trained models for text representations, e.g. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), have shown stateof-the-art performance on text classification tasks using only the simple classifier of a fully connected layer. However, a problem occurs when a classification task is adversarial to text encoders. Encoders aim to represent the meanings of texts; hence, semantically similar texts tend to have closer representations. Meanwhile, a classifier should distinguish subtle differences that lead to different label assignment"
2020.acl-main.33,C16-1329,0,0.0180987,"dWeb arXiv Ja En Zh SOTA 83.5 86.3 52.4 96.4 95.5 82.5 79.5 80.9 - Baseline ACE 86.5 86.3 89.2 88.8 54.0 53.2 97.0 97.0 96.5 96.5 86.1 86.2 83.1 82.8 86.9 86.8 36.0 35.8 AM AAN 86.4 86.8 89.1 89.4 52.9 53.0 97.2 96.9 96.3 96.6 86.5 87.1 83.2 83.6 87.1 86.4 36.3 36.4 Table 3: Evaluation results. The best scores are presented in the bold font, and scores higher than the Baseline are underlined. Our models consistently outperform the baseline and ACE, which indicates the effectiveness of negative supervision through the auxiliary task. Previous SOTA results are reported by Du et al. (2019) (MR), Zhou et al. (2016) (CR, SST-5), Howard and Ruder (2018) (TREC), Zhao et al. (2015) (SUBJ) and Iso et al. (2017) (MedWeb). the auxiliary task, we compared our model to one that predicts a sentence with the same label. Accurately, this model conducts classification given cosine similarities using cross entropy loss (referred to as ACE (the auxiliary task with cross entropy loss)). Furthermore, we evaluated two variations of our model. The first purely gives negative supervision, i.e., the auxiliary task only encourages the generation of distinct representation to negative examples, as described in Section 2.2 (re"
2020.coling-tutorials.3,Q19-1007,1,0.817811,"can be combined. We will see how MT models for new languages can be rapidly adapted from pre-trained MNMT models. Additionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2"
2020.coling-tutorials.3,2020.findings-emnlp.445,1,0.721245,"or Applied Researcher in the MT team at Microsoft India, where he is involved in building MT systems for multiple Indian languages. His research interests span in different aspects of machine translation, particularly: multilingual models, low-resource translation and translation involving related languages. More broadly, he is interested in different multilingual, cross-lingual and multi-task NLP approaches. He is passionate about building software and resources for NLP in Indian languages. He is the developer of the Indic NLP Library (Kunchukuttan, 2020), co-developer of the IndicNLP-Suite (Kakwani et al., 2020) and a co-founder of the AI4Bharat-NLP Initiative, a community initiative to build Indian language NLP technologies. He has published papers on MT and multilingual learning at ACL, NAACL, EMNLP, TACL and IJCNLP. He has been a member of the organizing committees for COLING 2012 and Workshop on Asian Translation. He received his Ph.D from the Indian Institute of Technology Bombay. References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively Multilingual Neural Machine Translation. In NAACL. Maruan Al-Shedivat and Ankur Parikh. 2019. Consistency by agreement in zero-shot neural machi"
2020.coling-tutorials.3,W18-6325,0,0.0229673,"3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 20"
2020.coling-tutorials.3,N19-1387,1,0.778287,"2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Fu"
2020.coling-tutorials.3,D18-1103,0,0.0161762,"ameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion"
2020.coling-tutorials.3,I17-2050,0,0.0220177,", 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterj"
2020.coling-tutorials.3,W18-2711,0,0.020594,"cmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. from IIT Bombay, India and his Ph.D. from Kyoto University, Japan. He is a post-doctoral researcher at NICT which is Japan’s natio"
2020.coling-tutorials.3,D18-1039,0,0.0186918,"ned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-re"
2020.coling-tutorials.3,W18-6327,0,0.0175966,"ntative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 20"
2020.coling-tutorials.3,P19-1297,0,0.0197742,"Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. fro"
2020.coling-tutorials.3,P16-1009,0,0.0502196,"tionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer"
2020.coling-tutorials.3,P16-1162,0,0.0231572,"tionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer"
2020.coling-tutorials.3,D18-1326,0,0.0261679,"tions. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30"
2020.coling-tutorials.3,P19-1579,0,0.0106457,"earning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre recei"
2020.coling-tutorials.3,N16-1004,0,0.00916043,"2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. from IIT Bombay, India and his Ph.D. from Kyoto University, Jap"
2020.coling-tutorials.3,D16-1163,0,0.0194072,"2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source transla"
2020.eamt-1.12,W18-6402,0,0.305768,"lication of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English→German and English→French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions. Object detection Abstract attention Figure 1: Overview of our MNMT model. many studies (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; H"
2020.eamt-1.12,W16-2358,0,0.0423569,"Missing"
2020.eamt-1.12,J82-2005,0,0.576686,"Missing"
2020.eamt-1.12,W17-4746,0,0.0849293,"Missing"
2020.eamt-1.12,W18-6438,0,0.0342887,"Missing"
2020.eamt-1.12,D17-1105,0,0.0955703,"ngly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; Huang et al., 2016; Calixto and Liu, 2017) incorporated global visual features extracted from the entire image by convolutional neural networks (CNNs) into a source word sequence or hidden states of a recurrent neural network (RNN). Furthermore, other studies started using local visual features in the context of an attention-based NMT. These features were extracted from equally-sized grids in an image by a CNN. For instance, multimodal attention (Caglayan et al., 2016b) has been designed for a mix of text and local visual features. Additionally, double attention mechanisms (Calixto et al., 2017) have been proposed for text homme en De"
2020.eamt-1.12,W16-2359,0,0.018129,"r hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms into decoder using grid visual features by VGG-19 CNN. Because the grid regions do not contain semantic visual features, the multimodal attention mechanism can not capture useful information with grid visual features. Therefore, instead of multimodal attention, Calixto, Liu, and Campbell (2017) proposed two individual attention mechanisms focusing on two modalities. Similarly, Libovick´y and Helcl (2017) proposed two attention strategies that can be applied to all hidden layers or context vectors of each modality. But they still used grid"
2020.eamt-1.12,P17-1175,0,0.0528235,"Missing"
2020.eamt-1.12,P19-1642,0,0.0192037,"features into source sentence vectors and encoder/decoder hidden states. Elliott and K´ad´ar (2017) utilized global visual features to learn both machine translation and visually grounding task simultaneously. As for the best system in WMT 2017,7 Caglayan et al. (2017) proposed different methods to incorporate global visual features based on attention-based NMT model such as initial encoder/decoder hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms into decoder using grid visual features by VGG-19 CNN. Because the grid regions do not contain semantic visual features, the multimodal attention"
2020.eamt-1.12,D17-1095,0,0.0549949,"s. Additionally, double attention mechanisms (Calixto et al., 2017) have been proposed for text homme en Decoder yt-1 yt chemise rouge st-1 st &lt;eos&gt; zt ct Att_text wp (max) rp Bi-directional GRU encoder 100 semantic image region feature vectores Att_img hi Faster R-CNN + ResNet-101 Man Source image in a red xi shirt &lt;eos&gt; Source sentence Figure 2: Our model of double attention-based MNMT with semantic image regions. and local visual features, respectively. Although previous studies improved the use of local visual features and the text modality, these improvements were minor. As discussed in (Delbrouck and Dupont, 2017), these local visual features may not be suitable to attention-based NMT, because the attention mechanism cannot understand complex relationships between textual objects and visual concepts. Other studies utilized richer local visual features to MNMT such as dense captioning features (Delbrouck et al., 2017). However, their efforts have not convincingly demonstrated that visual features can improve the translation quality. Caglayan et al. (2019) demonstrated that, when the textual context is limited, visual features can assist in generating better translations. MMT models disregard visual feat"
2020.eamt-1.12,W18-6439,0,0.113891,"studies have fused either global or local visual image features into MMT. 6.1 Global visual feature Calixto and Liu (2017) incorporated global visual features into source sentence vectors and encoder/decoder hidden states. Elliott and K´ad´ar (2017) utilized global visual features to learn both machine translation and visually grounding task simultaneously. As for the best system in WMT 2017,7 Caglayan et al. (2017) proposed different methods to incorporate global visual features based on attention-based NMT model such as initial encoder/decoder hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms i"
2020.eamt-1.12,W14-3348,0,0.0161004,"testset of Multi30k. All scores are averages of three runs. We present the results using the mean and the standard deviation. † and ‡ indicate that the result is significantly better than OpenNMT and double-attentive MNMT at p-value &lt; 0.01, respectively. Additionally, we report the best results of using grid and global visual features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, doubly-attentive MNMT and our model on M"
2020.eamt-1.12,P17-2031,0,0.0369791,"Missing"
2020.eamt-1.12,I17-1014,0,0.0393147,"Missing"
2020.eamt-1.12,P02-1040,0,0.106569,"models on the En→De and En→Fr 2016 testset of Multi30k. All scores are averages of three runs. We present the results using the mean and the standard deviation. † and ‡ indicate that the result is significantly better than OpenNMT and double-attentive MNMT at p-value &lt; 0.01, respectively. Additionally, we report the best results of using grid and global visual features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, d"
2020.eamt-1.12,W16-3210,0,0.111543,"Missing"
2020.eamt-1.12,W17-4718,0,0.0640345,"st, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English→German and English→French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions. Object detection Abstract attention Figure 1: Overview of our MNMT model. many studies (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent stud"
2020.eamt-1.12,W16-2346,0,0.39395,"Missing"
2020.eamt-1.12,D16-1044,0,0.0377199,"eline NMT for the English–German task. Helcl, Libovick´y, and Variˇs (2018) set an additional attention sub-layer after the self-attention based on the Transformer architecture, and integrated grid visual features extracted by a pretrained CNN. Caglayan et al. (2018) enhanced the multimodal attention into the filtered attention, which filters out grid regions irrelevant to translation and focuses on the most important part of the grid visual features. They made efforts to integrate a stronger attention function, but the considered regions were still grid visual features. Grid visual features. Fukui et al. (2016) applied multimodal compact bilinear pooling to combine the grid visual features and text vectors, but their model does not convincingly surpass an attentionbased NMT baseline. Caglayan et al. (2016a) integrated local visual features extracted by ResNet-50 and source text vectors into an NMT decoder using shared transformation. They reported that the results obtained by their method did not surpass the results obtained by NMT systems. Caglayan, Barrault, and Bougares (2016b) proposed a multimodal attention mechanism based on (Caglayan et al., 2016a). They integrated two modalities by Image reg"
2020.eamt-1.12,W18-6441,0,0.0269252,"Missing"
2020.eamt-1.12,W16-2360,0,0.0519306,") have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; Huang et al., 2016; Calixto and Liu, 2017) incorporated global visual features extracted from the entire image by convolutional neural networks (CNNs) into a source word sequence or hidden states of a recurrent neural network (RNN). Furthermore, other studies started using local visual features in the context of an attention-based NMT. These features were extracted from equally-sized grids in an image by a CNN. For instance, multimodal attention (Caglayan et al., 2016b) has been designed for a mix of text and local visual features. Additionally, double attention mechanisms (Calixto et al., 2017) have been propo"
2020.eamt-1.12,W04-3250,0,0.0908902,"features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, doubly-attentive MNMT and our model on Multi30k dataset. Additionally, we also compared with Caglayan et al. (2017), which achieved the best performance under the same condition with our experiments. Comparing the baselines, the doubly-attentive MNMT outperformed OpenNMT. Because there did not exist a big difference amongst the three image feature ext"
2020.lrec-1.238,L16-1715,0,0.0283516,"an of the … W, Acres or Land W.T MACMICHAEL Bags to… W, Acres or Land W.T MACMICHAEL Bags to… N. B.–Charts … N. B.–Charts … APLANDID PUR… APLANDID PUR… Of Engs il h Manufucture BYMR. T. Y. LOWES At the Exchange … Trimming New Incurance Company ATaPublic Meeting … Campbell Street BY MR. WOOD Sept17,1838 OCR Of Engs il h Manufucture BYMR. T. Y. LOWES At the Exchange … Sept17,1838 Filtering Figure 1: Overview of the corpus construction method. French, Dutch, and German. They annotated named entity tags for the Europeana Newspaper from the 17th to the 20th century using the INL Attestation Tool.3 Cassidy (2016) built an Australian historical newspaper corpus and published it on a website called Trove. They converted newspapers from the 19th century to the 21st century into text data using OCR. We construct our corpus based on Trove. Different from (Cassidy, 2016) and other previous studies, we propose a method to extract the specific topic of “public meeting” and analyze the temporal and semantic contents. 2.2. Search-based Content Analysis Search result clustering often ends with visualization (Carpineto et al., 2009) to help users target their own search (K¨aki, 2005). A few search engines visuall"
2020.lrec-1.238,2005.mtsummit-papers.11,0,0.0378244,"Missing"
2020.lrec-1.238,J93-2004,0,0.0924926,"he content of a corpus via search. 2.1. https://trove.nla.gov.au Historical Corpus Construction Several studies on corpus construction for historical documents have been conducted. Davies (2012) built an American English historical corpus. They collected text from magazines, newspapers and books from 1810 to 2000. They further lemmatised and labeled part-of-speech (POS) tags on the corpus. R¨ognvaldsson et al. (2012) built an Icelandic parsed historical document corpus. They collected text from the 12th to the 21st century and annotated them for parsing using the same schema as Penn Treebank (Marcus et al., 1993). S´anchez-Mart´ınez et al. (2013) built a Spanish historical corpus. They collected text from prose, theatre, and verse from 1481 to 1748, lemmatised them, and labeled POS tags. Neudecker (2016) built a corpus for named entity recognition from historical newspapers in 2 1 Related Work Public meetings is the main pillar of public opinion formation for Western Europe in the 19th century. 1934 Zouto and Ground New Incurance Company ATaPublic Meeting … Campbell Street BY MR. WOOD Zouto and Ground Onthe … Onthe … APlan of the … APlan of the … W, Acres or Land W.T MACMICHAEL Bags to… W, Acres or La"
2020.lrec-1.238,L16-1689,0,0.0286563,"t an American English historical corpus. They collected text from magazines, newspapers and books from 1810 to 2000. They further lemmatised and labeled part-of-speech (POS) tags on the corpus. R¨ognvaldsson et al. (2012) built an Icelandic parsed historical document corpus. They collected text from the 12th to the 21st century and annotated them for parsing using the same schema as Penn Treebank (Marcus et al., 1993). S´anchez-Mart´ınez et al. (2013) built a Spanish historical corpus. They collected text from prose, theatre, and verse from 1481 to 1748, lemmatised them, and labeled POS tags. Neudecker (2016) built a corpus for named entity recognition from historical newspapers in 2 1 Related Work Public meetings is the main pillar of public opinion formation for Western Europe in the 19th century. 1934 Zouto and Ground New Incurance Company ATaPublic Meeting … Campbell Street BY MR. WOOD Zouto and Ground Onthe … Onthe … APlan of the … APlan of the … W, Acres or Land W.T MACMICHAEL Bags to… W, Acres or Land W.T MACMICHAEL Bags to… N. B.–Charts … N. B.–Charts … APLANDID PUR… APLANDID PUR… Of Engs il h Manufucture BYMR. T. Y. LOWES At the Exchange … Trimming New Incurance Company ATaPublic Meeting"
2020.lrec-1.238,rognvaldsson-etal-2012-icelandic,0,0.0753094,"Missing"
2020.lrec-1.836,W19-5036,0,0.0131161,"ses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effects. More broadly, it is useful for research on knowledge extraction from patient-generated texts. Various types of tasks can be designed using our dataset. To name a few: 1. Automatic linking of texts describing drug reactions"
2020.lrec-1.836,W19-1909,0,0.0116813,"ern mining (Stilo et al., 2013) and applied deep neural networks (Limsopatham and Collier, 2016) to map colloquial expressions in posts onto formal writing used in medical concepts. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effects. More broadly, it is"
2020.lrec-1.836,N19-1423,0,0.00602232,"ious studies used pattern mining (Stilo et al., 2013) and applied deep neural networks (Limsopatham and Collier, 2016) to map colloquial expressions in posts onto formal writing used in medical concepts. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effe"
2020.lrec-1.836,W10-1915,0,0.291478,"of drugs, we focused on a third source of information—patient weblogs. There is a world-wide trend of sharing fight experiences against diseases on the internet. A typical example is PatientsLikeMe.3 Patients write about their real experiences in their weblog articles, exchange comments, and share information that they curated, to help each other fight and survive diseases. Because of their nature, patient weblogs provide lively descriptions of their physical conditions. Previous studies confirmed the usefulness of such patient self-reporting notes for finding information on adverse effects (Leaman et al., 2010; Nikfarjam and Gonzalez, 2011; Yang et al., 2012). Nikfarjam et al. (2015) annotated adverse effects on posts mined from a health-related online forum and Twitter,4 where shortness of content is typical. In contrast with their dataset, we targeted weblogs, 2 https://www.pmda.go.jp/safety/ info-services/drugs/adr-info/suspected-adr/ 0006.html (in Japanese) 3 https://www.patientslikeme.com/ 4 https://twitter.com/ 6769 タグリッソ服用丸２か月が経ちました。今のとこ ろの副作用は、相変わらずの舌炎症。はじめは、 米や小麦料理がザラザラした舌触りで不味いって 感じでした。 (It has been two months since the start of Tagrisso. Its adverse effect is glossitis, as I have always"
2020.lrec-1.836,P16-1096,0,0.0620381,"ose in general domains. To identify spans describing adverse drug reactions, previous studies used lexicons in the medical domain (Leaman et al., 2010; Yang et al., 2012), lexical patterns identified by association rule mining (Nikfarjam and Gonzalez, 2011), and word embedding (Nikfarjam et al., 2015). Differences in style of the texts is another issue. People write posts to forums and Twitter in a colloquial style, which is significantly different from the formal style used to define medical concepts. Previous studies used pattern mining (Stilo et al., 2013) and applied deep neural networks (Limsopatham and Collier, 2016) to map colloquial expressions in posts onto formal writing used in medical concepts. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments"
2020.lrec-1.836,N18-1100,0,0.0283833,"nowledge extraction from patient-generated texts. Various types of tasks can be designed using our dataset. To name a few: 1. Automatic linking of texts describing drug reactions to concept IDs in a medical ontology 2. Prediction of effect type given an article and drug names discussed in the article 3. Span identification describing drug effects given an article 4. Span identification and labelling of drug names, corresponding ICD-10 codes, and effect types The first task automatically identifies spans describing drug effects and maps onto standard codes or IDs defined in a medical ontology (Mullenbach et al., 2018; Limsopatham and Collier, 2016). The second task is a variant of the aspect-based sentiment analysis in the domain of drug reaction detection. The third task is a kind of sequential labelling problems, but with a number of labels as large as related drug names. The fourth task is an advanced sequential labelling problem with multiple types of labels with dependent relations. This task conforms to a more practical setting on drug effect mining. As discussed in Sec. 4.2., our annotation dataset consists of challenging examples that (a) have many-to-many correspondences between drugs and drug re"
2020.lrec-1.836,W95-0107,0,0.680353,"g these questions. The guideline was immediately updated whenever necessary then shared with annotators to maintain a consistent standard. 4. Analysis of Annotation Results Over the cumulative total of 1, 500 annotated articles, the number of identified drug names was 108, and the number of identified ICD-10 codes was 104. In this section, we discuss the quality and consolidate the annotation results. 4.1. Agreement Rates of Annotations To examine the agreement level of the annotations, we formatted the annotation results with the character-level Inside–Outside–Beginning (IOB) tagging scheme (Ramshaw and Marcus, 1995) and calculated Fleiss’ kappa. The IOB tagging scheme is common for evaluating named entity recognition, which is a sequential tagging task of named 10 9 4,147 27 693 The dictionary of Japanese drug names will be released at our web site. 6772 IOB (span only) IOB+drug name IOB+ICD-10 IOB+effect type IOB+all labels Fleiss’ kappa # of unique tags 0.635 0.615 0.565 0.589 0.520 3 287 211 59 1,020 Table 6: Frequency of types of effects Table 4: Agreement rates of annotations. Drug name ICD-10 code 169 677 87 78 8.1 328.5 2.5 1.6 40 30 20 10 0 10.0 Table 5: Statistics of consolidated annotations. en"
2020.lrec-1.836,R13-1084,0,0.0600326,"Missing"
2020.lrec-1.836,W18-5909,0,0.0131146,"s. Our dataset poses both challenges, as it annotates weblogs written in colloquial style, where adverse drug reactions are described not only as phrases, but also as sentences or even longer spans. Recent studies show that a pre-trained model for text representation achieves impressive performances on various downstream tasks, such as automatic question answering and natural language inference (Devlin et al., 2019). Alsentzer et al. (2019) adapted the pre-trained model to the medical domain. Further, sentiments in texts are useful clues to identify adverse effects (Sarker and Gonzalez, 2015; Wu et al., 2018; Alhuzali and Ananiadou, 2019), which is reasonable because adverse effects are firmly negative events for patients. These approaches are promising for adverse effect mining on patient-generated texts, from online forums to weblogs including our dataset. 6. Potential Task Designs with Our Dataset Our annotation dataset is a valuable resource to advance research on the automatic detection of drug effects. More broadly, it is useful for research on knowledge extraction from patient-generated texts. Various types of tasks can be designed using our dataset. To name a few: 1. Automatic linking of"
2020.wat-1.5,Q19-1038,0,0.0164061,"f-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER embeddings of two sentences. We observe that alignment quality tends to be high if the similarity score is over 0.6, so we use this value as the filtering threshConvS2S (Gehring et al., 2017). Compared with RNNs that maintain a hidden state of the entire past, convolution operations can be fully parallelized during training. ConvS2S integrates the convolution operations into the sequence-to-sequence framework, which not only improves computation effi"
2020.wat-1.5,P17-2061,1,0.890242,"longer sequences due to the quadratic complexity. Lightconv builds dynamic convolutions to predict a different kernel at each time-step rather than the entire sequence, which drastically reduces the number of parameters. 1 https://github.com/didi/iwslt2020_ open_domain_translation 2 https://dumps.wikimedia.org/ 3 https://tatoeba.org/eng/ 4 https://creativecommons.org/licenses/ by/2.0/fr/ 65 old for most experiments. This results in 1.5M filtered out-of-domain parallel sentences which we leverage to train the NMT system jointly with ASPEC-JC dataset. We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation tech"
2020.wat-1.5,D18-2012,0,0.018691,"t al., 1994; Morita et al., 2015) for Japanese and stanford parser pku7 for Chinese. Sentences over 175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT"
2020.wat-1.5,chu-etal-2012-chinese,1,0.724409,"requently being used in Japanese-Chinese translation tasks (Song et al., 2020; Chen et al., 2020) because there are a large number of shared Chinese characters in Chinese and Japanese. Usually they not only share the character but also share the semantic function within a sentence, so pre-mapping Chinese characters to the target-side can help amplify the cross-lingual supervision. More precisely, for Kana characters in Japanese, we remain them with the same tokenization granularity whereas for Chinese characters, we first tokenize them into by characters, then use the character mapping table (Chu et al., 2012) to pre-map them to target-side. 3.5 Meta Ensemble 4 Preprocessing and Training Details We conduct tokenization by using Juman6 (Kurohashi et al., 1994; Morita et al., 2015) for Japanese and stanford parser pku7 for Chinese. Sentences over 175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model trainin"
2020.wat-1.5,chu-etal-2014-constructing,1,0.795141,"on the size of the training data. Thus, high quality augmented training dataset help ameliorate NMT. There exist just around 0.68M parallel sentences in ASPEC-JC, so we expect that extra parallel data can significantly improve the translation quality. We use Japanese-Chinese parallel datasets on other domains collected by IWSLT 2020 (Ansari et al., 2020).1 All the out-of-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER embeddings of two sentences. We observe that alignment quality tends to"
2020.wat-1.5,2020.acl-main.703,0,0.0257025,"vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT / FT (1) Different model capacities 11 BT / FT (1) 12 BT / FT (1) 13 BT / FT (1) Character mapping 14 Fine-tune pre-trained models 15 mBART25 16 mBART25 + BT / FT"
2020.wat-1.5,L16-1147,0,0.0234976,"gmentation by Filtering Out-of-domain Parallel Data NMT quality depends highly on the size of the training data. Thus, high quality augmented training dataset help ameliorate NMT. There exist just around 0.68M parallel sentences in ASPEC-JC, so we expect that extra parallel data can significantly improve the translation quality. We use Japanese-Chinese parallel datasets on other domains collected by IWSLT 2020 (Ansari et al., 2020).1 All the out-of-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER emb"
2020.wat-1.5,2020.tacl-1.47,0,0.0329397,"Missing"
2020.wat-1.5,N19-1423,0,0.0108897,"175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT"
2020.wat-1.5,2020.lrec-1.454,1,0.840481,"esults in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT / FT (1) Different model capacities 11 BT / FT (1) 12 BT / FT (1) 13 BT / FT (1) Character mapping 14 Fine-tune pre-trained models 15 mBART25"
2020.wat-1.5,D18-1045,0,0.0160507,"We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation technique for NMT. For monolingual corpora, we use the Japanese sentences in ASPECJE as in-domain Japanese monolingual data, where 3M Japanese sentences are used to augment the ASPEC-JC corpus. We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available. We neither do not consider using other outof-domain monolingual data. For the translation direction of ja→zh, we name it forward translation because of the absence of the target-side monolingual data, which means we use the pre-trained syste"
2020.wat-1.5,D15-1276,1,0.756555,"and Japanese. Usually they not only share the character but also share the semantic function within a sentence, so pre-mapping Chinese characters to the target-side can help amplify the cross-lingual supervision. More precisely, for Kana characters in Japanese, we remain them with the same tokenization granularity whereas for Chinese characters, we first tokenize them into by characters, then use the character mapping table (Chu et al., 2012) to pre-map them to target-side. 3.5 Meta Ensemble 4 Preprocessing and Training Details We conduct tokenization by using Juman6 (Kurohashi et al., 1994; Morita et al., 2015) for Japanese and stanford parser pku7 for Chinese. Sentences over 175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART:"
2020.wat-1.5,N19-4009,0,0.0162351,"ayers and 1 decoder layer. “ls”, “dp” and “ffhd” represents label smoothing, dropout and feed-forward hidden dimension, respectively. Model settings without declarations of “ls”, “dp” and “ffhd” are set to “ls(0.1)”, “dp(0.3)” and “ffhd(4,096)” for transformer-big and default for other S2S frameworks. “Threshold” means the filtering threshold value for LASER embedding. for fine-tuning mBART25. For parts of the outof-domain Chinese sentences that are in traditional Chinese, we transfer Traditional Chinese characters to Simplified Chinese ones.8 We conduct all the experiments by using Fairseq9 (Ott et al., 2019), an open source sequenceto-sequence framework implementation. Most systems are set to the Transformer-big setting except those built up by other architectures or different model capacities. In particular, our model has a 6layer encoder and decoder, a hidden size of 1,024, a feed-forward hidden layer size of 4,096, batch-size of 2,048, dropout rate of 0.3 and 16 attention heads. For LSTM, we also use a 6-layer encoder and decoder architecture. For ConvS2S and Lightconv, we use the default settings in Fairseq. All the systems are early stopped if BLEU does not improve for continuous 50,000 step"
2020.wat-1.5,tiedemann-2012-parallel,0,0.0107677,"g information for longer periods of time on the sequence. 3.2 Data Augmentation by Filtering Out-of-domain Parallel Data NMT quality depends highly on the size of the training data. Thus, high quality augmented training dataset help ameliorate NMT. There exist just around 0.68M parallel sentences in ASPEC-JC, so we expect that extra parallel data can significantly improve the translation quality. We use Japanese-Chinese parallel datasets on other domains collected by IWSLT 2020 (Ansari et al., 2020).1 All the out-of-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score"
2020.wat-1.5,P02-1040,0,0.106558,"Missing"
2020.wat-1.5,D19-1071,0,0.0181312,"l., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT / FT (1) Different model capacities 11 BT / FT (1) 12 BT / FT (1) 13 BT / FT (1) Character mapping 14 Fine-tune pre-trained"
2020.wat-1.5,2021.eacl-main.115,0,0.0555276,"Missing"
2020.wat-1.5,P16-1009,0,0.239855,"y with ASPEC-JC dataset. We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation technique for NMT. For monolingual corpora, we use the Japanese sentences in ASPECJE as in-domain Japanese monolingual data, where 3M Japanese sentences are used to augment the ASPEC-JC corpus. We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available. We neither do not consider using other outof-domain monolingual data. For the translation direction of ja→zh, we name it forward translation because of the absence of the target-side monolingual data, which means we us"
2020.wat-1.5,P16-1162,0,0.334612,"y with ASPEC-JC dataset. We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation technique for NMT. For monolingual corpora, we use the Japanese sentences in ASPECJE as in-domain Japanese monolingual data, where 3M Japanese sentences are used to augment the ASPEC-JC corpus. We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available. We neither do not consider using other outof-domain monolingual data. For the translation direction of ja→zh, we name it forward translation because of the absence of the target-side monolingual data, which means we us"
2020.wat-1.5,2020.acl-srw.37,1,0.912193,"ormal University (KyotoU+ECNU) to WAT 2020 (Nakazawa et al., 2020). We participate in APSEC JapaneseChinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation. 1 • We revisit and explore the trick of character mapping (Song et al., 2020; Chen et al., 2020) between Chinese and Japanese on ASPEC translation task. Although only our team participated in the ASPEC Japanese-Chinese translation task this year, BLEU results we report on the WAT official leader-board rank 1st both on ja→zh and zh→ja compared with all the previous submitted systems. 2 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has led to large improvements in machine translation quality when large parallel corpora are available for training. In this work, we revisit several existing NMT based techniques on ASPEC Japan"
2020.wnut-1.62,W19-1909,0,0.0434744,"Missing"
2020.wnut-1.62,D19-1371,0,0.0194133,"et al., 2020) The denoising autoencoder based on a bidirectional transformer encoder and a left-to-right transformer decoder. We employ two types of pre-trained models, BART-base14 and BART-large.15 BART-base consists of 12 transformer layers, 16 selfattention heads per layer, and a hidden size of 768. BART-large consists of 24 transformer layers, 16 self-attention heads per layer, and a hidden size of 1,024. The language models mentioned above are pretrained on corpora in the general domain such as the BookCorpus (Zhu et al., 2015) and English Wikipedia. Recent studies (Lee and Hsiang, 2019; Beltagy et al., 2019) have revealed that language models pre-trained on a domain-specific corpus achieve better performance in that domain. We employ the following three types of BERT models pre-trained on large-scale corpora of the medical domain and Twitter domain to build a classifier suitable for COVID-19 English Tweets. ALBERT (Lan et al., 2020) The transformer encoder pre-trained by multitask learning of masked language modeling and sentence order prediction. ALBERT has significantly fewer parameters than the traditional BERT architecture due to two parameter reduction techniques, factorized embedding parame"
2020.wnut-1.62,Q17-1010,0,0.0292819,"man, 2001). 2 Dev Test Informative Uninformative 3,303 3,697 472 528 944 1,056 Total 7,000 1,000 2,000 Table 1: Statistics of the dataset. Introduction 1 Train WNUT-2020 Shared Task 2 In the shared task (Nguyen et al., 2020), systems are required to classify whether a COVID-19 English Tweet is informative or not. Such informative Tweets provide information about recovered, suspected, confirmed and death cases as well as location or travel history of the cases. The 10,000 COVID-19 English Tweets3 shown in Table 1 have been released for the shared task. The baseline system is based on fastText (Bojanowski et al., 2017). Systems are evaluated by accuracy, precision, recall and F1 score, and are ranked by F1 score, which is the main metric. Note that the latter three metrics are calculated for the informative class only. 3 IDSOU System We first introduce each base model in Section 3.1 and each loss function in Section 3.2. We then introduce the ensemble model in Section 3.3. Finally, Section 3.4 describes the implementation details. Institute for Datability Science, Osaka University http://noisy-text.github.io/2020/ 3.1 Base Models Recently, the fine-tuning approach for pre-trained language models (Devlin et"
2020.wnut-1.62,2020.acl-main.747,0,0.0936734,"Missing"
2020.wnut-1.62,N19-1423,0,0.117509,"he F1 score. 1 2 The spread of the COVID-19 is causing fear and panic to people around the world. To monitor the COVID-19 outbreaks in real-time, SNS analysis such as Twitter is attracting much attention. Although there are 4 million COVID-19 English Tweets posted daily on Twitter (Lamsal, 2020), most of them are uninformative. Against this background, WNUT-2020 held a shared task2 (Nguyen et al., 2020) to automatically identify whether a COVID-19 English Tweet is informative or not. Our system employs an ensemble approach based on pre-trained language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tas"
2020.wnut-1.62,2020.acl-main.703,0,0.219204,"To monitor the COVID-19 outbreaks in real-time, SNS analysis such as Twitter is attracting much attention. Although there are 4 million COVID-19 English Tweets posted daily on Twitter (Lamsal, 2020), most of them are uninformative. Against this background, WNUT-2020 held a shared task2 (Nguyen et al., 2020) to automatically identify whether a COVID-19 English Tweet is informative or not. Our system employs an ensemble approach based on pre-trained language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tasks. Finally, we ensemble 48 classifiers based on 16 pre-trained language models and 3 loss functio"
2020.wnut-1.62,2020.acl-main.45,0,0.227755,"language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tasks. Finally, we ensemble 48 classifiers based on 16 pre-trained language models and 3 loss functions with a random forest classifier (Breiman, 2001). 2 Dev Test Informative Uninformative 3,303 3,697 472 528 944 1,056 Total 7,000 1,000 2,000 Table 1: Statistics of the dataset. Introduction 1 Train WNUT-2020 Shared Task 2 In the shared task (Nguyen et al., 2020), systems are required to classify whether a COVID-19 English Tweet is informative or not. Such informative Tweets provide information about recovered, suspected, confirmed and death c"
2020.wnut-1.62,2021.ccl-1.108,0,0.0881054,"Missing"
2020.wnut-1.62,2020.acl-main.33,1,0.877017,"ystem employs an ensemble approach based on pre-trained language models. Such pretrained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Conneau et al., 2020; Lewis et al., 2020) have achieved high performance in various text classification tasks (Wang et al., 2019). In addition, we employ domain-specific pre-trained language models (Lee et al., 2019; Alsentzer et al., 2019; M¨uller et al., 2020) to build models suitable for COVID-19 and Twitter domains. Each model is optimized for three types of loss functions, cross-entropy, negative supervision (Ohashi et al., 2020), and Dice similarity coefficient (Li et al., 2020), which are useful for various text classification tasks. Finally, we ensemble 48 classifiers based on 16 pre-trained language models and 3 loss functions with a random forest classifier (Breiman, 2001). 2 Dev Test Informative Uninformative 3,303 3,697 472 528 944 1,056 Total 7,000 1,000 2,000 Table 1: Statistics of the dataset. Introduction 1 Train WNUT-2020 Shared Task 2 In the shared task (Nguyen et al., 2020), systems are required to classify whether a COVID-19 English Tweet is informative or not. Such informative Tweets provide informatio"
2021.acl-long.226,P19-1309,0,0.114523,"y-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models can be categorized into tw"
2021.acl-long.226,Q19-1038,0,0.157321,"y-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models can be categorized into tw"
2021.acl-long.226,W19-4330,0,0.091064,"tion of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models"
2021.acl-long.226,2020.acl-main.747,0,0.0967538,"Missing"
2021.acl-long.226,2020.acl-main.536,0,0.19767,"tive. In this section, we revisit the previous work in these 2 categories, which is crucial for designing a cross-lingual representation model. Generative Tasks. Generative tasks measure a generative probability between predicted tokens and real tokens by training a language model. BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sentence. For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al., 2019). The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019). Sequence to sequence methods (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence. The intermediate state between the encoder and the decoder are extracted as sentence representations. Particularly, the cross-lingual sen"
2021.acl-long.226,D15-1131,0,0.174526,"(Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines. In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, MultiFit (Eisenschlos et al., 2019). For the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al., 2019), and multilingual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines. Note that T-LASER and LASER are trained on 223M parallel sentences on 93 languages, which uses significantly more training data than ours. We also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations. 4.3 MLDoc: Zero-shot Cross-lingual Document Classification The MLDoc task, which consists of news documents given in 8 different languages, is a benchmark to evaluate cross-lingual sentence representations."
2021.acl-long.226,N19-1423,0,0.334077,"task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages."
2021.acl-long.226,D19-1572,0,0.278223,"-lingual sentence representation learning methods.10 4.2 Baselines For evaluation on the MLDoc benchmark, we use the state-of-the-art fixed-dimensional word representation methods MultiCCA+CNN method (Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines. In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, MultiFit (Eisenschlos et al., 2019). For the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al., 2019), and multilingual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines. Note that T-LASER and LASER are trained on 223M parallel sentences on 93 languages, which uses significantly more training data than ours. We also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations. 4.3 MLDo"
2021.acl-long.226,W18-6317,0,0.15151,"ion learning (Mikolov et al., 2013). Subsequently, contrastive tasks gradually emerged in many NLP tasks in various ways: negative sampling in knowledge graph embedding learning (Bordes et al., 2013; Wang et al., 2014), next sentence prediction in BERT (Devlin et al., 2019), token-level discrimination in ELECTRA (Clark et al., 2020), sentence-level discrimination in DeCLUTR (Giorgi et al., 2020), and hierarchical contrastive learning in HICTL (Wei et al., 2020). For the cross-lingual sentence representation training, typical ones include using correct and wrong translation pairs introduced by Guo et al. (2018); Yang et al. (2019); Chidambaram et al. (2019); Feng et al. (2020) or utilizing similarities between sentence pairs by introducing a regularization term (Yu et al., 2018). As another advantage, contrastive methods have proven to be more efficient than generative methods (Clark et al., 2020). Inspired by previous work, for our lightweight model, we propose a robust sentence-level contrastive task by leveraging similarity relationships arising from translation pairs. 3 Methodology We perform cross-lingual sentence representation learning by a lightweight dual-transformer framework. Concerning t"
2021.acl-long.226,2020.findings-emnlp.372,0,0.0359469,"., 2019) and XLM (Conneau and Lample, 2019) require being fine-tuned globally which results in a significant overhead of its own. On the other hand, fixeddimensional methods like LASER (Artetxe and Schwenk, 2019b) fix the sentence representations during the pre-training phase, and subsequently the fine-tuning for specific downstream tasks without back-propagating to the pre-trained model will be extremely computationally-lite. Lightweight models have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020). However, the lightweight models for the latter group have not been explored before, which may have a more promising future for deploying task-specific fine-tuning onto edge devices. In this work, we propose a variety of training tasks for a lightweight cross-lingual sentence model while retaining the robustness. To improve the computational efficiency, we utilize a lightweight dual-transformer architecture with just 2 layers, significantly decreasing the memory consumption and accelerating the training to further improve the efficiency. Our mode"
2021.acl-long.226,W19-4309,0,0.0206556,"stive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models can be categorized into two classes. On one h"
2021.acl-long.226,P18-1007,0,0.0235862,"build our PyTorch implementation on top of HuggingFace’s Transformers library (Wolf et al., 2020). Training data is composed of the ParaCrawl8 (Ba˜no´ n et al., 2020) v5.0 datasets for each language pair. We experiment on English– French, English–German, English–Spanish and English–Italian. We filter the parallel corpus for each language pair by removing sentences that cover tokens out of 2 languages. Raw and filtered number of the parallel sentences for each pair are shown in Table 2. 10,000 sentences are selected for validation on each language pair. We tokenize sentences by SentencePiece9 (Kudo, 2018) and build a shared vocabulary with the size of 50k for each language pair. For each encoder, we use the transformer architecture with 2 hidden layers, 8 attention heads, hidden size of 512 and filter size of 1,024, and the parameters of two encoders are shared with each other. The sentence representations generated are 512 dimensional. For the training phase, it minimizes the weighted losses for our proposed crosslingual language model jointly with 2 auxiliary tasks. We train 12 epochs for each language pair (30 epochs for English-Italian because of nearly half number of parallel sentences) w"
2021.acl-long.226,W15-1521,0,0.213113,"LDoc benchmark, we use the state-of-the-art fixed-dimensional word representation methods MultiCCA+CNN method (Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines. In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, MultiFit (Eisenschlos et al., 2019). For the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al., 2019), and multilingual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines. Note that T-LASER and LASER are trained on 223M parallel sentences on 93 languages, which uses significantly more training data than ours. We also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations. 4.3 MLDoc: Zero-shot Cross-lingual Document Classification The MLDoc task, which consists of"
2021.acl-long.226,D19-1410,0,0.14781,"training a language model. BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sentence. For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al., 2019). The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019). Sequence to sequence methods (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence. The intermediate state between the encoder and the decoder are extracted as sentence representations. Particularly, the cross-lingual sentence representation quality of LASER (Artetxe and Schwenk, 2019b) benefits from a massively multilingual machine translation task covering 93 languages. In our work, we revisit the BERT-style training tasks and introduce a novel 2903 generative loss enhanced by"
2021.acl-long.226,2020.emnlp-main.365,0,0.102774,"onneau and Lample, 2019) require being fine-tuned globally which results in a significant overhead of its own. On the other hand, fixeddimensional methods like LASER (Artetxe and Schwenk, 2019b) fix the sentence representations during the pre-training phase, and subsequently the fine-tuning for specific downstream tasks without back-propagating to the pre-trained model will be extremely computationally-lite. Lightweight models have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020). However, the lightweight models for the latter group have not been explored before, which may have a more promising future for deploying task-specific fine-tuning onto edge devices. In this work, we propose a variety of training tasks for a lightweight cross-lingual sentence model while retaining the robustness. To improve the computational efficiency, we utilize a lightweight dual-transformer architecture with just 2 layers, significantly decreasing the memory consumption and accelerating the training to further improve the efficiency. Our model uses significantly less nu"
2021.acl-long.226,D19-1071,0,0.0277913,"ies, which is crucial for designing a cross-lingual representation model. Generative Tasks. Generative tasks measure a generative probability between predicted tokens and real tokens by training a language model. BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sentence. For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al., 2019). The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019). Sequence to sequence methods (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence. The intermediate state between the encoder and the decoder are extracted as sentence representations. Particularly, the cross-lingual sentence representation quality of LASER (Artetxe and Schwenk, 2019b)"
2021.acl-long.226,W17-2619,0,0.135555,"ngual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during train"
2021.acl-long.226,W18-3023,0,0.281122,"ment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low"
2021.acl-long.226,L18-1560,0,0.21183,"uxiliary tasks to compensate for the learning bottleneck of lightweight transformer for generative tasks. Following the state-of-the-art fixeddimensional model LASER, we proceed to learn cross-lingual sentence representations from parallel sentences, where we employ 2-layer dualtransformer encoders to shrink the model architecture. By introducing the above-stated training tasks, we establish a computationally-lite framework for training cross-lingual sentence models. We evaluate the learned sentence representations on cross-lingual tasks including multilingual document classification (MLDoc) (Schwenk and Li, 2018) and XSR. Our results confirm the ability of our lightweight model to yield robust sentence representations. We also do a systematic study on the performance of our model in an ablative manner. The contributions of this work can be summarized as follows: • We implement fixed-dimensional crosslingual sentence representation learning in a lightweight model, achieving improved training efficiency and competitive performance of the learned sentence representations. • Our proposed novel generative and contrastive tasks allow cross-lingual sentence representation efficiently trainable by the lightwe"
2021.acl-long.226,2020.acl-main.195,0,0.0200492,"uire being fine-tuned globally which results in a significant overhead of its own. On the other hand, fixeddimensional methods like LASER (Artetxe and Schwenk, 2019b) fix the sentence representations during the pre-training phase, and subsequently the fine-tuning for specific downstream tasks without back-propagating to the pre-trained model will be extremely computationally-lite. Lightweight models have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020). However, the lightweight models for the latter group have not been explored before, which may have a more promising future for deploying task-specific fine-tuning onto edge devices. In this work, we propose a variety of training tasks for a lightweight cross-lingual sentence model while retaining the robustness. To improve the computational efficiency, we utilize a lightweight dual-transformer architecture with just 2 layers, significantly decreasing the memory consumption and accelerating the training to further improve the efficiency. Our model uses significantly less number of parameters"
2021.acl-srw.8,2020.acl-main.385,0,0.0316988,"ng. We explore three methods of supervising attention. Indirect attention supervision We learn the visual grounding using the final representations of grounded phrases and their aligned regions of interest. We train a binary classifier d that takes as input phrase embedding e as well as the representation xi of one of the regions in img, and predicts whether they align or not. We repeat this classification for every grounded phrase with every region. The loss is computed by 1 X Lossatt = LCE (zi , log d([e, xi ])), (5) nr Lossatt = Losstxt→img + Lossimg→txt . Semi-direct attention supervision Abnar and Zuidema (2020) introduced a transformation of raw attention called attention rollout and showed that it gives a more accurate quantification of how much information one token contains about another token than raw attention does. Therefore, we propose to replace the raw attention with the attention rollout in the direct supervision method. In other words, if we denote frollout(·) as the function that transforms raw attention vectors into attention rollout then our semi-direct attention lh supervision approach consists in replacing αij with frollout (α)lh ij in Equations (refeq6), (7) and (8). i where zi is t"
2021.acl-srw.8,D19-1542,0,0.118797,"g this direction and to improve a model’s reasoning abilities, we propose to further fine-tune a pre-trained model with the aim of learning visually grounded paraphrases (VGPs) (Chu et al., 2018; Otani et al., 2020). VGPs are two phrasal expressions that describe the same visual concept in an image. As shown in Figure 1, we fine-tune a model based on VGPs with three different tasks simultaneously as a multitask learning problem: an image description identification task (§2.2.1), a VGP classification task (§2.2.2), and an attention supervision task (§2.2.3). The first two tasks are inspired by Arase and Tsujii (2019), who showed that injecting semantic relations between a sentence pair can improve a BERT model’s performance on several downstream tasks. We adapted them to make the model learn from both visual and linguistic elements. Input The input of the fine-tuning process is composed of 1) an image, and 2) a pair of captions, 2.2.1 Image Description Identification In this task, a Softmax classifier f takes the output x0 of the Transformer corresponding to [CLS] and predicts which of the captions corresponds to the image (c1 , c2 , or both). The loss is given by Lossid = LCE (y, log f (x0 )), (1) where"
2021.acl-srw.8,C18-1295,1,0.915859,"Fine-tuning To provide guided attention supervision in visionand-language models, we devise a multi-task finetuning method that aims to improve the model’s ability to understand complex semantic relations (e.g. paraphrases) and align visual with linguistic elements. Li et al. (2020) hinted the importance of attention-based vision-and-language model’s ability to map entity-words to corresponding image regions. Following this direction and to improve a model’s reasoning abilities, we propose to further fine-tune a pre-trained model with the aim of learning visually grounded paraphrases (VGPs) (Chu et al., 2018; Otani et al., 2020). VGPs are two phrasal expressions that describe the same visual concept in an image. As shown in Figure 1, we fine-tune a model based on VGPs with three different tasks simultaneously as a multitask learning problem: an image description identification task (§2.2.1), a VGP classification task (§2.2.2), and an attention supervision task (§2.2.3). The first two tasks are inspired by Arase and Tsujii (2019), who showed that injecting semantic relations between a sentence pair can improve a BERT model’s performance on several downstream tasks. We adapted them to make the mode"
2021.acl-srw.8,2021.ccl-1.108,0,0.0334939,"Missing"
2021.acl-srw.8,N19-1423,0,0.0256251,"ttention mechanisms used by the models to learn to associate phrases with their visual grounding in the image have been conducted. In this work, we investigate how supervising attention directly to learn visual grounding can affect the behavior of such models. We compare three different methods on attention supervision and their impact on the performances of a state-of-the-art visually grounded language model on two popular vision-and-language tasks. 1 Introduction The introduction of Transformers (Vaswani et al., 2017) has been a major component of the success of pre-trained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020) which achieved new records in many natural language processing tasks. The same mechanism has been adapted to create models (Su et al., 2020; Chen et al., 2020; Li et al., 2019; Lu et al., 2019, 2020; LXM) that can now tackle vision-andlanguage tasks with impressive performances. A large body of research (Clark et al., 2019; Kovaleva et al., 2019) has been dedicated to understanding what attention heads learn during the pre-training of language models. Liu et al. (2016) have even shown how providing attention heads with guidance can impro"
2021.acl-srw.8,D14-1086,0,0.0302021,"wo worse performance are attained by the method without attention and the Indirect, which does not seem to improve the model’s ability to answer the question. Both the Direct and Semidirect methods, which use the attention heads as classifiers, fare better with a slight advantage for the Semi-direct method. 3.2.2 Referring Expression Comprehension The objective of this task is to locate the object in the image that is designated by the input phrase. The input is constituted of a referring expression and an image that contains the object that is being referred to. We used the RefCOCO+ dataset (Kazemzadeh et al., 2014) (141k expressions for 50k referred objects in 20k images). The dataset contains two test sets, where testA contains images with multiple persons and testB with multiple objects. We report results both with ground-truth RoIs and with the bounding boxes detected by Yu 3.2.3 Visualization To gain more insights into what models learn with the different attention supervision methods, we visualize attention heads using Bertviz2 (Vig, 2019). By zooming in on individual attention heads, we noticed that when the model was fine-tuned using 2 84 https://github.com/jessevig/bertviz either the Semi-direct"
2021.acl-srw.8,P18-1238,0,0.0671985,"Missing"
2021.acl-srw.8,D19-1445,0,0.0229675,"uage model on two popular vision-and-language tasks. 1 Introduction The introduction of Transformers (Vaswani et al., 2017) has been a major component of the success of pre-trained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020) which achieved new records in many natural language processing tasks. The same mechanism has been adapted to create models (Su et al., 2020; Chen et al., 2020; Li et al., 2019; Lu et al., 2019, 2020; LXM) that can now tackle vision-andlanguage tasks with impressive performances. A large body of research (Clark et al., 2019; Kovaleva et al., 2019) has been dedicated to understanding what attention heads learn during the pre-training of language models. Liu et al. (2016) have even shown how providing attention heads with guidance can improve performance on neural machine translation. On the other hand, the internal behaviors of vision-and-language models have attracted less interest from the research community. Li et al. (2020) have shown some attention heads in vision-andlanguage models are able to map entities to image 2 Our Method We use a state-of-the-art pre-trained vision-andlanguage model on which we propose multi-task fine-tunin"
2021.acl-srw.8,P19-3007,0,0.0172733,". The input is constituted of a referring expression and an image that contains the object that is being referred to. We used the RefCOCO+ dataset (Kazemzadeh et al., 2014) (141k expressions for 50k referred objects in 20k images). The dataset contains two test sets, where testA contains images with multiple persons and testB with multiple objects. We report results both with ground-truth RoIs and with the bounding boxes detected by Yu 3.2.3 Visualization To gain more insights into what models learn with the different attention supervision methods, we visualize attention heads using Bertviz2 (Vig, 2019). By zooming in on individual attention heads, we noticed that when the model was fine-tuned using 2 84 https://github.com/jessevig/bertviz either the Semi-direct or Direct attention supervision methods, every grounded entity text token attributes more attention to image tokens to image tokens corresponding to the visual grounding of the entity. We also observed that even though the Direct method seemed to have an uniform impact on all attention heads in every layer, with the Semidirect method attention heads displayed varying attention patterns across different layers. A possible explanation"
2021.acl-srw.8,2020.acl-main.469,0,0.154717,"els (Su et al., 2020; Chen et al., 2020; Li et al., 2019; Lu et al., 2019, 2020; LXM) that can now tackle vision-andlanguage tasks with impressive performances. A large body of research (Clark et al., 2019; Kovaleva et al., 2019) has been dedicated to understanding what attention heads learn during the pre-training of language models. Liu et al. (2016) have even shown how providing attention heads with guidance can improve performance on neural machine translation. On the other hand, the internal behaviors of vision-and-language models have attracted less interest from the research community. Li et al. (2020) have shown some attention heads in vision-andlanguage models are able to map entities to image 2 Our Method We use a state-of-the-art pre-trained vision-andlanguage model on which we propose multi-task fine-tuning methods focusing on attention supervision. After this proposed fine-tuning, we judge the success of our approach by further fine-tuning the model on downstream tasks of visual question answering and referring expressions, and evaluating it. We propose a fine-tuning approach after an initial pretraining step on a large unlabelled dataset because we believe the model would benefit fro"
2021.acl-srw.8,C16-1291,0,0.0740924,"en a major component of the success of pre-trained language models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020) which achieved new records in many natural language processing tasks. The same mechanism has been adapted to create models (Su et al., 2020; Chen et al., 2020; Li et al., 2019; Lu et al., 2019, 2020; LXM) that can now tackle vision-andlanguage tasks with impressive performances. A large body of research (Clark et al., 2019; Kovaleva et al., 2019) has been dedicated to understanding what attention heads learn during the pre-training of language models. Liu et al. (2016) have even shown how providing attention heads with guidance can improve performance on neural machine translation. On the other hand, the internal behaviors of vision-and-language models have attracted less interest from the research community. Li et al. (2020) have shown some attention heads in vision-andlanguage models are able to map entities to image 2 Our Method We use a state-of-the-art pre-trained vision-andlanguage model on which we propose multi-task fine-tuning methods focusing on attention supervision. After this proposed fine-tuning, we judge the success of our approach by further"
2021.acl-srw.9,W16-3210,0,0.0342243,"Missing"
2021.acl-srw.9,D15-1166,0,0.0247773,"dle layers between the object-to-frame layer and frameto-video layer in the original HAN. Experiments on the VATEX dataset (Wang et al., 2019) show that our VMT system achieves 35.86 corpus-level BLEU-4 score on the VATEX test set, yielding a 0.51 score improvement over the single model of the SOTA method (Hirasawa et al., 2020). 2 features and obtain ordered motion representations M ∗ , represented as: M ∗ = PE(M ) Target Decoder. The sentence embedding U from the source language encoder and the ordered motion embedding M∗ from the motion encoder are processed using two attention mechanisms (Luong et al., 2015): ru,t = Attentionu,t (ht−1 , U ) (2) ∗ rm,t = Attentionm,t (ht−1 , M ) (3) where Attention denotes a standard attention block, ht−1 denotes the hidden state at the previous decoding time step. Text representations ru,t and motion representations rm,t are allocated by another attention layer to obtain a contextual vector rc,t at decoding time step t. The contextual vector is fed into a GRU layer for decoding: VMT with Spatial HAN The overview of the proposed model is presented in Figure 2, which consists of components in the VMT baseline model (Hirasawa et al., 2020) and our proposed spatial H"
2021.acl-srw.9,W16-2346,0,0.0599618,"Missing"
2021.acl-srw.9,W18-6402,0,0.0197146,"5; Wu et al., 2016) have achieved high performance for domains where there is less ambiguity in data such as the newspaper domain. For some other domains, especially real-time domains such as spoken language or sports commentary, the verb and the noun sense ambiguity largely affects the translation quality. To solve the ambiguity problem, multimodal machine translation (MMT) (Specia et al., 2016) focuses on incorporating visual data as auxiliary information, where the spatiotemporal contextual information in the visual data helps reduce the ambiguity of nouns or verbs in the source text data (Barrault et al., 2018). Previous MMT studies mainly focus on imageguided machine translation (IMT) task (Zhao et al., 2020; Elliott et al., 2016). However, videos are better information sources than images because one 87 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 87–92 August 5–6, 2021. ©2021 Association for Computational Linguistics One person… teasing cats… Word Embeddings Text Encoder Text Attention 3D ConvNet Motion Encoder Motion Atten"
2021.acl-srw.9,D18-1325,0,0.0285582,"anguage Processing: Student Research Workshop, pages 87–92 August 5–6, 2021. ©2021 Association for Computational Linguistics One person… teasing cats… Word Embeddings Text Encoder Text Attention 3D ConvNet Motion Encoder Motion Attention Faster R-CNN Target Decoder ⼀个⼈… 逗两只猫… Spatial HAN Figure 2: The proposed model with spatial HAN. The text encoder and the motion encoder are the same as those in the VMT baseline model. problems by using both motion and spatial representations in a video. To obtain spatial representations efficiently, we propose to use a hierarchical attention network (HAN) (Werlen et al., 2018) to model the spatial information from object-level to video-level, thus we call it the spatial HAN module. Additionally, to obtain a better contextual spatial information, we add several kinds of middle layers between the object-to-frame layer and frameto-video layer in the original HAN. Experiments on the VATEX dataset (Wang et al., 2019) show that our VMT system achieves 35.86 corpus-level BLEU-4 score on the VATEX test set, yielding a 0.51 score improvement over the single model of the SOTA method (Hirasawa et al., 2020). 2 features and obtain ordered motion representations M ∗ , represent"
2021.acl-srw.9,2020.eamt-1.12,1,0.786724,"as the newspaper domain. For some other domains, especially real-time domains such as spoken language or sports commentary, the verb and the noun sense ambiguity largely affects the translation quality. To solve the ambiguity problem, multimodal machine translation (MMT) (Specia et al., 2016) focuses on incorporating visual data as auxiliary information, where the spatiotemporal contextual information in the visual data helps reduce the ambiguity of nouns or verbs in the source text data (Barrault et al., 2018). Previous MMT studies mainly focus on imageguided machine translation (IMT) task (Zhao et al., 2020; Elliott et al., 2016). However, videos are better information sources than images because one 87 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 87–92 August 5–6, 2021. ©2021 Association for Computational Linguistics One person… teasing cats… Word Embeddings Text Encoder Text Attention 3D ConvNet Motion Encoder Motion Attention Faster R-CNN Target Decoder ⼀个⼈… 逗两只猫… Spatial HAN Figure 2: The proposed model with spatial HA"
2021.naacl-main.169,Q17-1010,0,0.010028,"04) to tokenize Japanese text. The performance of the emotional intensity estimation models is evaluated by the mean absolute error (MAE) and the quadratic weighted kappa (QWK). We evaluated the model using both the emotional intensity labels given by the subjective annotators (subjective labels) and the average of the emotional intensity labels given by the three objective annotators (objective labels). 12 Each writer provided 500 posts for the training set and 100 posts for the validation and test sets. 13 https://taku910.github.io/mecab/ • fastText+SVM vectorizes each word with fastText14 (Bojanowski et al., 2017) and estimates the emotional intensity with a Support Vector Machine based on their average vector. • BERT is a model that fine-tunes the pretrained BERT15 (Devlin et al., 2019) and estimates the emotional intensity as y = softmax(hW ), where h is a feature vector obtained for the [CLS] token of BERT. We investigate the performance of both BERT trained with subjective labels (Subj. BERT) and BERT trained with objective labels (Obj. 14 https://dl.fbaipublicfiles.com/ fasttext/vectors-crawl/cc.ja.300.bin.gz 15 https://huggingface.co/cl-tohoku/ bert-base-japanese-whole-word-masking 2100 MAE Subje"
2021.naacl-main.169,2020.lrec-1.194,0,0.145658,"(Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such as story generation, it is worth considering the difference between the emotions the write"
2021.naacl-main.169,C18-1179,0,0.0379437,"Missing"
2021.naacl-main.169,E17-2092,0,0.324587,"t, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such a"
2021.naacl-main.169,N19-1423,0,0.11642,"e given in a four-point scale (no, weak, medium, and strong). Our comparative study over subjective and objective labels demonstrates that readers may not well infer the emotions of the writers, especially of anger and trust. For example, even for posts written by the writer with a strong anger emotion, our readers (i.e., the annotators) did not assign the anger label at all to more than half of the posts with the subjective anger label. Overall, readers may tend to underestimate the writers’ emotional intensities. In addition, experimental results on emotional intensity estimation with BERT (Devlin et al., 2019) show that predicting the subjective labels is a more difficult task than predicting the objective ones. This large gap between the subjective and objective annotations implies the challenge in predicting the subjective emotional intensity for a machine learning model, which can be viewed as a “reader” of the posts. 2 Related Work To estimate the emotional intensity of the text, datasets labeled with Ekman’s six emotions (Ekman, 1992) and Plutchik’s eight emotions (Plutchik, 1980) has been constructed for languages such as English, as shown in Table 1. EmoBank4 (Buechel and Hahn, 2017), which"
2021.naacl-main.169,P06-2059,0,0.0515809,"1 vs. Reader 2 Reader 1 vs. Reader 3 Reader 2 vs. Reader 3 0.697 0.662 0.700 0.607 0.545 0.597 0.594 0.567 0.632 0.342 0.443 0.415 0.627 0.581 0.630 0.359 0.429 0.476 0.527 0.455 0.512 0.203 0.196 0.295 0.547 0.549 0.585 Writer vs. Writer vs. Writer vs. Writer vs. 0.622 0.633 0.624 0.683 0.461 0.526 0.450 0.536 0.423 0.432 0.459 0.498 0.348 0.339 0.396 0.441 0.363 0.386 0.374 0.401 0.333 0.361 0.380 0.433 0.394 0.442 0.467 0.514 0.089 0.153 0.134 0.132 0.439 0.465 0.463 0.515 Reader 1 Reader 2 Reader 3 Avg. Readers Table 2: Inter-annotator agreement by quadratic weighted kappa. Some datasets (Kaji and Kitsuregawa, 2006; Suzuki, 2019) are available in Japanese. However, these are sentences with sentiment polarity, and do not cover the various emotions dealt with in this study. Our study is the first to label Japanese texts with various emotional intensity. • 3: I fully agree with the label given. 3 • 0: I do not think the annotator seriously engaged for this post. • 2: I can find the relevance between the post and label. • 1: I hardly find the relevance between the post and label. Emotional Intensity Annotation 3.1 Annotating Subjective Labels We hired 50 participants via crowdsourcing service Lancers.8 Thos"
2021.naacl-main.169,W04-3230,0,0.263034,"owing the standard emotional intensity estimation models (Acheampong et al., 2020), we train the following three types of four-class classification models for each emotion. • BoW+LogReg employs Bag-of-Words to extract features and Logistic Regression to the estimate emotional intensity. Experimental Settings In this experiment, we divided the dataset12 into training set of 15,000 posts from 30 writers, validation set of 1,000 posts from 10 writers, and evaluation set of 1,000 posts from 10 writers. That is, there is no duplication of writers between the splits. We used MeCab (IPADIC-2.7.0)13 (Kudo et al., 2004) to tokenize Japanese text. The performance of the emotional intensity estimation models is evaluated by the mean absolute error (MAE) and the quadratic weighted kappa (QWK). We evaluated the model using both the emotional intensity labels given by the subjective annotators (subjective labels) and the average of the emotional intensity labels given by the three objective annotators (objective labels). 12 Each writer provided 500 posts for the training set and 100 posts for the validation and test sets. 13 https://taku910.github.io/mecab/ • fastText+SVM vectorizes each word with fastText14 (Boj"
2021.naacl-main.169,S17-1007,0,0.308327,"urprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such as story generation, it is worth co"
2021.naacl-main.169,W17-5205,0,0.0389243,"Missing"
2021.naacl-main.169,S18-1001,0,0.0278752,"Missing"
2021.naacl-main.169,L18-1030,0,0.0711005,"y Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In other applications such as story generation, it is worth considering the difference between"
2021.naacl-main.169,D13-1170,0,0.00413074,"bjective labels than the readers’. The large gap between the subjective and objective emotions implies the complexity of the mapping from a post to the subjective emotional intensities, which also leads to a lower performance with machine learning models. 1 Introduction Emotion analysis is one of the major NLP tasks with a wide range of applications, such as a dialogue system (Tokuhisa et al., 2008) and social media mining (Stieglitz and Dang-Xuan, 2013). Since emotion analysis has been actively studied, not only the classification of the sentiment polarity (positive or negative) of the text (Socher et al., 2013), but also more detailed emotion detection and emotional intensity estimation (Bostan and Klinger, 2018) have been attempted in recent years. Previous studies on emotion analysis use six emotions (anger, disgust, fear, joy, sadness, and surprise) by Ekman (1992), eight emotions (anger, disgust, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions th"
2021.naacl-main.169,S07-1013,0,0.730795,"), eight emotions (anger, disgust, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD model (Valence, Arousal, and Dominance) by Russell (1980). Table 1 lists datasets with emotional intensity.1 1 In this paper, the emotions of the text writers themselves are called subjective emotions, and the emotions that the readers receive from the text are called objective emotions. These existing emotion analysis datasets include subjective emotional intensity labels by the writers (Scherer and Wallbott, 1994) and objective ones by the readers (Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2007; Buechel and Hahn, 2017; Mohammad and Bravo-Marquez, 2017a; Mohammad and Kiritchenko, 2018; Bostan et al., 2020), whereas the latter is mainly done by, e.g., expert or crowdsourcing annotators. It depends on the applications whether the writer’s emotions or the reader’s ones to be estimated in NLP-based emotion analysis. For example, in a dialogue system, it is important to estimate the reader’s emotion because we want to know how the user feels in response to the system’s utterance. On the other hand, in applications such as social media mining, we want to estimate the writer’s emotion. In o"
2021.naacl-main.169,C08-1111,0,0.0603374,"at the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer’s subjective labels than the readers’. The large gap between the subjective and objective emotions implies the complexity of the mapping from a post to the subjective emotional intensities, which also leads to a lower performance with machine learning models. 1 Introduction Emotion analysis is one of the major NLP tasks with a wide range of applications, such as a dialogue system (Tokuhisa et al., 2008) and social media mining (Stieglitz and Dang-Xuan, 2013). Since emotion analysis has been actively studied, not only the classification of the sentiment polarity (positive or negative) of the text (Socher et al., 2013), but also more detailed emotion detection and emotional intensity estimation (Bostan and Klinger, 2018) have been attempted in recent years. Previous studies on emotion analysis use six emotions (anger, disgust, fear, joy, sadness, and surprise) by Ekman (1992), eight emotions (anger, disgust, fear, joy, sadness, surprise, trust, and anticipation) by Plutchik (1980), and VAD mod"
2021.wat-1.1,2020.acl-main.703,0,0.187967,"news-commentary corpus.14 This year we also encouraged participants to use any corpora from WMT 202015 and WMT 202116 involving Japanese, Russian, and English as long as it did not belong to the news commentary domain to prevent any test set sentences from being unintentionally seen during training. 2,049 2,050 Table 5: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are Nway parallel. also encouraged the use of monolingual corpora expecting that it would be for pre-trained NMT models such as BART/MBART (Lewis et al., 2020; Liu et al., 2020). In Table 5 we give statistics of the aforementioned corpora which we used for the organizer’s baselines. Note that the evaluation corpora for both domains are created from documents and thus contain document level meta-data. Participants were encouraged to use document level approaches. Note that we do not exhaustively list6 all available corpora here and participants were not restricted from using any corpora as long as they are freely available. 2.7 Partition train development test train development test train development test 8 http://www.phontron.com/kftt/ https://data"
2021.wat-1.1,C18-2019,0,0.0200002,"task and 4 systems for the Japanese→ English.74 On the whole, all the submitted systems are basically lexical-constraint-aware NMT models with lexically constrained decoding method, where the restricted target vocabulary is concatenated into source sentences and, during the beam search at inference time, the models generate translation outputs containing the target vocabulary. We observed that these techniques boost the final translation performance of the NMT models in the restricted translation task. For human evaluation, we conducted the sourcebased direct assessment (Cettolo et al., 2017; Federmann, 2018) and source-based contrastive assessment (Sakaguchi and Van Durme, 2018; Federmann, 2018), to have the top-ranked systems of each team appraised by bilingual human annotators. In the human evaluation campaign, we also include the human reference data. Table 20 reports the final automatic evaluation score and the human evaluation results. In both tasks, the systems from the team “NTT” are the most highly evaluated in all the submitted systems in the final score and the human evaluation, consistently. We also note that our designed automation metric is well correlated Flickr30kEnt-JP Japanese↔En"
2021.wat-1.1,W18-1819,0,0.0552279,"Missing"
2021.wat-1.1,2007.mtsummit-papers.63,0,0.0610664,"om OPUS. We Test set II : A pair of test and reference sentences and context data that are articles including test sentences. The references were automatically extracted from English newswire sentences and manually selected. Therefore, the quality of the references of test set II is better than that of test set I. The statistics of JIJI Corpus are shown in Table 2. The definition of data use is shown in Table 3. Participants submit the translation results of one or more of the test data. The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from (Utiyama and Isahara, 2007). 2.5 ALT and UCSY Corpus The parallel data for Myanmar-English translation tasks at WAT2021 consists of two corpora, the ALT corpus and UCSY corpus. • The ALT corpus is one part from the Asian Language Treebank (ALT) project (Riza et al., 2016), consisting of twenty thousand Myanmar-English parallel sentences from news articles. • The UCSY corpus (Yi Mon Shwe Sin and Khin Mar Soe, 2018) is constructed by the NLP Lab, University of Computer Studies, 3 http://opus.nlpl.eu/ http://www.statmt.org/wmt20/ 5 Software Domain Evaluation Splits 4 3 Task Use Training Test set I Japanese to English Test"
2021.wat-1.20,W18-6402,0,0.0223804,"isual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has achieved state-of-the-art translation performance. However, there remain numerous situations where textual context alone is insufficient for correct translation, such as in the presence of ambiguous words and grammatical gender. Therefore, researchers in this field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the translation performance"
2021.wat-1.20,Q17-1010,0,0.0161645,"ject class instead of only the object class. We take these visual concepts to represent the image regions. We set each image labeled with 36 visual concepts of image regions, which are space-separated phrases. For the words, we lowercase and tokenize the source English sentences via the Moses toolkit.3 The soft alignment is a similarity matrix filled with the cosine similarity between source words and visual concepts. To avoid unknown words, we convert the words and concepts into subword units using the byte pair encoding (BPE) model (Sennrich et al., 2016). Subsequently, we utilize fastText (Bojanowski et al., 2017) to learn subword embeddings. We use a pre-trained model4 containing two million word vectors trained with subword information on Common Crawl (600B tokens). The source subword embeddings can be generated directly, whereas the generation of visual concept embeddings should take an average of the embeddings of all constituent subwords because they are phrases. As shown in Figure 1, source subwords are represented by W = {w1 , w2 , w3 , · · · , wn }, and the visual concepts are represented by C = {c1 , c2 , c3 , · · · , c36 }. These embeddings provide a mapping function from a subword to a 300-d"
2021.wat-1.20,W18-6439,0,0.118929,"e the |R |and |H |represent the length of source words and the numbers of image regions: n and 36; the CONCAT is a concatenation operator. 175 H Atxt h1 Bi-directional RNN man in red shirt watches dog on an agility course . polo (yt-1) GRU (3) GRU (2) hn GRU (1) RoI r3 en zt hn-1 rouge (yt) Aimg Halign agilité ct CONCAT r2 homme Text-attention a3txt h3 R r1 un h2 . Image-attention a3img &lt;eos&gt; r35 r36 Figure 2: The TMEKU system. 2.3 (1) img T eimg ) tanh(U img st t,j = (V Decoder To generate target word yt at time step t, a hidden (1) state proposal st is computed in the first cell of deepGRU (Delbrouck and Dupont, 2018) (GRU (1)) by function fgru1 (yt−1 , st−1 ). The function considers the previously emitted target word yt−1 and generated hidden state st−1 as follows. (1) st = (1 − ξˆt ) s˙ t + ξˆt st−1 ˆ γt = σ(Wγ EY [yt−1 ] + Uγ st−1 ) ξˆt = σ(Wξ EY [yt−1 ] + Uξ st−1 ) where Wξ , Uξ , Wγ , Uγ , W , and U are training parameters, and EY is the target word embedding. 2.3.1 Text-Attention At time step t, the text-attention focuses on every textual annotation atxt in Atxt and assigns an ati tention weight. The textual context vector zt is generated as follows. (1) img αt,j = softmax(eimg t,j ) ct = img img αt,"
2021.wat-1.20,W17-4718,0,0.0171891,"ween the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has achieved state-of-the-art translation performance. However, there remain numerous situations where textual context alone is insufficient for correct translation, such as in the presence of ambiguous words and grammatical gender. Therefore, researchers in this field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the"
2021.wat-1.20,2020.lrec-1.518,0,0.035343,"K, K 0 ∈ Rn×m and b, b0 ∈ Rn are the training parameters. 5 https://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf 176 To ensure that both representations have their own projections to compute the candidate probabilities, a textual GRU block and visual GRU block (Delbrouck and Dupont, 2018) obtained as below. bvt = fght (Wbv st ) (2) btt = fght (Wbt st ) t v yt ∼ pt = softmax(Wproj btt + Wproj bvt ), t ,Wv where Wbv , Wbt , Wproj proj are training parameters. 3 Experiments 3.1 Dataset Firstly, we conducted experiments for the En→Ja task using the official Flickr30kEnt-JP dataset (Nakayama et al., 2020), which was extended from the Flickr30k (Young et al., 2014) and Flickr30k Entities (Plummer et al., 2017) datasets, where manual Japanese translations were newly added. For training and validation, we used the Flickr30kEnt-JP dataset6 for Japanese sentences, the Flickr30k Entities dataset7 for English sentences, and the Flickr30k dataset8 for images. They were sharing the same splits of training and validation data made in Flickr30k Entities. For test data, we used the officially provided data of the Flickr30kEnt-JP task, and their corresponding images were in the Flickr30k dataset. Note that"
2021.wat-1.20,P02-1040,0,0.109413,"m; word embedding to 200dim; batch size to 32; beam size to 12; text dropout to 0.3; image region dropout to 0.5; dropout of source RNN hidden states to 0.5; and blocks btt and bvt to 0.5. Specifically, the textual annotation Atxt was 800dim, which was consistent with H. Further, the visual annotation Aimg was 4,096-dim by a concatenation of R and Halign , where R was 2,048-dim and Halign was 2,048-dim by a linear transformation from 800-dim. We trained the model using stochastic gradient descent with ADAM (Kingma and Ba, 2015) and a learning rate of 0.0004. We stopped training when the BLEU (Papineni et al., 2002) score did not improve for 20 evaluations on the validation set, 11 177 https://taku910.github.io/mecab/ Model Baseline NMT Baseline MNMT TMEKU System v.s. baseline NMT v.s. baseline MNMT Ensemble (top 10 models) Test NMT baseline by BLEU scores of 0.86 and outperformed the MNMT baseline by BLEU scores of 0.69 on the official test set. Our TMEKU system achieved significant improvement over both the NMT and MNMT baselines. Moreover, the result of ensembling the top 10 models has achieved the first place in the ranking of this task. We also participated in the Ambiguous MSCOCO task on the En→Ja"
2021.wat-1.20,P16-1162,0,0.0113637,"concept consisting of an attribute class followed by an object class instead of only the object class. We take these visual concepts to represent the image regions. We set each image labeled with 36 visual concepts of image regions, which are space-separated phrases. For the words, we lowercase and tokenize the source English sentences via the Moses toolkit.3 The soft alignment is a similarity matrix filled with the cosine similarity between source words and visual concepts. To avoid unknown words, we convert the words and concepts into subword units using the byte pair encoding (BPE) model (Sennrich et al., 2016). Subsequently, we utilize fastText (Bojanowski et al., 2017) to learn subword embeddings. We use a pre-trained model4 containing two million word vectors trained with subword information on Common Crawl (600B tokens). The source subword embeddings can be generated directly, whereas the generation of visual concept embeddings should take an average of the embeddings of all constituent subwords because they are phrases. As shown in Figure 1, source subwords are represented by W = {w1 , w2 , w3 , · · · , wn }, and the visual concepts are represented by C = {c1 , c2 , c3 , · · · , c36 }. These em"
2021.wat-1.20,W16-2346,0,0.0443301,"Missing"
2021.wat-1.20,2020.wat-1.7,1,0.745616,"field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the translation performance on the En→Ja task. In this study, we apply our system (Zhao et al., 2021) for the MMT task on the En→Ja language pair, which is called TMEKU system. This system is designed to translate a source word into a target word, focusing on a relevant image region. To guide the model to translate certain words based on certain image regions, explicit alignment over source words and image regions is needed. We propose to generate soft alignment of word-region based on cosine similarit"
2021.wat-1.20,Q14-1006,0,0.0188477,"ps://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf 176 To ensure that both representations have their own projections to compute the candidate probabilities, a textual GRU block and visual GRU block (Delbrouck and Dupont, 2018) obtained as below. bvt = fght (Wbv st ) (2) btt = fght (Wbt st ) t v yt ∼ pt = softmax(Wproj btt + Wproj bvt ), t ,Wv where Wbv , Wbt , Wproj proj are training parameters. 3 Experiments 3.1 Dataset Firstly, we conducted experiments for the En→Ja task using the official Flickr30kEnt-JP dataset (Nakayama et al., 2020), which was extended from the Flickr30k (Young et al., 2014) and Flickr30k Entities (Plummer et al., 2017) datasets, where manual Japanese translations were newly added. For training and validation, we used the Flickr30kEnt-JP dataset6 for Japanese sentences, the Flickr30k Entities dataset7 for English sentences, and the Flickr30k dataset8 for images. They were sharing the same splits of training and validation data made in Flickr30k Entities. For test data, we used the officially provided data of the Flickr30kEnt-JP task, and their corresponding images were in the Flickr30k dataset. Note that the Japanese training data size is originally 148,915 sente"
2021.wat-1.20,2020.eamt-1.12,1,0.81952,"abulary sizes of En→Ja were 9,578→22,274 tokens. For image regions, we used Faster-RCNN (Ren et al., 2015) in Anderson et al. (2018) to detect up to 36 salient visual objects per image and extracted their corresponding 2,048-dim image region features and attribute-object combined concepts. 3.3 Settings (i) NMT: the baseline NMT system (Bahdanau et al., 2015) is the architecture comprised a 2-layer bidirectional GRU encoder and a 2-layer cGRU decoder with attention mechanism, which only encodes the source sentence as the input. (ii) MNMT: the baseline MNMT system without word-region alignment (Zhao et al., 2020). This architecture comprised a 2-layer bidirectional GRU encoder and a 2-layer cGRU decoder with double attentions to integrate visual and textual features. (iii) TMEKU system: our proposed MNMT system with word-region alignment. We conducted all experiments on Nmtpy toolkit (Caglayan et al., 2017). 3.3.1 Parameters We ensured that the parameters were consistent in all the settings. We set the encoder and decoder hidden state to 400-dim; word embedding to 200dim; batch size to 32; beam size to 12; text dropout to 0.3; image region dropout to 0.5; dropout of source RNN hidden states to 0.5; an"
C16-1029,W09-2307,0,0.0222709,"rvation, these tokens have the ability to determine the syntactic role of the entire compound. For example, any compound that end with a nominal suffix “度” (degree) always act as nouns in a sentence. It should be noted that because of this characteristic of suffixes, we can tag the children of suffixes in compounds based on their meaning but not their syntactic roles. We show some examples in Table 2 to illustrate our POS tagging strategy for compounds. In Table 4 we present a dependency label set developed based on the Stanford Dependencies (De Marneffe et al., 2006) and its Chinese version (Chang et al., 2009), which defines 45 dependency relations for Chinese sentences. This label set is also closely related to the Universal Dependency1 with many of their labels compatible with each other. We explain the major characteristics of our label set in the following subsection. 3.1 Chinese Specific Labels dislocated The label “dislocated” is originally defined in the universal dependencies for languages such as Japanese to describe the syntactic relation of words in a topic–comment structure, but is not defined for Chinese. However, in Chinese it is frequent to see the topic–comment structure in a senten"
C16-1029,W02-1001,0,0.112792,"lower percentage of unknown words and unknown word-POS pairs found in the corresponding test set. This is consistent with our observation that compounds with internal structures are one of the major sources of OOV words. 4.2 Morphological Analysis Experiments We compared the performance of a state-of-the-art joint word segmentation and part-of-speech tagging system (Kruengkrai et al., 2009) on the original and our re-annotated CTB5. We used the position-ofcharacter (POC) tagset and the baseline feature set described in (Shen et al., 2014). We trained all models using the averaged perceptron (Collins, 2002), which is an efficient and stable online learning algorithm. The models applied on all test sets are those that result in the best performance on the dev sets. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 2 times. We use precision, recall and the F-score to measure the performance of the systems. Precision (P) is defined as the percentage of output tokens that are consistent with the gold standard test data, and recall (R) is the percentage of tokens in the gold standard test data that are recognize"
C16-1029,N06-1023,1,0.566142,"phrase based statistical machine translation toolkit Moses (Koehn et al., 2007) with default options. We trained the 5-gram language models on the target side of the parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses model"
C16-1029,W03-1722,0,0.036952,"is paper, we propose a new annotation approach to Chinese word segmentation, part-ofspeech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments. 1 Introduction The definition of “word” is an open problem in Chinese linguistics. In previous studies of Chinese corpus annotation (Duan et al., 2003; Huang et al., 1997; Xia, 2000), the judgement of word-hood of a meaningful string is based on the analysis of morphology: A morpheme in Chinese is defined as the smallest combination of meaning and phonetic sound in Chinese language, which can be classified into two major types: 1). Free morphemes, which can either be words by themselves or form words with other morphemes; and 2). Bound morphemes, which can only form words by attaching to other morphemes. An issue with word definition using morpheme classification is that, it potentially undermines the consistency of the representation of wo"
C16-1029,O97-4003,0,0.219847,"e a new annotation approach to Chinese word segmentation, part-ofspeech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments. 1 Introduction The definition of “word” is an open problem in Chinese linguistics. In previous studies of Chinese corpus annotation (Duan et al., 2003; Huang et al., 1997; Xia, 2000), the judgement of word-hood of a meaningful string is based on the analysis of morphology: A morpheme in Chinese is defined as the smallest combination of meaning and phonetic sound in Chinese language, which can be classified into two major types: 1). Free morphemes, which can either be words by themselves or form words with other morphemes; and 2). Bound morphemes, which can only form words by attaching to other morphemes. An issue with word definition using morpheme classification is that, it potentially undermines the consistency of the representation of words. For example, “论"
C16-1029,P08-1102,0,0.0702105,"Missing"
C16-1029,C08-1049,0,0.0408866,"Missing"
C16-1029,W04-3250,0,0.12997,"tures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-annotated” we segmented the Chinese sentences using the corresponding models described in 2 ? < 0.05 in McNemar’s test. http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 4 http://www.speech.sri.com/projects/srilm 5 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KyotoEBMT 3 306 the last subsection. The results show, with the underlying machine translation system being the same, the segmenter trained wit"
C16-1029,P09-1058,0,0.0604459,"Missing"
C16-1029,de-marneffe-etal-2006-generating,0,0.0183273,"Missing"
C16-1029,P03-1021,0,0.012317,"irs for tuning and testing, respectively. In the first set of experiments, we segmented the Japanese sentences using JUMAN (Kurohashi et al., 1994), and the Chinese sentences using the same morphological analyzer described in the last subsection. For decoding, we used the state-of-the-art phrase based statistical machine translation toolkit Moses (Koehn et al., 2007) with default options. We trained the 5-gram language models on the target side of the parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default opti"
C16-1029,W15-5006,1,0.786007,"mum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-annotated” we segmented the Chinese sentences using the corresponding models described in 2 ? < 0.05 in McNemar’s test. http"
C16-1029,Y12-1033,1,0.855948,"he parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-ann"
C16-1029,P14-2042,1,0.938798,"621 466 451 258 245 33 12 6 0 0 0 Proposed annotation 137,816 75,935 75,033 35,922 30,985 17,721 21,493 18,091 13,898 12,346 12,145 4,524 0 7,518 7,134 6,646 5,970 5,521 4,033 2,980 1,661 1,316 0 888 756 627 466 451 258 245 391 17 6 13,212 438 129 Table 3. Proposed tagset for part-of-speech tagging. The underlined characters in the examples correspond to the tags on the left-most column. The CTB POS are also shown. The key in our method to define the boundaries of common words is the character-level POS pattern. Character-level POS has been introduced in previous studies (Zhang et al., 2013; Shen et al., 2014) which captures the grammatical roles of Chinese characters inside words; we further develop this idea and use it as a criterion in word definition. We treat a meaningful disyllabic strings as a word if it falls into one of the character-level POS patterns listed in Table 1. The reason we focus on disyllabic patterns instead of other polysyllabic ones is that, based on our observation, meaningful strings with 3 or more syllables (other than names and idioms) are always compounds in Chinese, and therefore can be segmented into a sequence of monosyllabic and disyllabic tokens based on their inte"
C16-1029,P11-1139,0,0.0380763,"Missing"
C16-1029,P13-1013,0,0.0165029,"1,316 1,287 888 751 621 466 451 258 245 33 12 6 0 0 0 Proposed annotation 137,816 75,935 75,033 35,922 30,985 17,721 21,493 18,091 13,898 12,346 12,145 4,524 0 7,518 7,134 6,646 5,970 5,521 4,033 2,980 1,661 1,316 0 888 756 627 466 451 258 245 391 17 6 13,212 438 129 Table 3. Proposed tagset for part-of-speech tagging. The underlined characters in the examples correspond to the tags on the left-most column. The CTB POS are also shown. The key in our method to define the boundaries of common words is the character-level POS pattern. Character-level POS has been introduced in previous studies (Zhang et al., 2013; Shen et al., 2014) which captures the grammatical roles of Chinese characters inside words; we further develop this idea and use it as a criterion in word definition. We treat a meaningful disyllabic strings as a word if it falls into one of the character-level POS patterns listed in Table 1. The reason we focus on disyllabic patterns instead of other polysyllabic ones is that, based on our observation, meaningful strings with 3 or more syllables (other than names and idioms) are always compounds in Chinese, and therefore can be segmented into a sequence of monosyllabic and disyllabic tokens"
C16-1029,D10-1082,0,0.0356933,"Missing"
C18-1111,D16-1162,0,0.024687,"Incorporation How to use external knowledge such as dictionaries and knowledge bases for NMT remains a big research question. In domain adaptation, the use of domain specific dictionaries is a very crucial problem. In the practical perspective, many translation companies have created domain specific dictionaries but not domain specific corpora. If we can study a good way to use domain specific dictionaries, it will significantly promote the practical use of MT. There are some studies that try to use dictionaries for NMT, but the usage is limited to help low frequent or rare word translation (Arthur et al., 2016; Zhang and Zong, 2016a). Arcan and Buitelaar (2017) use a domain specific dictionary for terminology translation, 1312 but they simply apply the unknown word replacement method proposed by Luong et al. (2015), which suffers from noisy attention. 6.3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as crosslingual transfer learning, which transfers NMT model parameters among multiple languages. It is kno"
C18-1111,D11-1033,0,0.671459,"rcome the problem of the lack of substantial data in specific domains and languages. Most SMT domain adaptation methods can be broken down broadly into two main categories: 3.1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (C"
C18-1111,2010.amta-papers.16,0,0.0586287,"tences is crucial for good translation. To address this problem, a common method in SMT is to firstly classify the domains and then translate input sentences in classified domains using corresponding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron classifiers with various phrase-based features. For NMT, Kobus et al. (2016) propose an NMT domain control method, by appending either domain tags or features to the word embedding layer of NMT. They adopt an in-house classifier to distinguish the domain information. Li et al. (2016) propose to search similar sentences in the training data using the test sentence as a query, and"
C18-1111,2011.iwslt-evaluation.18,0,0.0265031,"tion or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re"
C18-1111,W17-4712,0,0.328656,"s is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayral"
C18-1111,2015.iwslt-evaluation.1,0,0.0685094,"Missing"
C18-1111,2016.amta-researchers.8,0,0.0555854,"1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selecti"
C18-1111,W17-3205,0,0.462343,"monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the diff"
C18-1111,P17-1110,0,0.254114,"monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the diff"
C18-1111,P16-1185,0,0.0525452,"have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu e"
C18-1111,D14-1179,0,0.0198359,"Missing"
C18-1111,P17-2061,1,0.891539,"d Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and"
C18-1111,D17-1158,0,0.28159,"uter vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the N"
C18-1111,P13-2119,0,0.149871,"rich scenarios (Bojar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English pa"
C18-1111,2015.mtsummit-papers.10,0,0.361618,"MT domain adaptation methods can be broken down broadly into two main categories: 3.1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly"
C18-1111,P08-1115,0,0.0405479,"lc¸ehre et al., 2015), the next word hypotheses generated by an NMT model is rescored by the weighted sum of the NMT and RNNLM probabilities (Figure 8). Ensembling Freitag and Al-Onaizan (2016) propose to ensemble the out-of-domain domain and the fine tuned in-domain models. Their motivation is exactly the same as the work of Dakwale and Monz (2017), which is preventing degradation of out-of-domain translation after fine tuning on in-domain data. Neural Lattice Search Khayrallah et al. (2017) propose a stack-based decoding algorithm over word lattices, while the lattices are generated by SMT (Dyer et al., 2008). In their domain adaptation experiments, they show that stack-based decoding is better than conventional decoding. 5 Domain Adaptation in Real-World Scenarios A domain adaptation method should be adopted according to the certain scenarios. For example, when there are some pseudo parallel in-domain data in the out-of-domain data, sentence selection is preferred; when only additional monolingual data is available, LM and NMT fusion can be adopted. In many cases, both out-of-domain parallel data and monolingual in-domain data are available, making the combination 1311 .&/01&apos; *,&1,&(,* 2,&1,&(,*&apos;"
C18-1111,W17-4713,0,0.049474,"Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron classifiers with various phrase-based features. For NMT, Kobus et al. (2016) propose an NMT domain control method, by appending either domain tags or features to the word embedding layer of NMT. They adopt an in-house classifier to distinguish the domain information. Li et al. (2016) propose to search similar sentences in the training data using the test sentence as a query, and then fine tune the NMT model using the retrieved training sentences for translating the test sentence. Farajian et al. (2017) follow the strategy of Li et al. (2016), but propose to dynamically set the hyperparameters (i.e., learning rate and number of epochs) of the learning algorithm based on the similarity of the input sentence and the retrieved sentences for updating the NMT model. Figure 9 shows an overview of domain adaptation for MT in the input domain unknown scenario. 6 6.1 Future Directions Domain Adaptation for State-of-the-art NMT Architectures Since the success of RNN based NMT (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), other architectures of NMT have been developed. One represen"
C18-1111,N16-1101,0,0.0279015,"3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as crosslingual transfer learning, which transfers NMT model parameters among multiple languages. It is known that a multilingual model, which relies on parameter sharing, helps in improving the translation quality for low resource languages especially when the target language is the same (Zoph et al., 2016). There are studies where either multilingual (Firat et al., 2016; Johnson et al., 2017) or multi-domain models (Sajjad et al., 2017) are trained, but none that attempt to package multiple language pairs and multiple domains into a single translation system. Even if out-of-domain data in the same language pair exists, it is possible that using both multilingual and multi-domain data can boost the translation performance. Therefore, we think that multilingual and multi-domain adaptation for NMT can be another future direction. Chu and Dabre (2018) conduct a preliminary study for this topic. 6.4 Adversarial Domain Adaptation and Domain Generation Generative a"
C18-1111,W07-0717,0,0.0696966,"criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight"
C18-1111,D10-1044,0,0.0359406,"els from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in N"
C18-1111,C14-1182,0,0.0639158,"Missing"
C18-1111,2015.mtsummit-papers.19,0,0.0583303,"nd any particular techniques in this paper but recommend readers to choose the best method for their own scenarios. Most of the above domain adaptation studies assume that the domain of the data is given. However, in a practical view such as an online translation engine, the domain of the sentences input by the users are not given. For such scenario, predicting the domains of the input sentences is crucial for good translation. To address this problem, a common method in SMT is to firstly classify the domains and then translate input sentences in classified domains using corresponding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on avera"
C18-1111,2016.amta-researchers.7,0,0.0292902,"est improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed"
C18-1111,Q13-1035,0,0.0256294,"n discriminator (Tzeng et al., 2017). They have been applied to domain adaptation tasks in computer vision and machine learning (Tzeng et al., 2017; Motiian et al., 2017; Volpi et al., 2017; Zhao et al., 2017; Pei et al., 2018). Recently, some of the adversarial methods began to be introduced into some NLP tasks (Liu et al., 2017; Chen et al., 2017b) and NMT (Britz et al., 2017). Most of the existing methods focus on adapting from a general domain into a specific domain. In the real scenario, training data and test data have different distributions and the target domains are sometimes unseen. Irvine et al. (2013) analyze the translation errors in such scenarios. Domain generalization aims to apply knowledge gained from labeled source domains to unseen target domains (Li et al., 2018). It provides a way to match the distribution of training data and test data in real-world MT, which may be a future trend of domain adaptation for NMT. 7 Conclusion Domain adaptation for NMT is a rather new but very important research topic to promote MT for practical use. In this paper, we gave the first comprehensive review of the techniques mainly being developed in the last two years. We compared domain adaptation tec"
C18-1111,P15-1001,0,0.0355654,"in adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective function (Chen et al., 2017a; Wang et al., 2017b). However, the model structures of SMT and NMT are quite different. SMT is a combination of several independent models; in comparison, NMT is an integral model itself. Therefore, most of these methods cannot be directly applied to NMT. 4 4.1 Domain Adaptation for NMT Data Centric 4.1.1 Using Monolingual Corpora Unlike SMT, in-domain monolingual data cannot be used as an LM for conventional NMT directly, and many studies have been conducted for this."
C18-1111,P07-1034,0,0.108018,".2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel inte"
C18-1111,I17-2004,0,0.09754,"., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the difficulty of low resource domains and languages in NMT (Section 2); Next, we briefly review the historical domain adaptation techniques being developed for SMT (Section 3); Under these background knowledge, we then present and compare the domain adaptation methods for 1305 Fig"
C18-1111,W17-3204,0,0.0549232,"However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013"
C18-1111,P07-2045,0,0.0069565,"both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domainspecific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are characteristics of statistical machine translation (SMT) systems (Koehn et al., 2007). NMT yields the state-of-the-art translation performance in resource rich scenarios (Bojar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perf"
C18-1111,W11-2132,0,0.0630701,"models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennri"
C18-1111,P17-1001,0,0.0288747,"ed in unsupervised machine learning, which are introduced by (Goodfellow et al., 2014). Adversarial methods have become popular in domain adaptation (Ganin et al., 2016), which minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator (Tzeng et al., 2017). They have been applied to domain adaptation tasks in computer vision and machine learning (Tzeng et al., 2017; Motiian et al., 2017; Volpi et al., 2017; Zhao et al., 2017; Pei et al., 2018). Recently, some of the adversarial methods began to be introduced into some NLP tasks (Liu et al., 2017; Chen et al., 2017b) and NMT (Britz et al., 2017). Most of the existing methods focus on adapting from a general domain into a specific domain. In the real scenario, training data and test data have different distributions and the target domains are sometimes unseen. Irvine et al. (2013) analyze the translation errors in such scenarios. Domain generalization aims to apply knowledge gained from labeled source domains to unseen target domains (Li et al., 2018). It provides a way to match the distribution of training data and test data in real-world MT, which may be a future trend of domain adap"
C18-1111,2015.iwslt-evaluation.11,0,0.0985932,"uses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of th"
C18-1111,P15-1002,0,0.0245704,"oblem. In the practical perspective, many translation companies have created domain specific dictionaries but not domain specific corpora. If we can study a good way to use domain specific dictionaries, it will significantly promote the practical use of MT. There are some studies that try to use dictionaries for NMT, but the usage is limited to help low frequent or rare word translation (Arthur et al., 2016; Zhang and Zong, 2016a). Arcan and Buitelaar (2017) use a domain specific dictionary for terminology translation, 1312 but they simply apply the unknown word replacement method proposed by Luong et al. (2015), which suffers from noisy attention. 6.3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as crosslingual transfer learning, which transfers NMT model parameters among multiple languages. It is known that a multilingual model, which relies on parameter sharing, helps in improving the translation quality for low resource languages especially when the target language is the same (Zoph et al., 2016). There"
C18-1111,2012.iwslt-papers.7,0,0.0152304,"el interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective function (Chen et al., 2017a;"
C18-1111,P17-2062,0,0.014107,"n data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category fo"
C18-1111,D09-1074,0,0.0380629,"on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to"
C18-1111,D17-1156,0,0.061316,"7; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017). An overview of these two categories is shown in Figure 1. Note that as model centric methods also use either monolingual or parallel corpora, there are overlaps between these two categories. The remainder of this paper is structured as follows: We first give a brief introduction of NMT, and describe the reason for the difficulty of low resource domains and languages in NMT (Section 2); Next, we briefly rev"
C18-1111,P10-2041,0,0.105905,"ve been proposed to overcome the problem of the lack of substantial data in specific domains and languages. Most SMT domain adaptation methods can be broken down broadly into two main categories: 3.1 Data Centric This category focuses on selecting or generating the domain-related data using existing in-domain data. i) When there are sufficient parallel corpora from other domains, the main idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and pa"
C18-1111,W17-5701,0,0.0369516,"ecific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are characteristics of statistical machine translation (SMT) systems (Koehn et al., 2007). NMT yields the state-of-the-art translation performance in resource rich scenarios (Bojar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016"
C18-1111,2012.amta-papers.19,0,0.0364559,"t are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 20"
C18-1111,2011.iwslt-evaluation.10,0,0.0475759,"Missing"
C18-1111,E17-2045,0,0.0482425,"of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and"
C18-1111,P13-1082,0,0.174721,"jar et al., 2017; Nakazawa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel co"
C18-1111,N16-1005,0,0.393461,"for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
C18-1111,P16-1009,0,0.711904,"for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
C18-1111,P16-1162,0,0.886962,"for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain adaptation for SMT, which can be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
C18-1111,W10-1759,0,0.027937,"nd Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective function (Chen et al., 2017a; Wang et al., 2017b). However, the model structures of SMT and NMT are quite different. SMT is a combination of several independent models; in comparison, NMT is an integral model itself. Therefore, most of these methods cannot be directly applied to NMT. 4 4.1 Do"
C18-1111,2012.amta-papers.21,0,0.0144443,"mains. i) Model level interpolation. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective functi"
C18-1111,P03-1010,0,0.0159135,"in idea is to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Sy"
C18-1111,D17-1147,0,0.0517826,"Missing"
C18-1111,2012.amta-papers.18,0,0.0196613,"nding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron classifiers with various phrase-based features. For NMT, Kobus et al. (2016) propose an NMT domain control method, by appending either domain tags or features to the word embedding layer of NMT. They adopt an in-house classifier to distinguish the domain information. Li et al. (2016) propose to search similar sentences in the training data using the test sentence as a query, and then fine tune the NMT model using the retrieved training sentences for translating the test sentence. Farajian et al. (2017) follow the strategy of Li et al. (2016), but propose to dynamic"
C18-1111,D14-1023,1,0.846543,"Ms (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selection or generation that are not related to NMT. Therefore, these methods can only achieve modest improvements in NMT (Wang et al., 2017a). 1307 Target Synthetic Source-Target Translate NMT Figure 3: Synthetic data generation for NMT (Sennrich et al., 2016b). 3.2 Model Centric This category focuses on interpolating the models from different domains. i) Model level interpolation. Several SMT models,"
C18-1111,C16-1295,1,0.943601,"r domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b)."
C18-1111,P17-2089,1,0.867528,"Missing"
C18-1111,D17-1155,1,0.900044,"ehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; G¨ulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ulc¸ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017)."
C18-1111,C18-1269,0,0.0248969,"of linear models, which means the instance weight cannot be integrated into NMT directly. There is only one work concerning instance weighting in NMT (Wang et al., 2017b). They set a weight for the objective function, and this weight is learned from the cross-entropy by an indomain LM and an out-of-domain LM (Axelrod et al., 2011) (Figure 5). Instead of instance weighting, Chen et al. (2017a) modify the NMT cost function with a domain classifier. The output probability of the domain classifier is transferred into the domain weight. This classifier is trained using development data. Recently, Wang et al. (2018) proposed a joint framework of sentence selection and weighting for NMT. Fine Tuning Fine tuning is the conventional way for domain adaptation (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). In this method, an NMT 1309 Sentence-1 Sentence-1 Score-1 Sentence-2 Score-2 … … Sentence-i Score-i … … Sentence-N Sentence-N Sentence-2 … Sentence-i Sentence Scoring by cross-entropy Sentence-1 Weight-1 Sentence-2 Weight-2 … … Sentence-i Weight-i … … … Score-N Sentence-N Weight-N Instance Weighting Figure 5: Instance weighting for NMT (Wang et al., 201"
C18-1111,2007.mtsummit-papers.68,0,0.100536,"chniques in this paper but recommend readers to choose the best method for their own scenarios. Most of the above domain adaptation studies assume that the domain of the data is given. However, in a practical view such as an online translation engine, the domain of the sentences input by the users are not given. For such scenario, predicting the domains of the input sentences is crucial for good translation. To address this problem, a common method in SMT is to firstly classify the domains and then translate input sentences in classified domains using corresponding models (Huck et al., 2015). Xu et al. (2007) perform domain classification for a Chinese-English translation task. The classifiers operate on whole documents rather than on individual sentences, using LM interpolation and vocabulary similarities. Huck et al. (2015) extend the work of Xu et al. (2007) on the sentence level. They use LMs and maximum entropy classifiers to predict the target domain. Banerjee et al. (2010) build a support vector machine classifier using tf-idf features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron cla"
C18-1111,D16-1160,0,0.233559,"ain adaptation surveys have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the first comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain adaptation for NMT into two main categories: data centric and model centric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale"
C18-1111,D16-1163,0,0.155622,"awa et al., 2017). However, currently, high quality parallel corpora of sufficient size are only available for a few language pairs such as languages paired with English and several European language pairs. Furthermore, for each language pair the sizes of the domain specific corpora and the number of domains available are limited. As such, for the majority of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain specific translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain specific machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform poorly and hence it is important to develop translation systems for specific domains (Koehn and Knowles, 2017). Leveraging out-of-domain parallel corpora and in-domain monolingual corpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentenc"
C18-1111,W17-4715,0,\N,Missing
C18-1111,W14-7001,0,\N,Missing
C18-1111,W17-4717,0,\N,Missing
C18-1111,2020.wmt-1.63,0,\N,Missing
C18-1111,2020.coling-tutorials.3,1,\N,Missing
C18-1295,P05-1074,0,0.260127,"Missing"
C18-1295,N03-1003,0,0.0695422,"VGPs (Section 5). Experiments show that our proposed NN-based method outperforms the other methods.1 2 2.1 Related Work Paraphrase Extraction Previous studies extract paraphrases from either monolingual corpora or bilingual parallel corpora. One major approach is to use the distributional similarity (Harris, 1954) with regular monolingual corpora (a large collection of text in a single language) (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008; Marton et al., 2009), or monolingual comparable corpora (a set of monolingual corpora that describe roughly the same topic in the same language) (Barzilay and Lee, 2003; Chen and Dolan, 2011). Distributional similarity stems from the distributional hypothesis (Harris, 1954), stating that words/phrases that share similar meanings should appear in similar distributions. This approach sometimes suffers from noisy results, because the distributed similarity often maps antonyms to closer points. Some methods try to extract paraphrases from monolingual parallel corpora (a collection of sentence level paraphrases) (Arase and Tsujii, 2017; MacCartney et al., 2008), but such monolingual parallel corpora are rarely available. Bilingual parallel corpora (a collection o"
C18-1295,P14-1133,0,0.0395457,"ng of a word, phrase, or sentence within the context of a specific language (e.g., “a red jersey” and “a red uniform shirt” in Figure 1 are paraphrases) (Bhagat and Hovy, 2013). Paraphrases have been exploited for natural language understanding, and shown to be very effective for various natural language processing (NLP) tasks, including question answering (Riezler et al., 2007), summarization (Zhou et al., 2006), machine translation (Chu and Kurohashi, 2016), text normalization (Ling et al., 2013), textual entailment recognition (Androutsopoulos and Malakasiotis, 2010), and semantic parsing (Berant and Liang, 2014). In this paper, we propose a novel task named, iParaphrasing, to extract visually grounded paraphrases (VGPs). We define VGPs as different phrasal expressions that describe the same visual concept in an image. Nowadays, with the spread of the web and social media, it is easy to collect large amounts of images with their describing text. For example, different news sites release news with the same topic using the same image; photos with many comments are posted to social networking sites and blogs. As these describing texts are written by different people but about the same image, there are po"
C18-1295,P08-1077,0,0.0370829,"ection 4.3). In addition, we propose a supervised neural network (NN)-based method using both textual and visual features to explicitly model the similarity of an entity pair as VGPs (Section 5). Experiments show that our proposed NN-based method outperforms the other methods.1 2 2.1 Related Work Paraphrase Extraction Previous studies extract paraphrases from either monolingual corpora or bilingual parallel corpora. One major approach is to use the distributional similarity (Harris, 1954) with regular monolingual corpora (a large collection of text in a single language) (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008; Marton et al., 2009), or monolingual comparable corpora (a set of monolingual corpora that describe roughly the same topic in the same language) (Barzilay and Lee, 2003; Chen and Dolan, 2011). Distributional similarity stems from the distributional hypothesis (Harris, 1954), stating that words/phrases that share similar meanings should appear in similar distributions. This approach sometimes suffers from noisy results, because the distributed similarity often maps antonyms to closer points. Some methods try to extract paraphrases from monolingual parallel corpora (a collection of sentence le"
C18-1295,J93-2003,0,0.0868637,"he similarity between entities i and j is defined as: s(i, j) = p(i|j)p(j|i) (5) where p(i|j) and p(j|i) are the direct and inverse translation probabilities of an entity pair i and j, which are calculated using a conventional statistical machine translation (SMT) (Koehn et al., 2007) method: 1. Generate a pseudo parallel corpus using the captions in the dataset, which treats the 5 captions  for 5 each image as monolingual parallel sentences and pair each of the sentences that leads to 2 = 10 sentence pairs per image. 2. Apply word alignment to the parallel corpus using IBM alignment models (Brown et al., 1993) in two directions with the grow-diag-final-and heuristic (Koehn et al., 2007) to align the words in each caption pair. 3. From the word-aligned parallel corpus, extract entity pairs such that the words inside an entity pair are aligned. Then p(i|j) and p(j|i) are calculated as follows: c(i, j) p(i|j) = P , k c(i, k) c(i, j) p(j|i) = P k c(j, k) (6) where c(i, j) is the number of co-occurrence of i and j in the word-aligned corpus. 4.3 Embedding-Based Similarity In this method, the similarity between entities i and j is defined as: s(i, j) = t&gt; i tj kti kktj k (7) where ti and tj are the phras"
C18-1295,D08-1021,0,0.0893761,"Missing"
C18-1295,P11-1020,0,0.0242249,"iments show that our proposed NN-based method outperforms the other methods.1 2 2.1 Related Work Paraphrase Extraction Previous studies extract paraphrases from either monolingual corpora or bilingual parallel corpora. One major approach is to use the distributional similarity (Harris, 1954) with regular monolingual corpora (a large collection of text in a single language) (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008; Marton et al., 2009), or monolingual comparable corpora (a set of monolingual corpora that describe roughly the same topic in the same language) (Barzilay and Lee, 2003; Chen and Dolan, 2011). Distributional similarity stems from the distributional hypothesis (Harris, 1954), stating that words/phrases that share similar meanings should appear in similar distributions. This approach sometimes suffers from noisy results, because the distributed similarity often maps antonyms to closer points. Some methods try to extract paraphrases from monolingual parallel corpora (a collection of sentence level paraphrases) (Arase and Tsujii, 2017; MacCartney et al., 2008), but such monolingual parallel corpora are rarely available. Bilingual parallel corpora (a collection of sentence-aligned bili"
C18-1295,L16-1101,1,0.858166,"rk-based method with image attention, and report the results of the first attempt toward iParaphrasing. 1 Introduction A paraphrase is a restatement of the meaning of a word, phrase, or sentence within the context of a specific language (e.g., “a red jersey” and “a red uniform shirt” in Figure 1 are paraphrases) (Bhagat and Hovy, 2013). Paraphrases have been exploited for natural language understanding, and shown to be very effective for various natural language processing (NLP) tasks, including question answering (Riezler et al., 2007), summarization (Zhou et al., 2006), machine translation (Chu and Kurohashi, 2016), text normalization (Ling et al., 2013), textual entailment recognition (Androutsopoulos and Malakasiotis, 2010), and semantic parsing (Berant and Liang, 2014). In this paper, we propose a novel task named, iParaphrasing, to extract visually grounded paraphrases (VGPs). We define VGPs as different phrasal expressions that describe the same visual concept in an image. Nowadays, with the spread of the web and social media, it is easy to collect large amounts of images with their describing text. For example, different news sites release news with the same topic using the same image; photos with"
C18-1295,D16-1044,0,0.0361081,"se localization is a task to find an image region that corresponds to a given phrase in a caption, which is closely related to our VGP extraction task. Plummer et al. (2015) pioneered this work, in which they annotated phrase-region alignment in the Flickr30k image-caption dataset (Young et al., 2014) and released it as the Flickr30k entities dataset. They also proposed a method based on canonical correlation analysis (CCA) (Hardoon et al., 2004) that learns joint embeddings of phrases and image regions for associating them. Wang et al. (2016a) proposed joint embeddings using a two-branch NN. Fukui et al. (2016) used a multimodal compact bilinear pooling method to combine textual and visual embeddings. Rohrbach et al. (2016) proposed a convolutional NN (CNN)-recurrent NN (RNN)-based method for this task. They learn to detect a region for a given phrase and then reconstruct the phrase using the detected region. Wang et al. (2016b) noticed that the relationships between phrases should agree with their corresponding regions, and proposed a joint matching method, but their method only considers the “has-a” relationship that is explicitly indicated by possessive pronouns. Previous studies rely on region p"
C18-1295,D15-1070,0,0.0303867,"s such as image object/region referring expression grounding (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017; Cirik et al., 2018) visual captioning (Vinyals et al., 2015; Xu et al., 2015; Bernardi et al., 2016; Laokulrat et al., 2016), text-image retrieval (Otani et al., 2016), visual question answering (Wu et al., 2017), visual dialog (Das et al., 2017a; Das et al., 2017b) and video event detection (Phan et al., 2016). Some researchers also have employed images for improving NLP tasks, such as multimodal machine translation (Specia et al., 2016), cross-lingual document retrieval (Funaki and Nakayama, 2015), and textual entailment recognition (Han 2 Although (Plummer et al., 2015) annotated the VGPs in the Filickr30k entities dataset, they did not propose any methods to extract them. (Regneri et al., 2013) collected sentence level paraphrases by aligning video scripts with the same time frame; these sentence level paraphrases are essentially similar to captions of an image. 3 If we treat multiple captions as a document forcibly, we could apply a coreference resolution approach for our task. 3481 a male the pitcher s(i, j) a ball a base Figure 2: An overview of our VGP extraction formulation. We"
C18-1295,ganitkevitch-callison-burch-2014-multilingual,0,0.0164131,"ingual text) enjoys more availability than monolingual parallel corpora as they are mandatory for training machine translation systems. Bilingual parallel corpora can be used for paraphrase extraction, with bilingual pivoting (Bannard and CallisonBurch, 2005). This method assumes that two source phrases are a paraphrase pair if they are translated to the same target phrase. Bilingual pivoting has been further refined by using syntax information (CallisonBurch, 2008) or mutual information (Kajiwara et al., 2017). These methods have led to the construction of a multilingual paraphrase database (Ganitkevitch and Callison-Burch, 2014). Note that our definition of paraphrases may look different from the studies mentioned above, as our paraphrases are a set of noun phrases that represent the same visual concept. Our idea to extract paraphrases under this definition is to use image captioning datasets (Young et al., 2014; Chen et al., 2015), 1 Codes and data for reproducing the results reported in this paper are available at https://github.com/ids-cv/ coling_iparaphrasing 3480 which usually contain several captions for each image, and currently scale to sub-million images, instead of a bilingual parallel corpus with limited a"
C18-1295,D17-1305,0,0.0546382,"Missing"
C18-1295,I17-1009,0,0.023768,"al parallel corpora are rarely available. Bilingual parallel corpora (a collection of sentence-aligned bilingual text) enjoys more availability than monolingual parallel corpora as they are mandatory for training machine translation systems. Bilingual parallel corpora can be used for paraphrase extraction, with bilingual pivoting (Bannard and CallisonBurch, 2005). This method assumes that two source phrases are a paraphrase pair if they are translated to the same target phrase. Bilingual pivoting has been further refined by using syntax information (CallisonBurch, 2008) or mutual information (Kajiwara et al., 2017). These methods have led to the construction of a multilingual paraphrase database (Ganitkevitch and Callison-Burch, 2014). Note that our definition of paraphrases may look different from the studies mentioned above, as our paraphrases are a set of noun phrases that represent the same visual concept. Our idea to extract paraphrases under this definition is to use image captioning datasets (Young et al., 2014; Chen et al., 2015), 1 Codes and data for reproducing the results reported in this paper are available at https://github.com/ids-cv/ coling_iparaphrasing 3480 which usually contain several"
C18-1295,D14-1086,0,0.0927529,"work that can search over all possible regions. Plummer et al. (2017) used spatial relationships between pairs of entities connected by verbs or prepositions, which achieved the state-of-the-art performance. In this paper, we use the current state-of-the-art phrase localization method of (Plummer et al., 2017) as a baseline for VGP extraction. 2.4 Other Vision and Language Tasks Vision and language tasks have been a hot research area recently in both the CV and NLP communities. Various efforts have been made for many multimodal tasks such as image object/region referring expression grounding (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017; Cirik et al., 2018) visual captioning (Vinyals et al., 2015; Xu et al., 2015; Bernardi et al., 2016; Laokulrat et al., 2016), text-image retrieval (Otani et al., 2016), visual question answering (Wu et al., 2017), visual dialog (Das et al., 2017a; Das et al., 2017b) and video event detection (Phan et al., 2016). Some researchers also have employed images for improving NLP tasks, such as multimodal machine translation (Specia et al., 2016), cross-lingual document retrieval (Funaki and Nakayama, 2015), and textual entailment recognition (Han 2 Although (Plumm"
C18-1295,P07-2045,0,0.0267756,"wn here for comprehensibility. regions (Plummer et al., 2015). Therefore, we can obtain a set of phrases annotated with the same image region. This set of phrases are used as the ground truth VGPs in our study. The goal of this work is to extract these VGPs. We formulate our task as a clustering task (Section 3), where the similarity between each entity pair is crucial for the performance. We apply many different unsupervised similarity computation methods (Section 4) including phrase localization-based similarity (Plummer et al., 2017) (Section 4.1), translation probability-based similarity (Koehn et al., 2007) (Section 4.2), and embedding-based similarity (Mikolov et al., 2013; Klein et al., 2014; Plummer et al., 2015) (Section 4.3). In addition, we propose a supervised neural network (NN)-based method using both textual and visual features to explicitly model the similarity of an entity pair as VGPs (Section 5). Experiments show that our proposed NN-based method outperforms the other methods.1 2 2.1 Related Work Paraphrase Extraction Previous studies extract paraphrases from either monolingual corpora or bilingual parallel corpora. One major approach is to use the distributional similarity (Harris"
C18-1295,C16-1005,0,0.0259128,"ich achieved the state-of-the-art performance. In this paper, we use the current state-of-the-art phrase localization method of (Plummer et al., 2017) as a baseline for VGP extraction. 2.4 Other Vision and Language Tasks Vision and language tasks have been a hot research area recently in both the CV and NLP communities. Various efforts have been made for many multimodal tasks such as image object/region referring expression grounding (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017; Cirik et al., 2018) visual captioning (Vinyals et al., 2015; Xu et al., 2015; Bernardi et al., 2016; Laokulrat et al., 2016), text-image retrieval (Otani et al., 2016), visual question answering (Wu et al., 2017), visual dialog (Das et al., 2017a; Das et al., 2017b) and video event detection (Phan et al., 2016). Some researchers also have employed images for improving NLP tasks, such as multimodal machine translation (Specia et al., 2016), cross-lingual document retrieval (Funaki and Nakayama, 2015), and textual entailment recognition (Han 2 Although (Plummer et al., 2015) annotated the VGPs in the Filickr30k entities dataset, they did not propose any methods to extract them. (Regneri et al., 2013) collected senten"
C18-1295,D17-1018,0,0.0282002,"es and data for reproducing the results reported in this paper are available at https://github.com/ids-cv/ coling_iparaphrasing 3480 which usually contain several captions for each image, and currently scale to sub-million images, instead of a bilingual parallel corpus with limited availability. To the best of our knowledge, this is the first study that aims to extract paraphrases from such multimodal datasets consisting of images and their captions.2 2.2 Coreference Resolution Coreference resolution is a task to find the expressions that refer to the same entity in a text (Soon et al., 2001; Lee et al., 2017). Our task in this paper focuses on extracting entities that describe the same visual concept, making the formulation similar to coreference resolution. Our task differs from conventional coreference resolution that it requires visual grounding. In addition, the targets of coreference resolution are the entities in a sentence or a document, while our targets are the entities in the captions of an image that are quasi-paraphrases but are not related to each other in discourse level like sentences in a document. For coreference resolution, the context in a sentence or discourse information in a"
C18-1295,D13-1008,0,0.0137758,"rt the results of the first attempt toward iParaphrasing. 1 Introduction A paraphrase is a restatement of the meaning of a word, phrase, or sentence within the context of a specific language (e.g., “a red jersey” and “a red uniform shirt” in Figure 1 are paraphrases) (Bhagat and Hovy, 2013). Paraphrases have been exploited for natural language understanding, and shown to be very effective for various natural language processing (NLP) tasks, including question answering (Riezler et al., 2007), summarization (Zhou et al., 2006), machine translation (Chu and Kurohashi, 2016), text normalization (Ling et al., 2013), textual entailment recognition (Androutsopoulos and Malakasiotis, 2010), and semantic parsing (Berant and Liang, 2014). In this paper, we propose a novel task named, iParaphrasing, to extract visually grounded paraphrases (VGPs). We define VGPs as different phrasal expressions that describe the same visual concept in an image. Nowadays, with the spread of the web and social media, it is easy to collect large amounts of images with their describing text. For example, different news sites release news with the same topic using the same image; photos with many comments are posted to social netw"
C18-1295,D08-1084,0,0.0283139,"parable corpora (a set of monolingual corpora that describe roughly the same topic in the same language) (Barzilay and Lee, 2003; Chen and Dolan, 2011). Distributional similarity stems from the distributional hypothesis (Harris, 1954), stating that words/phrases that share similar meanings should appear in similar distributions. This approach sometimes suffers from noisy results, because the distributed similarity often maps antonyms to closer points. Some methods try to extract paraphrases from monolingual parallel corpora (a collection of sentence level paraphrases) (Arase and Tsujii, 2017; MacCartney et al., 2008), but such monolingual parallel corpora are rarely available. Bilingual parallel corpora (a collection of sentence-aligned bilingual text) enjoys more availability than monolingual parallel corpora as they are mandatory for training machine translation systems. Bilingual parallel corpora can be used for paraphrase extraction, with bilingual pivoting (Bannard and CallisonBurch, 2005). This method assumes that two source phrases are a paraphrase pair if they are translated to the same target phrase. Bilingual pivoting has been further refined by using syntax information (CallisonBurch, 2008) or"
C18-1295,D09-1040,0,0.0332873,"opose a supervised neural network (NN)-based method using both textual and visual features to explicitly model the similarity of an entity pair as VGPs (Section 5). Experiments show that our proposed NN-based method outperforms the other methods.1 2 2.1 Related Work Paraphrase Extraction Previous studies extract paraphrases from either monolingual corpora or bilingual parallel corpora. One major approach is to use the distributional similarity (Harris, 1954) with regular monolingual corpora (a large collection of text in a single language) (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008; Marton et al., 2009), or monolingual comparable corpora (a set of monolingual corpora that describe roughly the same topic in the same language) (Barzilay and Lee, 2003; Chen and Dolan, 2011). Distributional similarity stems from the distributional hypothesis (Harris, 1954), stating that words/phrases that share similar meanings should appear in similar distributions. This approach sometimes suffers from noisy results, because the distributed similarity often maps antonyms to closer points. Some methods try to extract paraphrases from monolingual parallel corpora (a collection of sentence level paraphrases) (Aras"
C18-1295,C16-1313,0,0.0301505,"r Vision and Language Tasks Vision and language tasks have been a hot research area recently in both the CV and NLP communities. Various efforts have been made for many multimodal tasks such as image object/region referring expression grounding (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017; Cirik et al., 2018) visual captioning (Vinyals et al., 2015; Xu et al., 2015; Bernardi et al., 2016; Laokulrat et al., 2016), text-image retrieval (Otani et al., 2016), visual question answering (Wu et al., 2017), visual dialog (Das et al., 2017a; Das et al., 2017b) and video event detection (Phan et al., 2016). Some researchers also have employed images for improving NLP tasks, such as multimodal machine translation (Specia et al., 2016), cross-lingual document retrieval (Funaki and Nakayama, 2015), and textual entailment recognition (Han 2 Although (Plummer et al., 2015) annotated the VGPs in the Filickr30k entities dataset, they did not propose any methods to extract them. (Regneri et al., 2013) collected sentence level paraphrases by aligning video scripts with the same time frame; these sentence level paraphrases are essentially similar to captions of an image. 3 If we treat multiple captions a"
C18-1295,Q13-1003,0,0.119042,"et al., 2016; Laokulrat et al., 2016), text-image retrieval (Otani et al., 2016), visual question answering (Wu et al., 2017), visual dialog (Das et al., 2017a; Das et al., 2017b) and video event detection (Phan et al., 2016). Some researchers also have employed images for improving NLP tasks, such as multimodal machine translation (Specia et al., 2016), cross-lingual document retrieval (Funaki and Nakayama, 2015), and textual entailment recognition (Han 2 Although (Plummer et al., 2015) annotated the VGPs in the Filickr30k entities dataset, they did not propose any methods to extract them. (Regneri et al., 2013) collected sentence level paraphrases by aligning video scripts with the same time frame; these sentence level paraphrases are essentially similar to captions of an image. 3 If we treat multiple captions as a document forcibly, we could apply a coreference resolution approach for our task. 3481 a male the pitcher s(i, j) a ball a base Figure 2: An overview of our VGP extraction formulation. We extract VGP via clustering, where the entity-entity similarity s(i, j) is the key. We compare both unsupervised and supervised methods using entity-image and entity-entity associations for computing this"
C18-1295,P07-1059,0,0.017913,"sing. We apply various existing methods as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward iParaphrasing. 1 Introduction A paraphrase is a restatement of the meaning of a word, phrase, or sentence within the context of a specific language (e.g., “a red jersey” and “a red uniform shirt” in Figure 1 are paraphrases) (Bhagat and Hovy, 2013). Paraphrases have been exploited for natural language understanding, and shown to be very effective for various natural language processing (NLP) tasks, including question answering (Riezler et al., 2007), summarization (Zhou et al., 2006), machine translation (Chu and Kurohashi, 2016), text normalization (Ling et al., 2013), textual entailment recognition (Androutsopoulos and Malakasiotis, 2010), and semantic parsing (Berant and Liang, 2014). In this paper, we propose a novel task named, iParaphrasing, to extract visually grounded paraphrases (VGPs). We define VGPs as different phrasal expressions that describe the same visual concept in an image. Nowadays, with the spread of the web and social media, it is easy to collect large amounts of images with their describing text. For example, diffe"
C18-1295,J01-4004,0,0.134235,"t al., 2015), 1 Codes and data for reproducing the results reported in this paper are available at https://github.com/ids-cv/ coling_iparaphrasing 3480 which usually contain several captions for each image, and currently scale to sub-million images, instead of a bilingual parallel corpus with limited availability. To the best of our knowledge, this is the first study that aims to extract paraphrases from such multimodal datasets consisting of images and their captions.2 2.2 Coreference Resolution Coreference resolution is a task to find the expressions that refer to the same entity in a text (Soon et al., 2001; Lee et al., 2017). Our task in this paper focuses on extracting entities that describe the same visual concept, making the formulation similar to coreference resolution. Our task differs from conventional coreference resolution that it requires visual grounding. In addition, the targets of coreference resolution are the entities in a sentence or a document, while our targets are the entities in the captions of an image that are quasi-paraphrases but are not related to each other in discourse level like sentences in a document. For coreference resolution, the context in a sentence or discours"
C18-1295,W16-2346,0,0.05101,"Missing"
C18-1295,Q16-1019,0,0.0406956,"n, 2015) for an input image; vn is a 512 dimensional vector at position n of V . Given an entity feature vector ti and vn , we first transform them with fully connected (FC) layers whose unit sizes are 512: ˜ n = normL2 (W1 vn + b1 ), v ˜ti = normL2 (W2 ti + b2 ) (8) 6 The Fisher vector is constructed with 30 centers of both first and second order information, which results in a very sparse vector whose dimensionality is 300 × 30 × 2 = 18000. Therefore, we apply principal component analysis (PCA) to convert it to a lower dimensionality of 4,096. 7 https://github.com/ShaoqingRen/faster rcnn 8 (Yin et al., 2016) proposed a CNN network with attention for sentence level paraphrase identification; our model differs from theirs that we fuse both textual and visual information while theirs is a text only model. 9 An image is split into 14 × 14 = 196 sub-images, and represented as a 196 × 512 feature map. 3484 where normL2 (·) indicates L2 normalization to an input vector. We then compute an attention value an for vn as: exp(en ) hn = relu(˜ vn + ˜ti ), en = w&gt; hn , an = PN (9) n=1 exp(en ) where N = 196. After obtaining an , we fuse a visual and an entity feature vector to yi as: c= N X an vn , yi = U [no"
C18-1295,Q14-1006,0,0.309529,"a paraphrase pair if they are translated to the same target phrase. Bilingual pivoting has been further refined by using syntax information (CallisonBurch, 2008) or mutual information (Kajiwara et al., 2017). These methods have led to the construction of a multilingual paraphrase database (Ganitkevitch and Callison-Burch, 2014). Note that our definition of paraphrases may look different from the studies mentioned above, as our paraphrases are a set of noun phrases that represent the same visual concept. Our idea to extract paraphrases under this definition is to use image captioning datasets (Young et al., 2014; Chen et al., 2015), 1 Codes and data for reproducing the results reported in this paper are available at https://github.com/ids-cv/ coling_iparaphrasing 3480 which usually contain several captions for each image, and currently scale to sub-million images, instead of a bilingual parallel corpus with limited availability. To the best of our knowledge, this is the first study that aims to extract paraphrases from such multimodal datasets consisting of images and their captions.2 2.2 Coreference Resolution Coreference resolution is a task to find the expressions that refer to the same entity in"
C18-1295,N06-1057,0,0.0533861,"s as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward iParaphrasing. 1 Introduction A paraphrase is a restatement of the meaning of a word, phrase, or sentence within the context of a specific language (e.g., “a red jersey” and “a red uniform shirt” in Figure 1 are paraphrases) (Bhagat and Hovy, 2013). Paraphrases have been exploited for natural language understanding, and shown to be very effective for various natural language processing (NLP) tasks, including question answering (Riezler et al., 2007), summarization (Zhou et al., 2006), machine translation (Chu and Kurohashi, 2016), text normalization (Ling et al., 2013), textual entailment recognition (Androutsopoulos and Malakasiotis, 2010), and semantic parsing (Berant and Liang, 2014). In this paper, we propose a novel task named, iParaphrasing, to extract visually grounded paraphrases (VGPs). We define VGPs as different phrasal expressions that describe the same visual concept in an image. Nowadays, with the spread of the web and social media, it is easy to collect large amounts of images with their describing text. For example, different news sites release news with t"
C18-1295,D17-1001,0,\N,Missing
C18-1295,J13-3001,0,\N,Missing
chu-etal-2012-chinese,chou-etal-2008-extended,0,\N,Missing
chu-etal-2012-chinese,W08-1907,0,\N,Missing
chu-etal-2012-chinese,I05-1059,0,\N,Missing
chu-etal-2012-chinese,chou-huang-2006-hantology,0,\N,Missing
chu-etal-2014-constructing,C04-1151,0,\N,Missing
chu-etal-2014-constructing,J93-2003,0,\N,Missing
chu-etal-2014-constructing,J05-4003,0,\N,Missing
chu-etal-2014-constructing,P07-2045,0,\N,Missing
chu-etal-2014-constructing,W13-2505,1,\N,Missing
chu-etal-2014-constructing,P09-2057,0,\N,Missing
chu-etal-2014-constructing,P03-1010,0,\N,Missing
chu-etal-2014-constructing,2012.eamt-1.7,1,\N,Missing
D19-1146,2012.eamt-1.60,0,0.0681834,"and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson"
D19-1146,P17-2061,1,0.921873,"whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular,"
D19-1146,P15-1166,0,0.17506,"al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more difficult. Dong et al. (2015) have shown that a one-to-many model trained on middle-sized parallel data (200k sentence pairs) can improve the translation quality over a one-to-one model. However, it is unclear whether this works for much resource-poorer, more distant, and more diverse language pairs. Using pseudo-parallel data is a potential solution, but for most resource-poor languages, the amount of available clean and in-domain monolingual data are limited. It is also unclear what the real reason behind improvements in translation is: increase in the training data or multilingualism. This paper focuses on (a) improvin"
D19-1146,N16-1101,0,0.0294968,"Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed"
D19-1146,N18-1032,0,0.0267464,"9. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these appr"
D19-1146,D18-1398,0,0.0280459,"9. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these appr"
D19-1146,L18-1545,0,0.0789616,"from the overlapping vocabularies of Chinese and Japanese, the performance of other languages also improves despite no vocabulary overlap. Consequently, we believe that translation into a low-resource target language might benefit when the language is also target side of the helping corpus but multistage fine-tuning does not require such overlap and instead shows optimal performance when it leverages multilingualism during stage-wise tuning. In the future, we will test the above hypotheses thoroughly via some controlled experiments by using, for instance, larger scale multi-parallel corpora (Imamura and Sumita, 2018) and varieties of helping corpora. 5.2 How Does Multilingualism Help? The ALT corpus is multi-parallel and merely comprises of the same sentences in multiple languages. Although we used seven target languages, we did not introduce new English sentences to the source side. Table 1 shows that some languages are better translated (En-Vi) than others (En-Bn). We speculate that the representations for En-Vi might be better learned than those for En-Bn. When learning all language pairs jointly, the representations of sentences with the same meaning tend to be similar. As such, the representations fo"
D19-1146,W19-6613,1,0.78878,"from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular, we consider exploiting two types of corpora. One is a small-scale multi-parallel corpus, En-YY1 · · ·-YYN , consisting of English and N target languages of interest. T"
D19-1146,Q17-1024,0,0.0864411,"Missing"
D19-1146,2005.mtsummit-papers.11,0,0.188451,"Japanese, Khmer, Malay, and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, wherea"
D19-1146,I17-2050,0,0.0225962,"cs 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multi"
D19-1146,P02-1040,0,0.104338,"tter than the 1-to-1 “En-YY” model (#1), the best model without the multi-parallel corpus (#2–#4), and the strongest baselines (#5–#6). the BLEU score on the development set (simply concatenated unlike training data) did not vary by more than 0.1 BLEU over 10 checkpoints. Instead of choosing the model with the best development set BLEU, we averaged the last 10 checkpoints saved every after 1,000 updates, following Vaswani et al. (2017), and decoded the test sets with a beam size of 4 and a length penalty, α, of 0.6 consistently across all the models. 4.3 Results Table 1 gives the BLEU scores (Papineni et al., 2002) for all the configurations. Among the seven configurations, irrespective of the external parallel corpus for En-XX, the three-stage fine-tuned model (#7) achieved the highest BLEU scores for all the seven target languages. Results for #1 demonstrate that NMT systems trained on 18k parallel sentences can achieve only poor results for Bn and Ja, whereas reasonably high BLEU scores (> 20) are achieved for the other target languages. Introducing a large external En-XX parallel corpus improved the translation quality consistently and significantly for all the seven target languages,14 irrespective"
D19-1146,P16-1009,0,0.0870722,"arameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular, we consider exploiting two types of corpora. One is a small-scale multi-parallel corpus, En-YY1 · · ·-YYN , consisting of English and N target languages of interest. The other is a relatively larger helping parallel corpus, En-XX, where XX indicates the helping target language which needs not be one of the tar"
D19-1146,L16-1561,0,0.0220411,"s (Bengali, Filipino, Indonesian, Japanese, Khmer, Malay, and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked o"
D19-1146,N16-1004,0,0.0202007,"and German–English (Bojar et al., 2018), it performs poorly for resource-poor pairs (Zoph et al., 2016; Riza et al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more difficult. Dong et al. (2015) have shown that a one-to-many model trained on middle-sized parallel data (200k sentence pairs) can improve the translation quality over a one-to-one model. However, it is unclear whether this works for much resource-poorer, more distant, and more diverse language pairs. Using pseudo-parallel data is a potential solution, but for most resource-poor languages, the amount of available clean and in-domain monolingual data are limited. It is also unclear what the real reason behind improvem"
D19-1146,D16-1163,0,0.450445,"tent-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage finetuning can give 3–9 BLEU score gains over a simple one-to-one model. 1 Introduction Encoder-decoder based neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation model. While NMT is known to perform well for resource-rich language pairs, such as French– English and German–English (Bojar et al., 2018), it performs poorly for resource-poor pairs (Zoph et al., 2016; Riza et al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more di"
I13-1163,2012.eamt-1.7,1,0.845483,"s. Fragments with less than 3 words may be produced in this process, and we discard them like previous studies. 4 Experiments In our experiments, we compared our proposed fragment extraction method with (Munteanu and Marcu, 2006). We manually evaluated the accuracy of the extracted fragments. Moreover, we used the extracted fragments as additional MT training data, and evaluated the effectiveness of the fragments for MT. We conducted experiments on Chinese–Japanese data. In all our experiments, we preprocessed the data by segmenting Chinese and Japanese sentences using a segmenter proposed by Chu et al. (2012) and JUMAN (Kurohashi et al., 1994) respectively. 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST4 and NICT5 . This corpus was created by the Japanese project “Development and Research of Chinese–Japanese Natural Language Processing Technology”, containing 680k sentences (18.2M Chinese and 21.8M Japanese tokens respectively). This corpus contains various domains such as chemistry, physics, biology and agriculture etc. 4.1.2 Quasi–Comparable Corpora The quasi–comparable corpora we used are scientific paper abstracts collected from"
I13-1163,C04-1151,0,0.226919,"several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments"
I13-1163,W11-1209,0,0.0172353,"wo generative alignment models for extracting parallel fragments from comparable sentences. However, the extracted fragments slightly decrease MT performance when appending them to in–domain training data. We think the reason is that because the comparable sentences are quite noisy, the alignment models cannot accurately extract parallel fragments. To solve this problem we only use alignment models for parallel fragment candidate detection, and use an accurate lexicon filter to guarantee the accuracy of the extracted parallel fragments. Besides the above studies, there are some other efforts. Hewavitharana and Vogel (2011) propose a method that calculates both the inside and outside probabilities for fragments in a comparable sentence pair, and show that the context of the sentence helps fragment extraction. However, the proposed method only can be efficient in a controlled manner that supposes the source fragment was known, and search for the target fragment. Another study uses a syntax–based alignment model to extract parallel fragments from noisy parallel data (Riesa and Marcu, 2012). Since their method is designed for noisy parallel data, we believe that the method cannot accurately extract parallel fragmen"
I13-1163,P07-2045,0,0.0250392,"rom quasi–comparable corpora. To solve this problem, we propose an accurate parallel fragment extraction system that uses an alignment model to locate the parallel fragment candidates, and uses an accurate lexicon filter to identify the truly parallel ones. Experimental results indicate that our system can accurately extract parallel fragments, and our proposed method significantly outperforms a state–of–the–art approach. Furthermore, we investigate the factors that may affect the performance of our system in detail. 1 Introduction In statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009"
I13-1163,J05-4003,0,0.53616,"(Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot"
I13-1163,N10-1063,0,0.0218879,"since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accuratel"
I13-1163,P09-2057,0,0.013739,"et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel"
I13-1163,P03-1010,0,0.0486948,"machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the"
I13-1163,C12-1166,0,0.0237647,"Missing"
I13-1163,P11-2084,0,0.0442669,"Missing"
I13-1163,P06-1011,0,0.56179,"le in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable (Munteanu and Marcu, 2006). Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007). Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese–Japanese corpora show that our proposed method significantly outperforms a state–of–the– art approach, which indicate the effectiveness of our para"
I13-1163,2007.mtsummit-papers.50,0,0.444915,"However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable (Munteanu and Marcu, 2006). Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007). Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese–Japanese corpora show that our proposed method significantly outperforms a state–of–the– art approach, which indicate the effectiveness of our parallel fragment extraction system. Moreover, we investigate the factors that may affect the performance of our system"
I13-1163,N12-1061,0,0.0190235,"o guarantee the accuracy of the extracted parallel fragments. Besides the above studies, there are some other efforts. Hewavitharana and Vogel (2011) propose a method that calculates both the inside and outside probabilities for fragments in a comparable sentence pair, and show that the context of the sentence helps fragment extraction. However, the proposed method only can be efficient in a controlled manner that supposes the source fragment was known, and search for the target fragment. Another study uses a syntax–based alignment model to extract parallel fragments from noisy parallel data (Riesa and Marcu, 2012). Since their method is designed for noisy parallel data, we believe that the method cannot accurately extract parallel fragments from comparable sentences. 3 Proposed Method 3.1 System Overview Figure 1 shows an overview of our parallel fragment extraction system. We first apply comparable sentence extraction using a combination method of (Abdul-Rauf and Schwenk, 2011) (1)(2) and (Munteanu and Marcu, 2005) (3), which were originally used for extracting parallel sentences from comparable corpora. We translate the source sentences to target language with a SMT system trained on a parallel corpu"
I13-1163,J93-2003,0,\N,Missing
L16-1101,P98-1013,0,0.434745,"n et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous studies use paraphrases generated by bilingual pivoting (Callison-Burch et al., 2006), distributional similarity (Marton et al., 2009), and graph propagation (Razmara et al., 2013), which suffer from high computational complexity. In this study, we propose using word embeddings (Mikolov et al., 2013) to address this problem. We also propose using semantic lexicons including WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) for paraphrasing. In addition, we apply a method to combine these two types of paraphrases (Faruqui et al., 2015), which achieves further improvements in SMT. 2. Paraphrasing Out-of-Vocabulary Words In this paper, we study on phrase based SMT (Koehn et al., 2003). Figure 1 shows an overview of our proposed method. We first construct a phrase table based on unsupervised word alignments, containing phrase pairs together with their feature scores. From the development and test -.)///)0.)///),1-./0.2)$031-./0.2)4)) -5)///)05)///),1-5"
L16-1101,N06-1003,0,0.312393,"In statistical machine translation (SMT) (Koehn et al., 2003), because translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English-French, English-Arabic, English-Chinese and several European language pairs, parallel data remains a scarce resource. Moreover, even for these language pairs, the available domains are limited. The scarceness of parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in sufficient size in one domain, this OOV problem occurs when the domain shifts. Irvine et al. (2013a) showed that SMT performance decreases significantly when using a system trained on one domain to translate texts in different domains mainly because of OOVs. As one of the ways to address the OOV problem, paraphrasing has been proposed (Callison-Burch et al., 2006; Marton et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous stud"
L16-1101,D13-1167,0,0.0222894,"setting. We processed the training corpus using sub-sentence splitting following (Chu et al., 2012b). The development and test sets have only one reference, which contain 1,050 and 998 sentences respectively. For more details of this task, please refer to (Federico et al., 2012). 2.3. Combination 3.2. Settings One problem of word embeddings is that they are learnt without supervision, which limits of the quality. To achieve better quality embeddings, applying the existing semantic lexicons to change the objective of embedding training (Yu and Dredze, 2014), and relation-specific augmentation (Chang et al., 2013) have been studied. Here, we apply the word embedding retrofitting method (Faruqui et al., 2015), because of its independence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the parallel corpus using the SRILM toolkit4 with interpolated Kn"
L16-1101,2012.eamt-1.7,1,0.855467,"h we set both to 1 in our experiments. Taking the first derivative of Ψ with respect to one qi vector, we arrive at the following online update by equating it to zero: ∑ j:(i,j)∈E βij qj + αi qˆi qi = ∑ (2) j:(i,j)∈E βij + αi Following (Faruqui et al., 2015), we run 10 iterations for this update. After retrofitting, we use the new embeddings for paraphrasing in the same manner as before. 3. Experiments We conducted English-to-Chinese translation experiments in a low resource setting. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012a), and tokenizing English sentences. 3.1. Task We conducted our experiments on the OLYMPICS task of IWSLT 2012 (Federico et al., 2012). The OLYMPICS task is carried out using parts of the HIT Olympic Trilingual Corpus (HIT) (Yang et al., 2006) and the Basic Travel Expression Corpus (BTEC) as an additional training corpus. The HIT corpus is a multilingual corpus that covers 5 domains (traveling, dining, sports, traffic and business) that are closely related to the Beijing 2008 Olympic Games. The HIT corpus contains around 52k sentences 2.8 million words in total. The BTEC corpus is a multiling"
L16-1101,2012.iwslt-evaluation.12,1,0.823586,"h we set both to 1 in our experiments. Taking the first derivative of Ψ with respect to one qi vector, we arrive at the following online update by equating it to zero: ∑ j:(i,j)∈E βij qj + αi qˆi qi = ∑ (2) j:(i,j)∈E βij + αi Following (Faruqui et al., 2015), we run 10 iterations for this update. After retrofitting, we use the new embeddings for paraphrasing in the same manner as before. 3. Experiments We conducted English-to-Chinese translation experiments in a low resource setting. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012a), and tokenizing English sentences. 3.1. Task We conducted our experiments on the OLYMPICS task of IWSLT 2012 (Federico et al., 2012). The OLYMPICS task is carried out using parts of the HIT Olympic Trilingual Corpus (HIT) (Yang et al., 2006) and the Basic Travel Expression Corpus (BTEC) as an additional training corpus. The HIT corpus is a multilingual corpus that covers 5 domains (traveling, dining, sports, traffic and business) that are closely related to the Beijing 2008 Olympic Games. The HIT corpus contains around 52k sentences 2.8 million words in total. The BTEC corpus is a multiling"
L16-1101,P11-2071,0,0.0597613,"Missing"
L16-1101,N15-1184,0,0.116725,"ase pair. PPDB is packaged in 6 sizes from S to XXXL to leverage precision and coverage, and it is also divided into to lexical and phrasal paraphrases. In our experiments, we used the lexical paraphrases in the XL size. where n is the vocabulary size; E denotes a relation in the semantic lexicons; αi and βij control the relative strengths of associations, which we set both to 1 in our experiments. Taking the first derivative of Ψ with respect to one qi vector, we arrive at the following online update by equating it to zero: ∑ j:(i,j)∈E βij qj + αi qˆi qi = ∑ (2) j:(i,j)∈E βij + αi Following (Faruqui et al., 2015), we run 10 iterations for this update. After retrofitting, we use the new embeddings for paraphrasing in the same manner as before. 3. Experiments We conducted English-to-Chinese translation experiments in a low resource setting. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012a), and tokenizing English sentences. 3.1. Task We conducted our experiments on the OLYMPICS task of IWSLT 2012 (Federico et al., 2012). The OLYMPICS task is carried out using parts of the HIT Olympic Trilingual Corpus (HIT) (Yang et al., 200"
L16-1101,2012.iwslt-evaluation.1,0,0.0309765,"Missing"
L16-1101,N13-1092,0,0.104069,"Missing"
L16-1101,W13-2233,0,0.0525157,"Missing"
L16-1101,Q13-1035,0,0.0750334,"quantity of parallel data are crucial. However, except for a few language pairs, such as English-French, English-Arabic, English-Chinese and several European language pairs, parallel data remains a scarce resource. Moreover, even for these language pairs, the available domains are limited. The scarceness of parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in sufficient size in one domain, this OOV problem occurs when the domain shifts. Irvine et al. (2013a) showed that SMT performance decreases significantly when using a system trained on one domain to translate texts in different domains mainly because of OOVs. As one of the ways to address the OOV problem, paraphrasing has been proposed (Callison-Burch et al., 2006; Marton et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous studies use paraphrases generated by bilingual pivoting (Callison-Burch et al., 2006), distributional similarity (Marton et al., 2009), a"
L16-1101,D13-1109,0,0.0374,"Missing"
L16-1101,N03-1017,0,0.0240435,"em in statistical machine translation (SMT) with low resources. OOV paraphrasing that augments the translation model for the OOV words by using the translation knowledge of their paraphrases has been proposed to address the OOV problem. In this paper, we propose using word embeddings and semantic lexicons for OOV paraphrasing. Experiments conducted on a low resource setting of the OLYMPICS task of IWSLT 2012 verify the effectiveness of our proposed method. Keywords: Paraphrasing, Out-of-Vocabulary Word, Word Embedding, Semantic Lexicon 1. Introduction In statistical machine translation (SMT) (Koehn et al., 2003), because translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English-French, English-Arabic, English-Chinese and several European language pairs, parallel data remains a scarce resource. Moreover, even for these language pairs, the available domains are limited. The scarceness of parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in"
L16-1101,P07-2045,0,0.00855169,"applying the existing semantic lexicons to change the objective of embedding training (Yu and Dredze, 2014), and relation-specific augmentation (Chang et al., 2013) have been studied. Here, we apply the word embedding retrofitting method (Faruqui et al., 2015), because of its independence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the parallel corpus using the SRILM toolkit4 with interpolated Kneser-Ney discounting, and used it for all the experiments. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. The skip-gram model (Mikolov et al., 2013) was trained on the English Gigaword version 5.0,5 with the word2vec tool. We removed the punctuations in the corpus, obtaining about 3.95B tokens with a vocabulary size of 854k. The context window size was set to 5, and the vector size was"
L16-1101,W04-3250,0,0.0538852,"ord2vec” denotes the system paraphrased with the word embeddings obtained by word2vec; “WordNet synonyms”, “WordNet all”, “FrameNet” and “PPDB” denote the systems paraphrased with different semantic lexicons. “Word2vec retrofitted *” denote the systems paraphrased with the word2vec word embeddings retrofitted by different semantic lexicons. They were evaluated on BLEU-4 scores and OOV rates. The OOV rate was the percentage of the OOV words out of the total number of source words in the development/test sets. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). We can see that the OOV rate of the Baseline system is high, because of the small size of the parallel corpus for training. Both Word2vec and semantic lexicons decrease the OOV rate, and thus improve the MT performance. Although Word2vec is unsupervised learnt, it shows better results than the semantic lexicons that are either manual created or collected with supervised data. The reason for this is the lower coverage of the semantic lexicons compared to Word2vec, leading to lower OOV decreases. The combination of Word2vec and the semantic lexicons by retrofitting outperforms either method, b"
L16-1101,D09-1040,0,0.38028,"Missing"
L16-1101,P03-1021,0,0.00539277,"ndence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the parallel corpus using the SRILM toolkit4 with interpolated Kneser-Ney discounting, and used it for all the experiments. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. The skip-gram model (Mikolov et al., 2013) was trained on the English Gigaword version 5.0,5 with the word2vec tool. We removed the punctuations in the corpus, obtaining about 3.95B tokens with a vocabulary size of 854k. The context window size was set to 5, and the vector size was set to 200. Ψ(Q) = n ∑ ∑ {αi ||qi − qˆi ||2 + βij ||qi − qj ||2 } (1) i=1 3 (i,j)∈E 4 5 http://www.cis.upenn.edu/˜ccb/ppdb/ 645 http://www.speech.sri.com/projects/srilm LDC2011T07 Method Baseline Word2vec WordNet synonyms Word2vec retrofitted by WordNet synonyms WordNet all"
L16-1101,P13-1109,0,0.249507,"parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in sufficient size in one domain, this OOV problem occurs when the domain shifts. Irvine et al. (2013a) showed that SMT performance decreases significantly when using a system trained on one domain to translate texts in different domains mainly because of OOVs. As one of the ways to address the OOV problem, paraphrasing has been proposed (Callison-Burch et al., 2006; Marton et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous studies use paraphrases generated by bilingual pivoting (Callison-Burch et al., 2006), distributional similarity (Marton et al., 2009), and graph propagation (Razmara et al., 2013), which suffer from high computational complexity. In this study, we propose using word embeddings (Mikolov et al., 2013) to address this problem. We also propose using semantic lexicons including WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and the Paraphra"
L16-1101,P14-2089,0,0.0257282,"entences, we consider the OLYMPICS task as a low resource setting. We processed the training corpus using sub-sentence splitting following (Chu et al., 2012b). The development and test sets have only one reference, which contain 1,050 and 998 sentences respectively. For more details of this task, please refer to (Federico et al., 2012). 2.3. Combination 3.2. Settings One problem of word embeddings is that they are learnt without supervision, which limits of the quality. To achieve better quality embeddings, applying the existing semantic lexicons to change the objective of embedding training (Yu and Dredze, 2014), and relation-specific augmentation (Chang et al., 2013) have been studied. Here, we apply the word embedding retrofitting method (Faruqui et al., 2015), because of its independence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the para"
L16-1101,C98-1013,0,\N,Missing
L16-1348,W06-2810,0,0.0605838,"Missing"
L16-1348,P91-1022,0,0.765952,", and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are generally based on two kinds of algorithms: length-based algorithms align sentences according solely to their lengths (Brown et al., 1991), while lexicon-based algorithms use lexical information to calculate similarity between source and target sentences (Ma, 2006). Length-based algorithms are typically faster but not suited for processing noisy corpus (corpus containing article pairs with omitted or wrong translations). Lexicon-based algorithms, like the one used in Champollion (Ma, 2006)1 are more robust. However their performance highly depends on the coverage and quality of the lexicon, and large-scale lexicon with high quality is not easy to obtain especially for low resource language pairs. In this paper, we propose to use"
L16-1348,Y15-1033,1,0.941209,"sentences (Ma, 2006). Length-based algorithms are typically faster but not suited for processing noisy corpus (corpus containing article pairs with omitted or wrong translations). Lexicon-based algorithms, like the one used in Champollion (Ma, 2006)1 are more robust. However their performance highly depends on the coverage and quality of the lexicon, and large-scale lexicon with high quality is not easy to obtain especially for low resource language pairs. In this paper, we propose to use lexicons generated by a pivotbased MT system, which could be constructed even for low resource languages (Dabre et al., 2015). Experiments conducted on Chinese-Japanese scientific articles verify the effectiveness of our proposed method. 2. Simultaneous Sentence Boundary Detection and Alignment Our proposed alignment method consists in the following steps 1 1. Split source and target articles into sentence candidates. 2. Normalize words in sentence candidates to maximize matching rate between words in source, target, and lexicons. 3. Compute alignment path with the highest similarity between source and target. An alignment path is composed of pairs of article segments.2 4. Adjust sentence boundaries by merging sente"
L16-1348,W04-1101,0,0.0367831,"Missing"
L16-1348,D07-1103,0,0.100786,"Missing"
L16-1348,N03-1017,0,0.00709525,"he input article pairs using pivot-based MT, achieving better coverage of the input words with fewer entries than pre-existing dictionaries. Pivot-based MT makes it possible to build dictionaries for language pairs that have scarce parallel data. The alignment method is implemented in a tool that will be freely available in the near future. Keywords: Sentence Alignment, Sentence Segmentation, Pivot-based Machine Translation 1. Introduction Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair, which are crucial for machine translation (MT) (Koehn et al., 2003). Previous studies first split the source and target articles into sentences respectively using punctuation information, and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are"
L16-1348,C10-2081,0,0.0412132,"Missing"
L16-1348,ma-2006-champollion,0,0.123676,"l data. The alignment method is implemented in a tool that will be freely available in the near future. Keywords: Sentence Alignment, Sentence Segmentation, Pivot-based Machine Translation 1. Introduction Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair, which are crucial for machine translation (MT) (Koehn et al., 2003). Previous studies first split the source and target articles into sentences respectively using punctuation information, and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are generally based on two kinds of algorithms: length-based algorithms align sentences according solely to their lengths (Brown et al., 1991), while lexicon-based algorithms use lexical information to calculate similarity between so"
L16-1348,moore-2002-fast,0,0.168305,"Missing"
L16-1348,2010.amta-papers.14,0,0.0401683,"Missing"
L16-1348,P94-1012,0,0.0759164,".2871 0.1824 0.7646 0.7653 0.7883 0.7932 0.7955 0.7968 0.4820 0.2372 0.5619 0.5393 0.4352 0.3049 Table 5: Sentence alignment results of the proposed method “Hard+Soft” with similarity threshold tuned on the development set for maximum F measure and precision ≥ 0.9. Dictionary None EDR MT-Noun MT-NVAA EDR+MT-Noun EDR+MT-NVAA Coverage 0.27 0.39 0.42 0.45 0.46 0.48 Table 6: Lexicons coverage on the test set. segmentation and alignment issues, taking advantage of intermediate alignment data to adjust sentence boundaries. Lexicon-based algorithms require a dictionary that may be generated offline (Wu, 1994; Ma, 2006), or online, for example by comparing the number of occurrences and distribution of words on both sides of the bilingual corpus (Kay and R¨oscheisen, 1993). Our proposed method follows an intermediate approach, where lexicons are generated automatically using a pivot-based MT system. Since lexicons are generated from words in the corpus, they achieve a high coverage ratio with a low number of entries. MT systems have previously been used for sentence alignment, by calculating similarity scores between target and MT translation of the source. (Adafre and De Rijke, 2006) uses word-lev"
L16-1348,P11-2111,0,0.0604551,"Missing"
L16-1348,J93-1006,0,\N,Missing
L16-1468,J93-2003,0,0.0626585,"Missing"
L16-1468,D14-1179,0,0.0370302,"Missing"
L16-1468,2012.eamt-1.7,1,0.859034,"parallel sentences extracted by the baseline system. The models trained on the seed parallel corpus are used for producing the NN features for training and testing the classifier. We tried the use of both the two types of models to score the parallel sentence candidates for extraction. 3. Experiments We evaluated classification accuracy, and conducted extraction and translation experiments on Chinese-Japanese data to verify the effectiveness of our proposed NN features. In all our experiments, we preprocessed the data by segmenting Chinese and Japanese sentences using a segmenter proposed by Chu et al. (2012) and JUMAN (Kurohashi et al., 1994) respectively. 3.1. Data The seed parallel corpus we used is the Chinese-Japanese section of the Asian Scientific Paper Excerpt Corpus (ASPEC),5 containing 680k sentences pairs (18.2M Chinese and 21.8M Japanese tokens, respectively). Also, we downloaded Chinese6 (20120921) and Japanese7 (20120916) Wikipedia database dumps. We used an opensource Python script8 to extract and clean the text from the dumps. Since the Chinese dump is mixed of Traditional and Simplified Chinese, we converted all Traditional Chinese to Simplified Chinese using a conversion table pu"
L16-1468,chu-etal-2014-constructing,1,0.784656,"Missing"
L16-1468,Y15-1033,1,0.847103,"ning “+NN-ASPEC” and “+NN-WIKI” are also greatly different (680k versus 126k). As NMT is sensitive to the quality and quantity of the training data (Bahdanau et al., 2014), “+NN-ASPEC” outperformed “+NN-WIKI”. 4. Related Work Several studies have exploited the NN features for SMT. (Sutskever et al., 2014) used the NN features to rerank the N-best list of a SMT system, which achieved a BLEU score that is close to the previous state of the art. (Cho et al., 2014) scored the phrase pairs of a SMT system with a neural translation model, and used the scores as additional NN features for decoding. (Dabre et al., 2015) used the NN features for a pivot-based SMT system for dictionary construction. In contrast, we score the sentence pairs of with a neural translation model, and use the scores as NN features for parallel sentence extraction from comparable corpora. 5. Conclusion In this paper, we incorporated the NN features for parallel sentence extraction from comparable corpora for the first time. Experimental results verified the effectiveness of NN features for this task. As future work, we plan to address the domain problem of “+NN-ASPEC” by a NN based sentence selection method. Namely, we train NN model"
L16-1468,P07-2045,0,0.00628691,"Missing"
L16-1468,J05-4003,0,0.739012,"o be viewed as bilingual language models, they can be used to generate scores for candidate translations as neural network (NN) features for reranking the n-best lists produced by a statistical machine translation (SMT) system, whose quality rivals the state of the art (Sutskever et al., 2014). Comparable corpora are a set of monolingual corpora that describe roughly the same topic in different languages. Although they are not exact translation equivalents of each other, there are a large amount of parallel sentences contained in the comparable texts. The task of parallel sentence extraction (Munteanu and Marcu, 2005) is to identify truly parallel sentences from the erroneous ones from comparable corpora. Intuitively, because the NN features give a measure of the bilingual similarity of a sentence pair, they could be helpful for this task. However, this assumption has not been verified previously. In this paper, we incorporate the NN features into a robust parallel sentence extraction system (Chu et al., 2014), which consists of a parallel sentence candidate filter and a binary classifier for parallel sentence identification. The NN features are naturally used as additional features for the classifier. Exp"
P16-3002,N03-1017,0,0.0321069,"Missing"
P16-3002,li-etal-2010-enriching,0,0.0603641,"Missing"
P16-3002,W09-3818,0,0.0243735,"word if the nodes in dependency trees have different spans. For example, in Figure 1 there are two nodes for the word “boy” because they have different spans (i.e., (2, 4) and (2, 7)). The construction of a dependency forest from dependency trees is done by sharing the common nodes and edges (Line 1). The common nodes are those with the same span and part-of-speech (POS) . Note that the dependency forest obtained from this method does not necessarily encode exactly the dependency trees from which they are created. Usually there are more trees that can be extracted from the dependency forests (Boullier et al., 2009). In our experiment, when we use the term “a n-best dependency forest”, we indicate a dependency forest that is created from n-best dependency trees. 2.2 Finding Alignments over Forest Following the hierarchical alignment model (Riesa et al., 2011), our model searches for the best alignment by constructing partial alignments (hypotheses) over target dependency forests in a bottom-up manner as shown in Figure 1. The algorithm for constructing alignments is shown in Algorithm 2. Note that source dependency forests are included in the input to the algorithm. This is optional but can be included f"
P16-3002,I11-1089,1,0.748713,"e used 300, 100, 100 sentences from ASPEC-JE2 for training, development and test data, respectively.4 Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200.5 The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile6 , the Bayesian subtree alignment model (Nakazawa model) (Nakazawa and Kurohashi, 2011) and IBM Model4.7 We used Nile without automatically extracted rule features and constellation features to make a fair comparison with our model. We observed the improvement of alignments by using forests. We checked whether good parse trees were chosen when higher F-scores were achieved. It turned out that better parse trees led to higher F-scores, as shown in Figure 2a, but it was not always the case. Figure 2a shows an improved example by using 100-best trees on the target side. In the figure, we can observe that “の” and “of” are correctly aligned. We observe that the English 1-best parse t"
P16-3002,J04-4002,0,0.342213,"Missing"
P16-3002,J93-2003,0,0.150318,"Missing"
P16-3002,J07-2003,0,0.103676,"t, we compute its score using local features (Line 7) and pushed to a priority queue Bv (Line 8). These partial alignments are represented by black squares in a blue container in Figure 1. Then, we compute partial alignments for the target words covered by the node, by combining tails’ partial alignments and one column alignments for its word using nonlocal features (Line 10 - 14), which is represented by the orange arrows in Figure 1. k-best combined partial alignments are put in Yv (Line 14). They are represented by black squares in a yellow container in Figure 1. Here, we use cube pruning (Chiang, 2007) to get the approximate k-best combinations. Note that in the search over constituency parse trees, one column alignment matrices are generated only on the leaf node (Riesa et al., 2011), whereas we generate them also on nonleaf nodes in the search over dependency forests. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Input : Source and target sentence s, t Dependency forest Fs over s Dependency forest Ft over t Set of feature functions h Weight vector w Beam size k Output: A k-best list of alignments over s and t for v ∈TopologicalSort(Ft ) do links = ∅ Bv = ∅ i = word-index-of(v) links = {(0, i)} ∪Sin"
P16-3002,P11-1042,0,0.0350707,"Missing"
P16-3002,D11-1046,0,0.0967341,"one by sharing the common nodes and edges (Line 1). The common nodes are those with the same span and part-of-speech (POS) . Note that the dependency forest obtained from this method does not necessarily encode exactly the dependency trees from which they are created. Usually there are more trees that can be extracted from the dependency forests (Boullier et al., 2009). In our experiment, when we use the term “a n-best dependency forest”, we indicate a dependency forest that is created from n-best dependency trees. 2.2 Finding Alignments over Forest Following the hierarchical alignment model (Riesa et al., 2011), our model searches for the best alignment by constructing partial alignments (hypotheses) over target dependency forests in a bottom-up manner as shown in Figure 1. The algorithm for constructing alignments is shown in Algorithm 2. Note that source dependency forests are included in the input to the algorithm. This is optional but can be included for richer features. Each node in the forest has partial alignments sorted by alignment scores. Because it is computationally expensive to keep all possible partial alignments for each node, we keep a beam size of k. A partial alignment for a node i"
P16-3002,P08-1067,0,0.0388108,"alignments sorted by alignment scores. Because it is computationally expensive to keep all possible partial alignments for each node, we keep a beam size of k. A partial alignment for a node is an alignment matrix for target words that are cov9 ered by the node. In Figure 1, each partial alignment is represented as a black square. Scores of the partial alignments are a linear combination of features. There are two types of features: local and non-local features. A feature f is defined to be local if and only if it can be factored among the local productions in a tree, and non-local otherwise (Huang, 2008). We visit the nodes in the topological order, to guarantee that we visit a node after visiting all its tail nodes (Line 1). For each node, we first generates partial alignments, which are one column alignment matrices for its word. Because of time complexity, we only generates null, single link and double link alignment (Line 5). A single and double link alignment refer to a column matrix having exactly one and two alignments, respectively, as shown in Figure 1. For each partial alignment, we compute its score using local features (Line 7) and pushed to a priority queue Bv (Line 8). These par"
P16-3002,D10-1052,0,0.0546363,"Missing"
P16-3002,P08-1066,0,0.0553831,"Missing"
P16-3002,P14-1138,0,0.0195662,"ult: Black boxes represent golden alignments. Triangles represent 1-best model alignments. Circles represent the alignments of proposed model. Black and red arcs represent 1-best parses and chosen parses respectively. humans. However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests resp"
P16-3002,C10-1123,0,0.238834,"ing syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees. It achieved significantly better results than the IBM Model4 in Arabic-English and ChineseEnglish word alignment tasks, even though the model was trained on only 2,280 and 1,102 parallel sentences as gold standard alignments. However, their models rely only on 1-best source and target side parse trees, which are not necessarily good for word alignment tasks. In SMT, forest-based decoding has been proposed for both constituency and dependency parse trees (Mi et al., 2008; Tu et al., 2010). A forest is a compact representation of n-best parse trees. It provides more alternative parse trees to choose from during decoding, leading to significant improvements in translation quality. In this paper, we borrow this idea to build an alignment model using dependency forests rather than 1-best parses, which makes it possible to provide the model with more alternative parse trees that may be suitable for word alignment tasks. The motivation of using dependency forests instead of constituency forests in our model is that dependency forests are more appropriate for alignments between langu"
P16-3002,2008.amta-papers.18,0,0.0237822,". However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests respectively for decoding. The best path that is suitable for translation is chosen from the forest during decoding, leading to significant improvement in translation quality. Note that they do not use forests for obtaining word ali"
P16-3002,P13-1017,0,0.0179165,"riangles represent 1-best model alignments. Circles represent the alignments of proposed model. Black and red arcs represent 1-best parses and chosen parses respectively. humans. However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests respectively for decoding. The best path that is s"
P16-3002,N07-1051,0,\N,Missing
P16-3002,D09-1106,0,\N,Missing
P16-3002,D14-1197,0,\N,Missing
P17-2061,W04-3250,0,0.160581,"43 20.01 37.66 35.79 33.74 34.61 37.57 37.23 37.77 Table 2: Domain adaptation results (BLEU-4 scores) for WIKI-CJ using ASPEC-CJ. 5 Results Tables 1 and 2 show the translation results on the Chinese-to-English and Chinese-to-Japanese tasks, respectively. The entries with SMT and NMT are the PBSMT and NMT systems, respectively; others are the different methods described in Section 3. In both tables, the numbers in bold indicate the best system and all systems that were not significantly different from the best system. The significance tests were performed using the bootstrap resampling method (Koehn, 2004) at p < 0.05. We can see that without domain adaptation, the SMT systems perform significantly better than the NMT system on the resource poor domains, i.e., IWSLT-CE and WIKI-CJ; while on the resource rich domains, i.e., NTCIR-CE and ASPECCJ, NMT outperforms SMT. Directly using the SMT/NMT models trained on the out-of-domain data to translate the in-domain data shows bad performance. With our proposed “Mixed fine tuning” domain adaptation method, NMT significantly outperforms SMT on the in-domain tasks. Comparing different domain adaptation methods, “Mixed fine tuning” shows the best perfor6"
P17-2061,D14-1179,0,0.0132915,"Missing"
P17-2061,P07-2045,0,0.00716769,"1 2.55 16.12 16.56 15.02 16.41 18.77 18.10 17.65 test 2013 14.67 7.29 4.74 2.85 17.12 17.54 15.96 16.80 18.63 17.97 17.94 average 14.31 7.87 4.33 2.60 16.41 16.34 14.97 15.82 18.01 17.43 17.11 Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE. System WIKI-CJ SMT WIKI-CJ NMT ASPEC-CJ SMT ASPEC-CJ NMT Fine tuning Multi domain Multi domain w/o tags Multi domain + Fine tuning Mixed fine tuning Mixed fine tuning w/o tags Mixed fine tuning + Fine tuning For performance comparison, we also conducted experiments on phrase based SMT (PBSMT). We used the Moses PBSMT system (Koehn et al., 2007) for all of our MT experiments. For the respective tasks, we trained 5-gram language models on the target side of the training data using the KenLM toolkit6 with interpolated KneserNey discounting, respectively. In all of our experiments, we used the GIZA++ toolkit7 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. For both MT systems, we preprocessed the data as follows. For Chinese, we used KyotoMorph8 for segmentation, which was trained on the CTB version 5 (CTB5) and SCTB (Chu et al., 2016b). For English, we tokeniz"
P17-2061,L16-1468,1,0.872655,"Missing"
P17-2061,W16-5407,1,0.90265,"ag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain was automatically extracted (Chu et al., 2016a). We observed that “mixed fine tuning” works significantly better than methods that use fine tuning ∗ This work was done when the first author was a researcher of Japan Science and Technology Agency. 385 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 385–391 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2061 Source-Target (out-of-domain) NMT e control the politeness of NMT translations. The overview of this method is shown in the dotted section in Fig"
P17-2061,2015.iwslt-evaluation.11,0,0.147063,"em without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpu"
P17-2061,W16-4616,1,0.50204,"Missing"
P17-2061,D16-1046,0,0.0226074,"hat combines the best of existing approaches and show that it is effective. We can further fine tune the multi domain model on the in-domain data, which is named as “multi domain + fine tuning.” • To the best of our knowledge this is the first work on an empirical comparison of various domain adaptation methods. 2 3.3 Related Work The proposed mixed fine tuning method is a combination of the above methods (shown in Figure 2). The training procedure is as follows: Fine tuning has also been explored for various NLP tasks using neural networks such as sentiment analysis and paraphrase detection (Mou et al., 2016). Tag based NMT has also been shown to be effective for control the politeness of translations (Sennrich et al., 2016a) and multilingual NMT (Johnson et al., 2016). Besides fine tuning and multi domain NMT using tags, another direction of domain adaptation for NMT is using in-domain monolingual data. Either training an in-domain recurrent neural network (RNN) language model for the NMT decoder (G¨ulc¸ehre et al., 2015) or generating synthetic data by back translating target in-domain monolingual data (Sennrich et al., 2016b) have been studied. 3 1. Train an NMT model on out-of-domain data till"
P17-2061,W15-5001,1,0.85268,"Missing"
P17-2061,Q17-1024,0,0.0315585,"Missing"
P17-2061,P03-1021,0,0.0134846,"i domain w/o tags Multi domain + Fine tuning Mixed fine tuning Mixed fine tuning w/o tags Mixed fine tuning + Fine tuning For performance comparison, we also conducted experiments on phrase based SMT (PBSMT). We used the Moses PBSMT system (Koehn et al., 2007) for all of our MT experiments. For the respective tasks, we trained 5-gram language models on the target side of the training data using the KenLM toolkit6 with interpolated KneserNey discounting, respectively. In all of our experiments, we used the GIZA++ toolkit7 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. For both MT systems, we preprocessed the data as follows. For Chinese, we used KyotoMorph8 for segmentation, which was trained on the CTB version 5 (CTB5) and SCTB (Chu et al., 2016b). For English, we tokenized and lowercased the sentences using the tokenizer.perl script in Moses. Japanese was segmented using JUMAN9 (Kurohashi et al., 1994). For NMT, we further split the words into subwords using byte pair encoding (BPE) (Sennrich et al., 2016c), which has been shown to be effective for the rare word problem in NMT. Another motivation of using sub-word"
P17-2061,N16-1005,0,0.0989293,"Missing"
P17-2061,P16-1009,0,0.516582,"al with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain"
P17-2061,P16-1162,0,0.519258,"al with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain"
P17-2061,D16-1163,0,0.0622938,"shortcomings. 1 Introduction One of the most attractive features of neural machine translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted r"
P17-2061,W14-7001,1,\N,Missing
P17-2061,2015.iwslt-evaluation.1,0,\N,Missing
P18-3004,D17-1309,0,0.0161544,"2015) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. Neubig et al. (2012) and Nakagawa (2015) proposed methods that construct a binary tree and reordering simultaneously from a source sentence. These methods require a manual feature design for every language pair, which makes language dependent design costly. To overcome this challenge, methods based on feed forward neural networks that do not require a manual feature design have been proposed (de Gispert et al., 2015; Botha et al., 2017). However, these methods decide whether to reorder child nodes without considering the sub-trees, which contains important information for reordering. As a preordering method that is free of manual feature design and makes use of information in sub-trees, we propose a preordering method with a recursive neural network (RvNN). RvNN calculates reordering in a bottom-up manner (from the leaf nodes to the root) on a source syntax tree. Thus, preordering is performed considering the entire sub-trees. Specifically, RvNN learns whether to reorder nodes of a syntax tree1 with a vector representation o"
P18-3004,E14-1026,0,0.0287335,"Missing"
P18-3004,C10-1043,0,0.0302111,"Neural Network Based Preordering for English-to-Japanese Machine Translation Yuki Arase† Yuki Kawara† Chenhui Chu‡ † Graduate School of Information Science and Technology, Osaka University ‡ Institute for Datability Science, Osaka University {kawara.yuki,arase}@ist.osaka-u.ac.jp,chu@ids.osaka-u.ac.jp Abstract computational complexity issues (Jehl et al., 2014; Nakagawa, 2015). Rule-based preordering methods either manually create reordering rules (Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012) or extract reordering rules from a corpus (Xia and McCord, 2004; Genzel, 2010). On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; Nakagawa, 2015) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. Neubig et al. (2012) and Nakagawa (2015) proposed methods that construct a binary tree and reordering simultaneously from a source sentence. These methods require a manual feature design for every language pair, which makes language dependent design costly. To overcome this challenge, methods based on feed for"
P18-3004,N15-1105,0,0.0368797,"Missing"
P18-3004,E12-1074,0,0.0198702,"between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preordering effectively improves the translation quality because it solves long-distance reordering and 1 In this paper, we used binary syntax trees. 21 Proceedings of ACL 2018, Student Research Workshop, pages 21–27 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics meth"
P18-3004,W04-3250,0,0.119984,"ing was performed by minimum error rate training (Och, 2003). We repeated tuning and testing of each model 3 times and reported the average of scores. For NMT, we used the attention-based encoderdecoder model of (Luong et al., 2015) with 2-layer LSTM implemented in OpenNMT.10 The sizes of the vocabulary, word embedding, and hidden layer were set to 50k, 500, and 500, respectively. The batch size was set to 64, and the number of epochs was set to 13. The translation quality was evaluated using BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) using the bootstrap resampling method (Koehn, 2004) for the significance test. 3.2 Results Figure 2 shows the learning curve of our preordering model with λ = 200.11 Both the training and 2 http://stanfordnlp.github.io/CoreNLP/ http://www.nactem.ac.uk/enju/ 4 http://taku910.github.io/mecab/ 5 http://github.com/moses-smt/giza-pp 6 http://chainer.org/ 7 http://github.com/google/topdown-btg-preordering 3 8 http://www.statmt.org/moses/ http://github.com/kpu/kenlm 10 http://opennmt.net/ 11 The learning curve behaves similarly for different λ values. 9 23 Avogadro ’s hypothesis ( 1811 ) contributed to the development in since then Figure 4: Example"
P18-3004,P12-2061,0,0.023575,"es the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preordering effectively improves the translation quality because it solves long-distance reordering and 1 In this paper, we used binary syntax trees. 21 Proceedings of ACL 2018, Student Research Workshop, pages 21–27 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics method for English-to-Japanese translations using both phr"
P18-3004,2005.iwslt-1.8,0,0.0169266,"is paper, we propose a preordering method with a recursive neural network that learns features from raw inputs. Experiments show that the proposed method achieves comparable gain in translation quality to the state-of-the-art method but without a manual feature design. 1 Introduction The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preorderi"
P18-3004,D13-1049,0,0.0192538,"anslation Yuki Arase† Yuki Kawara† Chenhui Chu‡ † Graduate School of Information Science and Technology, Osaka University ‡ Institute for Datability Science, Osaka University {kawara.yuki,arase}@ist.osaka-u.ac.jp,chu@ids.osaka-u.ac.jp Abstract computational complexity issues (Jehl et al., 2014; Nakagawa, 2015). Rule-based preordering methods either manually create reordering rules (Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012) or extract reordering rules from a corpus (Xia and McCord, 2004; Genzel, 2010). On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; Nakagawa, 2015) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. Neubig et al. (2012) and Nakagawa (2015) proposed methods that construct a binary tree and reordering simultaneously from a source sentence. These methods require a manual feature design for every language pair, which makes language dependent design costly. To overcome this challenge, methods based on feed forward neural networks that do not require a manual feature design have been pr"
P18-3004,D13-1139,0,0.0222165,"reordering can effectively address this problem. Previous preordering methods require a manual feature design, making language dependent design costly. In this paper, we propose a preordering method with a recursive neural network that learns features from raw inputs. Experiments show that the proposed method achieves comparable gain in translation quality to the state-of-the-art method but without a manual feature design. 1 Introduction The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sen"
P18-3004,D15-1166,0,0.0584354,"ies 100 200 22.73 24.63 24.95 25.22 25.41 500 25.02 25.38 Table 1: BLEU scores with preordering by our model and without preordering under different λ settings (trained on a 500k subset of the training data). 50 words or the source to target length ratio exceeded 9. For SMT, we used Moses.8 We trained the 5-gram language model on the target side of the training corpus with KenLM.9 Tuning was performed by minimum error rate training (Och, 2003). We repeated tuning and testing of each model 3 times and reported the average of scores. For NMT, we used the attention-based encoderdecoder model of (Luong et al., 2015) with 2-layer LSTM implemented in OpenNMT.10 The sizes of the vocabulary, word embedding, and hidden layer were set to 50k, 500, and 500, respectively. The batch size was set to 64, and the number of epochs was set to 13. The translation quality was evaluated using BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) using the bootstrap resampling method (Koehn, 2004) for the significance test. 3.2 Results Figure 2 shows the learning curve of our preordering model with λ = 200.11 Both the training and 2 http://stanfordnlp.github.io/CoreNLP/ http://www.nactem.ac.uk/enju/ 4 http://taku"
P18-3004,P15-2023,0,0.0131131,"i Kawara† Chenhui Chu‡ † Graduate School of Information Science and Technology, Osaka University ‡ Institute for Datability Science, Osaka University {kawara.yuki,arase}@ist.osaka-u.ac.jp,chu@ids.osaka-u.ac.jp Abstract computational complexity issues (Jehl et al., 2014; Nakagawa, 2015). Rule-based preordering methods either manually create reordering rules (Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012) or extract reordering rules from a corpus (Xia and McCord, 2004; Genzel, 2010). On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; Nakagawa, 2015) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. Neubig et al. (2012) and Nakagawa (2015) proposed methods that construct a binary tree and reordering simultaneously from a source sentence. These methods require a manual feature design for every language pair, which makes language dependent design costly. To overcome this challenge, methods based on feed forward neural networks that do not require a manual feature design have been proposed (de Gispert et"
P18-3004,P06-1090,0,0.724324,"a preordering method with a recursive neural network that learns features from raw inputs. Experiments show that the proposed method achieves comparable gain in translation quality to the state-of-the-art method but without a manual feature design. 1 Introduction The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preordering effectively improve"
P18-3004,D10-1092,0,0.0865117,"Missing"
P18-3004,P15-1021,0,0.485328,"† Graduate School of Information Science and Technology, Osaka University ‡ Institute for Datability Science, Osaka University {kawara.yuki,arase}@ist.osaka-u.ac.jp,chu@ids.osaka-u.ac.jp Abstract computational complexity issues (Jehl et al., 2014; Nakagawa, 2015). Rule-based preordering methods either manually create reordering rules (Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012) or extract reordering rules from a corpus (Xia and McCord, 2004; Genzel, 2010). On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; Nakagawa, 2015) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. Neubig et al. (2012) and Nakagawa (2015) proposed methods that construct a binary tree and reordering simultaneously from a source sentence. These methods require a manual feature design for every language pair, which makes language dependent design costly. To overcome this challenge, methods based on feed forward neural networks that do not require a manual feature design have been proposed (de Gispert et al., 2015; Botha"
P18-3004,W10-1736,0,0.130313,"oduction The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preordering effectively improves the translation quality because it solves long-distance reordering and 1 In this paper, we used binary syntax trees. 21 Proceedings of ACL 2018, Student Research Workshop, pages 21–27 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Compu"
P18-3004,D12-1077,0,0.104765,"o-Japanese Machine Translation Yuki Arase† Yuki Kawara† Chenhui Chu‡ † Graduate School of Information Science and Technology, Osaka University ‡ Institute for Datability Science, Osaka University {kawara.yuki,arase}@ist.osaka-u.ac.jp,chu@ids.osaka-u.ac.jp Abstract computational complexity issues (Jehl et al., 2014; Nakagawa, 2015). Rule-based preordering methods either manually create reordering rules (Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012) or extract reordering rules from a corpus (Xia and McCord, 2004; Genzel, 2010). On the other hand, studies in (Neubig et al., 2012; Lerner and Petrov, 2013; Hoshino et al., 2015; Nakagawa, 2015) apply machine learning to the preordering problem. Hoshino et al. (2015) proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. Neubig et al. (2012) and Nakagawa (2015) proposed methods that construct a binary tree and reordering simultaneously from a source sentence. These methods require a manual feature design for every language pair, which makes language dependent design costly. To overcome this challenge, methods based on feed forward neural networks that do not require a manual fe"
P18-3004,P03-1021,0,0.194904,"pairs whose lengths were longer than Figure 2: Learning curve of our preordering model. Node dimensions w/o preordering w/o tags and categories w/ tags and categories 100 200 22.73 24.63 24.95 25.22 25.41 500 25.02 25.38 Table 1: BLEU scores with preordering by our model and without preordering under different λ settings (trained on a 500k subset of the training data). 50 words or the source to target length ratio exceeded 9. For SMT, we used Moses.8 We trained the 5-gram language model on the target side of the training corpus with KenLM.9 Tuning was performed by minimum error rate training (Och, 2003). We repeated tuning and testing of each model 3 times and reported the average of scores. For NMT, we used the attention-based encoderdecoder model of (Luong et al., 2015) with 2-layer LSTM implemented in OpenNMT.10 The sizes of the vocabulary, word embedding, and hidden layer were set to 50k, 500, and 500, respectively. The batch size was set to 64, and the number of epochs was set to 13. The translation quality was evaluated using BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) using the bootstrap resampling method (Koehn, 2004) for the significance test. 3.2 Results Figure 2"
P18-3004,P02-1040,0,0.107117,"We trained the 5-gram language model on the target side of the training corpus with KenLM.9 Tuning was performed by minimum error rate training (Och, 2003). We repeated tuning and testing of each model 3 times and reported the average of scores. For NMT, we used the attention-based encoderdecoder model of (Luong et al., 2015) with 2-layer LSTM implemented in OpenNMT.10 The sizes of the vocabulary, word embedding, and hidden layer were set to 50k, 500, and 500, respectively. The batch size was set to 64, and the number of epochs was set to 13. The translation quality was evaluated using BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a) using the bootstrap resampling method (Koehn, 2004) for the significance test. 3.2 Results Figure 2 shows the learning curve of our preordering model with λ = 200.11 Both the training and 2 http://stanfordnlp.github.io/CoreNLP/ http://www.nactem.ac.uk/enju/ 4 http://taku910.github.io/mecab/ 5 http://github.com/moses-smt/giza-pp 6 http://chainer.org/ 7 http://github.com/google/topdown-btg-preordering 3 8 http://www.statmt.org/moses/ http://github.com/kpu/kenlm 10 http://opennmt.net/ 11 The learning curve behaves similarly for different λ values. 9 23 Avogadro"
P18-3004,W16-4621,0,0.0254414,"o 0, while that without preordering was set to 6. Compared to the plain PBSMT without preordering, both BLEU and RIBES increased significantly with preordering by RvNN and BTG. These scores were comparable (statistically insignificant at p < 0.05) between RvNN and BTG,12 indicating that the proposed method achieves a translation quality comparable to BTG. In contrast to the case of PBSMT, NMT without preordering achieved a significantly higher BLEU score than NMT models with preordering by RvNN and BTG. This is the same phenomenon in the Chinese-to-Japanese translation experiment reported in (Sudoh and Nagata, 2016). We assume that one reason is the isolation between preordering and NMT models, where both models are trained using independent optimization functions. In the future, we will investigate this problem and consider a model that unifies Table 2: BLEU and RIBES scores on the test set. (All models are trained on the entire training corpus of 1.8M sentence pairs.) Numbers in bold indicate the best systems and the systems that are statistically insignificant at p < 0.05 from the best systems. 0.20 tau of w/o preordering tau of preordering with BTG tau of preordering with RvNN proportion 0.15 0.10 0."
P18-3004,N04-4026,0,0.071816,"e translation. Preordering can effectively address this problem. Previous preordering methods require a manual feature design, making language dependent design costly. In this paper, we propose a preordering method with a recursive neural network that learns features from raw inputs. Experiments show that the proposed method achieves comparable gain in translation quality to the state-of-the-art method but without a manual feature design. 1 Introduction The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preorderi"
P18-3004,2007.mtsummit-papers.63,0,0.0815263,"as training data, 1, 790 sentence pairs as development data, and 1, 812 sentence pairs as test data. We used Stanford CoreNLP2 for tokenization and POS tagging, Enju3 for parsing of English, and MeCab4 for tokenization of Japanese. For word alignment, we used MGIZA.5 Source-to-target and target-to-source word alignments were calculated using IBM model 1 and hidden Markov model, and they were combined with the intersection heuristic following (Nakagawa, 2015). We implemented our RvNN preordering model with Chainer.6 The ASPEC corpus was created using the sentence alignment method proposed in (Utiyama and Isahara, 2007) and was sorted based on the alignment confidence scores. In this paper, we used 100k sentences sampled from the top 500k sentences as training data for preordering. The vocabulary size N was set to 50k. We used Adam (Kingma and Ba, 2015) with a weight decay and gradient clipping for optimization. The mini batch size K was set to 500. We compared our model with the state-of-theart preordering method proposed in (Nakagawa, 2015), which is hereafter referred to as BTG. We used its publicly available implementation,7 and trained it on the same 100k sentences as our model. We used the 1.8M source"
P18-3004,D07-1077,0,0.0480034,"hout a manual feature design. 1 Introduction The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preordering effectively improves the translation quality because it solves long-distance reordering and 1 In this paper, we used binary syntax trees. 21 Proceedings of ACL 2018, Student Research Workshop, pages 21–27 c Melbourne, Australia, July 15"
P18-3004,C04-1073,0,0.0509849,"the-art method but without a manual feature design. 1 Introduction The word order between source and target languages significantly influences the translation quality in statistical machine translation (SMT) (Tillmann, 2004; Hayashi et al., 2013; Nakagawa, 2015). Models that adjust orders of translated phrases in decoding have been proposed to solve this problem (Tillmann, 2004; Koehn et al., 2005; Nagata et al., 2006). However, such reordering models do not perform well for long-distance reordering. In addition, their computational costs are expensive. To address these problems, preordering (Xia and McCord, 2004; Wang et al., 2007; Xu et al., 2009; Isozaki et al., 2010b; Gojun and Fraser, 2012; Nakagawa, 2015) and post-ordering (Goto et al., 2012, 2013; Hayashi et al., 2013) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preordering effectively improves the translation quality because it solves long-distance reordering and 1 In this paper, we used binary syntax trees. 21 Proceedings of ACL 2018, Student Research Workshop, pages 21–27 c Melbourne,"
P18-3004,N09-1028,0,\N,Missing
W13-2505,C04-1151,0,0.794908,", 606-8501, Japan {chu,nakazawa}@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp Abstract Non–parallel corpora include various levels of comparability: noisy parallel, comparable and quasi–comparable. Noisy parallel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is signi"
W13-2505,I05-1059,0,0.0349896,"n Hanzi and Kanji. Table 1 gives some examples of common Chinese characters in Traditional Chinese, Simplified Chinese and Japanese with their Unicode. Since Chinese characters contain significant semantic information, and common Chinese characters share the same meaning, they can be valuable linguistic clues for many Chinese–Japanese NLP tasks. Many studies have exploited common Chinese characters. Tan et al. (1995) used the occurrence of identical common Chinese characters in Chinese and Japanese (e.g. “snow” in Table 1) in automatic sentence alignment task for document– level aligned text. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese, but different from Simplified Chinese (e.g. “love” in Table 1). Using a Chinese encoding converter1 that can convert Traditional Chinese into Simplified Chinese, they built a Japanese–Simplified Chinese dictionary partly using direct conversion of Japanese into Chinese for Japanese Kanji words. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chi"
W13-2505,P07-2045,0,0.00752983,"rable corpora. We adopt a system proposed by Munteanu and Marcu (2005), which is for parallel sentence extraction from comparable corpora. We extend the system in several aspects to make it even suitable for quasi–comparable corpora. The core component of the system is a classifier which can identify parallel sentences from non–parallel sentences. Previous method of classifier training by the Cartesian product is not practical, because it differs from the real process of parallel sentence extraction. We propose a novel Introduction In statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), the quality and quantity of the parallel sentences are crucial, because translation knowledge is acquired from a sentence–level aligned parallel corpus. However, except for a few language pairs, such as English–French, English–Arabic and English– Chinese, parallel corpora remain a scarce resource. The cost of manual construction for parallel corpora is high. As non–parallel corpora are far more available, constructing parallel corpora from non–parallel corpora is an attractive research field. 34 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 34–42, c Sofia, B"
W13-2505,N03-2016,0,0.0538577,"Missing"
W13-2505,W06-2810,0,0.0821037,"Missing"
W13-2505,2011.mtsummit-papers.53,1,0.83952,"Missing"
W13-2505,J05-4003,0,0.130834,"lel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conduct"
W13-2505,2012.eamt-1.7,1,0.810272,"training and testing method is that features from the IR results can be used. Here, we use the ranks of the retrieved documents returned by the IR framework as feature. 4 4.2.1 Settings • Probabilistic dictionary: We took the top 5 translations with translation probability larger than 0.1 created from the parallel corpus. Experiments We conducted classification and translation experiments to evaluate the effectiveness of our proposed parallel sentence extraction system. • IR tool: Indri7 with the top 10 results. • Segmenter: For Chinese, we used a segmenter optimized for Chinese–Japanese SMT (Chu et al., 2012a). For Japanese, we used JUMAN (Kurohashi et al., 1994). 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST3 and NICT4 . This corpus was created by the Japanese project “Development and Research of Chinese– Japanese Natural Language Processing Technology”, containing various domains such as chemistry, physics, biology and agriculture etc. This corpus is aligned in both sentence–level and document–level, containing 680k sentences and 100k articles. • Alignment: GIZA++8 . • SMT: We used the state–of–the–art phrase– based SMT toolkit"
W13-2505,P06-1011,0,0.0243421,"ank (Proposed)”, where parallel subsentential fragments are in bold. We investigated the alignment results of the extracted sentences. We found that most of the parallel subsentential fragments were correctly aligned with the help of the parallel sentences in the baseline system. Therefore, translation performance was improved by appending the extracted sentences. However, it also led to many wrong alignments among the non– parallel fragments which are harmful to translation. In the future, we plan to further extract these parallel subsentential fragments, which can be more effective for SMT (Munteanu and Marcu, 2006). 6 Conclusion and Future Work In this paper, we proposed a novel method of classifier training and testing that simulates the real parallel sentence extraction process. Furthermore, we used linguistic knowledge of Chinese character features. Experimental results of parallel sentence extraction from quasi–comparable corpora indicated that our proposed system performs significantly better than the previous study. 5 Related Work As parallel sentences trend to appear in similar document pairs, many studies first conduct document matching, then identify the parallel sen40 Our approach can be impro"
W13-2505,chu-etal-2012-chinese,1,0.833746,"training and testing method is that features from the IR results can be used. Here, we use the ranks of the retrieved documents returned by the IR framework as feature. 4 4.2.1 Settings • Probabilistic dictionary: We took the top 5 translations with translation probability larger than 0.1 created from the parallel corpus. Experiments We conducted classification and translation experiments to evaluate the effectiveness of our proposed parallel sentence extraction system. • IR tool: Indri7 with the top 10 results. • Segmenter: For Chinese, we used a segmenter optimized for Chinese–Japanese SMT (Chu et al., 2012a). For Japanese, we used JUMAN (Kurohashi et al., 1994). 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST3 and NICT4 . This corpus was created by the Japanese project “Development and Research of Chinese– Japanese Natural Language Processing Technology”, containing various domains such as chemistry, physics, biology and agriculture etc. This corpus is aligned in both sentence–level and document–level, containing 680k sentences and 100k articles. • Alignment: GIZA++8 . • SMT: We used the state–of–the–art phrase– based SMT toolkit"
W13-2505,N10-1063,0,0.489733,"anslated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extract Chinese–Japanese parallel sentences from quasi–comparable corpora, which are"
W13-2505,2012.eamt-1.37,0,0.0343286,"Missing"
W13-2505,P09-2057,0,0.767195,"igned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting"
W13-2505,P03-1010,0,0.55095,"asi–comparable. Noisy parallel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many"
W13-2505,2007.mtsummit-papers.63,0,0.0421083,"lations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extra"
W13-2505,I05-1023,0,0.0313432,"with two different approaches: binary classification (Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; S¸tefˇanescu et al., 2012) and translation similarity measures (Utiyama and Isahara, 2003; Fung and Cheung, 2004; Abdul-Rauf and Schwenk, 2011). We adopt the binary classification approach with a novel classifier training and testing method and Chinese character features. Few studies have been conducted for extracting parallel sentences from quasi–comparable corpora. We are aware of only two previous efforts. Fung and Cheung (2004) proposed a multi-level bootstrapping approach. Wu and Fung (2005) exploited generic bracketing Inversion Transduction Grammars (ITG) for this task. Our approach differs from the previous studies that we extend the approach for comparable corpora in several aspects to make it work well for quasi–comparable corpora. Table 4: BLEU scores for Chinese–to–Japanese translation experiments (“†” and “‡” denotes the result is better than “Munteanu+ 2005 (Cartesian)” significantly at p &lt; 0.05 and p &lt; 0.01 respectively, “*” denotes the result is better than “Baseline” significantly at p &lt; 0.01). 4.3.3 Discussion The translation results indicate that compared to the pre"
W13-2505,J93-2003,0,\N,Missing
W15-5006,P05-1022,0,0.129705,"Missing"
W15-5006,N12-1047,0,0.0279359,"2014), which makes use of our dependency parses in order to capture non-local reorderings. 3.2 • Forest parse scores • Number of content/function words aligned to content/function words • Number of times a subtree is inserted in a position (left or right of parent) that is not the most common in the training data • Number of examples sharing the same information used to create an initial hypothesis • Similarity between source and input word embeddings (Mikolov et al., 2013) Forest Input The optimal weights for each feature are as before estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. We found that the quality of the source-side dependency parsing had an important impact 3 http://kheaﬁeld.com/code/kenlm/ 57 3.4 Reranking translation in the n-best list. These features were added to those used in the ﬁrst round of tuning, then one ﬁnal iteration of tuning was run. The tuning algorithm and settings were the same as for standard tuning. This retuning step was added in order to ﬁnd an optimal combination of the additional features with related features such as sentence length and the score given by the 5-gram language model used inside the decoder. A ﬁnal rer"
W15-5006,J07-2003,0,0.0620462,"X2, morphological forms of “be”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the"
W15-5006,D11-1047,1,0.850442,"view Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and sco"
W15-5006,C12-1120,1,0.883803,"Missing"
W15-5006,P14-2024,0,0.0236533,"ble combination of hypotheses. This linear model is based on a linear combination of both local features (local to each translation hypothesis) and non-local features (such as a 5-gram language model score of the ﬁnal translation). Despite our already relatively large set of dense features, we found there were a number of cases where these features were not enough to diﬀerentiate between good and bad translation hypotheses. This year we have added ten new features, now reaching a total of 52, a selection of which are shown below: Improvements from WAT2014 3.1 Alignment Based on the ﬁndings of Neubig and Duh (2014), we experimented with supervised alignment using Nile (Riesa et al., 2011) as part of our translation framework. We found that using supervised alignments made a considerable improvement to translation quality. Since Nile supports only constituency parses, we also perform constituency parsing for source and target languages for generating bidirectional word alignments. For the initial alignments for Nile, we use the alignments generated from the model described in last year’s system description (Richardson et al., 2014), which makes use of our dependency parses in order to capture non-local r"
W15-5006,W11-2123,0,0.0137398,"”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-term"
W15-5006,N15-3009,0,0.0231441,"Missing"
W15-5006,2011.iwslt-evaluation.24,0,0.0153808,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,P06-1055,0,0.256442,"Missing"
W15-5006,D12-1107,0,0.0117256,"n-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the decoder to make choices between various morphological variations of a wo"
W15-5006,P05-1034,0,0.166664,"Missing"
W15-5006,N06-1023,1,0.726271,"Missing"
W15-5006,W08-0402,0,0.0128267,"hat we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the"
W15-5006,D11-1046,0,0.0924047,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,Y12-1033,1,0.720835,"Missing"
W15-5006,D07-1104,0,0.0236985,"g. We believe that 2 System Overview Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full"
W16-2201,P06-1121,0,0.0156779,"arser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on"
W16-2201,P05-1033,0,0.0890526,"from the language side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source an"
W16-2201,W02-1001,0,0.122912,"ady have a partially projected LQ subtree. Next, we want to project a new dependency in the HQ tree to the LQ side. If adding this newly projected dependency to the partially projected subtree leads to non-projectivity,2 we give up this projection and leave the dependency as null. Many alignment errors can be detected by the property of projectivity. For example, in Figure F ∈Y The score function for each factor is denoted as the inner product of a feature and a weight vector: score(F, X) = w · f (F, X) (3) The weight vector can be learnt by e.g., the averaged structured perceptron algorithm (Collins, 2002) on an annotated treebank. During parsing, the parser would utilize the learnt weight vector to determine the best parse tree. In our partial parsing method, we aim to keep the dependencies in partial projected trees, while 2 Note that not all non-projectivites are caused by alignment errors; a few of them are also due to translation shift. 5 complement the null dependencies to construct a projective tree. To realize this, we set extremely high scores to the projected dependencies to maximize the score(F, X) for these dependencies, while for the null dependencies we set relatively small scores"
W16-2201,D11-1047,1,0.900779,"Missing"
W16-2201,D14-1063,1,0.892513,"Missing"
W16-2201,W04-3250,0,0.11489,"-trained partial parsing denotes the systems that used the Chinese parser re-trained on the projected trees for the partial parsing process. For reference, we also show the MT performance of the phrase based, string-to-tree, and tree-to-string systems, which are based on the open-source GIZA++/Moses pipeline (Koehn et al., 2007). Note that in all of the Moses, string-totree, and tree-to-string settings, Japanese is always in the string format, and Chinese is parsed by the Berkeley parser14 (Petrov and Klein, 2007).15 The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that, the Baseline KyotoEBMT system outperforms the Moses, string-to-tree, and tree-to-string systems, which verifies the effectiveness of the tree-to-tree approach. The performance difference of KyotoEBMT against the other three MT approaches on the Ja-to-Zh direction is much larger than those of the Zh-to-Ja direction. The reason for this is that KyotoEBMT is much more sensitive to the parsing accuracy on the source side, because the source tree is utilized in the ordering of the final translation. Therefore using Chinese as the source side limits the effectiveness Ja-to-Zh 13.1"
W16-2201,P09-1042,0,0.193816,"es. This extremely limits the translation quality of tree-totree MT. In this paper, we present an approach that projects dependency trees from a high quality (HQ) parser to a low quality (LQ) parser using alignment information. The projection could reduce the parsing errors on the LQ side, and address the annotation criterion difference problem. This can make the LQ trees isomorphic to the HQ trees, which can benefit the translation rule extraction in tree-to-tree MT, and thus improve the MT performance. The idea of cross-language projection of parse trees has been proposed previously, e.g., (Ganchev et al., 2009; Jiang et al., 2010; Goto Tree-to-tree machine translation (MT) that utilizes syntactic parse trees on both source and target sides suffers from the non-isomorphism of the parse trees due to parsing errors and the difference of annotation criterion between the two languages. In this paper, we present a method that projects dependency parse trees from the language side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then co"
W16-2201,P06-1077,0,0.237594,"e to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 2016. 2016 Associatio"
W16-2201,D08-1022,0,0.0294423,"l parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational L"
W16-2201,W10-1736,0,0.0700554,"Missing"
W16-2201,W15-5001,1,0.774902,"trees with the ratio higher than the threshold for training the parser. We tried several thresholds in our preliminary experiments, and selected the best threshold of 0.78 (170k trees) based on the MT performance.3 4 Experiments We conducted Japanese-Chinese MT experiments to verify the effectiveness of our constrained partial parsing based projection method. 4.1 Settings 3.3 Re-train a New Low Quality Side Parser We conducted experiments on the scientific domain MT task on the Japanese-Chinese paper excerpt corpus (ASPEC-JC),4 which is one subtask of the workshop on Asian translation (WAT)5 (Nakazawa et al., 2015). The ASPEC-JC task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. We used the tree-to-tree MT system KyotoEBMT6 (Richardson et al., 2015) for all of our MT experiments. For Chinese, we used the Chinese analyzing tool KyotoMorph7 proposed by Shen et al. (2014) for segmentation and part-of-speech (POS) tagging, and the SKP parser8 (Shen et al., 2012) for parsing. As the baseline Chinese parser, we trained SKP with the Penn Chinese treebank version 5 (CTB5)9 containing 18k sentences in news domain, and an in-house scientific domain treebank of 10k"
W16-2201,D11-1110,0,0.0211491,"trees might only contain a part of dependencies over a threshold. Our proposed method differs from the previous studies in several aspects: we propose the use of the projectivity criterion for partial projection; we utilize the original target parser and propose a constrained partial parsing algorithm; we re-train a target parser on the full trees generated by the partial parsing. To address the annotation criterion difference problem in projection, Hwa et al. (2005) firstly projected the dependency parse trees, and then applied post projection transformations based on manually created rules. Jiang et al. (2011) presented a method that tolerates the syntactic nonisomorphism between languages. This allows the projected parse trees do not have to follow the annotation criterion of the source parse trees. Our proposed method does not adjust the annotation criterion difference between the source and the projected trees, because in our tree-to-tree MT task, we prefer isomorphic trees. Only a few studies have been conducted to improve MT performance via projection. For string-to-string MT (Koehn et al., 2007), Goto et al. (2015) proposed a pre-ordering method that projects target side constituency trees to"
W16-2201,N06-1023,1,0.721456,"ee-to-tree MT system KyotoEBMT6 (Richardson et al., 2015) for all of our MT experiments. For Chinese, we used the Chinese analyzing tool KyotoMorph7 proposed by Shen et al. (2014) for segmentation and part-of-speech (POS) tagging, and the SKP parser8 (Shen et al., 2012) for parsing. As the baseline Chinese parser, we trained SKP with the Penn Chinese treebank version 5 (CTB5)9 containing 18k sentences in news domain, and an in-house scientific domain treebank of 10k sentences. For Japanese, we used JUMAN10 (Kurohashi et al., 1994) for morphological analyzing, and the KNP parser for parsing11 (Kawahara and Kurohashi, 2006). We trained two 5-gram language models for Chinese and Japanese, respectively, on the training data of the ASPEC corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting, and used them for all the experiments. In all of our experiments, we used the discriminative alignment model Nile13 (Riesa et al., 2011) for word alignment; tuning was performed by the k-best batch Re-training a new LQ parser on the projected trees is necessary for two reasons. Initially, we use the original LQ parser for the partial parsing process, because we do not have a better choice; due to the low acc"
W16-2201,P05-1034,0,0.066477,"with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 201"
W16-2201,P07-2045,0,0.0607713,"uage side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are di"
W16-2201,D15-1039,0,0.0191895,"vided into two categories: word alignment errors and annotation criterion difference (Ganchev et al., 2009). To address the word alignment error problem, several studies have proposed to train a target parser on high confidence partially projected trees. Ganchev et al. (2009) presented a partial projection method with constraints such as language-specific annotation rules. They then trained a target parser using the partially projected trees. Spreyer and Kuhn (2009) proposed a similar method that trains both graph-based and transition-based dependency parsers on the partially projected trees. Rasooli and Collins (2015) proposed a method to train a target parser on 8 “dense” projected trees. The “dense” projected trees might only contain a part of dependencies over a threshold. Our proposed method differs from the previous studies in several aspects: we propose the use of the projectivity criterion for partial projection; we utilize the original target parser and propose a constrained partial parsing algorithm; we re-train a target parser on the full trees generated by the partial parsing. To address the annotation criterion difference problem in projection, Hwa et al. (2005) firstly projected the dependency"
W16-2201,W15-5006,1,0.861406,"Missing"
W16-2201,D11-1046,0,0.0576875,"Missing"
W16-2201,P08-1066,0,0.0355068,"isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation,"
W16-2201,Y12-1033,1,0.922019,"projected dependency of (13:及其 (extremely), 19:+), which is obviously incorrect. Alignment errors could happen due to many factors, one of which is translation shift. The erroneous alignment in Figure 2 is caused by this. 3.2 Partial Parsing After the partial projection step, we obtain partial projected trees, with null dependencies discussed in Section 3.1.2. We then perform partial parsing to complement these null dependencies. Before the description of the partial parsing method, we first review the formalism of dependency parsing used in many previous studies such as (Kubler et al., 2009; Shen et al., 2012): Because of the existence of the above cases, we only apply the direct mapping method for partial projection. For the (1) and (2) cases, we leave the dependencies for these words as null. For the (3) case, we propose a projectivity criterion to detect the alignment error, and again leave the dependencies as null. Note that all of these three cases are processed during the top-down projection process. Y ∗ = argmaxY ∈Φ(X) score(Y, X) (1) where X = x1 ...xi ...xn is the input sentence, Y is a candidate tree, Φ(X) is a set of all possible dependency trees over X. Y can be denoted as Y = {(m, h) :"
W16-2201,P14-2042,1,0.853752,"strained partial parsing based projection method. 4.1 Settings 3.3 Re-train a New Low Quality Side Parser We conducted experiments on the scientific domain MT task on the Japanese-Chinese paper excerpt corpus (ASPEC-JC),4 which is one subtask of the workshop on Asian translation (WAT)5 (Nakazawa et al., 2015). The ASPEC-JC task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. We used the tree-to-tree MT system KyotoEBMT6 (Richardson et al., 2015) for all of our MT experiments. For Chinese, we used the Chinese analyzing tool KyotoMorph7 proposed by Shen et al. (2014) for segmentation and part-of-speech (POS) tagging, and the SKP parser8 (Shen et al., 2012) for parsing. As the baseline Chinese parser, we trained SKP with the Penn Chinese treebank version 5 (CTB5)9 containing 18k sentences in news domain, and an in-house scientific domain treebank of 10k sentences. For Japanese, we used JUMAN10 (Kurohashi et al., 1994) for morphological analyzing, and the KNP parser for parsing11 (Kawahara and Kurohashi, 2006). We trained two 5-gram language models for Chinese and Japanese, respectively, on the training data of the ASPEC corpus using the KenLM toolkit12 wit"
W16-2201,Y15-2010,1,0.739736,"Goto et al. (2015) proposed a pre-ordering method that projects target side constituency trees to the source side, and then generates pre-ordering rules based on the projected trees. For tree-to-string MT, Jiang et al. (2010) combined projection and supervised constituency parsing by guiding the parsing procedure of the supervised parser with the projected parser. They showed that the guided parser achieved comparable MT results on a treeto-string system (Liu et al., 2006), compared to a normal supervised parser trained on thousands of CTB trees. For tree-to-tree MT (Richardson et al., 2015), Shen et al. (2015) proposed a naive projection method. They complemented the remaining dependencies for a partially projected tree with a backtracking method. Namely, they reused the dependencies in the original target tree for the complement without considering the partially projected dependencies. In contrast, in this paper we propose partial parsing for the complement, in which we search for the best parse tree by taking account of the partially projected dependencies. isomorphic parse tree problem in a dependency based tree-to-tree MT system. Experiments verified the effectiveness of our proposed method. As"
W16-2201,W09-1104,0,0.0318536,"(e.g., English) to a low resource language, to improve the parsing accuracy of the low resource language. The difficulties in projection can be mainly divided into two categories: word alignment errors and annotation criterion difference (Ganchev et al., 2009). To address the word alignment error problem, several studies have proposed to train a target parser on high confidence partially projected trees. Ganchev et al. (2009) presented a partial projection method with constraints such as language-specific annotation rules. They then trained a target parser using the partially projected trees. Spreyer and Kuhn (2009) proposed a similar method that trains both graph-based and transition-based dependency parsers on the partially projected trees. Rasooli and Collins (2015) proposed a method to train a target parser on 8 “dense” projected trees. The “dense” projected trees might only contain a part of dependencies over a threshold. Our proposed method differs from the previous studies in several aspects: we propose the use of the projectivity criterion for partial projection; we utilize the original target parser and propose a constrained partial parsing algorithm; we re-train a target parser on the full tree"
W16-2201,P08-1064,0,0.0311285,"arsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics tative dependency based tree-to-tree MT system. Figure 1 shows"
W16-2201,N07-1051,0,\N,Missing
W16-2201,C10-2059,0,\N,Missing
W16-2201,W14-7001,1,\N,Missing
W16-2201,N12-1047,0,\N,Missing
W16-4616,C16-2064,1,0.550675,"Japanese translation direction of the ASPEC-JC task, and further updated the state-of-the-art result (46.04 → 46.36 BLEU). 3 NMT NMT is a new approach to MT that, although recently proposed, has quickly achieved state-of-the-art results (Bojar et al., 2016). We implemented our own version of the sequence-to-sequence with attention mechanism model, first proposed in (Bahdanau et al., 2015). Our implementation was done using the Chainer3 toolkit (Tokui et al., 2015). We make this implementation available under a GPL license.4 3 4 http://chainer.org/ https://github.com/fabiencro/knmt . See also (Cromieres, 2016) 167 Figure 2: The structure of a NMT system with attention, as described in (Bahdanau et al., 2015) (but with LSTMs instead of GRUs). The notation “&lt;1000&gt;” means a vector of size 1000. The vector sizes shown here are the ones suggested in the original paper. 3.1 Overview of NMT We describe here briefly the (Bahdanau et al., 2015) model. As shown in Figure 2, an input sentence is first converted into a sequence of vector through an embedding layer; these vectors are then fed to two LSTM layers (one going forward, the other going backward) to give a new sequence of vectors that encode the input"
W16-4616,W16-4601,1,0.866497,"Missing"
W16-4616,L16-1350,1,0.87803,"Missing"
W16-4616,W15-5006,1,0.864664,"Missing"
W16-4616,W16-2323,0,0.0339875,"els instead of one. The two “classic” ways of combining the prediction of different systems are to either take the geometric average or the arithmetic average of their predicted probabilities. Interestingly, although it seems other researchers have reported that using the arithmetic average works better (Luong et al., 2016), we actually found that geometric average was giving better results for us. Ensembling usually works best with independently trained parameters. We actually found that even using parameters from a single run could improve results. This had also been previously observed by (Sennrich et al., 2016). Therefore, for the cases when we could only run one training session, we ensembled the three parameters corresponding to the best development loss, the best development BLEU, and the final parameters (obtained after continuing training for some time after the best development BLEU was found). We refer to this as “self-ensembling” in the result section. When we could do n independent training, we kept these three parameters for each of the independent run and ensembled the 3 · n parameters. 3.7 Preprocessing The preprocessing steps were essentially the same as for KyotoEBMT. We lowercased the"
W16-4616,C16-1029,1,0.86956,"Missing"
W16-4616,P16-1008,0,0.0308784,"luency and meaning of the translations. This is due to the reason that the word order of the translations in EBMT depends on the parse trees of the input sentences, but the parsing accuracy is not perfect especially for Chinese. NMT tends to produce fluent translations, however it lacks of adequacy sometimes. The most common problem of NMT is that it could produce under or over translated translations, due to the lack of a way for the attention mechanism to memorize the source words that have been translated during decoding. We plan to address this problem with the coverage model proposed in (Tu et al., 2016). The UNK words are also a big problem. Although we deal with them using the UNK replacement method (Luong et al., 2015), it could simply fail because of errors for finding the corresponding source words using attention. We show a Japanese-to-English translation example by our EBMT and NMT systems in Table 2 to illustrate some of the above problems. The translation produced by the EBMT system has a word order problem that changes the meaning, making “the basic composition, standard” independent from “this flow sensor.” It also has an agreement violation problem between the argument and the pre"
W16-5407,C96-2184,0,0.0597353,"nnotation are selected from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chin"
W16-5407,P07-2045,0,0.00552999,"bowl wall section 26 defines an opening 28 and a leachate chamber 29 is located in the wall 24 beneath the opening 28. annular 碗状壁区 section 26 defines opening 28, leach liquid chamber 29 in the wall 24 opening 28 below. the annular bowl-shaped wall section 26 defines opening 28, leach liquid chamber 29 is positioned below the opening 28 in the wall 24. Table 5: An improved MT example. task at the NTCIR-10 workshop10 (Goto et al., 2013). The NTCIR-CE task uses 1,000,000, 2,000, and 2,000 sentences for training, development, and testing, respectively. We used the Moses tree-to-string MT system (Koehn et al., 2007) for all of our MT experiments. In our experiments, Chinese is in the tree format, and Japanese/English is in the string format. For Chinese, we used KyotoMorph for segmentations and the Berkeley parser for joint POS tagging and parsing. We binarized the parsing results for better translation rule extraction. We compared the MT performance of the “Baseline” and “Baseline+SCTB” settings in Section 3.1. For Japanese, we used JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-g"
W16-5407,W04-3250,0,0.13052,"d a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements are due to analysis improvements of the source sentences. Table 5 shows an improved MT example from the NTCIR-CE task. We can see that there is an out-ofvocabulary word"
W16-5407,J93-2004,0,0.0540819,"Missing"
W16-5407,W15-5001,1,0.854089,"ences to the baseline treebanks for training the analyzers. Figure 3 shows the results. We can see that for segmentation and POS tagging, the accuracy improvements slow down when more annotated sentences are used for training the analyzers; while for parsing, there is still a large potential of improvement by annotating more sentences. 3.2 MT Experiments For Chinese-to-Japanese translation, we conducted experiments on the scientific domain MT task on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ)8 (Nakazawa et al., 2016), which is one subtask of the workshop on Asian translation (WAT)9 (Nakazawa et al., 2015). The ASPEC-CJ task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. For Chinese-to-English translation, we conducted experiments on the Chinese-English subtask (NTCIR-CE) of the patent MT 8 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ http://orchid.kuee.kyoto-u.ac.jp/WAT/ 63 System Baseline Baseline+SCTB ASPEC-CJ 39.12 40.08† NTCIR-CE 33.19 33.90† Table 4: BLEU-4 scores for ASPEC-CJ and NTCIR-CE translation tasks (“†” indicates that the result is significantly better than “Baseline” at p < 0.01). Source Reference Baseline Baseline +SCTB 环形碗状壁区段２６限定了开口"
W16-5407,L16-1262,0,0.021801,"Missing"
W16-5407,P03-1021,0,0.0123856,"d JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements"
W16-5407,C14-1026,0,0.0271706,"urce sentence in Table 5 “浸出 (leach ) /液 (liquid) /腔室 (chamber) /２９/位于 (is located) /壁 (wall) /２４/中 (in) /开口 (opening) /２８/之下 (beneath)”. chamber)” into “leach liquid chamber”, this is due to the similar analysis results of both systems, while the correct analysis for this noun phrase should be “(NP (NP 浸出 NN 液 SFN) 腔室 NN) (leachate chamber)”. 4 Related Work Besides the widely used CTB (Xue et al., 2005), there are two other treebanks for Chinese. The Peking University (PKU) annotated a Chinese treebank, firstly only for segmentations and POS tags (Yu et al., 2003), and later also for syntax (Qiu et al., 2014). The Harbin Institute of Technologys (HIT) also annotated a treebank for dependency structures (Che et al., 2012). Besides the difference in annotation standards and syntactic structures, all the three treebanks are in news domain. CTB selected the raw sentences from People’s Daily, Hong Kong newswire, Xinhua newswire etc., and PKU and HIT selected the raw sentences from People’s Daily newswire. To the best of our knowledge, our treebank is the first publicly available Chinese treebank in scientific domain. Three are two types of syntactic grammars for treebanking: phrase structures and depen"
W16-5407,P14-2042,1,0.898261,"Missing"
W16-5407,C16-1029,1,0.912447,"of downstream applications such as text mining and machine translation (MT). Motivated by this, we decide to construct a Chinese treebank in the scientific domain (SCTB) to promote Chinese NLP research in this domain. This paper presents the details of our treebank annotation process and the experiments conducted on the annotated treebank. The raw sentences are selected from Chinese scientific papers. Our annotation process follows that of CTB (Xue et al., 2005) with an exception of the segmentation standard. We apply a Chinese word segmentation standard based on character-level POS patterns (Shen et al., 2016), aiming to circumvent inconsistency and address data 1 2 https://catalog.ldc.upenn.edu/LDC2005T01 Statistics from Japan Patent Office. 59 Proceedings of the 12th Workshop on Asian Language Resources, pages 59–67, Osaka, Japan, December 12 2016. Figure 1: A screenshot of the annotation interface containing an annotation example of a Chinese sentence “烟草 (tobacco) /使用 (use) /是 (is) /当今 (nowadays) /世界 (world) /最大 (biggest) /的 (’s) /可 (can) /预防 (prevention) /死因 (cause of death) /，/烟草 (tobacco) /使用 (use) /者 (person) /中 (among) /近 (about) /一半 (half) /将 (will) /死于 (die) /烟草 (tobacco) /使用 (use) /。” ("
W16-5407,L16-1249,0,0.024799,"rase structures and dependency structures. We adopt the phrase structures used in CTB (Xue et al., 2005), because phrase structures can be converted to dependency structures based on predefined head rules using e.g. the Penn2Malt toolkit.14 Treebanks with multi-view of both phrase structures and dependency structures also have been proposed (Qiu et al., 2014). Recently, with more needs of multilingual NLP, the interests of constructing multilingual treebanks have increased. Multilingual treebanks such as the universal dependency treebank15 (Nivre et al., 2016) and the Asian language treebank (Thu et al., 2016) are being constructed. As the raw sentences of our treebank were selected from parallel data and the translated Japanese and English sentences are available, we leave the potential to develop our treebank to a trilingual one. 5 Conclusion In this paper, we presented the details of the annotation of SCTB: a Chinese treebank in scientific domain. Experiments conducted for Chinese analysis and MT verified the effectiveness of the annotated SCTB. As future work, firstly, we plan to annotate more sentences, and we aim to finish the annotation for 10k sentences within this year. Secondly, we also p"
W16-5407,xia-etal-2000-developing,0,0.591073,"ed from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chinese word segmentat"
W16-5407,zhang-etal-2004-interpreting,0,0.102482,"Missing"
Y14-1032,N13-1075,0,0.210807,"262–271 !262 PACLIC 28 pairs to compare the similarity between them. Data sparseness makes the vector representations sparse (e.g., the vector of a low frequent word tends to have many zero entries), thus they do not always reliably represent the meanings of words. Therefore, the similarity of word pairs can be inaccurate. Smoothing technology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc.,"
Y14-1032,P05-1074,0,0.0563113,"rase pairs more accurately. Finally, we compute the similarity of phrase pairs based on the smoothed source and target vectors. In this way, we improve the quality of comparable features, which can improve the accuracy of the phrase table thus improve SMT performance. Details of paraphrase generation, comparable feature estimation and vector smoothing with paraphrases will be described in Section 4.1, 4.2 and 4.3 respectively. 4.1 Paraphrase Generation In this paper, we generate both source and target phrasal level paraphrases from the parallel corpus used for SMT4 through bilingual pivoting (Bannard and Callison-Burch, 2005). The idea of this method is that if two source phrases f1 and f2 are translated to the same target phrase e, we can assume that f1 and f2 are a paraphrase pair. Probability of this paraphrase pair can be assigned by marginalizing over 4 Paraphrases also can be generated from external parallel corpora and monolingual corpora, however we leave it as future work. !265 all shared target translations e in the parallel corpus, defined as follows: p(f1 |f2 ) = ! e φ(f1 |e)φ(e|f2 ) (1) where, φ(f1 |e) and φ(e|f2 ) are phrase translation probability. Target paraphrases can be generated in a similar wa"
Y14-1032,N06-1003,0,0.0324651,"udi, 2011; Irvine et al., 2013). Moreover, studies have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and th"
Y14-1032,2012.eamt-1.7,1,0.851342,"vectors for the phrase “unemployment figures” before and after smoothing. 5 Experiments In our experiments, we compared our proposed method with (Klementiev et al., 2012). We estimated comparable features from comparable corpora using the method of (Klementiev et al., 2012) and our proposed method respectively. We appended the comparable features to the phrase table, and evaluated the two methods in the perspective of SMT performance. We conducted experiments on Chinese-English data. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012), and tokenizing English sentences. 5.1 Experimental Settings SMT Settings We conducted Chinese-to-English translation experiments. The parallel corpus we used is from Chinese-English NIST open MT.6 The “NIST” column of Table 4 shows the statistics of this parallel corpus. For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting"
Y14-1032,P11-2071,0,0.0449146,"Missing"
Y14-1032,D10-1041,0,0.014692,"ng φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are incorrect. Another method is constructing a paraphrase lattice for the tuning and testing data, and performing lattice decoding (Du et al., 2010; Bar and Dershowitz, 2014). Paraphrases also can be incorporated as additional training data, which may improve both coverage and accuracy of SMT (Pal et al., 2014). Previous studies require external data in addition to the parallel corpus used for SMT for paraphrase generation to make their methods effective. These paraphrases can be generated from external parallel corpora (Callison-Burch et al., 2006; Du et al., 2010), or monolingual corpora based on distributional similarity (Marton et al., 2009; Razmara et al., 2013; Pal et al., 2014; Bar and Dershowitz, 2014). Our study differs from pre"
Y14-1032,N04-1035,0,0.0348047,"hnology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc., therefore it is suitable for the data sparseness problem. We generate paraphrases from the parallel corpus used for translation model learning. Then, we use the paraphrases to smooth the vectors of the translation pairs in the translation model for comparable feature estimation. Smoothing is done by learning vectors that combine the v"
Y14-1032,W12-3134,0,0.0156949,"hrases 6,273 46,191 # paraphrases 39.8 21.6 # Bigrams w/ paraphrases 15,026 223,299 Avg # paraphrases 34.6 17.7 # Trigrams w/ paraphrases 5,419 185,609 # paraphrases 20.0 14.9 Table 6: Statistics the generated paraphrases for the phrases and individual words inside the phrases in the filtered phrase table. tered phrase table. We can see that each Chinese phrase has a large number of translations on average especially for the lower order n-gram phrases, which can indicate the inaccuracy of the filtered phrase table. Our proposed method requires paraphrases for vector smoothing. We used Joshua (Ganitkevitch et al., 2012) to generate both Chinese and English paraphrases from the parallel corpus. We kept the paraphrase pairs that satisfy logp(x1 |x2 ) &gt; −7 and logp(x2 |x1 ) &gt; −7 14 for smoothing, where p(x1 |x2 ) is the probability that x1 is a paraphrase of x2 , and p(x2 |x1 ) is the probability that x2 is a paraphrase of x1 . Table 6 shows the statistics of the paraphrase generation results for the Chinese and English phrases, and individual words inside the phrases in the filtered phrase table. Note that, for some phrase pairs, their comparable feature scores may be 0, because of data sparseness. In that cas"
Y14-1032,W09-1117,0,0.016764,"ntax-based context etc. In this paper, we use window-based context, and leave the comparison of using different definitions of context as future work. Given a phrase, we count all its immediate context words, with a window size of 4 (2 preceding words and 2 following words). We build a context by collecting the counts in a bag of words fashion, namely we do not distinguish the positions that the context words appear in. The number of dimensions of the constructed vector is equal to the vocabulary size. We further reweight each component in the vector by multiplying by the IDF score following (Garera et al., 2009; Chu et al., 2014), which is defined as follows: IDF (t, D) = log |D| 1 + |{d ∈ D : t ∈ d}| (2) where |D |is the total number of documents in the corpus, and |{d ∈ D : t ∈ d} |denotes number of documents where the term t appears.5 We model the source and target vectors using the method described above, and project the source vector onto the vector space of the target language using a seed dictionary. The contextual similarity of the phrase pair is the similarity of the vectors, which is computed using cosine similarity defined as follows: &quot;K Fk × Ek #&quot; K 2 2 k=1 (Fk ) × k=1 (Ek ) Cos(f, e) ="
Y14-1032,P08-1088,0,0.0286005,"ation. The con6 LDC2007T02, LDC2002T01, LDC2003T17, LDC2004T07, HK News part of LDC2004T08, LDC2005T10 and LDC2006T04 7 http://www.speech.sri.com/projects/srilm !268 # Zh articles # En articles # Zh sentences # En sentences # Zh tokens # En tokens NIST N/A N/A 991k 991k 26.1M 27.2M Gigaword 3.6M 4.3M 42.6M 56.9M 1.1B 1.3B Wikipedia 248k 248k 2.8M 10.1M 70.5M 240.5M Table 4: Statistics of the comparable data used for comparable feature estimation. textual feature was estimated on the parallel corpus. We treated the two sides of the parallel corpus as independent monolingual corpora, following (Haghighi et al., 2008; Klementiev et al., 2012). Contextual feature estimation requires a seed dictionary. The seed dictionary we used is NIST ChineseEnglish translation lexicon Version 3.0,8 containing 82k entries. The temporal feature was estimated on Chinese9 and English10 Gigaword version 5.0. We used the afp, cna and xin sections with date range 1994/05-2010/12 of the corpora. The topical feature was estimated on Chinese and English Wikipedia data. We downloaded Chinese11 (2012/09/21) and English12 (2012/10/01) Wikipedia database dumps. We used an open-source Python script13 to extract and clean the text from"
Y14-1032,W13-2233,0,0.23776,"ns.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1 Scarceness of parallel corpora al"
Y14-1032,N13-1056,0,0.109849,"ns.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1 Scarceness of parallel corpora al"
Y14-1032,D13-1109,0,0.0221904,"Missing"
Y14-1032,P06-1103,0,0.128113,"odels. !263 BLE (Klementiev et al., 2012; Irvine and CallisonBurch, 2013a). The results verify the effectiveness of using BLE together with paraphrases for the accuracy problem of SMT. 2 Related Work 2.1 Bilingual Lexicon Extraction (BLE) for SMT From the pioneering work of (Rapp, 1995), BLE from comparable corpora has been studied for a long time. BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages. Contextual similarity (Rapp, 1995), topical similarity (Vuli´c et al., 2011) and temporal similarity (Klementiev and Roth, 2006) can be important clues for BLE. Orthographic similarity may also be used for BLE for some similar language pairs (Koehn and Knight, 2002). Moreover, some studies try to use the combinations of different similarities for BLE (Irvine and Callison-Burch, 2013b; Chu et al., 2014). To address the data sparseness problem of BLE, smoothing technology has been proposed (Pekar et al., 2006; Andrade et al., 2013). BLE can be used to address the accuracy problem of SMT, which estimates comparable features for the translation pairs in the translation model (Klementiev et al., 2012). BLE also can be used"
Y14-1032,E12-1014,0,0.0677873,"language pairs and domains.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1"
Y14-1032,W02-0902,0,0.136879,"Missing"
Y14-1032,N03-1017,0,0.120216,"d pairs can be inaccurate. Smoothing technology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc., therefore it is suitable for the data sparseness problem. We generate paraphrases from the parallel corpus used for translation model learning. Then, we use the paraphrases to smooth the vectors of the translation pairs in the translation model for comparable feature estimation. Smoothing is do"
Y14-1032,P07-2045,0,0.00477633,"e phrase table, and evaluated the two methods in the perspective of SMT performance. We conducted experiments on Chinese-English data. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012), and tokenizing English sentences. 5.1 Experimental Settings SMT Settings We conducted Chinese-to-English translation experiments. The parallel corpus we used is from Chinese-English NIST open MT.6 The “NIST” column of Table 4 shows the statistics of this parallel corpus. For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting, and used it for all the experiments. We used NIST open MT 2002 and 2003 data sets for tuning and testing, containing 878 and 919 sentence pairs respectively. Note that both MT 2002 and 2003 data sets contain 4 references for each Chinese sentence. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experi"
Y14-1032,W04-3250,0,0.0529101,"“Baseline” denotes the baseline system that does not use comparable features. “Klementiev+” denotes the system that appends the comparable features estimated following (Klementiev et al., 2012) to the phrase table. “Proposed” denotes the system that uses the comparable features estimated by our proposed method. “+Contextual”, “+Topical” and “+Temporal” denote the systems that append contextual, topical and temporal features respectively. “+All” denotes the system that appends all the three types of features. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). We can see that “Klementiev+” does not always outperform “Baseline”. The reason for this is that the comparable features estimated by (Klementiev et al., 2012) are inaccurate. “Proposed” performs significantly better than both “Baseline” and “Klementiev+”. The reason for this is that “Proposed” deals with the data sparseness problem of BLE for comparable feature estimation, making the features more accurate thus improve the SMT performance. As for different comparable features of “Proposed”, “+Contextual”, “+Topical” and “+Temporal” are all helpful, and combining them can be more effective."
Y14-1032,D09-1040,0,0.125371,"have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are incorrect. Another me"
Y14-1032,P03-1021,0,0.0189184,"heart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting, and used it for all the experiments. We used NIST open MT 2002 and 2003 data sets for tuning and testing, containing 878 and 919 sentence pairs respectively. Note that both MT 2002 and 2003 data sets contain 4 references for each Chinese sentence. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. Comparable Feature Estimation Settings Table 4 shows the statistics of the comparable data used for comparable feature estimation. The con6 LDC2007T02, LDC2002T01, LDC2003T17, LDC2004T07, HK News part of LDC2004T08, LDC2005T10 and LDC2006T04 7 http://www.speech.sri.com/projects/srilm !268 # Zh articles # En articles # Zh sentences # En sentences # Zh tokens # En tokens NIST N/A N/A 991k 991k 26.1M 27.2M Gigaword 3.6M 4.3M 42.6M 56.9M 1.1B 1.3B Wikipedia 248k 248k 2.8M 10.1M 70.5M 240.5M Table 4: Statistics of the comparable data used for comparable fea"
Y14-1032,P95-1050,0,0.428112,"h Phrase-based SMT (PBSMT) (Koehn et al., 2003).2 Experimental results show that our proposed method can improve SMT performance, compared to the previous studies that estimate comparable features without dealing with the data sparseness problem of 2 Our proposed method can also be applied to other language pairs and SMT models. !263 BLE (Klementiev et al., 2012; Irvine and CallisonBurch, 2013a). The results verify the effectiveness of using BLE together with paraphrases for the accuracy problem of SMT. 2 Related Work 2.1 Bilingual Lexicon Extraction (BLE) for SMT From the pioneering work of (Rapp, 1995), BLE from comparable corpora has been studied for a long time. BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages. Contextual similarity (Rapp, 1995), topical similarity (Vuli´c et al., 2011) and temporal similarity (Klementiev and Roth, 2006) can be important clues for BLE. Orthographic similarity may also be used for BLE for some similar language pairs (Koehn and Knight, 2002). Moreover, some studies try to use the combinations of different similarities for BLE (Irvine and Callison-Burch, 2013b; C"
Y14-1032,P13-1109,0,0.0524129,"3). Moreover, studies have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are"
Y14-1032,P11-2084,0,0.0522009,"Missing"
Y14-1032,J93-2003,0,\N,Missing
Y15-1033,D14-1179,0,0.0191739,"Missing"
Y15-1033,D07-1103,0,0.657787,"ntences and 4.5M terms) parallel data via pivot-based SMT. We generate a large pivot translation model using the Ja-En and En-Zh parallel data. Moreover, a small direct Ja-Zh translation model is generated using small-scale Ja-Zh parallel data. (680k sentences and 561k terms). Both the direct and pivot translation models are used to translate the Ja terms in the Ja-En dictionaries to Zh and the Zh terms in the Zh-En dictionaries to Ja to construct a large-scale Ja-Zh dictionary (about 3.6M terms). We address the noisy nature of pivoting large phrase tables by statistical significance pruning (Johnson et al., 2007). In addition, we exploit linguistic knowledge of common Chinese characters (Chu et al., 2013) shared in Ja-Zh to further improve the translation model. Large-scale experiments on scientific domain data indicate that our proposed method achieves high quality dictionaries which we manually verify to have a high quality. Reranking the n-best list produced by the SMT decoder is known to help improve the translation quality given that good quality features are used (Och et al., 2004). In this paper, we use bilingual neural network language model features for reranking the n-best list produced by t"
Y15-1033,P07-2045,0,0.143247,"o be a possible way of constructing a dictionary for the language pairs that have scarce parallel data (Tsunakawa et al., 2009; Chu et al., 2015). The assumption of this method is that there is a pair of large-scale parallel data: one between the source language and an intermediate resource rich language (henceforth called pivot), and one between that pivot and the target language. We can use the source-pivot and pivot-target parallel data to develop a source-target term1 translation model for dictionary construction. Pivot-based SMT uses the log linear model as conventional phrase-based SMT (Koehn et al., 2007) does. This method can address the data sparseness problem of directly merging the source-pivot and pivot-target terms, because it can use the portion of terms to generate new terms. Small-scale experiments in (Tsunakawa et al., 2009) showed very low 1 In this paper, we call the entries in the dictionary terms. A term consists of one or multiple tokens. accuracy of pivot-based SMT for dictionary construction.2 This paper presents our study to construct a largescale Japanese-Chinese (Ja-Zh) scientific dictionary, using large-scale Japanese-English (Ja-En) (49.1M sentences and 1.4M terms) and En"
Y15-1033,D09-1141,0,0.0502113,"iplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007) has shown to be quite effective in a bilingual scenario, it has never been considered in a pivot language scenario. (Tsunakawa et al., 2009) was the first work that constructs a dictionary"
Y15-1033,P02-1040,0,0.092256,"ioned in Section 4. The following scores are reported: • BS+RRCBLEU: Using character BLEU to rerank the n-best list. • BS+RRWBLEU: Using word BLEU to rerank the n-best list. • BS+RRSVM: Using SVM to rerank the n-best list. This is followed by substituting the OOVs with the character level translations using the learned neural translation models (which we label as +OOVsub). 5.2.3 Evaluation Criteria Following (Tsunakawa et al., 2009), we evaluated the accuracy on the test set using three metrics: 1 best, 20 best and Mean Reciprocal Rank (MRR)(Voorhees, 1999). In addition, we report the BLEU-4 (Papineni et al., 2002) scores that were computed on the word level. 5.2.4 Results of Automatic Evaluation Table 3 shows the evaluation results. We also show the percentage of OOV terms,11 and the accuracy with and without OOV terms respectively. In general, we can see that Pivot performs better than Direct, because the data of Ja-En and En-Zh is larger than that of Ja-Zh. Direct+Pivot shows better performance than either method. Different pruning methods show different performances, where Pr:P-T improves the accuracy, while the other two not. To understand the reason for this, we also investigated the statistics of"
Y15-1033,E12-1055,0,0.0162567,"loped a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007"
Y15-1033,P14-2042,1,0.828997,"AS LCAS title ISTIC pc ASPEC Size 3,588,800 22,610,643 19,905,978 3,013,886 6,090,535 1,070,719 1,562,119 680,193 shows the statistics of the bilingual dictionaries used for training. • Parallel corpora: the scientific Ja-En, En-Zh and Ja-Zh corpora we used were also provided by JST and ISTIC, containing 49.1M , 8.7M and 680k sentence pairs respectively. Table 2 shows the statistics of parallel corpora used for training. Among which ISTIC pc was provided by ISTIC, and the others were provided by JST. 5.2.1 In our experiments, we segmented the Chinese and Japanese data using a tool proposed by Shen et al. (2014) and JUMAN (Kurohashi et al., 1994) respectively. For decoding, we used Moses (Koehn et al., 2007) with the default options. We trained a word 5-gram language model on the Zh side of all the En-Zh and Ja-Zh training data (14.4M sentences) using the SRILM toolkit10 with interpolated Keneser-Ney discounting. Tuning was performed by minimum error rate training which also provides us with the n-best lists used to learn reranking weights. As a baseline, we compared following three methods for training the translation model: • Direct: Only use the Ja-Zh data to train a direct Ja-Zh model. Table 2: S"
Y15-1033,N07-1061,0,0.02745,"character based neural MT to eliminate the out-of-vocabulary (OOV) terms, which further improves the quality. The rest of this paper is structured as follows: Section 2 reviews related work. Section 3 presents our dictionary construction using pivot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weig"
Y15-1033,P07-1108,0,0.0645188,"Missing"
Y15-1033,P09-1018,0,0.0175042,"ot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked we"
Y15-2010,P05-1033,0,0.101997,"de which has a high quality parser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indica"
Y15-2010,P11-1061,0,0.0167906,"ject modifier. In this paper we consider combining these two trees and get improved results. We show in our experiments that combining the LQ-parsed trees with the transferred trees yield better translation results rather than only using them individually. 2 Related Work 2.1 Syntax Transfer for Non-MT Task There are many previous works describing methods to improve the performance of NLP tasks for a resource poor language by using a related resource rich language (mainly English). Amongst these the ones which employ methods which transfer information perform better than unsupervised methods. (Das and Petrov, 2011) describe an approach for inducing unsupervised part-of-speech tags for languages that have no labeled training data. (Jiang et al., 2010) show a transfer strategy to construct a constituency parser. (Ganchev et al., 2009) present a partial, approximate transfer through linear expectation constraints to project only parts of the parse trees to the low resource language side. However, improving monolingual parsing accuracy does not directly lead to higher MT performance, as it does not address the annotation criteria difference problem. 2.2 Syntax Transfer for MT Task For MT tasks, most transfe"
Y15-2010,P06-1121,0,0.0248273,"ser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which ha"
Y15-2010,P09-1042,0,0.603471,"about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ or LQ side since the Chinese parser is of a relatively lower quality and makes a number of parsing mistakes. One advantage is that the transferred parse information will possibly be more similar to the other side’s parse. This will also reduce the parsing error on the LQ side and unify the syntactic annotation on both sides. This idea has been proposed before, but not much has been done in the case of d"
Y15-2010,N06-1023,1,0.774953,"ti ,tj and undo the modification if so. We run several iterations of the procedure mentioned above till it reaches the worst case which means reverse T reenew is the same as T reeold T T . It is not so simple to test the effectiveness of projectivity on tasks other than MT. We manually check the percentage of errors our method has solved by manually evaluating 50 sentences (Section 7.2). tific domain, with default parameters. The new combined parser that we proposed used the training data obtained from the ASPEC Ja-Zh parallel corpus,3 containing 670k sentences. We used a Japanese parser KNP (Kawahara and Kurohashi, 2006)4 and the baseline SKP to automatically parse these sentences. We then created combined Chinese dependency trees.5 Finally, we trained a new parser using these combined Chinese dependency trees with the same parameters. As test data we used an additional 1k sentences from our in-house treebank. Table 1 shows the results of these two parsers. Parser Baseline Combined UAS 0.7433 0.5890 Root-Accuracy 0.6950 0.6140 Table 1: Parsing accuracy 6 Re-train a New LQ Side Parser Using the word alignments and original monolingual dependency trees, we successfully create combined trees using the parallel t"
Y15-2010,P07-2045,0,0.00835035,"from the language side which has a high quality parser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or"
Y15-2010,P06-1077,0,0.148062,"transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be re"
Y15-2010,D08-1022,0,0.192001,"ne the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ or LQ side since the"
Y15-2010,P05-1034,0,0.223368,"trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ"
Y15-2010,P14-5014,1,0.878661,"Missing"
Y15-2010,P08-1066,0,0.104227,"has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality par"
Y15-2010,Y12-1033,1,0.438635,"Missing"
Y15-2010,P08-1064,0,0.223917,"original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ or LQ side since the Chinese parser is of a relatively lowe"
Y15-2010,C10-2059,0,\N,Missing
Y18-3015,C18-1111,1,0.83113,"the preordering model improves the results on the PBSMT (4.62, 1.99, 1.22 for En-Ja, Ja-En, En-My, respectively). Translation quality of the En-My task is improved less than the En-Ja task (1.22 point and 4.62 point, respectively). We think this is caused by unbalanced corpus sizes of ALT and UCSY. The ALT corpus, from which the test set was derived, is significantly smaller than the UCSY corpus. This makes the English-Myanmar translation task difficult. 5 Domain Adaptation 5.1 Method It has been known that vanilla NMT performs poorly for domain specific translation in low-resource scenarios (Chu and Wang, 2018). The WAT 2018 Myanmar-English task is a low-resource setting that only contains 18k in-domain training sentences for the ALT task. However, it also provides the UCSY out-of-domain corpus, containing 208k training sentences. This is a proper domain adaptation setting, where out-of-domain data can be leveraged for indomain translation. 12 13 https://github.com/moses-smt/mosesdecoder http://github.com/kpu/kenlm 1123 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT"
Y18-3015,P17-2061,1,0.926792,"sh and Myanmar-English tasks. We use three different methods that we have been proposed in the past. For the first system, we use the rewarding model boosting target words in the decoder of NMT (Takebayashi et al., 2018). It predicts target words that are promising to be used in a correct translation and rewards them to give them better chances to be output. For the second system, we preorder source sentences before translation so that the word order becomes similar to target sentences, which is applied to PBSMT (Kawara et al., 2018). For the third system, we use our mixed fine tuning method (Chu et al., 2017). It is a domain adaptation method that Datasets We conduct English-to-Japanese, Japanese-toEnglish, English-to-Myanmar, and Myanmar-toEnglish translation, referred to as En-Ja, Ja-En, En-My, and My-En for short, hereafter. Table 1 shows statistics of the datasets provided in the ASPEC (Asian Scientific Paper Excerpt Corpus) (Nakazawa et al., 2016) Japanese-English and Myanmar-English tasks. The ASPEC JapaneseEnglish task is of a scientific domain, providing 3M, 1,790, and 1,812 sentences for training, development, and test, respectively. The Myanmar-English task provides two corpora, namely,"
Y18-3015,P18-3004,1,0.919531,". This year, Osaka University participated in two tasks: the ASPEC Japanese-English and Myanmar-English tasks. We use three different methods that we have been proposed in the past. For the first system, we use the rewarding model boosting target words in the decoder of NMT (Takebayashi et al., 2018). It predicts target words that are promising to be used in a correct translation and rewards them to give them better chances to be output. For the second system, we preorder source sentences before translation so that the word order becomes similar to target sentences, which is applied to PBSMT (Kawara et al., 2018). For the third system, we use our mixed fine tuning method (Chu et al., 2017). It is a domain adaptation method that Datasets We conduct English-to-Japanese, Japanese-toEnglish, English-to-Myanmar, and Myanmar-toEnglish translation, referred to as En-Ja, Ja-En, En-My, and My-En for short, hereafter. Table 1 shows statistics of the datasets provided in the ASPEC (Asian Scientific Paper Excerpt Corpus) (Nakazawa et al., 2016) Japanese-English and Myanmar-English tasks. The ASPEC JapaneseEnglish task is of a scientific domain, providing 3M, 1,790, and 1,812 sentences for training, development, a"
Y18-3015,W17-5706,0,0.0172608,") + λryj , (1) where λ is the weight of reward that will be tuned using a development set. This means that our model boosts the probabilities of predicted words that might have been slipped away during beam search in the conventional decoder. We use a simple binary rewarding that performed the best in Takebayashi et al. (2018):  1 (yj ∈ Df 2e ), ryj = (2) 0 (otherwise). Finally, a target word is output as: yj = arg max Q(yj |y&lt;j , X). yj 3.2 Experiments For the ASPEC Japanese-English task, we used the first 2M parallel sentence pairs among the entire 3M pairs sentences for training following Morishita et al. (2017), because the remaining 1M sentences were noisy. As preprocessing, we segmented Japanese sentences using MeCab,1 and tokenized and truecased the English sentences with the truecase.perl script in Moses2 . We further split the words into sub-words using joint BPE (Sennrich et al., 2016) with 32, 000 merge operations. The vocabulary sizes of the Japanese and English side were 28, 852 and 22, 340, respectively. For the MyanmarEnglish task, we simply concatenated the available ALT and UCSY corpora for training. We tokenized and truecased the English corpus, and used the tokenized and romanized Mya"
Y18-3015,P15-1021,0,0.157565,"ing En-My pre. rec. 67.65 48.35 67.61 48.34 My-En pre. rec. 56.44 46.31 56.23 46.29 Table 4: The precision and recall of unigram calculated by comparing the translation hypotheses against the reference translations on WAT 2018 Myanmar-English task. 4 Preordering Model 4.1 Model The word order between source and target languages significantly influences the translation quality in MT. Preordering, arranging words of source sentences so that the order is similar to that of the target language before translation, can effectively address this problem and significantly improves BLEU score of PBSMT (Nakagawa, 2015). Although NMT has been shown its strong performance in translation, it requires a large amount of training corpus, which is not the case for the Myanmar-English task. Hence, we use our preordering model with PBSMT for WAT submission. We applied the preordering model based on recursive neural networks (RvNN) (Kawara et al., 2018) to En-Ja and Ja-En translation for ASPEC and EnMy translation for the Myanmar-English tasks.6 We first parse source sentences to obtain their syntax trees with a parser, then assign either Inverted (I) or Straight (S) labels at each node of the source syn6 We could no"
Y18-3015,P03-1021,0,0.0105148,"ub.io/mecab/ 10 http://odaemon.com/?page=tools ckylark 11 http://github.com/moses-smt/giza-pp 8 Ja-En 15.31 17.30 En-My 19.71 20.93 I PP My/PRP parents/NNS live/VBP En-Ja 24.54 29.16 Table 5: PBSMT results (BLEU-4) with and without preordering on the WAT 2018 ASPEC and MyanmarEnglish tasks. Adam (Kingma and Ba, 2015) with a weight decay (10−4 ) and gradient clipping (5) for optimization. The mini batch size was set to 500. For PBSMT, we used Moses.12 We trained the 5gram language model on the target side of the training corpus with KenLM.13 Tuning was performed by minimum error rate training (Och, 2003). We repeated tuning and testing of each model 3 times and reported the average of scores. The distortion limit of PBSMT system trained by preordered sentences was set to 0, while that without preordering was set to 20. Table 5 shows the results. We can see that the preordering model improves the results on the PBSMT (4.62, 1.99, 1.22 for En-Ja, Ja-En, En-My, respectively). Translation quality of the En-My task is improved less than the En-Ja task (1.22 point and 4.62 point, respectively). We think this is caused by unbalanced corpus sizes of ALT and UCSY. The ALT corpus, from which the test s"
Y18-3015,P16-1162,0,0.0402931,"med the best in Takebayashi et al. (2018):  1 (yj ∈ Df 2e ), ryj = (2) 0 (otherwise). Finally, a target word is output as: yj = arg max Q(yj |y&lt;j , X). yj 3.2 Experiments For the ASPEC Japanese-English task, we used the first 2M parallel sentence pairs among the entire 3M pairs sentences for training following Morishita et al. (2017), because the remaining 1M sentences were noisy. As preprocessing, we segmented Japanese sentences using MeCab,1 and tokenized and truecased the English sentences with the truecase.perl script in Moses2 . We further split the words into sub-words using joint BPE (Sennrich et al., 2016) with 32, 000 merge operations. The vocabulary sizes of the Japanese and English side were 28, 852 and 22, 340, respectively. For the MyanmarEnglish task, we simply concatenated the available ALT and UCSY corpora for training. We tokenized and truecased the English corpus, and used the tokenized and romanized Myanmar corpus released by the organizers. We used the mlpnlp-nmt system3 that is an LSTM based encoder-decoder NMT model with attention, which achieved the best translation performance in human evaluations for both the Ja-En, and EnJa tasks at WAT 2017 (Nakazawa et al., 2017). We impleme"
