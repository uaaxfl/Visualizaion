2020.aacl-main.48,P04-1077,0,0.113254,"Missing"
2020.aacl-main.48,2021.ccl-1.108,0,0.16019,"Missing"
2020.aacl-main.48,D19-1514,0,0.042113,"nticipation, and action localization. An independent and concurrent work (UniViLM) by Luo et al. (2020) is closely related to ours in that we share some similar pretraining objectives, some of the pretraining setup – HowTo100M (Alayrac et al., 2016), and the down-stream video captioning benchmark using YouCook2 (Zhou et al., 2018a). The main difference is that they use BERT-style pretraining for encoder and language-modeling style pretraining for decoder, whereas we use MASS-style pre-training to pretrain encoder and decoder jointly. Other approaches such as ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), Unicoder-VL (Li et al., 2019), VL-BERT (Su et al., 2019), and UNITER (Chen et al., 2019) focus on pretraining joint representations for text and image, evaluating them on downstream tasks such as visual question answering, image-text retrieval and referring expressions. Related Work Text-only Pretraining. Language pretraining models based on the Transformer neural net2 Available at https://github. com/google-research-datasets/ Video-Timeline-Tags-ViTT Dense Video Captioning. In this paper, we focus on generating captions at the segment-level, which is a sub-task of the so-called dense video"
2020.acl-main.16,D18-1399,0,0.0488069,"Missing"
2020.acl-main.16,W05-0909,0,0.187355,"ranslation noise. We call the latter the Pivot-Language Generation Stabilization (PLuGS) approach. Examples of outputs produced by these three solutions are shown in Fig. 1. We perform extensive evaluations across five different languages (French, Italian, German, Spanish, Hindi) to compare these three approaches. The results indicate that the bilingual PLuGS models consistently perform the best in terms of captioning accuracy. Since there is very little support in the literature regarding the ability of standard evaluation metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016) to accurately measure captioning accuracy for non-English languages, our evaluations are done using fine-grained, side-by-side human evaluations using paid raters; we explain the evaluation protocol in detail in Sec. 5. Besides the evaluations on bilingual PLuGS models, we also train and evaluate a multilingual PLuGS model, in which all five non-English languages considered are supported through a single model capable of generating outputs in all 5 languages. The results indicate that similar languages are reinforcing each othe"
2020.acl-main.16,N19-1422,0,0.249177,"odels (Sharma et al., 2018; Zhao et al., 2019; Changpinyo et al., 2019) based on the web-scale Conceptual Captions dataset (Sharma et al., 2018). Generating image captions in languages other than English has been explored in the context of the WMT 2017-2018 multimodal translation sub-task on multilingual caption generation (Elliott et al., 2017). The goal of the task is to generate image captions in German and French, using a small training corpus with images and captions available in English, German and French (based on Flickr30K). In the context of that work, we use the results reported in (Caglayan et al., 2019) to quantitatively compare it against our approach. Another relevant connection is with the work in (Jaffe, 2017), which explores several LSTM-based encoder-decoder models that generate captions in different languages. The model most similar to our work is their Dual Attention model, which first generates an English caption, then an LSTM with attention over the image and the generated English caption produces a German caption. Their quantitative evaluations do not find any additional benefits for this approach. Our work is related to this idea, but there are key technical differences. In the P"
2020.acl-main.16,D19-1155,1,0.855271,"- & X-language outputs, with respect to accuracy; therefore, directly applying an English QE model appears to be the most appropriate scalable solution. 2 Related Work There is a large body of work in automatic image captioning for English, starting with early work (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Kiros et al., 2015; Xu et al., 2015) based on data offered by manually annotated datasets such as Flickr30K (Young et al., 2014b) and MS-COCO (Lin et al., 2014), and more recently with work using Transformer-based models (Sharma et al., 2018; Zhao et al., 2019; Changpinyo et al., 2019) based on the web-scale Conceptual Captions dataset (Sharma et al., 2018). Generating image captions in languages other than English has been explored in the context of the WMT 2017-2018 multimodal translation sub-task on multilingual caption generation (Elliott et al., 2017). The goal of the task is to generate image captions in German and French, using a small training corpus with images and captions available in English, German and French (based on Flickr30K). In the context of that work, we use the results reported in (Caglayan et al., 2019) to quantitatively compare it against our approac"
2020.acl-main.16,W17-4718,0,0.0128702,"., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Kiros et al., 2015; Xu et al., 2015) based on data offered by manually annotated datasets such as Flickr30K (Young et al., 2014b) and MS-COCO (Lin et al., 2014), and more recently with work using Transformer-based models (Sharma et al., 2018; Zhao et al., 2019; Changpinyo et al., 2019) based on the web-scale Conceptual Captions dataset (Sharma et al., 2018). Generating image captions in languages other than English has been explored in the context of the WMT 2017-2018 multimodal translation sub-task on multilingual caption generation (Elliott et al., 2017). The goal of the task is to generate image captions in German and French, using a small training corpus with images and captions available in English, German and French (based on Flickr30K). In the context of that work, we use the results reported in (Caglayan et al., 2019) to quantitatively compare it against our approach. Another relevant connection is with the work in (Jaffe, 2017), which explores several LSTM-based encoder-decoder models that generate captions in different languages. The model most similar to our work is their Dual Attention model, which first generates an English caption"
2020.acl-main.16,W16-3210,0,0.0480432,"Missing"
2020.acl-main.16,W17-4750,0,0.0178592,"(Sharma et al., 2018). Generating image captions in languages other than English has been explored in the context of the WMT 2017-2018 multimodal translation sub-task on multilingual caption generation (Elliott et al., 2017). The goal of the task is to generate image captions in German and French, using a small training corpus with images and captions available in English, German and French (based on Flickr30K). In the context of that work, we use the results reported in (Caglayan et al., 2019) to quantitatively compare it against our approach. Another relevant connection is with the work in (Jaffe, 2017), which explores several LSTM-based encoder-decoder models that generate captions in different languages. The model most similar to our work is their Dual Attention model, which first generates an English caption, then an LSTM with attention over the image and the generated English caption produces a German caption. Their quantitative evaluations do not find any additional benefits for this approach. Our work is related to this idea, but there are key technical differences. In the PLuGS approach, we train an end-to-end model based on a Transformer (Vaswani et al., 2017) decoder that exploits t"
2020.acl-main.16,W04-1013,0,0.0424959,"ion. ing potential translation noise. We call the latter the Pivot-Language Generation Stabilization (PLuGS) approach. Examples of outputs produced by these three solutions are shown in Fig. 1. We perform extensive evaluations across five different languages (French, Italian, German, Spanish, Hindi) to compare these three approaches. The results indicate that the bilingual PLuGS models consistently perform the best in terms of captioning accuracy. Since there is very little support in the literature regarding the ability of standard evaluation metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016) to accurately measure captioning accuracy for non-English languages, our evaluations are done using fine-grained, side-by-side human evaluations using paid raters; we explain the evaluation protocol in detail in Sec. 5. Besides the evaluations on bilingual PLuGS models, we also train and evaluate a multilingual PLuGS model, in which all five non-English languages considered are supported through a single model capable of generating outputs in all 5 languages. The results indicate that similar"
2020.acl-main.16,P02-1040,0,0.10719,"tabilizer is already a translation. ing potential translation noise. We call the latter the Pivot-Language Generation Stabilization (PLuGS) approach. Examples of outputs produced by these three solutions are shown in Fig. 1. We perform extensive evaluations across five different languages (French, Italian, German, Spanish, Hindi) to compare these three approaches. The results indicate that the bilingual PLuGS models consistently perform the best in terms of captioning accuracy. Since there is very little support in the literature regarding the ability of standard evaluation metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016) to accurately measure captioning accuracy for non-English languages, our evaluations are done using fine-grained, side-by-side human evaluations using paid raters; we explain the evaluation protocol in detail in Sec. 5. Besides the evaluations on bilingual PLuGS models, we also train and evaluate a multilingual PLuGS model, in which all five non-English languages considered are supported through a single model capable of generating outputs in all 5 languages. The results ind"
2020.acl-main.16,P16-1162,0,0.00679183,"abilizer Vocabtext Pre-trained/fixed) Trainable Figure 2: The Transformer based PLuGS model. The text on the input side is used for the translation and multimodal translation experiments with the Multi30K dataset. For image captioning, no text input is provided. 256. Each member of this sequence of embeddings is projected to match Transformer dimensions by a 2 layer DNN with linear activation. This sequence of projected object-label embeddings is fed to the encoder together with the global image embedding. parked Text Embeddings: All text (input or output) is encoded using byte-pair encoding (Sennrich et al., 2016) with a shared source-target vocabulary of about 4000 tokens, then embedded as described in (Vaswani et al., 2017), resulting in a sequence of text embeddings. The embeddings dimensions are chosen to match the Transformer dimensions. When performing the translation (MT) and multimodal translation (MMT) experiments in Sec. 6.1, the sequence of source text embeddings are fed to the encoder after the LangId embedding. Additionally, we reserve a token-id in the text vocabulary for each language (e.g. hdei for German) for use as a separator in the PLuGS model output and also have a separate start-o"
2020.acl-main.16,P18-1238,1,0.951221,"al English model. 1 Introduction Data hungry state-of-the-art neural models for language generation have the undesired potential to widen the quality gap between English and non-English languages, given the scarcity of nonEnglish labeled data. One notable exception is machine translation, which benefits from large amounts of bilingually or multilingually annotated data. But cross-modal language generation tasks, such as automatic image captioning, tend to be directly hurt by this trend: existing datasets such as Flickr (Young et al., 2014a), MSCOCO (Lin et al., 2014), and Conceptual Captions (Sharma et al., 2018) have extensive labeled data for English, but labeled data is extremely scarce in other languages (Elliott et al., 2016) (at 2 orders of magnitude less for a couple of languages, and none for the rest). In this paper, we conduct a study aimed at answering the following question: given a large annotated web-scale dataset such as Conceptual Captions (Sharma et al., 2018) in one language, and a baseline machine translation system, what is the optimal way to scale a cross-modality language generation system to new languages at web-scale? We focus our study on the task of automatic image captioning"
2020.acl-main.16,Q14-1006,0,0.439088,"S models are better than the captions generated by the original, monolingual English model. 1 Introduction Data hungry state-of-the-art neural models for language generation have the undesired potential to widen the quality gap between English and non-English languages, given the scarcity of nonEnglish labeled data. One notable exception is machine translation, which benefits from large amounts of bilingually or multilingually annotated data. But cross-modal language generation tasks, such as automatic image captioning, tend to be directly hurt by this trend: existing datasets such as Flickr (Young et al., 2014a), MSCOCO (Lin et al., 2014), and Conceptual Captions (Sharma et al., 2018) have extensive labeled data for English, but labeled data is extremely scarce in other languages (Elliott et al., 2016) (at 2 orders of magnitude less for a couple of languages, and none for the rest). In this paper, we conduct a study aimed at answering the following question: given a large annotated web-scale dataset such as Conceptual Captions (Sharma et al., 2018) in one language, and a baseline machine translation system, what is the optimal way to scale a cross-modality language generation system to new language"
2020.acl-main.16,P19-1650,1,0.854111,"en the generated EN- & X-language outputs, with respect to accuracy; therefore, directly applying an English QE model appears to be the most appropriate scalable solution. 2 Related Work There is a large body of work in automatic image captioning for English, starting with early work (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Kiros et al., 2015; Xu et al., 2015) based on data offered by manually annotated datasets such as Flickr30K (Young et al., 2014b) and MS-COCO (Lin et al., 2014), and more recently with work using Transformer-based models (Sharma et al., 2018; Zhao et al., 2019; Changpinyo et al., 2019) based on the web-scale Conceptual Captions dataset (Sharma et al., 2018). Generating image captions in languages other than English has been explored in the context of the WMT 2017-2018 multimodal translation sub-task on multilingual caption generation (Elliott et al., 2017). The goal of the task is to generate image captions in German and French, using a small training corpus with images and captions available in English, German and French (based on Flickr30K). In the context of that work, we use the results reported in (Caglayan et al., 2019) to quantitatively comp"
2020.acl-main.583,N19-1056,1,0.901411,"Missing"
2020.acl-main.583,D17-1098,0,0.0281789,"Missing"
2020.acl-main.583,D18-2029,0,0.0798455,"Missing"
2020.acl-main.583,D19-1469,0,0.190763,"Missing"
2020.acl-main.583,W19-8643,0,0.0514994,"Missing"
2020.acl-main.583,D14-1162,0,0.0825401,"Missing"
2020.acl-main.583,prasad-etal-2008-penn,0,0.689279,"(Chunseong Park et al., 2017), forced attentions (Sadler et al., 2019) and modeling and learning compositional semantics using fine-grained annotations of entities in MSCOCO (Cornia et al., 2019). 2 The first step toward our goals is to characterize image–text coherence and annotate a sizable corpus of image–text pairs with coherence relations. We use an overlapping set of high-level relations, inspired both by theoretical work linking discourse coherence to discourse structure and discourse goals (Roberts, 2012; Webber et al., 1999), and by previous successful discourse annotation campaigns (Prasad et al., 2008). Crucially, following previous work on text (Rohde et al., 2018) and multimodal discourse (Alikhani et al., 2019), Prior Work There are diverse ways to characterize the communicative functions of text and images in multimodal documents (Marsh and Domas White, 2003), any of which can provide the basis for computational work. Some studies emphasize the distinctive cognitive effects of imagery in directing attention; engaging perceptual, spatial and embodied 1 https://github.com/malihealikhani/Crossmodal Coherence Modeling 3 6526 Coherence in Images and Captions Visible, Meta (a) C APTION : fore"
2020.acl-main.583,P19-1272,0,0.0297845,"Missing"
2020.acl-main.583,P12-2018,0,0.0175938,"a different textual-feature extractor. We train and test using an encoder that takes sentence embeddings as input using the hCLSi representation produced by the BERT-base model (Devlin et al., 2018). Results The results of all of our models are presented in Table 3, where we present the F1 scores over each of the individual relations, as well as an overall weighted average. The BERT+ResNet model achieves the highest performance (|t |&gt; 9.54, p &lt; 0.01), with an overall F1 score of 0.67. For the interested reader, we present in the GitHub page the top features of the Naive Bayes SVM classifier (Wang and Manning, 2012). 4.2 Single-Label Prediction To achieve the goal of generating captions with a desired coherence relation to the image, it is important to clearly differentiate between often cooccurring label types (such as Visible and Meta). To this end, we introduce a label-mapping strategy for predicting coherence relations, such that each image–caption pair is assigned a single coherence label. We map the set of human-annotated coherence relations for an image–caption pair to a single label using the following heuristic: 1. If the set contains the Meta label, then the image–caption pair is assigned the M"
2020.acl-main.583,P18-1210,0,0.0242759,"019) and modeling and learning compositional semantics using fine-grained annotations of entities in MSCOCO (Cornia et al., 2019). 2 The first step toward our goals is to characterize image–text coherence and annotate a sizable corpus of image–text pairs with coherence relations. We use an overlapping set of high-level relations, inspired both by theoretical work linking discourse coherence to discourse structure and discourse goals (Roberts, 2012; Webber et al., 1999), and by previous successful discourse annotation campaigns (Prasad et al., 2008). Crucially, following previous work on text (Rohde et al., 2018) and multimodal discourse (Alikhani et al., 2019), Prior Work There are diverse ways to characterize the communicative functions of text and images in multimodal documents (Marsh and Domas White, 2003), any of which can provide the basis for computational work. Some studies emphasize the distinctive cognitive effects of imagery in directing attention; engaging perceptual, spatial and embodied 1 https://github.com/malihealikhani/Crossmodal Coherence Modeling 3 6526 Coherence in Images and Captions Visible, Meta (a) C APTION : forest on a sunny day Visible, Action, Subjective (b) C APTION : youn"
2020.acl-main.583,P99-1006,1,0.458806,"arch (Anderson et al., 2017), a memory network with multiple context information (Chunseong Park et al., 2017), forced attentions (Sadler et al., 2019) and modeling and learning compositional semantics using fine-grained annotations of entities in MSCOCO (Cornia et al., 2019). 2 The first step toward our goals is to characterize image–text coherence and annotate a sizable corpus of image–text pairs with coherence relations. We use an overlapping set of high-level relations, inspired both by theoretical work linking discourse coherence to discourse structure and discourse goals (Roberts, 2012; Webber et al., 1999), and by previous successful discourse annotation campaigns (Prasad et al., 2008). Crucially, following previous work on text (Rohde et al., 2018) and multimodal discourse (Alikhani et al., 2019), Prior Work There are diverse ways to characterize the communicative functions of text and images in multimodal documents (Marsh and Domas White, 2003), any of which can provide the basis for computational work. Some studies emphasize the distinctive cognitive effects of imagery in directing attention; engaging perceptual, spatial and embodied 1 https://github.com/malihealikhani/Crossmodal Coherence M"
2020.acl-main.583,D18-1437,0,0.0293183,"most beautiful horse in the world. Story: horse competes in the event. Introduction The task of image captioning is seemingly straightforward to define: use natural language to generate a description that captures the salient content of an image. Initial datasets, such as MSCOCO (Lin et al., 2014) and Flickr (Young et al., 2014), approached this task directly, by asking crowd workers to describe images in text. Unfortunately, such dedicated annotation efforts cannot yield enough data for training robust generation models; the resulting generated captions are plagued by content hallucinations (Rohrbach et al., 2018; Sharma et al., 2018) that effectively preclude them for being used in real-world applications. In introducing the Conceptual Captions dataset, Sharma et al. (2018) show that this dataset is large enough, at 3.3M examples, to significantly alleviate content hallucination. However, because the technique for creating such a large-scale resource relies on harvesting existing data from the web, it no longer guarantees consistent image–text relations. For example, along with descriptive captions (e.g.,“this is a person in a suit”), the dataset also includes texts that provide contextual background"
2020.acl-main.583,Q14-1006,0,0.0592089,"Output of a coherence-aware model for various coherence relations. Content that establishes the intended relation is underlined. (Photo credit: Blue Destiny / Alamy Stock Photo) Visible: horse and rider jumping a fence. Meta: horse and rider jumping a fence during a race. Subjective: the most beautiful horse in the world. Story: horse competes in the event. Introduction The task of image captioning is seemingly straightforward to define: use natural language to generate a description that captures the salient content of an image. Initial datasets, such as MSCOCO (Lin et al., 2014) and Flickr (Young et al., 2014), approached this task directly, by asking crowd workers to describe images in text. Unfortunately, such dedicated annotation efforts cannot yield enough data for training robust generation models; the resulting generated captions are plagued by content hallucinations (Rohrbach et al., 2018; Sharma et al., 2018) that effectively preclude them for being used in real-world applications. In introducing the Conceptual Captions dataset, Sharma et al. (2018) show that this dataset is large enough, at 3.3M examples, to significantly alleviate content hallucination. However, because the technique for"
2020.acl-main.583,W19-8653,0,0.0370346,"Missing"
2020.acl-main.583,P18-1238,1,0.917076,"Missing"
2020.acl-main.583,miltsakaki-etal-2004-penn,0,\N,Missing
2020.emnlp-main.702,P19-1355,0,0.0301998,"aForN fully embraces the teacher-forcing paradigm and Radu Soricut Google Research Venice, CA 90291 rsoricut@google.com extends it to N-grams, thereby addressing the problem at the level of teacher-forcing itself. The advent of large-scale pretraining has pushed the state-of-the-art on Natural Language benchmarks to impressive heights, often showing gains across many tasks at once (Devlin et al., 2019; Raffel et al., 2019; Zhang et al., 2019). A negative consequence of this is the tendency towards large, data-hungry models, which have a negative impact on energy-consumption and accessibility (Strubell et al., 2019), as well as higher latency and production costs. As such, it is of increasing importance to develop techniques that counteract these tendencies. While TeaForN does increase training cost moderately, it can be used to drive down latency and inference cost, which dominate the overall cost of a production model. Many sequence generation models use beam search to improve generation quality (Vaswani et al., 2017; Raffel et al., 2019; Zhang et al., 2019; Yan et al., 2020). In contrast with greedy decoding, beam search keeps the k most-likely candidates at each decoding timestep. While beam search h"
2020.emnlp-main.702,2020.findings-emnlp.217,0,0.0715756,"ing a technique called teacherforcing (Goodfellow et al., 2016). Teacher-forcing is popular because it improves sample efficiency and provides training stability, but models trained with teacher-forcing are known to suffer from issues such as exposure bias (Venkatraman et al., 2015; Bengio et al., 2015; Ding and Soricut, 2017) and a lack of differentiability across timesteps (i.e., training updates made when decoding at time-step t cannot fully propagate to time-step t − 1). Previous attempts to address these issues include scheduled sampling (Bengio et al., 2015), parallel N-gram prediction (Yan et al., 2020), and sampling from previous predictions (Zhang et al., 2019). Our proposed method, Teacher-Forcing with Ngrams (TeaForN), imposes few requirements on the decoder architecture and does not require curriculum learning or sampling model outputs. TeaForN fully embraces the teacher-forcing paradigm and Radu Soricut Google Research Venice, CA 90291 rsoricut@google.com extends it to N-grams, thereby addressing the problem at the level of teacher-forcing itself. The advent of large-scale pretraining has pushed the state-of-the-art on Natural Language benchmarks to impressive heights, often showing ga"
2020.emnlp-main.702,P19-1426,0,0.157056,"Missing"
2020.emnlp-main.709,N15-1015,0,0.14542,"ns, and the resulting representations are fine-tuned for downstream tasks. The assumption underlying this family of approaches is that, in the pretraining corpus, spoken words have some consistent, grounded relationship with the temporally corresponding visual content. However, in contrast to the highly diverse corpora utilized for text-based pretraining (Wikipedia, Common Crawl, etc.), pretraining for web videos (so far) has been limited to instructional videos. This domain restriction is motivated by the commonly accepted notion that “procedural knowledge tends to be inherently multimodal” (Malmaud et al., 2015). We expect that the semantic information in video frames and ASR tokens is readily correlated in instructional videos. But corpus diversity brings significant benefits: in the text-only case, models can effectively represent diverse real-world entities (Roberts et al., 2020) precisely because pretraining is not restricted to, e.g., only fictional stories (Zhu et al., 2015). In search of more general representations, our main question is: does video-ASR pretraining “work” for more diverse pretraining corpora? Are certain categories of non-instructional videos “groundable,” thus enabling divers"
2020.emnlp-main.709,D19-1517,0,0.0303248,"ideo understanding tasks (Gupta et al., 2017; Huang et al., 2017; Huang* et al., 2018; Moriya et al., 2019), e.g., action detection/classification (Yu et al., 2014; Alayrac et al., 2017; Chang et al., 2019; Kuehne et al., 2019), 8812 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8812–8822, c November 16–20, 2020. 2020 Association for Computational Linguistics segmentation/captioning (Sener et al., 2015), and instruction alignment (Malmaud et al., 2015; Alayrac et al., 2016). A number of multimodal instructional video datasets have been proposed (Wang et al., 2019; Tang et al., 2019; Sanabria et al., 2018). A notable recent example of work addressing a non-instructional video corpus is Ignat et al. (2019), who analyze grounded-ness in lifestyle vlogs. Fouhey et al. (2018) highlight the difference between keyword search vs. implicitly mining action data of interest from a broader corpus (e.g., Bregler (1997); Gu et al. (2018)). Operational grounding. Our work builds upon prior operational notions of grounding: if an algorithm is able to consistently predict specific visualtextual relationships, then that relationship is said to be “grounded” (Lu et al.,"
2020.eval4nlp-1.6,S16-1081,0,0.045936,"Missing"
2020.eval4nlp-1.6,P02-1040,0,0.10639,"egy that improves statistical properties; and a computationally efficient tempered Word Mover Distance, for better fusion of the information in the contextualized word representations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERTbackbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks. 1 Introduction Automatic evaluation metrics play an important role in comparing candidate sentences generated by machines against human references. Firstgeneration metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) use predefined handcrafted rules to measure surface similarity between sentences and have no ability, or very little ability (Banerjee and Lavie, 2005), to go beyond word surface level. To address this problem, later work (Kusner et al., 2015; Zhelezniak et al., 2019) utilize static embedding techniques such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) to represent the words in sentences as vectors in a low-dimensional continuous ∗ † Work done during the internship at Google. Corresponding author. 51 Proceedings of the First Workshop on Evaluati"
2020.eval4nlp-1.6,D14-1162,0,0.101055,"Missing"
2020.eval4nlp-1.6,W05-0909,0,0.236661,"resentations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERTbackbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks. 1 Introduction Automatic evaluation metrics play an important role in comparing candidate sentences generated by machines against human references. Firstgeneration metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) use predefined handcrafted rules to measure surface similarity between sentences and have no ability, or very little ability (Banerjee and Lavie, 2005), to go beyond word surface level. To address this problem, later work (Kusner et al., 2015; Zhelezniak et al., 2019) utilize static embedding techniques such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) to represent the words in sentences as vectors in a low-dimensional continuous ∗ † Work done during the internship at Google. Corresponding author. 51 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 51–59, c November 20, 2020. 2020 Association for Computational Linguistics For word similarity, x represents a single word"
2020.eval4nlp-1.6,D19-1410,0,0.0987429,"e across sentences and paragraphs. Modern deep learning models based on the Transformer (Vaswani et al., 2017) utilize a multilayered self-attention structure that encodes not only a global representation of each word (a word embedding), but also its contextualized information within the context considered. Such contextualized word representations have yielded significant improvements on various tasks, including machine translation (Vaswani et al., 2017), NLU tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020), summarization (Zhang et al., 2019a), and automatic evaluation metrics (Reimers and Gurevych, 2019; Zhang et al., 2019b; Zhao et al., 2019; Sellam et al., 2020). In this paper, we investigate how to better use BERT-based contextualized embeddings in order to arrive at effective evaluation metrics for generated text. We formalize a unified family of text similarity metrics, which operate either at the word/token or sentence level, and show how a number of existing embedding-based similarity metrics belong to this family. In this context, we present a tempered Word Mover Distance (TWMD) formulation by utilizing the Sinkhorn distance (Cuturi, 2013), which adds an entropy regularizer to the ob"
2020.eval4nlp-1.6,2020.acl-main.704,0,0.285719,"on the Transformer (Vaswani et al., 2017) utilize a multilayered self-attention structure that encodes not only a global representation of each word (a word embedding), but also its contextualized information within the context considered. Such contextualized word representations have yielded significant improvements on various tasks, including machine translation (Vaswani et al., 2017), NLU tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020), summarization (Zhang et al., 2019a), and automatic evaluation metrics (Reimers and Gurevych, 2019; Zhang et al., 2019b; Zhao et al., 2019; Sellam et al., 2020). In this paper, we investigate how to better use BERT-based contextualized embeddings in order to arrive at effective evaluation metrics for generated text. We formalize a unified family of text similarity metrics, which operate either at the word/token or sentence level, and show how a number of existing embedding-based similarity metrics belong to this family. In this context, we present a tempered Word Mover Distance (TWMD) formulation by utilizing the Sinkhorn distance (Cuturi, 2013), which adds an entropy regularizer to the objective of WMD (Kusner et al., 2015). Compared to WMD, our TWM"
2020.eval4nlp-1.6,N19-1423,0,0.140024,"eir cosine similarity. However, static embeddings cannot capture the rich syntactic, semantic, and pragmatic aspects of word usage across sentences and paragraphs. Modern deep learning models based on the Transformer (Vaswani et al., 2017) utilize a multilayered self-attention structure that encodes not only a global representation of each word (a word embedding), but also its contextualized information within the context considered. Such contextualized word representations have yielded significant improvements on various tasks, including machine translation (Vaswani et al., 2017), NLU tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020), summarization (Zhang et al., 2019a), and automatic evaluation metrics (Reimers and Gurevych, 2019; Zhang et al., 2019b; Zhao et al., 2019; Sellam et al., 2020). In this paper, we investigate how to better use BERT-based contextualized embeddings in order to arrive at effective evaluation metrics for generated text. We formalize a unified family of text similarity metrics, which operate either at the word/token or sentence level, and show how a number of existing embedding-based similarity metrics belong to this family. In this context, we present a temper"
2020.eval4nlp-1.6,D19-1053,0,0.346966,"arning models based on the Transformer (Vaswani et al., 2017) utilize a multilayered self-attention structure that encodes not only a global representation of each word (a word embedding), but also its contextualized information within the context considered. Such contextualized word representations have yielded significant improvements on various tasks, including machine translation (Vaswani et al., 2017), NLU tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020), summarization (Zhang et al., 2019a), and automatic evaluation metrics (Reimers and Gurevych, 2019; Zhang et al., 2019b; Zhao et al., 2019; Sellam et al., 2020). In this paper, we investigate how to better use BERT-based contextualized embeddings in order to arrive at effective evaluation metrics for generated text. We formalize a unified family of text similarity metrics, which operate either at the word/token or sentence level, and show how a number of existing embedding-based similarity metrics belong to this family. In this context, we present a tempered Word Mover Distance (TWMD) formulation by utilizing the Sinkhorn distance (Cuturi, 2013), which adds an entropy regularizer to the objective of WMD (Kusner et al., 2015). Co"
2020.eval4nlp-1.6,D19-1008,0,0.0766365,"er various BERTbackbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks. 1 Introduction Automatic evaluation metrics play an important role in comparing candidate sentences generated by machines against human references. Firstgeneration metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) use predefined handcrafted rules to measure surface similarity between sentences and have no ability, or very little ability (Banerjee and Lavie, 2005), to go beyond word surface level. To address this problem, later work (Kusner et al., 2015; Zhelezniak et al., 2019) utilize static embedding techniques such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) to represent the words in sentences as vectors in a low-dimensional continuous ∗ † Work done during the internship at Google. Corresponding author. 51 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 51–59, c November 20, 2020. 2020 Association for Computational Linguistics For word similarity, x represents a single word vector. A standard choice is defining C(x1 , x2 ) = hx1 , x2 i, the inner product between the two vectors. The result"
2020.eval4nlp-1.6,P19-1493,0,0.0454258,"Missing"
2020.eval4nlp-1.6,W04-1013,0,0.0428434,"erties; and a computationally efficient tempered Word Mover Distance, for better fusion of the information in the contextualized word representations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERTbackbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks. 1 Introduction Automatic evaluation metrics play an important role in comparing candidate sentences generated by machines against human references. Firstgeneration metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) use predefined handcrafted rules to measure surface similarity between sentences and have no ability, or very little ability (Banerjee and Lavie, 2005), to go beyond word surface level. To address this problem, later work (Kusner et al., 2015; Zhelezniak et al., 2019) utilize static embedding techniques such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) to represent the words in sentences as vectors in a low-dimensional continuous ∗ † Work done during the internship at Google. Corresponding author. 51 Proceedings of the First Workshop on Evaluation and Comparison of N"
2020.eval4nlp-1.6,2021.ccl-1.108,0,0.0719345,"Missing"
2021.acl-long.294,2020.emnlp-main.19,0,0.0430894,"r row in the attention matrix. The nonzero entry selection is either content-based (Kitaev et al., 2020; Roy et al., 2020; Tay et al., 2020b; Zhou et al., 2020), hand-crafted (Beltagy et al., 2020; Brown et al., 2020; Child et al., 2019; Ho et al., 2019) or simply random (Zaheer et al., 2020). It is also well known in the NLP literature that long-range contextual information is necessary for many NLP tasks (Khandelwal et al., 2018; Liu and Lapata, 2019). So a set of global tokens are also considered. This adds O(1) number of dense rows and columns to the attention matrix (Zaheer et al., 2020; Ainslie et al., 2020; Beltagy et al., 2020). A third approach is to approximate the attention matrix with a low-rank factored form (Choromanski et al., 2020; Wang et al., 2020; Tay et al., 2020a). The first two approaches are based on the premise that one needs to explicitly zero out entries in the attention matrix in order to reduce the quadratic complexity. Decades of research by the scientific computing and numerical analysis community has resulted in more sophisticated algorithms to sparsify matrices. A 1 Eq. (11) and (12) offer a simple illustration of this intuition. small set of samples of these algorithms"
2021.acl-long.294,P18-1027,0,0.057961,"Missing"
2021.acl-long.294,P19-1500,0,0.14617,"mounts to truncating off-diagonal entries in the attention matrix beyond a user-specified sequence distance. A second approach is to keep O(1) number of nonzeros per row in the attention matrix. The nonzero entry selection is either content-based (Kitaev et al., 2020; Roy et al., 2020; Tay et al., 2020b; Zhou et al., 2020), hand-crafted (Beltagy et al., 2020; Brown et al., 2020; Child et al., 2019; Ho et al., 2019) or simply random (Zaheer et al., 2020). It is also well known in the NLP literature that long-range contextual information is necessary for many NLP tasks (Khandelwal et al., 2018; Liu and Lapata, 2019). So a set of global tokens are also considered. This adds O(1) number of dense rows and columns to the attention matrix (Zaheer et al., 2020; Ainslie et al., 2020; Beltagy et al., 2020). A third approach is to approximate the attention matrix with a low-rank factored form (Choromanski et al., 2020; Wang et al., 2020; Tay et al., 2020a). The first two approaches are based on the premise that one needs to explicitly zero out entries in the attention matrix in order to reduce the quadratic complexity. Decades of research by the scientific computing and numerical analysis community has resulted i"
2021.acl-long.294,D15-1166,0,0.0607984,"tive in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models. 1 Introduction Linearly combining information using contentbased weights, a method generically known as attention, is a key building block in many deep neural networks such as recurrent neural networks (RNN) (Luong et al., 2015), convolutional neural networks (CNN) (Bello et al., 2019) and graph convolutional networks (GCN) (Velickovic et al., 2018). One particular type of such attention, called multi-head scaled dot-product attention, is one of the main components of the Transformer architecture proposed by Vaswani et al. (2017), which has been shown to push the state-of-theart (SOTA) performance for various understanding and generation tasks. These include standard natural language processing (NLP) tasks such as machine translation, document classification, entailment, summarization and question answering (Zaheer e"
2021.acl-long.294,D18-1325,0,0.0273005,"of about 200 tokens, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. The observation is succinctly summarized by the title of the paper ”sharp nearby, fuzzy far away”. Remarkably, this is in spirit very close to the key insight into the Multilevel Methods. A few recent attention-related studies have explored this direction with some success, such as word-level and sentence-level attentions in (Miculicich et al., 2018; Abreu et al., 2019), and sentence-level and paragraph-level attentions in (Liu and Lapata, 2019). Even though the proposed hierarchical attention in these studies only has two levels, as opposed to ten or more levels typically used by the Multilevel Methods, the reported positive results are quite suggestive. We therefore hypothesize that the same hierarchical low-rank structure as shown in Eq (13) might also hold for the attention matrix in many NLP tasks. And we treat it as the inductive bias in the hierarchical attention mechanism proposed in this paper. As pointed out in (Goyal and Bengi"
2021.acl-long.294,2020.findings-emnlp.232,0,0.0746093,"Missing"
2021.conll-1.14,D17-1098,0,0.0180083,"ce the set of test images and guiding text pairs used for our human evaluation experiments in order to encourage future work in this direction and facilitate direct comparisons with our results. 2 de Vries, 2015; Devlin et al., 2015) provide image grounding, but lack the ability to produce diverse captions, while early encoder-decoder methods (Donahue et al., 2015; Vinyals et al., 2015b; Karpathy and Li, 2015; Zheng et al., 2019; Wu et al., 2015; Sharma et al., 2018) produce diverse captions, but lack image grounding. Contemporary encoder-decoder methods (Lu et al., 2018; Cornia et al., 2019; Anderson et al., 2017; You et al., 2016; Chen et al., 2020; Mun et al., 2017; Xu et al., 2015; Fang et al., 2015), including the method proposed in this work, can generate diverse captions with image grounding. Most similar to our work, (Zheng et al., 2019) propose the use of a guiding object and produces a caption by using a forward and backward LSTM to separately generate the caption text before and after the guiding object. In contrast, our approach involves a multi-modal Transformer model that uses layers of self-attention and cross-attention to better ground the target caption using early-fused representation"
2021.conll-1.14,D19-1155,1,0.835628,"o transform 184 Trainable Caption Frozen Transformer Decoder Transformer Encoder FC FC FC G RGR Graph RISE R-Graph RISE RFRCNN R-CNN Subword Embed T Subword Tokenizer RPN Image Guiding Text Figure 2: Our model architecture. Thick arrows indicate a sequence, such as bounding boxes for image regions, or list of tokens in a text. the image features in the context of the guiding text concepts and vice versa, such that the decoder input is conditioned on the joint text-image representation. We represent the input image as a sequence of global and regional features, G and R, respectively. Following Changpinyo et al. (2019), we use Graph-Regularized Image Semantic Embedding (GraphRISE) as the global image features G. These 64-dimensional features, trained to discriminate O(40M) ultra-fine-grained semantic labels, have been shown to outperform other state of the art image embeddings (Juan et al., 2019). 3.1 Encoder All models in the image feature extraction pipeline, are pre-trained and are not fine-tuned during our model training process. To transform them into the encoder input space, we use a trainable fullyconnected network for each type of feature, denoted as F C in Figure 2. The guiding text is provided to"
2021.conll-1.14,P15-2017,0,0.0296703,"particular, we investigate the underlying characteristics of image captioning datasets required to produce higher quality guided captions. Our results suggest that the key to solving in-thewild guided image captioning may not be laborious human annotations, Visual Genome– style, but rather access to noisy, unrestricteddomain training datasets with high style diversity. • We open-source the set of test images and guiding text pairs used for our human evaluation experiments in order to encourage future work in this direction and facilitate direct comparisons with our results. 2 de Vries, 2015; Devlin et al., 2015) provide image grounding, but lack the ability to produce diverse captions, while early encoder-decoder methods (Donahue et al., 2015; Vinyals et al., 2015b; Karpathy and Li, 2015; Zheng et al., 2019; Wu et al., 2015; Sharma et al., 2018) produce diverse captions, but lack image grounding. Contemporary encoder-decoder methods (Lu et al., 2018; Cornia et al., 2019; Anderson et al., 2017; You et al., 2016; Chen et al., 2020; Mun et al., 2017; Xu et al., 2015; Fang et al., 2015), including the method proposed in this work, can generate diverse captions with image grounding. Most similar to our wo"
2021.conll-1.14,P15-1005,0,0.0677433,"Missing"
2021.conll-1.14,W17-2214,0,0.0331721,"ptioning uses a short-text input form (the guiding text) and a long-text output form (the caption). 183 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 183–193 November 10–11, 2021. ©2021 Association for Computational Linguistics tion”, “love”) and actions (e.g. “swimming”). A key question we ask in this work is what kind of training data can lead to models that work better in a real-world setting. Ideally, the training data should contain multiple captions for a given image, each associated with a different guiding text. The Visual Genome dataset (Krishna et al., 2017a) provides exactly that, with a total of 3.6M objectlevel captions created through human annotation. In contrast, the Conceptual Captions (Sharma et al., 2018) contains only image-level captions created through an automatic pipeline obtaining images and captions from the internet. We perform human evaluations that measure caption informativeness, correctness and fluency to test our approach in a real-world setting. Interestingly, while the Conceptual Captions dataset contains a similar number of captions (3.3M), and has a lower number of unique tokens than Visual Genome, models trained on thi"
2021.conll-1.14,P18-1238,1,0.93695,"oved performance. 1 Figure 1: An illustration comparing the difference between an image captioning model and a guided image captioning model. The guided image captioning model generates captions that focus on a specific object, concept or action of the image, provided to the model as free-form guiding text. Introduction Describing the content of an image using natural language is generically referred to as image captioning, but there are a variety of ways in which this can be achieved: by focusing on the most salient aspects of an image, as in MSCOCO (Lin et al., 2014) or Conceptual Captions (Sharma et al., 2018); on most of the groundable concepts in an image, as in Image Paragraphs (Krause et al., 2017) or Localized Narratives (Pont-Tuset et al., 2020); or on a predefined set of objects, as in dense captioning (Johnson et al., 2016). These various approaches acknowledge that a typical realworld image may contain a varying number of objects/concepts/actions that may be of interest to the caption consumer, and therefore the optimal description depends on the degree to which the ∗ Work done as a part of the Google AI Residency. caption covers what the user is interested in at any given moment. However,"
2021.emnlp-main.164,2020.acl-main.586,1,0.858065,"chine with little human intervention. 1 Introduction Multiple datasets have been proposed to measure the progress on visual question answering (VQA) (Antol et al., 2015; Zhu et al., 2016b; Goyal et al., 2017; Gurari et al., 2018; Hudson and Manning, 2019; Yang et al., 2016; Tu et al., 2014; Qi et al., 2015; Liu et al., 2016). However, these datasets often possess biases introduced in the data collection process and by the human annotators. It has been shown that existing VQA models leverage these spurious biases and take shortcuts (Goyal et al., 2017; Agrawal et al., 2018; Chao et al., 2018a; Akula et al., 2020a). As a result, the performance of those models on a specific VQA dataset can only serve as a rough proxy for the true learning of the VQA task (Bras et al., 2020). Test sets Ivqa2 Ivzwz Ioid QAvqa2 QAvzwz 7 X 7 X 7 7 (a) Test sets Ivqa2 Ivzwz Ioid QAvqa2 QAvzwz X X X X X X (b) Table 1: (a) Existing VQA test sets; (b) CrossVQA (disentangled) test sets generated by our VQAG model. One common remedy to this is to go beyond indomain evaluation, in which the test set exhibits some form of “distribution shifts” from the training set (Agrawal et al., 2018; Chao et al., 2018b). The key idea is that"
2021.emnlp-main.164,D19-1219,0,0.0283637,"ls Figure 2: Overview of CrossVQA. We train a controllable visual question-answer generation (VQAG) engine and use the dataset indicators and control signals to generate the desired cross-dataset shifts. trieval and question answering (Antol et al., 2015; Zhu et al., 2016b; Akula, 2015; Palakurthi et al., 2015). While the task of generating question automatically is well studied in the language domain, it has been under-explored for image-related natural questions (Mostafazadeh et al., 2016). Prior works explored VQG using autoencoder-based architectures (Jain et al., 2017; Yang et al., 2018; Alberti et al., 2019; Krishna et al., 2019). Jain et al. (2017) employ a variational autoencoder paradigm where they first learn to embed a given question and image into a low dimensional latent space. The latent codes are subsequently mapped to a highdimensional representation using RNNs during inference to generate the question. Krishna et al. (2019) model question generation as a process that maximizes mutual information between the image and the expected answer’s category. They incorporate fine-grained answer type as the guidance to generate goal-driven questions. Xu et al. (2020) propose an answer-centric ap"
2021.emnlp-main.164,W05-0909,0,0.0611185,"Missing"
2021.emnlp-main.164,2020.tacl-1.43,0,0.0244814,"tion mismatches. To complicate things even more, the frequency of objects occurring in natural images follows a long-tail distribution (Salakhutdinov et al., 2011; Zhu et al., 2014, 2016a). Lack of sufficient instances of minority classes in the test sets further complicates the estimation of generalization capabilities from one dataset to another. A possible solution to address this issue is to use an iterative, human-in-the-loop approach for dataset collection where human annotators carefully devise new test samples by incorporating visual and language distribution shifts (Nie et al., 2020; Bartolo et al., 2020; Kaushik et al., 2020; Gardner et al., 2020). However, this approach is not scalable and training the human annotators, be they seasoned AI experts or non-experts, would incur huge annotation time and cost. In this work, we propose to make the process of creating distribution shifts more systematic and automatic. Inspired by recent work on ∗ Work done in part while AA was an intern at Google. dynamic benchmarks that co-evolve with strong 2148 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2148–2166 c November 7–11, 2021. 2021 Association for Comp"
2021.emnlp-main.164,W04-1013,0,0.0305889,"Missing"
2021.emnlp-main.164,D16-1155,1,0.836183,"d datasets and demonstrate its utility by using them to evaluate several state-of-theart VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the language shifts. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention. 1 Introduction Multiple datasets have been proposed to measure the progress on visual question answering (VQA) (Antol et al., 2015; Zhu et al., 2016b; Goyal et al., 2017; Gurari et al., 2018; Hudson and Manning, 2019; Yang et al., 2016; Tu et al., 2014; Qi et al., 2015; Liu et al., 2016). However, these datasets often possess biases introduced in the data collection process and by the human annotators. It has been shown that existing VQA models leverage these spurious biases and take shortcuts (Goyal et al., 2017; Agrawal et al., 2018; Chao et al., 2018a; Akula et al., 2020a). As a result, the performance of those models on a specific VQA dataset can only serve as a rough proxy for the true learning of the VQA task (Bras et al., 2020). Test sets Ivqa2 Ivzwz Ioid QAvqa2 QAvzwz 7 X 7 X 7 7 (a) Test sets Ivqa2 Ivzwz Ioid QAvqa2 QAvzwz X X X X X X (b) Table 1: (a) Existing VQA te"
2021.emnlp-main.164,P16-1170,0,0.0252429,"ol signals B’s image A’s indicator Other control signals VQAG Question & Answer C’s image A’s or B’s indicator Other control signals Figure 2: Overview of CrossVQA. We train a controllable visual question-answer generation (VQAG) engine and use the dataset indicators and control signals to generate the desired cross-dataset shifts. trieval and question answering (Antol et al., 2015; Zhu et al., 2016b; Akula, 2015; Palakurthi et al., 2015). While the task of generating question automatically is well studied in the language domain, it has been under-explored for image-related natural questions (Mostafazadeh et al., 2016). Prior works explored VQG using autoencoder-based architectures (Jain et al., 2017; Yang et al., 2018; Alberti et al., 2019; Krishna et al., 2019). Jain et al. (2017) employ a variational autoencoder paradigm where they first learn to embed a given question and image into a low dimensional latent space. The latent codes are subsequently mapped to a highdimensional representation using RNNs during inference to generate the question. Krishna et al. (2019) model question generation as a process that maximizes mutual information between the image and the expected answer’s category. They incorpora"
2021.emnlp-main.164,2020.acl-main.441,0,0.210978,"language distribution mismatches. To complicate things even more, the frequency of objects occurring in natural images follows a long-tail distribution (Salakhutdinov et al., 2011; Zhu et al., 2014, 2016a). Lack of sufficient instances of minority classes in the test sets further complicates the estimation of generalization capabilities from one dataset to another. A possible solution to address this issue is to use an iterative, human-in-the-loop approach for dataset collection where human annotators carefully devise new test samples by incorporating visual and language distribution shifts (Nie et al., 2020; Bartolo et al., 2020; Kaushik et al., 2020; Gardner et al., 2020). However, this approach is not scalable and training the human annotators, be they seasoned AI experts or non-experts, would incur huge annotation time and cost. In this work, we propose to make the process of creating distribution shifts more systematic and automatic. Inspired by recent work on ∗ Work done in part while AA was an intern at Google. dynamic benchmarks that co-evolve with strong 2148 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2148–2166 c November 7–11, 2021. 202"
2021.emnlp-main.164,N16-1000,0,0.0952777,"re there is no or limited labeled data (Ganin and Lempitsky, 2015; Gong et al., 2012; Guo and Xiao, 2012; Tzeng et al., 2015; Akula et al., 2020b). However, these works focus either on language modeling or visual recognition tasks. Here, we investigate adaptation skills using the multi-modal VQA task, for which distribution mismatches can occur in both language and visual features. There are a few works that study systematic compositional skills in multi-modal tasks. For example, Lampert et al. (2009) study the use of attributes in transferring information between object classes. Jabri et al. (2016) explore several variants of the VQA task and show that VQA models struggle with transferring knowledge across datasets. Agrawal et al. (2018) study the extent to which a model is visually grounded, by evaluating its ability to generalize to a different answer distribution for each question type. Chao et al. (2018b) investigate the issue of cross-dataset generalization, using a specific setting where the source domain contains a large amount of training data and the target domain contains insufficient data to train a VQA system from scratch. Unlike these works, our work performs a more fine-gr"
2021.emnlp-main.164,2020.acl-main.16,1,0.887389,"Missing"
2021.emnlp-main.164,P19-1472,0,0.150541,"vzwz What color is the screen’s background? Green There is a screen above my finger. Can you please tell me what it says? Swipe QAvqa2 QAvzwz Figure 1: Existing works on VQA domain adaptation between source and target datasets (e.g. VQA2. and VizWiz) can only compare the model’s performance on the entangled test splits hIvqa2 , QAvqa2 i and hIvzwz , QAvzwz i. In this work, we propose a VQAG module to generate novel and scalable VQA test sets, called CrossVQA, consisting of additional test sets hIvqa2 , QAvzwz i and hIvzwz , QAvqa2 i where visual and language features are disentangled. models (Zellers et al., 2019b), we propose to bring in visual question-answer generation (VQAG) module in the evaluation process. More specifically, we first build a strong, controllable VQAG engine that is capable of creating particular dataset-style question-answer pairs. Then, we use it to generate novel himage, question, answeri test splits, while controlling distribution shifts in vision and language features. This is summarized in Table 1 and exemplified with the VQA2 and VizWiz datasets in Figure 1. Collectively, we refer to the resulting VQA test sets as CrossVQA. through a combination of transformer-based archit"
2021.emnlp-main.164,D17-1090,0,0.0669528,"Missing"
2021.emnlp-main.164,N16-1019,1,0.802513,"ages datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-theart VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the language shifts. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention. 1 Introduction Multiple datasets have been proposed to measure the progress on visual question answering (VQA) (Antol et al., 2015; Zhu et al., 2016b; Goyal et al., 2017; Gurari et al., 2018; Hudson and Manning, 2019; Yang et al., 2016; Tu et al., 2014; Qi et al., 2015; Liu et al., 2016). However, these datasets often possess biases introduced in the data collection process and by the human annotators. It has been shown that existing VQA models leverage these spurious biases and take shortcuts (Goyal et al., 2017; Agrawal et al., 2018; Chao et al., 2018a; Akula et al., 2020a). As a result, the performance of those models on a specific VQA dataset can only serve as a rough proxy for the true learning of the VQA task (Bras et al., 2020). Test sets Ivqa2 Ivzwz Ioid QAvqa2 QAvzwz 7 X 7 X 7 7 (a) Test sets Ivqa2 Ivzwz Ioid QAvqa"
2021.findings-emnlp.291,2020.acl-main.583,1,0.651743,"ormation goals for these captions, and assigns a more adequate score when comparing the Model caption against the Human caption. In this case where a caption that does not just describe the image but elaborates on it, our metric recognizes that the model output is potentially successful (Photo credit: Moorthy Gounder) Introduction An investigation of the descriptions used with im- Lin, 2004; Denkowski and Lavie, 2014; Anderson et al., 2016a). ages on the web shows that image descriptions can have different functions and goals (Kruk et al., So far, however, efforts to develop such expres2019a; Alikhani et al., 2020). For instance, cap- sive captioning models have been hindered by the tions may describe visible entities, activities and lack of automatic metrics that can evaluate their relationships, provide background information that output with respect to their information goals in goes beyond what’s visible, or report the writer’s context. Previous approaches to automatic capown subjective reactions to what’s displayed. By tion evaluation have mostly focused on n-gram drawing on such diverse examples, image caption- measures of similarity to reference output (Vedaning models can learn the different inf"
2021.findings-emnlp.291,N19-1423,0,0.013806,"metric to pre- words are common and used appropriately in the vious ones using a common methodology, ranking generated text. Embedding-based metrics such as the performance of several different caption genera- BLEURT (Sellam et al., 2020) and BERTScore tion systems on out-of-domain images—relying on (Zhang et al., 2020) designed to address this lima new benchmark out-of-domain test set, which we itation are closer to human ratings. BLEURT is publish, providing reference captions for a subset a data-intensive training scheme that is based on of OpenImages (Kuznetsova et al., 2020b). Our BERT (Devlin et al., 2019) fine-tuned on human 3420 Facade of a glass building. A pink flower bush in a garden. The underside of the Arc de Triomphe. Close-up of a fly sitting on a daisy. Man sitting by his artwork looking at a large statue of a man on a horse in a royal courtyard. Woman with an umbrella reading a book sitting in the grass in front of a city skyline. Cowboy on a horse and cowboy on the ground working together to lasso a calf in a pen. Black and white artwork painted on a blue wall. Figure 2: Examples of the ground truth captions that we collected for the COIN dataset. (Photo credits from left to right,"
2021.findings-emnlp.291,2020.emnlp-main.191,0,0.0233321,"ni et al. (2020)’s relations that charory (Hobbs, 1985), which characterizes the infer- acterize inferences between text and images. ences that give discourse units a coherent joint inCoherence-aware models have benefited sevterpretation using a constrained inventory of coher- eral NLP tasks such as gesture interpretation (Lasence relations. In particular, we use the taxonomy carides and Stone, 2009; Pustejovsky and Krishfor image–text coherence developed by Alikhani naswamy, 2020), text summarization (Xu et al., et al. (2020), which for example includes Visible, 2019), machine comprehension (Gao et al., 2020). Story and Subjective relations between the text and The majority of these works use Rhetorical Structhe image. A description and an image stand in a ture Theory (RST) (Mann and Thompson, 1987) Visible relation if the text includes information that and Penn Discourse TreeBank (PDTB) (Prasad is recognizably depicted in the image. Subjective et al., 2008b) datasets to learn and predict these captions react to the content of the image and Story relations between two adjacent text spans. In this captions provide a free-standing description of the line of work, we are the first to present a cohere"
2021.findings-emnlp.291,W19-5357,0,0.0114082,"tion. A subset of 50 random images is used of the reference or generated captions. These serve to rank the captioning systems as described above, as baselines for COSMic. resulting in 400 machine-generated captions total N-gram based The most popular image caption- for the 8 captioning systems. These were then ing metrics are based on precision and recall of n- evaluated by human annotators using the process described in Section 3. The human-scored system grams from generated and reference captions. We level performance for each captioning system on compare with Bleu1 , Bleu2 , Bleu3 , Bleu4 (Guo and Hu, 2019), ROUGEL (Lin, 2004), CIDEr (Vedan- this test set is reported in Table 1 in “Average Hutam et al., 2015), and SPICE (Anderson et al., man Rating”. We measure the alignment between metric2016b). We compute these using their popular 6 assigned and human-assigned scores using the open-source implementation . Kendall (Kendall, 1938) correlation coefficient. In BLEURT We use a pre-trained BLEURT model7 order to calculate the score, we first aggregate all as a baseline for our work. Unlike N-gram based the sample scores and average them. Then we approaches, BLEURT uses BERT-based word em- calculate"
2021.findings-emnlp.291,W14-3348,0,0.00939303,"different coherence relation than the reference (Human) caption. “Coh.” represents the coherence labels for generated and reference captions. Our coherence-aware metric COSMic is aware of the different information goals for these captions, and assigns a more adequate score when comparing the Model caption against the Human caption. In this case where a caption that does not just describe the image but elaborates on it, our metric recognizes that the model output is potentially successful (Photo credit: Moorthy Gounder) Introduction An investigation of the descriptions used with im- Lin, 2004; Denkowski and Lavie, 2014; Anderson et al., 2016a). ages on the web shows that image descriptions can have different functions and goals (Kruk et al., So far, however, efforts to develop such expres2019a; Alikhani et al., 2020). For instance, cap- sive captioning models have been hindered by the tions may describe visible entities, activities and lack of automatic metrics that can evaluate their relationships, provide background information that output with respect to their information goals in goes beyond what’s visible, or report the writer’s context. Previous approaches to automatic capown subjective reactions to w"
2021.findings-emnlp.291,D19-1469,0,0.0253748,"Missing"
2021.findings-emnlp.291,W04-1013,0,0.455583,"that has a different coherence relation than the reference (Human) caption. “Coh.” represents the coherence labels for generated and reference captions. Our coherence-aware metric COSMic is aware of the different information goals for these captions, and assigns a more adequate score when comparing the Model caption against the Human caption. In this case where a caption that does not just describe the image but elaborates on it, our metric recognizes that the model output is potentially successful (Photo credit: Moorthy Gounder) Introduction An investigation of the descriptions used with im- Lin, 2004; Denkowski and Lavie, 2014; Anderson et al., 2016a). ages on the web shows that image descriptions can have different functions and goals (Kruk et al., So far, however, efforts to develop such expres2019a; Alikhani et al., 2020). For instance, cap- sive captioning models have been hindered by the tions may describe visible entities, activities and lack of automatic metrics that can evaluate their relationships, provide background information that output with respect to their information goals in goes beyond what’s visible, or report the writer’s context. Previous approaches to automatic capow"
2021.findings-emnlp.291,P19-1654,0,0.018747,"m left to right, top to bottom: Sharron Mollerus, Northfielder, George M. Groutas, davebloggs007, Tim Adams, Brisbane City Council, Colin Brown, Guilhem Vellut) ratings of generated text. BERTScore, however, computes the similarity score as the average of cosine similarities between predicted tokens and their top matching reference tokens. These metrics however, do not respect the information goal and the purpose for which the model has generated the text. We address this problem by introducing the first coherence-aware generation metric. Similar to SPICE (Anderson et al., 2016b) and VIFIDEL (Madhyastha et al., 2019) we use the information encoded in images. We further propose the addition of coherence relations that facilitate learning with fewer samples by a multimodal metric using pre-trained BERT and ViLBERT. 3 Data Collection We collect two datasets: human judgments for image captions that are generated by coherenceaware captioning systems using Conceptual Captions dataset; and ground-truth labels for the Open Images dataset. With Conceptual Captions corpora we fine-tune ViLBERT with ratings and show that addition of coherence relations can make automated scoring closer to human scoring. We use OpenI"
2021.findings-emnlp.291,P02-1040,0,0.110516,"displayed. By tion evaluation have mostly focused on n-gram drawing on such diverse examples, image caption- measures of similarity to reference output (Vedaning models can learn the different inferential links tam et al., 2014); such surface-level models fail between text and images and use that information to deal with the lexical and syntactic diversity of at generation time to produce descriptions that can image descriptions. More recent approaches more fulfill different discourse goals and inject the de- closely approximate semantic similarity using word sired context into their output (Papineni et al., 2002; embedding-based techniques. These models show 3419 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3419–3430 November 7–11, 2021. ©2021 Association for Computational Linguistics robust performance and achieve a higher correlation with human judgments than that of previous metrics. Nevertheless, they too fail to generalize to the different kinds of content that successful descriptions may exhibit across different goals and contexts. That is, they cannot distinguish reasonable descriptions that happen to differ from reference output in their goals and perspective,"
2021.findings-emnlp.291,prasad-etal-2008-penn,0,0.428949,"in 4 different coherence classes of Meta, Visible, Subjective, Story. These 4,000 image/caption pairs are then presented to human annotators who are asked to select the correct coherence label for each pair: Protocol We hired two expert linguists for data annotation and designed an annotation website to facilitate the annotation procedure. They are native English speakers who identify themselves as 3421 • Meta: the caption talks about when, where, and how the picture is taken. Meta-talk in Schiffrin (1980) • Visible: the caption is true just by looking at the picture. Restatement relation in Prasad et al. (2008a). • Subjective: the captions is the matter of opinion. Evaluation relation in Hobbs (1985). • Story: text and image work like story and illustration. Occasion relation in Hobbs (1985). 1 https://github.com/Merterm/COSMic Figure 3: An illustration of different flavors of COSMic that outputs a score for the generated caption given the image, reference caption, and the coherence-labels for both the captions. (a) COSMic Vanilla uses only global textual and visual features, while (b) COSMic ViLBERT uses combined visio-linguistic features with both local and global focus. This model takes into acc"
2021.findings-emnlp.291,2020.acl-main.704,0,0.0820457,"ing a ROUGE (Lin, 2004), and CIDEr (Vedantam et al., coherence-aware metric. We present a model to 2015), . The major problem of the n-gram simscore a generated caption given the image, refer- ilarity metrics is that they give no credit to synence caption, and the discourse goals of both these onym matches of reference n-grams, even if those captions (Section 4). We compare this metric to pre- words are common and used appropriately in the vious ones using a common methodology, ranking generated text. Embedding-based metrics such as the performance of several different caption genera- BLEURT (Sellam et al., 2020) and BERTScore tion systems on out-of-domain images—relying on (Zhang et al., 2020) designed to address this lima new benchmark out-of-domain test set, which we itation are closer to human ratings. BLEURT is publish, providing reference captions for a subset a data-intensive training scheme that is based on of OpenImages (Kuznetsova et al., 2020b). Our BERT (Devlin et al., 2019) fine-tuned on human 3420 Facade of a glass building. A pink flower bush in a garden. The underside of the Arc de Triomphe. Close-up of a fly sitting on a daisy. Man sitting by his artwork looking at a large statue of a"
2021.findings-emnlp.291,P18-1238,1,0.812053,"Missing"
2021.naacl-main.253,2020.acl-main.583,1,0.839831,"g embedding sequence is O = (o1 , . . . , o|O |), where each oj has dimension Do = 256. Caption Universal Sentence Embedding The caption text is embedded using a pretrained version of the Universal Sentence Encoder (USE) (Cer et al., 2018) into a Ds = 512 dimensional vector s. The USE model itself is trained on large amounts of English sources (Wikipedia, web news, discussion forums, etc.) and fine-tuned using supervised labels from the SNLI corpus (Bowman et al., 2015). We have alternatively tried a BERT (Devlin et al., 2019) model as an encoder, but observed it provides no additional gains (Alikhani et al., 2020) vation (Xu et al., 2015), and then combines each of the resulting vector pairs using bilinear layers (see below). All bilinear outputs are then concatenated and fed to a dense layer with a sigmoid activation, to produce the quality estimation yˆ. 5.1.1 Bilinear Layers A bilinear layer models the inner product of its two inputs after applying a linear transformation to the second input. This layer is defined as: b(x, y; B) = xT By = hx, Byi (1) where x ∈ RDx and y ∈ RDy are input features, and B ∈ RDx ×Dy is the learned parameter matrix. Linear and bias terms can be added by appending a consta"
2021.naacl-main.253,D15-1075,0,0.0342805,"intuition is that the caption should likely mention the more salient objects. We use the object label model mentioned in Sec. 3.1, whose resulting embedding sequence is O = (o1 , . . . , o|O |), where each oj has dimension Do = 256. Caption Universal Sentence Embedding The caption text is embedded using a pretrained version of the Universal Sentence Encoder (USE) (Cer et al., 2018) into a Ds = 512 dimensional vector s. The USE model itself is trained on large amounts of English sources (Wikipedia, web news, discussion forums, etc.) and fine-tuned using supervised labels from the SNLI corpus (Bowman et al., 2015). We have alternatively tried a BERT (Devlin et al., 2019) model as an encoder, but observed it provides no additional gains (Alikhani et al., 2020) vation (Xu et al., 2015), and then combines each of the resulting vector pairs using bilinear layers (see below). All bilinear outputs are then concatenated and fed to a dense layer with a sigmoid activation, to produce the quality estimation yˆ. 5.1.1 Bilinear Layers A bilinear layer models the inner product of its two inputs after applying a linear transformation to the second input. This layer is defined as: b(x, y; B) = xT By = hx, Byi (1) whe"
2021.naacl-main.253,D18-2029,0,0.0200043,"Missing"
2021.naacl-main.253,D19-1155,1,0.840907,"previous work indicates (Sharma et al., 2018), for out-of-domain images (OID), captions produced by Conceptual Captions trained models tend to have higher quality compared to captions produced by COCO-trained models. All of the models are trained to minimize the ground-truth caption perplexity; however, they differ on several important aspects (which contributes to caption diversity): the image feature representations, the number of object detection results they use, and the caption decoding procedure. We briefly discuss these differences below; for further details, see (Sharma et al., 2018; Changpinyo et al., 2019). Global Image Representation Our captioning models use one of the following pretrained image encoders: (1) The Inception-ResNet-v2 model (Szegedy et al., 2016), (2) The Picturebook image encoder (Kiros et al., 2018), or, (3) The GraphRISE model (Juan et al., 2019), a ResNet-101 model (He et al., 2016) trained for an image classification task at ultra-fine granularity levels. Object Representations The identification of objects in an image is done using a Faster R-CNN model, training it to predict both 1,600 object and 400 attribute labels in Visual Genome (Krishna et al., 2017), following the"
2021.naacl-main.253,N19-1423,0,0.0135898,"ore salient objects. We use the object label model mentioned in Sec. 3.1, whose resulting embedding sequence is O = (o1 , . . . , o|O |), where each oj has dimension Do = 256. Caption Universal Sentence Embedding The caption text is embedded using a pretrained version of the Universal Sentence Encoder (USE) (Cer et al., 2018) into a Ds = 512 dimensional vector s. The USE model itself is trained on large amounts of English sources (Wikipedia, web news, discussion forums, etc.) and fine-tuned using supervised labels from the SNLI corpus (Bowman et al., 2015). We have alternatively tried a BERT (Devlin et al., 2019) model as an encoder, but observed it provides no additional gains (Alikhani et al., 2020) vation (Xu et al., 2015), and then combines each of the resulting vector pairs using bilinear layers (see below). All bilinear outputs are then concatenated and fed to a dense layer with a sigmoid activation, to produce the quality estimation yˆ. 5.1.1 Bilinear Layers A bilinear layer models the inner product of its two inputs after applying a linear transformation to the second input. This layer is defined as: b(x, y; B) = xT By = hx, Byi (1) where x ∈ RDx and y ∈ RDy are input features, and B ∈ RDx ×Dy"
2021.naacl-main.253,U13-1004,0,0.0751966,"Missing"
2021.naacl-main.253,N16-1059,0,0.017239,"This is done by collecting additional fine-grained caption annotations from trained human raters, over images that are out-of-domain for the QE model. 2 Related Work Our paper is most similar to work done on evaluation metrics of image captions, where the main difference is that QE does not have access to the ground truth captions. Quality estimation has more than a decade long history in the Machine Translation (MT) field, from the early work based on feature engineering (Specia et al., 2009; Soricut and Echihabi, 2010), to more recent neural-network–based approaches (Kreutzer et al., 2015; Kim and Lee, 2016; Kim et al., 2017). The QE track at the WMT conference (Specia et al., 2019) has been running for several years, with multiple participants and notable improvements in model performance over the years. However, there are significant differences in the formulation of the QE task between MT and image captioning, most notably the fact that the MT formulation is 1 https://github.com/ google-research-datasets/ image-caption-quality-dataset uni-modal (text-only alignment). As a result, solutions for QE in the MT context tend to focus on feature-engineering that exploits this aspect (Specia et al.,"
2021.naacl-main.253,P18-1085,0,0.0216887,"of the models are trained to minimize the ground-truth caption perplexity; however, they differ on several important aspects (which contributes to caption diversity): the image feature representations, the number of object detection results they use, and the caption decoding procedure. We briefly discuss these differences below; for further details, see (Sharma et al., 2018; Changpinyo et al., 2019). Global Image Representation Our captioning models use one of the following pretrained image encoders: (1) The Inception-ResNet-v2 model (Szegedy et al., 2016), (2) The Picturebook image encoder (Kiros et al., 2018), or, (3) The GraphRISE model (Juan et al., 2019), a ResNet-101 model (He et al., 2016) trained for an image classification task at ultra-fine granularity levels. Object Representations The identification of objects in an image is done using a Faster R-CNN model, training it to predict both 1,600 object and 400 attribute labels in Visual Genome (Krishna et al., 2017), following the Bottom-Up Top-Down setting (Anderson et al., 2018). In terms of featurization for the identified bounding boxes, we use variants that include a ResNet-101 model pretrained on ImageNet (Russakovsky et al., 2015) Figu"
2021.naacl-main.253,W15-3037,0,0.0251043,"a fine-grained signal. This is done by collecting additional fine-grained caption annotations from trained human raters, over images that are out-of-domain for the QE model. 2 Related Work Our paper is most similar to work done on evaluation metrics of image captions, where the main difference is that QE does not have access to the ground truth captions. Quality estimation has more than a decade long history in the Machine Translation (MT) field, from the early work based on feature engineering (Specia et al., 2009; Soricut and Echihabi, 2010), to more recent neural-network–based approaches (Kreutzer et al., 2015; Kim and Lee, 2016; Kim et al., 2017). The QE track at the WMT conference (Specia et al., 2019) has been running for several years, with multiple participants and notable improvements in model performance over the years. However, there are significant differences in the formulation of the QE task between MT and image captioning, most notably the fact that the MT formulation is 1 https://github.com/ google-research-datasets/ image-caption-quality-dataset uni-modal (text-only alignment). As a result, solutions for QE in the MT context tend to focus on feature-engineering that exploits this aspe"
2021.naacl-main.253,P10-1063,1,0.705701,"than incorrect or unhelpful ones, even though they were never exposed to such a fine-grained signal. This is done by collecting additional fine-grained caption annotations from trained human raters, over images that are out-of-domain for the QE model. 2 Related Work Our paper is most similar to work done on evaluation metrics of image captions, where the main difference is that QE does not have access to the ground truth captions. Quality estimation has more than a decade long history in the Machine Translation (MT) field, from the early work based on feature engineering (Specia et al., 2009; Soricut and Echihabi, 2010), to more recent neural-network–based approaches (Kreutzer et al., 2015; Kim and Lee, 2016; Kim et al., 2017). The QE track at the WMT conference (Specia et al., 2019) has been running for several years, with multiple participants and notable improvements in model performance over the years. However, there are significant differences in the formulation of the QE task between MT and image captioning, most notably the fact that the MT formulation is 1 https://github.com/ google-research-datasets/ image-caption-quality-dataset uni-modal (text-only alignment). As a result, solutions for QE in the"
2021.naacl-main.253,2009.eamt-1.5,0,0.06547,"pful captions higher than incorrect or unhelpful ones, even though they were never exposed to such a fine-grained signal. This is done by collecting additional fine-grained caption annotations from trained human raters, over images that are out-of-domain for the QE model. 2 Related Work Our paper is most similar to work done on evaluation metrics of image captions, where the main difference is that QE does not have access to the ground truth captions. Quality estimation has more than a decade long history in the Machine Translation (MT) field, from the early work based on feature engineering (Specia et al., 2009; Soricut and Echihabi, 2010), to more recent neural-network–based approaches (Kreutzer et al., 2015; Kim and Lee, 2016; Kim et al., 2017). The QE track at the WMT conference (Specia et al., 2019) has been running for several years, with multiple participants and notable improvements in model performance over the years. However, there are significant differences in the formulation of the QE task between MT and image captioning, most notably the fact that the MT formulation is 1 https://github.com/ google-research-datasets/ image-caption-quality-dataset uni-modal (text-only alignment). As a res"
2021.naacl-main.253,P13-4014,0,0.163118,"Missing"
2021.naacl-main.253,W18-6465,0,0.057317,"Missing"
2021.naacl-main.253,P18-1238,1,0.92937,"starting point for our dataset is the Open Images Dataset (OID) (Kuznetsova et al., 2018) from which we randomly sample 16,000 images and then, for legal and privacy concerns, filter out those which contain faces2 . The choice for OID images is driven by their image copyright status (CC BY) and the fact that they are out-of-domain for popular image captioning datasets such as COCO and Conceptual Captions. To generate a diverse set of captions for annotation, we used several variants of Transformerbased (Vaswani et al., 2017) image-captioning models, trained on the Conceptual Captions dataset (Sharma et al., 2018), which consists of 3.3M training and ∼15,000 validation imagescaption pairs. As previous work indicates (Sharma et al., 2018), for out-of-domain images (OID), captions produced by Conceptual Captions trained models tend to have higher quality compared to captions produced by COCO-trained models. All of the models are trained to minimize the ground-truth caption perplexity; however, they differ on several important aspects (which contributes to caption diversity): the image feature representations, the number of object detection results they use, and the caption decoding procedure. We briefly"
D08-1093,P07-1038,0,0.020617,"slations can be produced and measured. Although a real estimate of the impact of a parser design decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision. On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly training procedure. 2 Related Work and Experimental Framework There have been previous studies which explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is neces"
D08-1093,P08-1087,0,0.0121278,"notation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et"
D08-1093,H91-1060,0,0.0220532,"hn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee reg"
D08-1093,P05-1022,0,0.765075,"mains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). Training Set Test Set WSJ sec. 02-21 (39,832 sent.) WSJ sec. 24 WSJ sec. 23 Brown-test Sent. count 1308 2343 2186 Charniak accuracy 90.48 91.13 86.34 Here we investigate algorithms for predicting the accuracy of a parser P on sentences, chunks of sentences, and whole corpora. We also investigate and contrast several scenarios for prediction: (1) the predictor looks at the input text only, (2) the predic"
D08-1093,2003.mtsummit-papers.6,1,0.676395,"accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse"
D08-1093,P98-1035,0,0.00969915,"es. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees pr"
D08-1093,P05-1066,0,0.0104477,"ta from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of"
D08-1093,J03-4003,0,0.0828782,"rees for the Brown test set helps us decide which prediction is better. Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experi"
D08-1093,N04-1035,1,0.557497,"ser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample s"
D08-1093,W01-0521,0,0.112482,"h explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-m"
D08-1093,W97-0302,0,0.049047,"tical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. This choice has been made to better reflect a scenario in which parser P would be used in a data-intensive application such as syntax-driven machine translation, in which the parser must be able to run through hundreds of millions of training words in a timely manner. We use the more accurate, but slower Charniak parser (Charniak and Johnson, 2005) as the reference parser Pref in our predictor (see Section 3.3). In order to predict the Collinsstyle parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that"
D08-1093,W01-1203,0,0.0212184,"the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain o"
D08-1093,2006.amta-papers.8,1,0.772156,"have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to c"
D08-1093,P08-1067,0,0.0354199,"r prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool1000 Length lexCount100 lexBool100 rootSYN UNK LM-PPL puncSYN de"
D08-1093,P03-1054,0,0.011043,"e of parser P against Pref . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool100"
D08-1093,N07-1006,0,0.0122313,"olean-based counterparts. Since these features measure different but overlapping pieces of the information available, it is to be expected that some of the feature combinations would provide better correlation that the individual features, but the gains are not strictly additive. By taking the individual features that provide the best discriminative power, we are able to get a correlation score of 0.42 on the test set. 3.5 dev (r) 0.55 test (r) 0.42 Optimizing for Maximum Correlation If our goal is to obtain the highest correlations with the F-score measure, is SVM regression the best method? Liu and Gildea (2007) recently introduced Maximum Correlation Training (MCT), a search procedure that follows the gradient of the formula for correlation coefficient (r). We implemented MCT, but obtained no better results. Moreover, it required many random re-starts just to obtain results comparable to SVM regression (Table 1). 4 WSJ-test (r) 0.42 0.61 0.62 0.69 0.79 WSJ-test (rms error) 0.098 0.026 0.019 0.015 0.011 Table 2: Performance of predictor on n-sentence chunks from WSJ-test (Correlation and rms error between actual/predicted accuracies). Table 1: Comparison of correlation (r) obtained using MCT versus S"
D08-1093,W06-1606,1,0.742809,"ins. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and conseq"
D08-1093,J93-2004,0,0.0309262,"e is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee regarding the performance of a parser on a new domain, and, as experiments in this paper show, can give wrong indications regarding important decisions for the design of NLP systems that use a syntactic parser as an important component. This paper proposes another method for measuring the performance of a parser on a given domain that is both cheap and effective. It is a fully automated procedure (no expensive annotation involved) that uses properties of bo"
D08-1093,P06-1043,0,0.604855,"y predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are c"
D08-1093,P03-1021,0,0.00309409,"Missing"
D08-1093,N07-1051,0,0.0277637,"f . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool1000 Length lexCount100 lex"
D08-1093,J01-2004,0,0.0635174,"automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is"
D08-1093,C98-1035,0,\N,Missing
D08-1093,P06-1121,1,\N,Missing
D19-1155,P04-1077,0,0.0169535,"ther models were not obtained due to the limited number of submissions per time period. As of August 30, 2019, the G + B-Ultra + L model outperforms all other single baselines6 , for both CIDEr and SPICE (and tie on ROUGE-L). 5 Table 1: Automatic metric scores for the image captioning task on Conceptual Captions. Ablation results are reported for our model using different sets of visual features. The top two baselines are from the Conceptual Captions Leaderboard as of August 30, 2019. Metrics We adopt the standard automatic metrics for image captioning: CIDEr (Vedantam et al., 2015), ROUGE-L (Lin and Och, 2004), and SPICE (Anderson et al., 2016), as implemented in the COCO-caption evaluation toolkit5 . Results We report results on both the dev and test sets for Conceptual Captions in Table 1. “Base” uses the G feature only. We first compare the Base G against each of the feature types (B-FRCNN, B-Ultra, and L). We then perform ablations under the +B condition (FRCNN/Ultra) to the Base G or stronger G + L models. According to dev CIDEr scores, global or box Graph-RISE features G and B-Ultra are (individually) clearly stronger than box features by Faster R-CNN B-FRCNN or label embeddings L features. N"
D19-1155,P18-1238,1,0.925371,"le supervised data for these downstream vision and language tasks. We empirically demonstrate the abovementioned advantages through a focused study of the effect of improved featurization on image captioning and VQA in transfer learning settings. In particular, we (i) leverage ultra-fine-grained semantic labels (e.g., “golden gate bridge” vs. “bridge”) for featurization (Juan et al., 2019); and, (ii) focus on scenarios in which object detection modules trained on Visual Genome (VG) (Krishna et al., 2017) are applied to out-of-domain images: image captioning on the Conceptual Captions dataset (Sharma et al., 2018), and VQA on the VizWiz dataset (Gurari et al., 2018). Our results indicate that there are ways to incorporate low-level pre-training tasks that benefit vision and language models via higher-quality bottom-up signals. 2 Related Work Attention-based deep models are popular in image captioning and VQA. Early work used fixed partitions of images as candidate regions (Xu et al., 2015). However, variable sized regions that are better correlated with object boundaries have gained momentum (Fu et al., 2017; Pedersoli et al., 2017; Anderson et al., 2018). Indeed, Anderson et al. (2018) established new"
K19-1039,D17-1042,0,0.0156931,"entarity of Video and ASR We now turn to the question of why multimodal models produce better captions: what type of signal does video contain that speech does not (and vice versa)? Our initial idea was to quantitatively compare the captions generated by AT versus AT+Video; however, because the dataset is relatively small, we were unable to make observations about the generated captions that were statistically significant.8 8 In general, making concrete statements about the causal link between inputs and outputs of sequence-to-sequence models is challenging, even in the text-to-text case, see Alvarez-Melis and Jaakkola (2017). 7 For instance, the constant prediction baseline we consider would score low in both vocab coverage and uniqueness. 424 Video AUCv, w 1.0 knead nori yeast mozzarella lettuce pancake wrapper patty dal grill pizza oven bake .75 .50 .50 .75 1.0 ASR Token AUCt, w 97.8 97.1 96.1 95.8 95.3 94.7 94.3 93.4 93.0 92.9 92.9 92.7 92.3 4 bit about prepare mixed then spoon or it ready 3 few more (a) Easiest (AUCµ,w ) 43.8 51.7 52.1 52.3 54.5 54.5 54.9 55.9 56.2 56.7 57.1 58.3 58.5 (b) Hardest (AUCµ,w ) fat turn sea white chilies dried beer pancetta mustard spice sliced cinnamon warm 39.7 36.4 35.9 31.7 30"
K19-1039,W05-0909,0,0.0496375,"otator agreement. How2 (Sanabria et al., 2018) tackles the different task of predicting video uploader-provided descriptions/captions, which are not always appropriate summarizations. 4 421 task, in particular, they found it difficult to maintain a consistent level of specificity in terms of how many factual details to include (e.g., “mix together” vs. “mix the peppers and mushrooms together.”) Results: We compute corpus-level performance statistics using four standard generation evaluation metrics: ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), BLEU-4 (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) (higher is better in all cases). Note that our evaluation is micro-averaged at the segment level, and differs slightly from prior work on this dataset, which has mostly reported metrics macro-averaged at the video level. We switched the evaluation because some metrics like BLEU-4 exhibit undesirable sparsity artifacts when macro-averaging, e.g., any video without a correct 4-gram gets a zero BLEU score, even if there are many 1/2/3-grams correct. Segment-level averaging, the standard evaluation practice in fields like machine translation, is insensitive to this sparsity concern, and (we belie"
K19-1039,P15-1162,0,0.0414694,"Missing"
K19-1039,P18-1241,0,0.0138877,"r example, when finishing a dish, a user says “that’s perfection in my book right there” rather than “put the dish on a plate and serve.” There are also cases where no ASR tokens are available at all. Despite these potential difficulties, previous work has demonstrated that ASR can be informative in a variety of instructional video understanding tasks (Naim et al., 2014, 2015; Malmaud et al., 2015; Sener et al., 2015; Alayrac et al., 2016; Huang et al., 2017); though less work has focused on instructional caption generation, which is known to be difficult and sensitive to input perturbations (Chen et al., 2018). We find that incorporating ASR-token–based features significantly improves performance over visual-features–only models (e.g., CIDEr improves 0.53 ⇒ 1.0, BLEU-4 improves 4.3 ⇒ 8.5). We also show that combining ASR tokens and visual features results in the highest performing models, suggesting that the modalities contain complementary information. We conclude by asking: what information is captured by the visual features that is not captured by the ASR tokens (and vice versa)? Auxiliary experiments examining performance of models in predicting the presence/absence of individual word types sug"
K19-1039,P19-1659,0,0.0325331,"Missing"
K19-1039,P02-1040,0,0.104255,"initive, exact measure of inter-annotator agreement. How2 (Sanabria et al., 2018) tackles the different task of predicting video uploader-provided descriptions/captions, which are not always appropriate summarizations. 4 421 task, in particular, they found it difficult to maintain a consistent level of specificity in terms of how many factual details to include (e.g., “mix together” vs. “mix the peppers and mushrooms together.”) Results: We compute corpus-level performance statistics using four standard generation evaluation metrics: ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), BLEU-4 (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) (higher is better in all cases). Note that our evaluation is micro-averaged at the segment level, and differs slightly from prior work on this dataset, which has mostly reported metrics macro-averaged at the video level. We switched the evaluation because some metrics like BLEU-4 exhibit undesirable sparsity artifacts when macro-averaging, e.g., any video without a correct 4-gram gets a zero BLEU score, even if there are many 1/2/3-grams correct. Segment-level averaging, the standard evaluation practice in fields like machine translation, is insensitive t"
K19-1039,Q13-1003,0,0.0294222,"idered, current state-of-the-art, visual-features–only models only slightly outperform a constant prediction baseline, e.g., by 1.5 BLEU/METEOR points. 2 Related Work Narrated instructional videos. While several works have matched audio and video signals in an unconstrained setting (Arandjelovic and Zisserman, 2017; Tian et al., 2018), our work builds upon previous efforts to utilize accompanying speech signals to understand online instructional videos, specifically. Several works focus on learning video-instruction alignments, and match a fixed set of instructions to temporal video segments (Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015; Hendricks et al., 2017; Kuehne et al., 2017). Another line of previous work uses speech to extract and align language fragments, e.g., verb-noun pairs, with instructional videos (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Alayrac et al., 2016; Huang et al., 2017, 2018; Hahn et al., 2018). Sener et al. (2015), as part of their parsing pipeline, train a 3-gram language model on segmented ASR token inputs to produce recipe steps. Dense Video Captioning. Recent work in computer vision addresses dense video captioning (Krishna et al., 2017; Li et al"
K19-1039,N15-1015,0,0.151945,"features–only models only slightly outperform a constant prediction baseline, e.g., by 1.5 BLEU/METEOR points. 2 Related Work Narrated instructional videos. While several works have matched audio and video signals in an unconstrained setting (Arandjelovic and Zisserman, 2017; Tian et al., 2018), our work builds upon previous efforts to utilize accompanying speech signals to understand online instructional videos, specifically. Several works focus on learning video-instruction alignments, and match a fixed set of instructions to temporal video segments (Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015; Hendricks et al., 2017; Kuehne et al., 2017). Another line of previous work uses speech to extract and align language fragments, e.g., verb-noun pairs, with instructional videos (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Alayrac et al., 2016; Huang et al., 2017, 2018; Hahn et al., 2018). Sener et al. (2015), as part of their parsing pipeline, train a 3-gram language model on segmented ASR token inputs to produce recipe steps. Dense Video Captioning. Recent work in computer vision addresses dense video captioning (Krishna et al., 2017; Li et al., 2018; Wang et al., 2018), a supervised"
K19-1039,P19-1641,0,0.555928,"Missing"
K19-1039,N15-1017,0,0.0252117,"of-the-art, visual-features–only models only slightly outperform a constant prediction baseline, e.g., by 1.5 BLEU/METEOR points. 2 Related Work Narrated instructional videos. While several works have matched audio and video signals in an unconstrained setting (Arandjelovic and Zisserman, 2017; Tian et al., 2018), our work builds upon previous efforts to utilize accompanying speech signals to understand online instructional videos, specifically. Several works focus on learning video-instruction alignments, and match a fixed set of instructions to temporal video segments (Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015; Hendricks et al., 2017; Kuehne et al., 2017). Another line of previous work uses speech to extract and align language fragments, e.g., verb-noun pairs, with instructional videos (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Alayrac et al., 2016; Huang et al., 2017, 2018; Hahn et al., 2018). Sener et al. (2015), as part of their parsing pipeline, train a 3-gram language model on segmented ASR token inputs to produce recipe steps. Dense Video Captioning. Recent work in computer vision addresses dense video captioning (Krishna et al., 2017; Li et al., 2018; Wang et al"
K19-1039,W18-1819,0,0.0554087,"Missing"
K19-1039,W04-1013,0,\N,Missing
N03-1030,kingsbury-palmer-2002-treebank,0,\N,Missing
N03-1030,J97-2002,0,\N,Missing
N03-1030,A00-2018,0,\N,Missing
N03-1030,J93-2004,0,\N,Missing
N03-1030,W01-1605,1,\N,Missing
N03-1030,J02-3001,0,\N,Missing
N03-1030,P95-1037,0,\N,Missing
N03-1030,fillmore-etal-2002-framenet,0,\N,Missing
N04-1008,J93-2003,0,\N,Missing
N04-1008,P02-1040,0,\N,Missing
N04-1008,P03-1003,0,\N,Missing
N04-1008,W03-1210,0,\N,Missing
N04-1008,N03-1020,0,\N,Missing
N04-1008,P03-1021,0,\N,Missing
N15-1186,N06-2001,0,0.0784517,"uang et al., 2012; Le and Mikolov, 2014). The main advantage offered by these techniques is that they can be both trained in an unsupervised manner, and also tuned using supervised labels. However, most of these approaches treat words as units, and fail to account for phenomena involving the relationship between various morphological forms that affect word semantics, especially for rare or unseen words. Previous attempts at dealing with sub-word units and their compositionality have looked at explicitlyengineered features such as stems, cases, POS, etc., and used models such as factored NLMs (Alexandrescu and Kirchhoff, 2006) to obtain representations for unseen words, or compositional distributional semantic models (Lazaridou et al., 2013) to derive representations for morphologically-inflected words, based on the composing morphemes. A more recent trend has seen proposals that deal with mor1628 phology using vector-space representations (Luong et al., 2013; Botha and Blunsom, 2014). Given word morphemes (affixes, roots), a neural-network architecture (recursive neural networks in the work of Luong et al (2013), log-bilinear models in the case of Botha and Blunsom (2014)), is used to obtain embedding representati"
N15-1186,P14-2131,0,0.0247038,"factored NLMs (Alexandrescu and Kirchhoff, 2006) to obtain representations for unseen words, or compositional distributional semantic models (Lazaridou et al., 2013) to derive representations for morphologically-inflected words, based on the composing morphemes. A more recent trend has seen proposals that deal with mor1628 phology using vector-space representations (Luong et al., 2013; Botha and Blunsom, 2014). Given word morphemes (affixes, roots), a neural-network architecture (recursive neural networks in the work of Luong et al (2013), log-bilinear models in the case of Botha and Blunsom (2014)), is used to obtain embedding representations for existing morphemes, and also to combine them into (possibly novel) embedding representations for words that may not have been seen at training time. Common to these proposals is the fact that the morphological analysis of words is treated as an external, preprocessing-style step. This step is done using off-the-shelf analyzers such as Morfessor (Creutz and Lagus, 2007). As a result, the morphological analysis happens within a different model compared to the model in which the resulting morphemes are consequently used. In contrast, the work pre"
N15-1186,D09-1124,0,0.0622909,"ublicly-available word-similarity datasets. Most relevant for our approach is the Stanford English Rare-Word (RW) dataset (Luong et al., 2013), consisting of 2034 word pairs with a higher degree of English morphology compared to other word-similarity datasets. We also use for English the WS353 (Finkelstein et al., 2002) and RG65 datasets (Rubenstein and Goodenough, 1965). For German, we use the Gur350 and ZG222 datasets (Zesch and Gurevych, 2006). For French we use the RG65 French version (Joubarne and Inkpen, 2011); for Spanish, Romanian, and Arabic we use their respective versions of WS353 (Hassan and Mihalcea, 2009). Results We present in Table 4 the results obtained across 6 language pairs and 9 datasets, using a count threshold for SG+Morph of C = 100. We also include the results obtained by two previouslyproposed methods, LSM2013 (Luong et al., 2013) and BB2014 (Botha and Blunsom, 2014), which share some of the characteristics of our method. Even in the absence of any morphological treatment, our word representations are better than previously used ones. For instance, LSM2013 uses exactly the same EN Wikipedia (Shaoul and Westbury, 2010) training data, and achieves 26.8 and 34.4 Spearman ρ correlation"
N15-1186,P12-1092,0,0.0496334,", French, Spanish, Romanian, Arabic, and Uzbek. The results indicate that the induced morphological analysis deals successfully with sophisticated morphological variations. 2 Previous Work Many recent proposals in the literature use wordrepresentations as the basic units for tackling sentence-level tasks such as language modeling (Mnih and Hinton, 2007; Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), discriminative parsing (Collobert, 2011), as well as similar tasks involving larger units such as documents (Glorot et al., 2011; Huang et al., 2012; Le and Mikolov, 2014). The main advantage offered by these techniques is that they can be both trained in an unsupervised manner, and also tuned using supervised labels. However, most of these approaches treat words as units, and fail to account for phenomena involving the relationship between various morphological forms that affect word semantics, especially for rare or unseen words. Previous attempts at dealing with sub-word units and their compositionality have looked at explicitlyengineered features such as stems, cases, POS, etc., and used models such as factored NLMs (Alexandrescu and"
N15-1186,J14-1004,0,0.0212892,"method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages. 1 Introduction Word representations obtained via neural networks (Bengio et al., 2003; Socher et al., 2011a) or specialized models (Mikolov et al., 2013a) have been used to address various natural language processing tasks (Mnih et al., 2009; Huang et al., 2014; Bansal et al., 2014). These vector representations capture various syntactic and semantic properties of natural language (Mikolov et al., 2013b). In many instances, natural language uses a small set of concepts to render a much larger set of meaning variations via morphology. We show in this paper that morphological transformations can be captured by exploiting regularities present in wordrepresentations as the ones trained using the SkipGram model (Mikolov et al., 2013a). In contrast to previous approaches that combine morphology with vector-based word representations (Luong et al., 2013; B"
N15-1186,P13-1149,0,0.141675,"unsupervised manner, and also tuned using supervised labels. However, most of these approaches treat words as units, and fail to account for phenomena involving the relationship between various morphological forms that affect word semantics, especially for rare or unseen words. Previous attempts at dealing with sub-word units and their compositionality have looked at explicitlyengineered features such as stems, cases, POS, etc., and used models such as factored NLMs (Alexandrescu and Kirchhoff, 2006) to obtain representations for unseen words, or compositional distributional semantic models (Lazaridou et al., 2013) to derive representations for morphologically-inflected words, based on the composing morphemes. A more recent trend has seen proposals that deal with mor1628 phology using vector-space representations (Luong et al., 2013; Botha and Blunsom, 2014). Given word morphemes (affixes, roots), a neural-network architecture (recursive neural networks in the work of Luong et al (2013), log-bilinear models in the case of Botha and Blunsom (2014)), is used to obtain embedding representations for existing morphemes, and also to combine them into (possibly novel) embedding representations for words that m"
N15-1186,P14-2050,0,0.0409262,"rast to previous approaches that combine morphology with vector-based word representations (Luong et al., 2013; Botha and Blunsom, 2014), we do not rely on an external morphological analyzer, such as Morfessor (Creutz and La∗ Work done at Google, now at Human Longevity Inc. gus, 2007). Instead, our method automatically induces morphological rules and transformations, represented as vectors in the same embedding space. At the heart of our method is the SkipGram model described in (Mikolov et al., 2013a). We further exploit the observations made by Mikolov et al (2013b), and further studied by (Levy and Goldberg, 2014; Pennington et al., 2014), regarding the regularities exhibited by such embedding spaces. These regularities have been shown to allow inferences of certain types (e.g., king is to man what queen is to woman). Such regularities also hold for certain morphological relations (e.g., car is to cars what dog is to dogs). In this paper, we show that one can exploit these regularities to model, in a principled way, prefix- and suffix-based morphology. The main contributions of this paper are as follows: 1. provides a method by which morphological rules are learned in an unsupervised, languageagnostic"
N15-1186,W13-3512,0,0.46238,"Missing"
N15-1186,N13-1090,0,0.317691,"present a language agnostic, unsupervised method for inducing morphological transformations between words. The method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages. 1 Introduction Word representations obtained via neural networks (Bengio et al., 2003; Socher et al., 2011a) or specialized models (Mikolov et al., 2013a) have been used to address various natural language processing tasks (Mnih et al., 2009; Huang et al., 2014; Bansal et al., 2014). These vector representations capture various syntactic and semantic properties of natural language (Mikolov et al., 2013b). In many instances, natural language uses a small set of concepts to render a much larger set of meaning variations via morphology. We show in this paper that morphological transformations can be captured by exploiting regularities present in wordrepresentations as the ones trained using the SkipGram model (Mikolov et al., 2013a). In contrast"
N15-1186,D14-1162,0,0.116762,"Missing"
N15-1186,D11-1014,0,0.121673,"Missing"
N15-1186,W06-1104,0,0.0778886,"illustrate the richness of the morphological phenomena present in languages such as German, Romanian, Arabic, and Uzbek, compared to English. As test sets, we use standard, publicly-available word-similarity datasets. Most relevant for our approach is the Stanford English Rare-Word (RW) dataset (Luong et al., 2013), consisting of 2034 word pairs with a higher degree of English morphology compared to other word-similarity datasets. We also use for English the WS353 (Finkelstein et al., 2002) and RG65 datasets (Rubenstein and Goodenough, 1965). For German, we use the Gur350 and ZG222 datasets (Zesch and Gurevych, 2006). For French we use the RG65 French version (Joubarne and Inkpen, 2011); for Spanish, Romanian, and Arabic we use their respective versions of WS353 (Hassan and Mihalcea, 2009). Results We present in Table 4 the results obtained across 6 language pairs and 9 datasets, using a count threshold for SG+Morph of C = 100. We also include the results obtained by two previouslyproposed methods, LSM2013 (Luong et al., 2013) and BB2014 (Botha and Blunsom, 2014), which share some of the characteristics of our method. Even in the absence of any morphological treatment, our word representations are better"
N15-1186,W13-2201,1,\N,Missing
N18-1138,Q16-1031,0,0.0292844,"le embedding A competitive approach to modeling different styles is to directly encode the style information into the embedding space. In (Johnson et al., 2016), the style label is converted into a one-hot vector and is concatenated with the word embedding at each time step in the S model. The outputs of this model are at 36.68 ROUGE-L, slightly higher than the baseline S model, but significantly lower than the SP model performance (37.52 ROUGE-L). Another style embedding approach is to augment the S model with continuous trainable style embeddings for each predefined style label, similar to (Ammar et al., 2016). The resulting outputs achieve 37.2 ROUGE-L, which is better than the S model with one-hot style embedding, but still worse than the SP method (statistically significant at p-value=0.025 using paired t-test). However, neither of these approaches apply to the cases when the style is out-of-domain or unknown during testing. In contrast, such cases are handled naturally by the proposed M-SP model. Ensemble model Another question is whether the SP model simply benefits from ensembling Style vs. Content Previous experiments indicate that the SP and M-SP models have superior generation accuracy, bu"
N18-1138,S17-1008,0,0.121671,"learned styles to perform on-thefly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities. 1 Introduction Encoder-decoder models have recently pushed forward the state-of-the-art performance on a variety of language generation tasks, including machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), text summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), dialog systems (Li et al., 2016; Asghar et al., 2017), and image captioning (Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017). This framework consists of an encoder that reads the input data and encodes it as a sequence of vectors, which is in turn used by a decoder to generate an∗ Work done as an intern at Google AI. other sequence of vectors used to produce output symbols step by step. The prevalent approach to training such a model is to update all the model parameters using all the examples in the training data (over multiple epochs). This is a reasonable approach, under the assumption that we are modeling a single underlying distrib"
N18-1138,N16-1012,0,0.0139687,"nput style to existing styles learned at training time, and use the correlations learned at training time between input and output style characteristics to generate style-appropriate token sequences. 2 Related Work Encoder-Decoder Models for Structured Output Prediction Encoder-decoder architectures have been successfully applied to a variety of structure prediction tasks recently. Tasks for which such architectures have achieved state-of-the-art results include machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), automatic text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Paulus et al., 2017; Nema et al., 2017), sentence simplification (Filippova et al., 2015; Zhang and Lapata, 2017), dialog systems (Li et al., 2016, 2017; Asghar et al., 2017), image captioning (Vinyals et al., 2015; Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017), etc. By far the most used implementation of such architectures is based on the original sequenceto-sequence model (Sutskever et al., 2014), augmented with its attention-based extension (Bahdanau et al., 2015). Although our SHAPED and Mix-SHAPED model formulations do not depend on a particular archit"
N18-1138,W17-4513,0,0.0418863,"to domain adaptation and sequence tagging. They use a shared encoder to represent instances from all of the domains, and use a domain projection layer to project the shared layer into a domain-specific space. They only consider the supervised domain-adaptation case, in which labeled training data exists for the target domain. Glorot et al. (2011) use auto-encoders for learning a high-level feature extraction across domains for sentiment analysis, while Zhou et al. (2016) employ auto-encoders to directly transfer the examples across different domains also for the same sentiment analysis task. Hua and Wang (2017) perform an experimental analysis on domain adaptation for neural abstractive summarization. An important requirement of all the methods in the related work described above is that they require access to the (unlabeled) target domain data, in order to learn a domain-invariant representation across source and target domains. In contrast, our Mix-SHAPED model does not need access to a target domain or style at training time, and instead performs the adaptation on-the-fly, according to the specifics of the input data and the correlations learned at training time between available input and output"
N18-1138,Q17-1024,0,0.0781639,"Missing"
N18-1138,C16-1038,0,0.0203553,"sequence model (Sutskever et al., 2014), augmented with its attention-based extension (Bahdanau et al., 2015). Although our SHAPED and Mix-SHAPED model formulations do not depend on a particular architecture implementation, we do make use of the (Bahdanau et al., 2015) model to instantiate our models. Domain Adaptation for Neural Network Models One general approach to domain adaptation for natural language tasks is to perform data/feature augmentation that represents inputs as both general and domain-dependent data, as originally proposed in (Daum´e III, 2009), and ported to neural models in (Kim et al., 2016). For computer vision tasks, a line of work related to our approach has been proposed by Bousmalis et al. (2016) using what they call domain separation networks. As a tool for studying unsupervised domain adaptation 1529 for image recognition tasks, their proposal uses CNNs for encoding an image into a feature representation, and also for reconstructing the input sample. It also makes use of a private encoder for each domain, and a shared encoder for both the source and the target domain. The approach we take in this paper shares this idea of model parametrization according to the domain/style"
N18-1138,D16-1127,0,0.128233,"butions from all learned styles to perform on-thefly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities. 1 Introduction Encoder-decoder models have recently pushed forward the state-of-the-art performance on a variety of language generation tasks, including machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), text summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), dialog systems (Li et al., 2016; Asghar et al., 2017), and image captioning (Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017). This framework consists of an encoder that reads the input data and encodes it as a sequence of vectors, which is in turn used by a decoder to generate an∗ Work done as an intern at Google AI. other sequence of vectors used to produce output symbols step by step. The prevalent approach to training such a model is to update all the model parameters using all the examples in the training data (over multiple epochs). This is a reasonable approach, under the assumption that we are modeling a sin"
N18-1138,D17-1230,0,0.0617493,"Missing"
N18-1138,K16-1028,0,0.314308,"describe an extension that uses a mixture of output distributions from all learned styles to perform on-thefly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities. 1 Introduction Encoder-decoder models have recently pushed forward the state-of-the-art performance on a variety of language generation tasks, including machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), text summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), dialog systems (Li et al., 2016; Asghar et al., 2017), and image captioning (Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017). This framework consists of an encoder that reads the input data and encodes it as a sequence of vectors, which is in turn used by a decoder to generate an∗ Work done as an intern at Google AI. other sequence of vectors used to produce output symbols step by step. The prevalent approach to training such a model is to update all the model parameters using all the examples in the training data (over multiple epochs). This is a reasonable appro"
N18-1138,D15-1042,0,0.0499437,"Missing"
N18-1138,W12-3018,0,0.0395481,"Missing"
N18-1138,P17-1098,0,0.0139699,"e correlations learned at training time between input and output style characteristics to generate style-appropriate token sequences. 2 Related Work Encoder-Decoder Models for Structured Output Prediction Encoder-decoder architectures have been successfully applied to a variety of structure prediction tasks recently. Tasks for which such architectures have achieved state-of-the-art results include machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), automatic text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Paulus et al., 2017; Nema et al., 2017), sentence simplification (Filippova et al., 2015; Zhang and Lapata, 2017), dialog systems (Li et al., 2016, 2017; Asghar et al., 2017), image captioning (Vinyals et al., 2015; Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017), etc. By far the most used implementation of such architectures is based on the original sequenceto-sequence model (Sutskever et al., 2014), augmented with its attention-based extension (Bahdanau et al., 2015). Although our SHAPED and Mix-SHAPED model formulations do not depend on a particular architecture implementation, we do make use of the (Bahdanau et al., 20"
N18-1138,W17-2612,0,0.030111,"g an image into a feature representation, and also for reconstructing the input sample. It also makes use of a private encoder for each domain, and a shared encoder for both the source and the target domain. The approach we take in this paper shares this idea of model parametrization according to the domain/style, but goes further with the Mix-SHAPED model, performing on-the-fly adaptation of the model outputs. Other CNN-based domain adaptation methods for object recognition tasks are presented in (Long et al., 2016; Chopra et al., 2013; Tzeng et al., 2015; Sener et al., 2016). For NLP tasks, Peng and Dredze (2017) take a multi-task approach to domain adaptation and sequence tagging. They use a shared encoder to represent instances from all of the domains, and use a domain projection layer to project the shared layer into a domain-specific space. They only consider the supervised domain-adaptation case, in which labeled training data exists for the target domain. Glorot et al. (2011) use auto-encoders for learning a high-level feature extraction across domains for sentiment analysis, while Zhou et al. (2016) employ auto-encoders to directly transfer the examples across different domains also for the sam"
N18-1138,D15-1044,0,0.618143,"na. Furthermore, we describe an extension that uses a mixture of output distributions from all learned styles to perform on-thefly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities. 1 Introduction Encoder-decoder models have recently pushed forward the state-of-the-art performance on a variety of language generation tasks, including machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), text summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), dialog systems (Li et al., 2016; Asghar et al., 2017), and image captioning (Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017). This framework consists of an encoder that reads the input data and encodes it as a sequence of vectors, which is in turn used by a decoder to generate an∗ Work done as an intern at Google AI. other sequence of vectors used to produce output symbols step by step. The prevalent approach to training such a model is to update all the model parameters using all the examples in the training data (over multiple epochs). Th"
N18-1138,D17-1062,0,0.0306487,"e characteristics to generate style-appropriate token sequences. 2 Related Work Encoder-Decoder Models for Structured Output Prediction Encoder-decoder architectures have been successfully applied to a variety of structure prediction tasks recently. Tasks for which such architectures have achieved state-of-the-art results include machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), automatic text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Paulus et al., 2017; Nema et al., 2017), sentence simplification (Filippova et al., 2015; Zhang and Lapata, 2017), dialog systems (Li et al., 2016, 2017; Asghar et al., 2017), image captioning (Vinyals et al., 2015; Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017), etc. By far the most used implementation of such architectures is based on the original sequenceto-sequence model (Sutskever et al., 2014), augmented with its attention-based extension (Bahdanau et al., 2015). Although our SHAPED and Mix-SHAPED model formulations do not depend on a particular architecture implementation, we do make use of the (Bahdanau et al., 2015) model to instantiate our models. Domain Adaptation for Neural Network"
N18-1138,P16-1031,0,0.0299596,"g et al., 2016; Chopra et al., 2013; Tzeng et al., 2015; Sener et al., 2016). For NLP tasks, Peng and Dredze (2017) take a multi-task approach to domain adaptation and sequence tagging. They use a shared encoder to represent instances from all of the domains, and use a domain projection layer to project the shared layer into a domain-specific space. They only consider the supervised domain-adaptation case, in which labeled training data exists for the target domain. Glorot et al. (2011) use auto-encoders for learning a high-level feature extraction across domains for sentiment analysis, while Zhou et al. (2016) employ auto-encoders to directly transfer the examples across different domains also for the same sentiment analysis task. Hua and Wang (2017) perform an experimental analysis on domain adaptation for neural abstractive summarization. An important requirement of all the methods in the related work described above is that they require access to the (unlabeled) target domain data, in order to learn a domain-invariant representation across source and target domains. In contrast, our Mix-SHAPED model does not need access to a target domain or style at training time, and instead performs the adapt"
N18-1138,P17-1099,0,0.0301631,"hat uses a mixture of output distributions from all learned styles to perform on-thefly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities. 1 Introduction Encoder-decoder models have recently pushed forward the state-of-the-art performance on a variety of language generation tasks, including machine translation (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017), text summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), dialog systems (Li et al., 2016; Asghar et al., 2017), and image captioning (Xu et al., 2015; Ranzato et al., 2015; Liu et al., 2017). This framework consists of an encoder that reads the input data and encodes it as a sequence of vectors, which is in turn used by a decoder to generate an∗ Work done as an intern at Google AI. other sequence of vectors used to produce output symbols step by step. The prevalent approach to training such a model is to update all the model parameters using all the examples in the training data (over multiple epochs). This is a reasonable approach, under the assu"
N18-1138,1983.tc-1.13,0,0.249252,"Missing"
P04-1078,N03-1020,0,0.714545,"duction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by BLEU but twisted towards the specifics of the summarization task. An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations. Human evaluations, however, are subject to specific guidelines given to the human assessors when performing the evaluation task; the variation in human judgment is therefore highly influenced by these guidelines. It follows that, in order for an automatic evaluation to agree with a humanperformed evaluation, the evaluation metric used by th"
P04-1078,P02-1040,0,0.0927742,"ce statistics. The automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization are particular instances from the family of metrics we propose. We show that different members of the same family of metrics explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Automatic Question Answering) and the evaluation guidelines used by humans for evaluating such applications. 1 Introduction With the introduction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by B"
P04-1078,P03-1021,0,0.0123013,"swering) and the evaluation guidelines used by humans for evaluating such applications. 1 Introduction With the introduction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by BLEU but twisted towards the specifics of the summarization task. An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations. Human evaluations, however, are subject to specific guidelines given to the human assessors when performing the evaluation task; the variation in human judgment is therefore highly influenced by these guidelines. It"
P04-1078,N04-1008,1,0.718984,"to be handled to so-called factoid questions. Automatic evaluation of factoid QA is often straightforward, as the number of correct answers is most of the time limited, and exhaustive lists of correct answers are available. When removing the factoid constraint, however, the set of possible answer to a (complex, beyondfactoid) question becomes unfeasibly large, and consequently automatic evaluation becomes a challenge. In this section, we focus on an evaluation carried out in order to assess the performance of a QA system for answering questions from the Frequently-Asked-Question (FAQ) domain (Soricut and Brill, 2004). These are generally questions requiring a more elaborated answer than a simple factoid (e.g., questions such as: “How does a film qualify for an Academy Award?”). In order to evaluate such a system a humanperformed evaluation was performed, in which 11 versions of the QA system (various modules were implemented using various algorithms) were separately evaluated. Each version was evaluated by a human evaluator, with no reference answer available. For this evaluation 115 test questions were used, and the human evaluator was asked to assess whether the proposed answer was correct, somehow rela"
P05-1009,J99-4005,0,\N,Missing
P05-1009,J93-2003,0,\N,Missing
P05-1009,P02-1040,0,\N,Missing
P05-1009,P95-1034,0,\N,Missing
P05-1009,J98-4003,0,\N,Missing
P05-1009,W00-2004,0,\N,Missing
P05-1009,W02-2105,0,\N,Missing
P06-1139,W00-2004,0,0.02382,"resent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep,"
P06-1139,W02-2105,0,0.0549781,"e realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGEN"
P06-1139,W03-0501,0,0.155099,"the WIDL-expressions with -gram language model distributions. The output presented in Figure 4 is the most likely headline realization produced by our system. &gt; &gt;  4 Headline Generation using WIDL-expressions We employ the WIDL formalism (Section 2) and the WIDL-NGLM-A V algorithm (Section 3) in a summarization application that aims at producing both informative and fluent headlines. Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases. A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al., 2003) and annotated with additional information (Zajic et al., 2004). Automatic Creation of WIDL-expressions for Headline Generation. We generate WIDLexpressions starting from an input document. First, we extract a weighted list of topic keywords from the input document using the algorithm of Zhou and Hovy (2003). This list is enriched with phrases created from the lexical dependencies the topic keywords have in the input document. We associate probability distributions with these phrases using their frequency (we assume QÃ ê  Ã &  M? 3 · Q/ Headline Generation Evaluation. To evaluate the ac"
P06-1139,P95-1034,0,0.0402743,"rocessing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and sema"
P06-1139,N03-1017,1,0.0296461,"Missing"
P06-1139,koen-2004-pharaoh,0,0.0190861,"= corresponding to a Chinese span = , we use a probabilistic beam Z and a histogram beam : to beam out low probability translation alternatives. At this point, each = span is “tiled” with likely transla= tions = taken from the translation table. Tiles that are adjacent are joined together in  by ] &gt; a a larger &gt;[Rtile operator, where  . That is, reordering of opthe component tiles are permitted by the erators (assigned non-zero probability), but the longer the movement from the original order of the tiles, the lower the probability. (This distortion model is similar with the one used in (Koehn, 2004).) When multiple tiles are available for the , they are joined by a operasame span tor, where is specified by the probability distributions specified in the translation table. Usually, statistical phrase-based translation tables specify not only one, but multiple distributions that account for context preferences. In our experiments, we consider four probability distributions: &apos; (&apos; (&apos;Ya &apos;ca ^`_ B B_ ^ .b ^`_ B , and . b B3_ ^ , where ^ and B are Chinese-English phrase translations as they appear in the translation table. In Figure 6, we show an example of WIDL-expression created by this algo"
P06-1139,W04-1013,0,0.00676458,"2.9 SRI LANKA ’S JOINT VENTURE TO EXPAND EXPORTS OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO OF INDIA AND BANGLADESH WATER BARRAGE Abstractive Keywords Webcl WIDL-A  Figure 5: Headlines generated automatically using a WIDL-based sentence realization system. Table 1: Headline generation evaluation. We compare extractive algorithms against abstractive algorithms, including our WIDL-based algorithm. and the other half is used as test set (273 documents). We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGE (Lin, 2004). For each input document, we train two language models, using the SRI Language Model Toolkit (with modified Kneser-Ney smoothing). A general trigram language model, trained on 170M English words from the Wall Street Journal, is used to model fluency. A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluency and content validity. We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to p"
P06-1139,P03-1021,0,0.00448332,"on 170M English words from the Wall Street Journal, is used to model fluency. A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluency and content validity. We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to produce headlines that have restrictions in the number of words allowed (10, in our case). The interpolation weights A (Equation 2) are trained using discriminative training (Och, 2003) using ROUGE as the objective function, on the development set. The results are presented in Table 1. We compare the performance of several extractive algorithms (which operate on an extracted sentence to arrive at a headline) against several abstractive algorithms (which create headlines starting from scratch). For the extractive algorithms, Lead10 is a baseline which simply proposes as headline the lead sentence, cut after the first 10 words.  HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al., 2003), and Topiary  is our implementation of the Topiary system (Zajic e"
P06-1139,P02-1040,0,0.0713665,"Missing"
P06-1139,P05-1009,1,0.934841,"n be written under a log-linear future (admissible) scores for the bIceGflgkh states. model as in Equation 1: The algorithm PUSH es each state from the current bdc5eGfNgih into a priority queue m , which sorts EGFIH J? A J &gt; J DB C DB (1) the states according to their total score (current n ?J G E I F H A J &gt; J DB .LK admissible). In the next iteration, XZY5[^]o_la is a singleton set containing the state POP ed out from the We can formulate the search problem of finding top of m . The admissible heuristic function we use the most probable realization B under this model is the one defined in (Soricut and Marcu, 2005), as shown in Equation 2, and therefore we do not using Equation 1 (unnormalized) for computing need to be concerned about computing expensive the event costs. Given the existence of the adnormalization factors. missible heuristic and the monotonicity property ? J NMPORQ . F C DB NMPOSQ . F EGFIH A J &gt; J DB (2) of the unfolding provided by the priority queue m , the proof for A V optimality (Russell and Norvig, = For a given WIDL-expression over , the set 1995) guarantees that WIDL-NGLM-A V finds a is defined by  , and feature function path in 9qp that provides an optimal solution.  C  ¶"
P06-1139,2003.mtsummit-papers.20,0,0.0236122,"onsiderable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, etc., for Penman and FUF), syntactic relations (such as subject, object, premod, etc., for HALogen), or lexical dependencies (Fergus, Amalgam). Such inputs cannot be accurately produced by state-of-the-art analysis components from arbitrary textual input in the context of text-to-text applications. Second, most of the recent"
P06-1139,P01-1030,1,\N,Missing
P06-2103,W02-2111,0,0.0771219,"Missing"
P06-2103,J04-4001,0,0.0189338,"anner that takes advantage of their individual strengths. An integral part of this framework are algorithms for searching and training these stochastic coherence models. We evaluate the performance of our models and algorithms and show empirically that utilitytrained log-linear coherence models outperform each of the individual coherence models considered. 1 Introduction Various theories of discourse coherence (Mann and Thompson, 1988; Grosz et al., 1995) have been applied successfully in discourse analysis (Marcu, 2000; Forbes et al., 2001) and discourse generation (Scott and de Souza, 1990; Kibble and Power, 2004). Most of these efforts, however, have limited applicability. Those that use manually written rules model only the most visible discourse constraints (e.g., the discourse connective “although” marks a CONCESSION relation), while being oblivious to fine-grained lexical indicators. And the methods that utilize manually annotated corpora (Carlson et al., 2003; Karamanis et al., 2004) and supervised learning algorithms have high costs associated with the annotation procedure, and cannot be easily adapted to different domains and genres. In contrast, more recent research has focused on stochastic a"
P06-2103,P03-1069,0,0.429431,"ntion, using large collections of existing human-authored documents. These models are attractive due to their increased scalability and portability. As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators. A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005)? Because the problem is NP-complete (Althaus et al., 2005), it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another. If the search is exhaustive, the ordering procedure may take too long to be useful. In this paper, we propose an A search algorithm for the discourse ordering problem that comes with strong theoretical guarantees. For a wide range of practical problems (discourse order"
P06-2103,W98-1411,0,0.131518,"Missing"
P06-2103,P05-1018,0,0.268516,"ting human-authored documents. These models are attractive due to their increased scalability and portability. As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators. A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005)? Because the problem is NP-complete (Althaus et al., 2005), it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another. If the search is exhaustive, the ordering procedure may take too long to be useful. In this paper, we propose an A search algorithm for the discourse ordering problem that comes with strong theoretical guarantees. For a wide range of practical problems (discourse ordering of up to 15 units), the algorithm finds an optim"
P06-2103,J91-1002,0,0.0331286,"rties, similar to those stipulated by Centering Theory (Grosz et al., 1995). Their model learns distribution patterns for transitions between discourse entities that are abstracted into their syntactic roles – subject (S), object (O), other (X), missing (-). The feature values are computed using an entity-grid representation for the discourse that records the syntactic role of each entity as it appears in each sentence. Also, salient entities are differentiated from casually occurring entities, based on the widely used assumption that occurrence frequency correlates with discourse prominence (Morris and Hirst, 1991; Grosz et al., 1995). We exclude the coreference information from this model, as the discourse ordering problem cannot accommodate current coreference solutions, which assume a pre-specified order (Ng, 2005). In the jargon of (Barzilay and Lapata, 2005), the model we implemented is called Syntax+Salience. The probability assigned to a text AAB""   . by this Entity-Based model (henceforth called EB) can be locally computed (i.e., at sentence transition level) using C feature functions, as follows: 0 ED    "" = #$  37F $ HG 7 I 7  #;   #  0  Here, I 7  #;   #  are"
P06-2103,N04-1015,0,0.364034,"arge collections of existing human-authored documents. These models are attractive due to their increased scalability and portability. As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators. A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005)? Because the problem is NP-complete (Althaus et al., 2005), it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another. If the search is exhaustive, the ordering procedure may take too long to be useful. In this paper, we propose an A search algorithm for the discourse ordering problem that comes with strong theoretical guarantees. For a wide range of practical problems (discourse ordering of up to 15 units),"
P06-2103,P05-1020,0,0.0314204,", object (O), other (X), missing (-). The feature values are computed using an entity-grid representation for the discourse that records the syntactic role of each entity as it appears in each sentence. Also, salient entities are differentiated from casually occurring entities, based on the widely used assumption that occurrence frequency correlates with discourse prominence (Morris and Hirst, 1991; Grosz et al., 1995). We exclude the coreference information from this model, as the discourse ordering problem cannot accommodate current coreference solutions, which assume a pre-specified order (Ng, 2005). In the jargon of (Barzilay and Lapata, 2005), the model we implemented is called Syntax+Salience. The probability assigned to a text AAB""   . by this Entity-Based model (henceforth called EB) can be locally computed (i.e., at sentence transition level) using C feature functions, as follows: 0 ED    "" = #$  37F $ HG 7 I 7  #;   #  0  Here, I 7  #;   #  are feature values, and G 7 are Local Models of Discourse Coherence Stochastic local models of coherence work under the assumption that well-formed discourse can be characterized in terms of specific distribution"
P06-2103,P03-1021,0,0.00442779,"to the reference order). The range of BLEU scores is between 0 (the worst) and 1 (the best). We run different discriminative training sessions using TAU and BLEU, and train two different sets of the + parameters for Equation 1. The log-linear models thus obtained are called Loglinear   and Log-linear    , respectively. Utility-based Training In addition to the modeling problem, we must also address the training problem, which amounts to finding appropriate values for the   parameters from Equation 1. The solution we employ here is the discriminative training procedure of Och (2003). This procedure learns an optimal setting of the A parameters using as optimality criterion the utility of the proposed solution. There are two necessary ingredients to implement Och’s (2003) training procedure. First, it needs a search algorithm that is able to produce ranked  -best lists of the most promising candidates in a reasonably fast manner (Huang  -best and Chiang, 2005). We accommodate  8H8 algorithm, computation within the IDL-CH-HB which decodes bag-of-units IDL-expressions at an average speed of 75.4 sec./exp. on a 3.0 GHz CPU Linux machine, for an average input of 11.5 uni"
P06-2103,J93-2003,0,0.00424429,"cal models of coherence work under the assumption that well-formed discourse can be characterized in terms of specific distributions of local recurring patterns. These distributions can be defined at the lexical level or entity-based levels. Word-Coocurrence Coherence Models. We propose a new coherence model, inspired by (Knight, 2003), that models the intuition that the usage of certain words in a discourse unit (sentence) tends to trigger the usage of other words in subsequent discourse units. (A similar intuition holds for the Machine Translation models generically known as the IBM models (Brown et al., 1993), which assume that certain words in a source language sentence tend to trigger the usage of certain words in a target language translation of that sentence.) We train models able to recognize local recurring patterns of word usage across sentences in an unsupervised manner, by running an ExpectationMaximization (EM) procedure over pairs of consecutive sentences extracted from a large collection of training documents1 . We expect EM to detect and assign higher probabilities to recurring word patterns compared to casually occurring word patterns. A local coherence model based on IBM Model 1 ass"
P06-2103,P02-1040,0,0.0887367,"of our algorithms and models in a well-controlled setting. As described in Section 3, our framework can be used in applications such as multi-document summarization. In fact, Barzilay et al. (2002) formulate the multi-document summarization problem as an information ordering problem, and show that naive ordering algorithms such as majority ordering (select most frequent orders across input documents) and chronological ordering (order facts according to publication date) do not always yield coherent summaries. BLEU. One of the most successful metrics for judging machine-generated text is BLEU (Papineni et al., 2002). It counts the number of unigram, bigram, trigram, and four-gram matches between hypothesis and reference, and combines them using geometric mean. For the discourse ordering problem, we represent hypotheses and references by index sequences (e.g., “4 2 3 1” is a hypothesis order over four discourse units, in which the first and last units have been swapped with reData. For training and testing, we use documents from two different genres: newspaper articles and accident reports written by government officials (Barzilay and Lapata, 2005). The first collection (henceforth called EARTHQUAKES) con"
P06-2103,J95-2003,0,0.968336,"benefit from their complementary strengths. The model comWe describe a generic framework for integrating various stochastic models of discourse coherence in a manner that takes advantage of their individual strengths. An integral part of this framework are algorithms for searching and training these stochastic coherence models. We evaluate the performance of our models and algorithms and show empirically that utilitytrained log-linear coherence models outperform each of the individual coherence models considered. 1 Introduction Various theories of discourse coherence (Mann and Thompson, 1988; Grosz et al., 1995) have been applied successfully in discourse analysis (Marcu, 2000; Forbes et al., 2001) and discourse generation (Scott and de Souza, 1990; Kibble and Power, 2004). Most of these efforts, however, have limited applicability. Those that use manually written rules model only the most visible discourse constraints (e.g., the discourse connective “although” marks a CONCESSION relation), while being oblivious to fine-grained lexical indicators. And the methods that utilize manually annotated corpora (Carlson et al., 2003; Karamanis et al., 2004) and supervised learning algorithms have high costs a"
P06-2103,P05-1009,1,0.821264,"computed 1 as current admissible) to control the unfolding of an input IDL-graph, by processing, at each unfolding step, the most inexpensive state (extracted from the top of G ). The admissibility of the future costs and the monotonicity property enforced by the priority queue guarantees that IDL-CH-A finds an optimal solution to Equation 1 (Russell and Norvig, 1995). The IDL-CH-HB  algorithm uses a histogram beam  to control the unfolding of an input IDLgraph, by processing, at each unfolding step, the Search Algorithms Algorithms that operate on IDL-graphs have been recently proposed by Soricut and Marcu (2005). We extend these algorithms to take as input IDLgraphs over non-atomic symbols (such that the coherence models can operate inside terms like C E D , and F from Figure 1), and also to work under models with hidden variables such as CM (Section 2.2). These algorithm, called IDL-CH-A (A search for IDL-expressions under Coherence models) and IDL-CH-HB  (Histogram-Based beam search for IDL-expressions under Coherence models, with histogram beam  ), assume an alphabet 3 of nonatomic (visible) variables (over which the input IDL-expressions are defined), and an alphabet  of hidden variables. The"
P06-2103,W05-1506,0,\N,Missing
P06-2103,W01-1605,1,\N,Missing
P06-2103,P04-1050,0,\N,Missing
P06-2103,P04-1051,0,\N,Missing
P10-1063,2004.tmi-1.8,0,\N,Missing
P10-1063,N09-2055,0,\N,Missing
P10-1063,D08-1093,1,\N,Missing
P10-1063,P02-1040,0,\N,Missing
P10-1063,W09-0429,0,\N,Missing
P10-1063,N09-1028,0,\N,Missing
P10-1063,W06-1610,0,\N,Missing
P10-1063,2005.eamt-1.15,0,\N,Missing
P10-1063,J04-4002,0,\N,Missing
P10-1063,2005.eamt-1.35,0,\N,Missing
P10-1063,N06-1058,0,\N,Missing
P10-1063,P09-1035,0,\N,Missing
P10-1063,W07-0734,0,\N,Missing
P10-1063,C04-1046,0,\N,Missing
P10-1063,P07-1038,0,\N,Missing
P10-1063,P05-1066,0,\N,Missing
P10-1063,W08-0330,0,\N,Missing
P10-1063,W08-0331,0,\N,Missing
P10-1063,W05-0904,0,\N,Missing
P10-1063,W07-0736,0,\N,Missing
P18-1238,P04-1077,0,0.0165014,"nceptual T2T8x8 Training COCO COCO COCO COCO Conceptual Conceptual Conceptual Conceptual CIDEr 0.340 0.356 0.341 0.359 0.269 0.275 0.226 0.227 ROUGE-L 0.414 0.413 0.404 0.416 0.310 0.309 0.280 0.277 SPICE 0.101 0.103 0.101 0.103 0.076 0.076 0.068 0.066 Table 7: Auto metrics on the Flickr 1K Test. 5.4.2 Automatic Evaluation Results In this section, we report automatic evaluation results, using established image captioning metrics. For the COCO C40 test set (Fig. 5), we report the numerical values returned by the COCO online evaluation server‡ , using the CIDEr (Vedantam et al., 2015), ROUGE-L (Lin and Och, 2004), and METEOR (Banerjee and Lavie, 2005) metrics. For Conceptual Captions (Fig. 6) and Flickr (Fig. 7) test sets, we report numerical values for the CIDEr, ROUGE-L, and SPICE (Anderson et al., 2016)§ . For all metrics, higher number means closer distance between the candidates and the groundtruth captions. The automatic metrics are good at detecting invs out-of-domain situations. For COCO-models tested on COCO, the results in Fig. 5 show CIDEr scores in the 1.02-1.04 range, for both RNN- and Transformer-based models; the scores drop in the 0.35-0.41 range (CIDEr) for the Conceptual-based models"
P18-1238,Q14-1006,0,0.870186,"using a Flume (Chambers et al., 2010) pipeline. This pipeline processes billions of Internet webpages in parallel. From these webpages, it extracts, filters, and processes candidate himage, captioni pairs. The filtering and processing steps are described in detail in the following sections. Related Work Automatic image captioning has a long history (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Kiros et al., 2015). It has accelerated with the success of Deep Neural Networks (Bengio, 2009) and the availability of annotated data as offered by datasets such as Flickr30K (Young et al., 2014) and MS-COCO (Lin et al., 2014). The COCO dataset is not large (order of 106 images), given the training needs of DNNs. In spite of that, it has been very popular, in part because it offers annotations for images with non-iconic views, or non-canonical perspectives of objects, and therefore reflects the composition of everyday scenes (the same is true about Flickr30K (Young et al., 2014)). COCO annotations–category labeling, instance spotting, and instance segmentation– are done for all objects in an image, including those † https://en.wikipedia.org/wiki/Alt attribute 3 Conceptual Captions Dat"
P19-1650,P84-1044,0,0.115328,"Missing"
P19-1650,D18-1435,0,0.0347583,"Missing"
P19-1650,P18-1000,0,0.214723,"Missing"
P19-1650,Q18-1027,0,0.0263516,"this important distinction. 3.2.3 Label Coverage Control Because of the asymmetry between the image input and the label inputs, we introduce a mechanism to control the coverage of the supplied object and web entity labels in the generated caption. This mechanism consists of two parts: (i) two regressor models that learn coverage scores correlating input labels (one for objects, one for web entities) with output mentions, and (ii) two control “knobs” that allow us to specify desired coverage scores at inference time. This coverage control mechanism is inspired by the Label-FineTuning model of Niu and Bansal (2018), although it is used here to achieve a different goal. Coverage of Object Labels An interesting property of the object labels is that they may be repetitive, often at various levels of granularity, for instance “table”, “office table” and “office”. A model that would require to reproduce all of them in the output caption will likely produce a disfluent caption containing repetitive mentions of the same object. We introduce object level coverage p as a precision-like score for object labels, Covobj , defined as the fraction of output caption tokens present in the input object labels (Eq. 6). F"
P19-1650,P18-1238,1,0.14285,"ased on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative. 1 Our Approach: “extra large movie poster image for Season of the Witch” Baseline Model: “folk rock artist performs on stage during festival” Our Approach: “Eric Clapton performs on stage during the Crossroads Guitar Festival” Figure 1: Generating informative captions using finegrained entity information from external sources; baseline outputs from Sharma et al. (2018). Introduction Much of the visual information available on the web is in the form of billions of images, but that information is not readily accessible to those with visual impairments, or those with slow internet speeds. Automatic image captioning can help alleviate this problem, but its usefulness is directly proportional to how much information an automatically-produced caption can convey. As it happens, the goal of learning good models for image captioning (in terms of generalization power) is at odds with the goal of producing highly informative captions (in terms of fine-grained entity m"
Q16-1001,E14-1060,0,0.0554739,"Missing"
Q16-1001,N15-1107,0,0.0870818,"Missing"
Q16-1001,banea-etal-2008-bootstrapping,0,0.0765869,"Missing"
Q16-1001,D12-1133,0,0.0746248,"Missing"
Q16-1001,W10-0701,0,0.100991,"Missing"
Q16-1001,E03-1009,0,0.309131,"onship between the attributes of the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters."
Q16-1001,P11-1061,0,0.0856553,"Missing"
Q16-1001,N12-1086,0,0.0512554,"Missing"
Q16-1001,de-marneffe-etal-2014-universal,0,0.0615903,"Missing"
Q16-1001,P07-1116,0,0.0877182,"Missing"
Q16-1001,Y09-1013,0,0.0251906,"ges; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-synt"
Q16-1001,C12-2026,0,0.0604854,"Missing"
Q16-1001,D11-1057,0,0.196897,"Missing"
Q16-1001,dukes-habash-2010-morphological,0,0.0178039,"we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing. playing awesome Radu Soricut Google Inc. rsoricut@google.com POS:V ERB, T ENSE :PAST, VF ORM :F IN, . . . POS:V ERB, T ENSE :P RES, VF ORM :G ER, . . . POS:A DJ, D EGREE :P OS Table 1: A sample English morpho-syntactic lexicon. They are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green a"
Q16-1001,N13-1138,0,0.0977876,"Missing"
Q16-1001,dzeroski-etal-2000-morphosyntactic,0,0.143082,"Missing"
Q16-1001,erjavec-2004-multext,0,0.140248,"Missing"
Q16-1001,D13-1105,0,0.0544315,"Missing"
Q16-1001,N16-1077,1,0.886147,"Missing"
Q16-1001,P13-1057,0,0.036681,"Missing"
Q16-1001,gimenez-marquez-2004-svmtool,0,0.125427,"Missing"
Q16-1001,E09-1038,0,0.0329114,"l. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-syntactic lexicon generation as a graph-based semi-supervised"
Q16-1001,P12-1016,0,0.0160868,", 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of w"
Q16-1001,P98-1080,0,0.316876,"Missing"
Q16-1001,A00-2013,0,0.230514,"Missing"
Q16-1001,huet-etal-2008-morphosyntactic,0,0.0707153,"Missing"
Q16-1001,W10-0717,0,0.0783632,"Missing"
Q16-1001,kokkinakis-etal-2000-annotating,0,0.00855555,"language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing. playing awesome Radu Soricut Google Inc. rsoricut@google.com POS:V ERB, T ENSE :PAST, VF ORM :F IN, . . . POS:V ERB, T ENSE :P RES, VF ORM :G ER, . . . POS:A DJ, D EGREE :P OS Table 1: A sample English morpho-syntactic lexicon. They are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Min"
Q16-1001,P08-1068,0,0.0642064,"n the attributes of the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was intro"
Q16-1001,P13-2017,1,0.875506,"Missing"
Q16-1001,N13-1090,0,0.0160739,"Missing"
Q16-1001,P07-1017,0,0.0315038,"000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes"
Q16-1001,D09-1063,0,0.090751,"Missing"
Q16-1001,D15-1151,0,0.0159257,"ailable lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-syntactic lexicon"
Q16-1001,N15-1055,0,0.0503508,"Missing"
Q16-1001,D13-1032,0,0.0729096,"Missing"
Q16-1001,W11-1107,0,0.0735884,"Missing"
Q16-1001,Q15-1012,0,0.0930734,"Missing"
Q16-1001,N15-1093,0,0.0487045,"Missing"
Q16-1001,J04-2003,0,0.0247049,"(Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth refe"
Q16-1001,A94-1024,0,0.492659,"Missing"
Q16-1001,N13-1039,0,0.0320114,"Missing"
Q16-1001,W11-4644,0,0.0722929,"Missing"
Q16-1001,N09-1024,0,0.219471,"Missing"
Q16-1001,W96-0213,0,0.533254,"nd Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word pair sharing the same word cluster and a feature for the cluster is fired. Thus, there are 256 possible cluster features on an edge, though in our case only a single one can fire. Suffix & Prefix. Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003). For example, in English, -ing denotes gerund verb forms like, studying, playing and -ed denotes past tense like studied, played etc. Prefixes like un-, in- often denote adjectives. Thus we include both 2-gram and 3-gram 1 Some of these features can cause the graph to become very dense making label propagation prohibitive. We keep the size of the graph in check by only allowing a word node to be connected to at most 100 other (randomly selected) word nodes sharing one particular feature. This reduces edges while still keeping the graph connected. suffix and prefix as edge featur"
Q16-1001,W13-5005,0,0.0554608,"Missing"
Q16-1001,simov-etal-2004-language,0,0.109415,"Missing"
Q16-1001,H05-1060,0,0.0500174,"e), with seed lexicon (Seed), with propagated lexicon (Propagation). (Oflazer and Kuru¨oz, 1994; Hajiˇc and Hladk´a, 1998). The model we use is a standard atomic sequence classifier, that classifies the morphological bundle for each word independent of the others (with the exception of features derived from these words). Specifically, we use a linear SVM model classifier with hand tuned features. This is similar to commonly used analyzers like SVMTagger (Gim´enez and Marquez, 2004) and MateTagger (Bohnet and Nivre, 2012). Our taggers are trained in a language independent manner (Hajiˇc, 2000; Smith et al., 2005; M¨uller et al., 2013). The list of features used in training the tagger are listed in Table 6. In addition to the standard features, we use the morpho-syntactic attributes present in the lexicon for every word as features in the tagger. As shown in M¨uller and Schuetze (2015), this is typically the most important feature for morphological tagging, even more useful than clusters or word embeddings. While predicting the contextual morphological tags for a given word, the morphological attributes present in the lexicon for the current word, the previous word and the next word are used as featur"
Q16-1001,P08-1084,0,0.263349,"Missing"
Q16-1001,N15-1186,1,0.766624,"like studied, played etc. Prefixes like un-, in- often denote adjectives. Thus we include both 2-gram and 3-gram 1 Some of these features can cause the graph to become very dense making label propagation prohibitive. We keep the size of the graph in check by only allowing a word node to be connected to at most 100 other (randomly selected) word nodes sharing one particular feature. This reduces edges while still keeping the graph connected. suffix and prefix as edge features.2 We introduce an edge between two words sharing a particular suffix or prefix feature. Morphological Transformations. Soricut and Och (2015) presented an unsupervised method of inducing prefix- and suffix-based morphological transformations between words using word embeddings. In their method, statistically, most of the transformations are induced between words with the same lemma (without using any prior information about the word lemma). For example, their method induces the transformation between played and playing as suffix:ed:ing. This feature indicates T ENSE :PAST to turn off and T ENSE :P RES to turn on.3 We train the morphological transformation prediction tool of Soricut and Och (2015) on the news corpus (same as the one"
Q16-1001,D08-1114,0,0.279809,"Missing"
Q16-1001,D10-1017,0,0.0938659,"Missing"
Q16-1001,N12-1052,1,0.899187,"Missing"
Q16-1001,N07-1037,0,0.0803348,"Missing"
Q16-1001,P10-1149,0,0.0821283,"Missing"
Q16-1001,W02-1028,0,0.213563,"Missing"
Q16-1001,tron-etal-2006-morphdb,0,0.086576,"Missing"
Q16-1001,P10-1040,0,0.0756671,"f the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word"
Q16-1001,P08-1086,0,0.0458253,"ubgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word pair sharing the same word cluster and a feature for the cluster is fired. Thus, there are 256 possible cluster features on an edge, though in our case only a single one can fire. Suffix & Prefix. Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003). For example, in English, -ing"
Q16-1001,N10-1119,1,0.867886,"Missing"
Q16-1001,W04-0109,0,0.0792441,"Missing"
Q16-1001,P00-1027,0,0.070682,"Missing"
Q16-1001,P11-2033,0,0.119186,"Missing"
Q16-1001,C98-1077,0,\N,Missing
soricut-etal-2002-using,1999.tc-1.8,0,\N,Missing
soricut-etal-2002-using,J93-2003,0,\N,Missing
soricut-etal-2002-using,knight-al-onaizan-1998-translation,1,\N,Missing
soricut-etal-2002-using,W98-0301,1,\N,Missing
soricut-etal-2002-using,P01-1030,1,\N,Missing
W12-3102,W10-1703,1,0.557844,"Missing"
W12-3102,W11-2103,1,0.709065,"- ANNOTATOR AGREEMENT P (A) 0.567 0.576 0.595 0.598 0.540 0.504 0.568 0.519 0.568 0.601 P (A) 0.660 0.566 0.733 0.732 0.792 0.566 0.719 0.634 0.671 0.722 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.272 0.312 0.323 0.336 0.222 0.176 0.272 0.214 0.284 0.375 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.428 0.296 0.554 0.557 0.648 0.279 0.526 0.401 0.455 0.564 Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7). Agreement rates vary widely across languages. For inter-annotator agreements, the range is 0.176 to 0.336, while intra-annotator agreement ranges from 0.279 to 0.648. We note in particular the low agreement rates among judgments in the English-Spanish task, which is reflected in the relative lack of statistical significance Table 4. The agreement rates for this year were somewhat lower than last year. 3.3 Results of the Translation Task We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In ou"
W12-3102,D09-1030,1,0.149941,"Missing"
W12-3102,W12-3103,0,0.0434403,"Missing"
W12-3102,W11-2107,0,0.311039,"Missing"
W12-3102,W12-3131,0,0.0259292,"Missing"
W12-3102,W12-3133,0,0.0166865,"Missing"
W12-3102,W12-3134,1,0.0973948,"Missing"
W12-3102,W12-3135,0,0.0202653,"Missing"
W12-3102,W12-3111,0,0.0311487,"Missing"
W12-3102,W12-3136,0,0.0403021,"Missing"
W12-3102,W12-3112,0,0.176358,"Missing"
W12-3102,2011.eamt-1.32,0,0.0209858,"with only two linear equations. System “SDLLW SVM” uses a 20-feature set and an SVM epsilon regression model with radial basis function kernel with parameters C, gamma, and epsilon tuned on a development set (305 training instances). The model was trained with 10-fold cross validation and the tuning process was restarted several times using different starting points and step sizes to avoid overfitting. The final model was selected based on its performance on the development set and the number of support vectors. UU (R, S): System “UU best” uses the 17 baseline features, plus 82 features from Hardmeier (2011) (with some redundancy and some overlap with baseline features), and constituency trees over input sentences generated by the Stanford parser and dependency trees over both input and output sentences generated by the MaltParser. System “UU bltk” uses only the 17 baseline features plus constituency and dependency trees as above. The machine learning component in both cases is SVM regression (SVMlight software). For the ranking task, 29 the ranking induced by the regression output is used. The system uses polynomial kernels of degree 2 (UU best) and 3 (UU bltk) as well as two different types of"
W12-3102,W12-3137,0,0.0533752,"Missing"
W12-3102,W12-3106,0,0.0413119,"Missing"
W12-3102,W12-3145,0,0.0522389,"Missing"
W12-3102,W12-3146,0,0.0998404,"Missing"
W12-3102,W09-0415,0,0.0172168,"Missing"
W12-3102,W12-3150,1,0.188192,"Missing"
W12-3102,W12-3119,0,0.030516,"Missing"
W12-3102,W12-3151,0,0.046244,"Missing"
W12-3102,W12-3117,0,\N,Missing
W12-3102,W12-3138,0,\N,Missing
W12-3102,W12-3130,0,\N,Missing
W12-3102,W12-3148,0,\N,Missing
W12-3102,W12-3114,0,\N,Missing
W12-3102,W12-3107,0,\N,Missing
W12-3102,W12-3139,1,\N,Missing
W12-3102,W11-2108,0,\N,Missing
W12-3102,W09-0401,1,\N,Missing
W12-3102,W12-3140,0,\N,Missing
W12-3102,W12-3132,0,\N,Missing
W12-3102,W12-3109,0,\N,Missing
W12-3102,W10-1711,0,\N,Missing
W12-3102,2010.iwslt-evaluation.22,0,\N,Missing
W12-3102,W07-0718,1,\N,Missing
W12-3102,W06-3114,1,\N,Missing
W12-3102,2009.eamt-1.5,1,\N,Missing
W12-3102,W12-3144,0,\N,Missing
W12-3102,W11-2158,0,\N,Missing
W12-3102,W12-3110,1,\N,Missing
W12-3102,W08-0309,1,\N,Missing
W12-3102,W12-3105,0,\N,Missing
W12-3102,W12-3149,0,\N,Missing
W12-3102,2011.eamt-1.12,1,\N,Missing
W12-3102,W04-3250,1,\N,Missing
W12-3102,W12-3142,0,\N,Missing
W12-3102,W11-2113,0,\N,Missing
W12-3102,W12-3143,0,\N,Missing
W12-3102,W11-2145,0,\N,Missing
W12-3102,W12-3147,0,\N,Missing
W12-3102,W12-3101,0,\N,Missing
W12-3102,W12-3115,0,\N,Missing
W12-3102,W11-2101,0,\N,Missing
W12-3102,W12-3113,0,\N,Missing
W12-3102,W12-3104,0,\N,Missing
W12-3102,W12-3108,0,\N,Missing
W12-3102,W12-3118,1,\N,Missing
W12-3118,W12-3102,1,0.854328,"Missing"
W12-3118,P07-2045,0,0.00656602,"d a feature-selection algorithm that has been designed to directly optimize towards the official metrics used in this shared-task. The resulting submissions placed 1st (the M5P model) and 2nd (the SVM model), respectively, on both the Ranking task and the Scoring task, out of 11 participating teams. This shared-task focused on estimating the quality of English to Spanish automatic translations. The training set distributed for the shared task comprised of 1, 832 English sentences taken from the news domain and their Spanish translations. The translations were produced by the Moses SMT system (Koehn et al., 2007) trained on Europarl data. Translations also had a quality score derived from an average of three human judgements of Post-Editing effort using a 15 scale (1 for worse-quality/most-effort, and 5 for best-quality/least-effort). Submissions were evaluated using a blind official test set of 422 sentences produced in the same fashion as the training set. Two sub-tasks were considered: (i) scoring translations using the 1-5 quality scores (Scoring), and (ii) ranking translations from best to worse (Ranking). The official metrics used for the Ranking task were DeltaAvg (measuring how valuable a prop"
W12-3118,P10-1063,1,0.103611,"ror (MAE) and Root Mean Squared Error (RMSE). The interested reader is referred to (Callison-Burch et al., 2012) for detailed descriptions of both the data and the evaluation metrics used in the shared-task. Introduction The WMT 2012 Quality Estimation shared-task focused on automatic methods for estimating machine translation output quality at run-time (sentence-level estimation). Different from MT evaluation metrics, quality prediction (QP) systems do not rely on reference translations and are generally built using machine learning techniques to estimate quality scores (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011). Some interesting uses of sentence-level MT quality prediction are the following: decide whether a given translation is good enough for publishing asis (Soricut and Echihabi, 2010), or inform monolingual (target-language) readers whether or not they can rely on a translation; filter out sentences that are not good enough for post-editing by professional translators (Specia, 2011); select the best translation The SDL Language Weaver team participated with two submissions based on M5P and SVM regression models in both the Ranking and the Scoring 145 Proceedings"
W12-3118,W12-3121,1,0.846787,"Missing"
W12-3118,2009.eamt-1.5,0,0.319652,"were Mean-Absolute-Error (MAE) and Root Mean Squared Error (RMSE). The interested reader is referred to (Callison-Burch et al., 2012) for detailed descriptions of both the data and the evaluation metrics used in the shared-task. Introduction The WMT 2012 Quality Estimation shared-task focused on automatic methods for estimating machine translation output quality at run-time (sentence-level estimation). Different from MT evaluation metrics, quality prediction (QP) systems do not rely on reference translations and are generally built using machine learning techniques to estimate quality scores (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011). Some interesting uses of sentence-level MT quality prediction are the following: decide whether a given translation is good enough for publishing asis (Soricut and Echihabi, 2010), or inform monolingual (target-language) readers whether or not they can rely on a translation; filter out sentences that are not good enough for post-editing by professional translators (Specia, 2011); select the best translation The SDL Language Weaver team participated with two submissions based on M5P and SVM regression models in both the Ranking and"
W12-3118,2011.eamt-1.12,0,0.0334175,"he interested reader is referred to (Callison-Burch et al., 2012) for detailed descriptions of both the data and the evaluation metrics used in the shared-task. Introduction The WMT 2012 Quality Estimation shared-task focused on automatic methods for estimating machine translation output quality at run-time (sentence-level estimation). Different from MT evaluation metrics, quality prediction (QP) systems do not rely on reference translations and are generally built using machine learning techniques to estimate quality scores (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011). Some interesting uses of sentence-level MT quality prediction are the following: decide whether a given translation is good enough for publishing asis (Soricut and Echihabi, 2010), or inform monolingual (target-language) readers whether or not they can rely on a translation; filter out sentences that are not good enough for post-editing by professional translators (Specia, 2011); select the best translation The SDL Language Weaver team participated with two submissions based on M5P and SVM regression models in both the Ranking and the Scoring 145 Proceedings of the 7th Workshop on Statistica"
W12-3118,P11-1022,1,\N,Missing
W12-3121,P07-1038,0,0.0407033,"Missing"
W12-3121,W08-0330,0,0.554341,"e use here are computed as perplexity numbers using a 5gram language model trained on the MT training set. This can be achieved, for instance, by using the publicly-available SRILM toolkit 2 . These two features are useful in modeling the relationship between the likelihood of a string (or set of strings) under an n-gram language model and the expected BLEU score for that input/output pair. Pseudo-reference–based features Previous work has shown that, in the absence of human-produced references, automaticallyproduced ones are still helpful in differentiating between good and bad translations (Albrecht and Hwa, 2008). When computed on the target side, this type of features requires one (or possibly more) secondary MT system(s), used to generate translations starting from the same input. These pseudoreferences are useful in gauging translation convergence, using BLEU scores as feature values. In intuitive terms, their usefulness can be summarized as follows: “if system X produced a translation A and system Y produced a translation B starting from the same input, and A and B are similar, then A is probably a good translation”. An important property here is that systems X and Y need to be as different as pos"
W12-3121,C04-1046,0,0.387067,"recht and Hwa, 2007), and ranking (Ye et al., 2007; Duh, 2008). The focus of these approaches is on system performance evaluation, as they use a constant test set and measure various MT systems against it. In contrast, we are interested in evaluating the quality of the translations themselves, while treat163 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 163–170, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics ing the MT components as constants. In this respect, the goal is more related to the area of confidence estimation for MT (Blatz et al., 2004). Confidence estimation is usually concerned with identifying words/phrases for which one can be confident in the quality of the translation. A sentencelevel approach to quality estimation is taken on the classification-based work of Gamon et al. (2005) and regression-based work of Specia et al. (2009). Our approach to quality estimation focuses on both sentence-level and document-level estimation. We improve on the quality estimation technique that is proposed for document-level estimation in (Soricut and Echihabi, 2010). Furthermore, we exploit the availability of multiple translation hypoth"
W12-3121,W12-3102,1,0.87934,"Missing"
W12-3121,P05-1066,0,0.271287,"rting from the same input, and A and B are similar, then A is probably a good translation”. An important property here is that systems X and Y need to be as different as possible from each other. This property ensures that a convergence on similar translations is not just an artifact of the systems sharing the same translation model/resources, but a true indication that the translations converge. The secondary systems we use in this work are 2 Available at www-speech.sri.com/projects/srilm. still phrase-based, but equipped with linguisticallyoriented modules similar with the ones proposed in (Collins et al., 2005; Xu et al., 2009). Our experiments indicate that this single feature is one of the most powerful ones in terms of its predictive power. Example-based features For example-based features, we use a development set of parallel sentences, for which we produce translations and compute sentence-level BLEU scores. We set aside the top BLEU scoring sentences and bottom BLEU scoring sentences. These sets are used as positive examples (with better-thanaverage BLEU) and negative examples (with worsethan-average BLEU), respectively. We define a positive-example–based feature function as a geometric mean"
W12-3121,W08-0331,0,0.112353,"Missing"
W12-3121,2005.eamt-1.15,0,0.164744,"e quality of the translations themselves, while treat163 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 163–170, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics ing the MT components as constants. In this respect, the goal is more related to the area of confidence estimation for MT (Blatz et al., 2004). Confidence estimation is usually concerned with identifying words/phrases for which one can be confident in the quality of the translation. A sentencelevel approach to quality estimation is taken on the classification-based work of Gamon et al. (2005) and regression-based work of Specia et al. (2009). Our approach to quality estimation focuses on both sentence-level and document-level estimation. We improve on the quality estimation technique that is proposed for document-level estimation in (Soricut and Echihabi, 2010). Furthermore, we exploit the availability of multiple translation hypotheses to perform system combination. Our system combination methods are based on generic Machine Learning techniques, applied on 1-best output strings. In contrast, most of the approaches to MT system combination combine N-best lists from multiple MT sys"
W12-3121,P08-2021,0,0.184488,"tion, sentencelevel predictions are used for system selection, improving the quality of the output translations. We present three system selection techniques and perform evaluations that quantify the gains across multiple domains and language pairs. 1 Introduction Aside from improving the performance of coretranslation models, there additionally exist two orthogonal approaches via which fully-automatic translations can achieve increased acceptance and better integration in real-world use cases. These two approaches are: improved translation accuracy via system combination (Rosti et al., 2008; Karakos et al., 2008; Hildebrand and Vogel, 2008), and automatic quality-estimation techniques used as an additional layer on top of MT systems, which present the user only with translations that are predicted as being accurate (Soricut and Echihabi, 2010; Specia, 2011). In this paper, we describe new contributions to both these approaches. First, we present a novel and superior technique for performing quality estimation at document level. We achieve this by chang∗ Research was completed before the author started in his current role at Google Inc. The opinions stated are his own and not of Google Inc. ing the gr"
W12-3121,W09-0429,0,0.0556239,"Missing"
W12-3121,2004.tmi-1.8,0,0.0672071,"Missing"
W12-3121,J04-4002,0,0.0613906,"Missing"
W12-3121,P02-1040,0,0.0907388,"res. However, even if the predicted value is at document-level, the actual feature computation and model prediction does not necessarily need to happen at document-level. It is one of the goals of this work to determine if the models of prediction work better at a coarser granularity (such as document level) or finer granularity (such as sentence-level). We describe here a mechanism for predicting BLEU scores at sentence level, and then combining these scores into document-level scores. To make explicit our prediction mechanism, we present here in detail the formula for computing BLEU scores (Papineni et al., 2002). First, n-gram precision scores Pn are computed as follows: P Pn = C∈Candidates P P n-gram∈C Countclip (n-gram) P Count(n-gram) (1) where Countclip (n-gram) is the maximum number of n-grams co-occurring in a candidate translation and a reference translation, and Count(n-gram) is the number of n-grams in the candidate translation. To prevent very short translations that try to maximize their precision scores, BLEU adds a brevity penalty, BP, to the formula: C∈Candidates ( BP = n-gram∈C 1 if |c |> |r| (1−|r|/|c|) e if |c |≤ |r| (2) where |c |is the length of the candidate translation and |r |is"
W12-3121,W08-0329,0,0.147733,"level quality estimation, sentencelevel predictions are used for system selection, improving the quality of the output translations. We present three system selection techniques and perform evaluations that quantify the gains across multiple domains and language pairs. 1 Introduction Aside from improving the performance of coretranslation models, there additionally exist two orthogonal approaches via which fully-automatic translations can achieve increased acceptance and better integration in real-world use cases. These two approaches are: improved translation accuracy via system combination (Rosti et al., 2008; Karakos et al., 2008; Hildebrand and Vogel, 2008), and automatic quality-estimation techniques used as an additional layer on top of MT systems, which present the user only with translations that are predicted as being accurate (Soricut and Echihabi, 2010; Specia, 2011). In this paper, we describe new contributions to both these approaches. First, we present a novel and superior technique for performing quality estimation at document level. We achieve this by chang∗ Research was completed before the author started in his current role at Google Inc. The opinions stated are his own and not of"
W12-3121,P10-1063,1,0.786243,"and language pairs. 1 Introduction Aside from improving the performance of coretranslation models, there additionally exist two orthogonal approaches via which fully-automatic translations can achieve increased acceptance and better integration in real-world use cases. These two approaches are: improved translation accuracy via system combination (Rosti et al., 2008; Karakos et al., 2008; Hildebrand and Vogel, 2008), and automatic quality-estimation techniques used as an additional layer on top of MT systems, which present the user only with translations that are predicted as being accurate (Soricut and Echihabi, 2010; Specia, 2011). In this paper, we describe new contributions to both these approaches. First, we present a novel and superior technique for performing quality estimation at document level. We achieve this by chang∗ Research was completed before the author started in his current role at Google Inc. The opinions stated are his own and not of Google Inc. ing the granularity of the prediction mechanism from document-level (Soricut and Echihabi, 2010) to sentence-level, and predicting BLEU scores via directly modeling the sufficient statistics for BLEU computation. A document-level score is then r"
W12-3121,2009.eamt-1.5,0,0.25253,"reat163 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 163–170, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics ing the MT components as constants. In this respect, the goal is more related to the area of confidence estimation for MT (Blatz et al., 2004). Confidence estimation is usually concerned with identifying words/phrases for which one can be confident in the quality of the translation. A sentencelevel approach to quality estimation is taken on the classification-based work of Gamon et al. (2005) and regression-based work of Specia et al. (2009). Our approach to quality estimation focuses on both sentence-level and document-level estimation. We improve on the quality estimation technique that is proposed for document-level estimation in (Soricut and Echihabi, 2010). Furthermore, we exploit the availability of multiple translation hypotheses to perform system combination. Our system combination methods are based on generic Machine Learning techniques, applied on 1-best output strings. In contrast, most of the approaches to MT system combination combine N-best lists from multiple MT systems via confusion network decoding (Karakos et al"
W12-3121,2011.eamt-1.12,0,0.14753,"duction Aside from improving the performance of coretranslation models, there additionally exist two orthogonal approaches via which fully-automatic translations can achieve increased acceptance and better integration in real-world use cases. These two approaches are: improved translation accuracy via system combination (Rosti et al., 2008; Karakos et al., 2008; Hildebrand and Vogel, 2008), and automatic quality-estimation techniques used as an additional layer on top of MT systems, which present the user only with translations that are predicted as being accurate (Soricut and Echihabi, 2010; Specia, 2011). In this paper, we describe new contributions to both these approaches. First, we present a novel and superior technique for performing quality estimation at document level. We achieve this by chang∗ Research was completed before the author started in his current role at Google Inc. The opinions stated are his own and not of Google Inc. ing the granularity of the prediction mechanism from document-level (Soricut and Echihabi, 2010) to sentence-level, and predicting BLEU scores via directly modeling the sufficient statistics for BLEU computation. A document-level score is then recreated based"
W12-3121,N09-1028,0,0.0460895,"put, and A and B are similar, then A is probably a good translation”. An important property here is that systems X and Y need to be as different as possible from each other. This property ensures that a convergence on similar translations is not just an artifact of the systems sharing the same translation model/resources, but a true indication that the translations converge. The secondary systems we use in this work are 2 Available at www-speech.sri.com/projects/srilm. still phrase-based, but equipped with linguisticallyoriented modules similar with the ones proposed in (Collins et al., 2005; Xu et al., 2009). Our experiments indicate that this single feature is one of the most powerful ones in terms of its predictive power. Example-based features For example-based features, we use a development set of parallel sentences, for which we produce translations and compute sentence-level BLEU scores. We set aside the top BLEU scoring sentences and bottom BLEU scoring sentences. These sets are used as positive examples (with better-thanaverage BLEU) and negative examples (with worsethan-average BLEU), respectively. We define a positive-example–based feature function as a geometric mean of 1-to-4–gram pre"
W12-3121,W07-0736,0,0.203358,"Missing"
W12-3121,2008.amta-srw.3,0,\N,Missing
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W18-1406,Q13-1005,0,0.0222811,"otation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evaluations with implicit feedback rather than prediction and evaluation of labels on text and images. Spatial tasks are natural fits for this strategy, since both evaluation metrics and reward functions (in reinforcement learning) can use spatial proximity to an end location (MacMahon et al., 2006; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Artzi and Zettlemoyer, 2013) or spatial configuration (Bisk et al., 2018; Misra et al., 2017; Tan and Bansal, 2018). There are trade-offs between model-driven and user-driven corpus building. The former defines inventories of spatial relations and generating assignments that will cover them. This may omit phenomena or distinctions not covered in the model and requires considerable expertise and tooling—both of which increase cost and limit scale. User-driven annotation is more exploratory and may be limited by the preferences and tendencies of contributors. We will mitigate such effects by composing diverse crowds from v"
W18-1406,W16-1721,1,0.844969,"d image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real world navigation tasks. Paths Understanding salient features and spatial relations in images and text naturally extends into navigation tasks that connect such points. To avoid"
W18-1406,N18-1177,0,0.108584,"have access to a God’s eye view, like that available to mapping applications (with access to full geographic features via databases). Instead, such tasks must be solvable by interpreting visual and textual stimuli relevant to the locations. This should put a greater emphasis on challenging spatial descriptions and relationships rather than known and named routes. Nonetheless, maps as visual artifacts (e.g., PDFs) may be incorporated in some cases, giving automated agents the ability to use them as a hiker might use a paper map without access to a GPS-based mapping application. Mirowski et al. (2018) is a recent example that takes a first-person perspective in a real world simulation, though one that does not incorporate language. They learn a model for navigating the Google Street View graph via reinforcement learning, where the goal location is specified via its distance to several other landmark locations and no explicit maps are used. Two especially interesting aspects of their approach are their use of curriculum learning (start with nearby goals and then tackle more distant ones) and showing successful adaptation from one city to another. These ideas are complementary to those that"
W18-1406,L18-1254,1,0.897777,"h spatially relevant points and paths—on the scale of at least hundreds of thousands. Multilinguality For both theoretical and practical reasons, we cannot focus on just one language. Different languages have different spatial relations, often involving the three different frames of reference—relative, intrinsic, and absolute (Levinson, 2003)—in different ways. Navigational systems supporting vague reference off the grid are needed even more in locations where English and other majority languages are not spoken. One way we already target multilinguality is via community-driven crowd-sourcing (Funk et al., 2018). In our approach, we intentionally cycle our iterations throughout the world and we involve developers from each locale because they have insights into how the local context affects how language is used and how the task is performed. User-driven annotation We seek to complement previous efforts that have focused on finegrained linguistic annotation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evalua"
W18-1406,W16-3202,0,0.0497127,"Missing"
W18-1406,Q18-1004,0,0.0227929,"Missing"
W18-1406,D14-1086,0,0.0338832,"nguage artifacts provides a natural and mutually reinforcing progression from points to paths to playscapes. Points Scene understanding—building a model for a point in space—is the bedrock of real world spatial language tasks. We must be able to observe and describe visible objects and the spatial relationships between them. Before addressing paths and navigation tasks, we can make considerable progress by improving our data and modeling for spatial relations in tasks like image segmentation and image captioning (Hall et al., 2011; H¨urlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities fo"
W18-1406,D17-1015,0,0.0208315,"to playscapes. Points Scene understanding—building a model for a point in space—is the bedrock of real world spatial language tasks. We must be able to observe and describe visible objects and the spatial relationships between them. Before addressing paths and navigation tasks, we can make considerable progress by improving our data and modeling for spatial relations in tasks like image segmentation and image captioning (Hall et al., 2011; H¨urlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague region"
W18-1406,P17-2033,0,0.0135746,"ounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real world navigation task"
W18-1406,P10-1083,0,0.0402714,"finegrained linguistic annotation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evaluations with implicit feedback rather than prediction and evaluation of labels on text and images. Spatial tasks are natural fits for this strategy, since both evaluation metrics and reward functions (in reinforcement learning) can use spatial proximity to an end location (MacMahon et al., 2006; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Artzi and Zettlemoyer, 2013) or spatial configuration (Bisk et al., 2018; Misra et al., 2017; Tan and Bansal, 2018). There are trade-offs between model-driven and user-driven corpus building. The former defines inventories of spatial relations and generating assignments that will cover them. This may omit phenomena or distinctions not covered in the model and requires considerable expertise and tooling—both of which increase cost and limit scale. User-driven annotation is more exploratory and may be limited by the preferences and tendencies of contributors. We will mitigate such effects by c"
W18-1406,D14-1039,1,0.842799,"rlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real"
W18-1406,D17-1106,0,0.0455013,"Missing"
