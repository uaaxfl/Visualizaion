2020.acl-demos.27,W12-3202,0,0.667392,"Missing"
2020.acl-demos.27,bird-etal-2008-acl,0,0.340091,"., 2015; Radev et al., 2016). Various subsets of AA have been used in the past for a number of tasks, including: to study citation patterns and intent (Radev et al., 2016; Zhu et al., 2015; Nanba et al., 2011; Mohammad et al., 2009; Teufel et al., 2006; Aya et al., 2005; Pham and Hoffmann, 2003), to generate summaries of scientific articles (Qazvinian et al., 2013), to study gender disparities in NLP (Schluter, 2018), to study subtopics within NLP (Anderson et al., 233 5 http://saifmohammad.com/WebPages/nlpscholar.html 2012), and to create corpora of scientific articles (Mariani et al., 2018; Bird et al., 2008). However, none of these works provide an interactive visualization for users to explore NLP literature and their citations. 3 Data We now briefly describe how we extracted information from the ACL Anthology and Google Scholar. (Further details about the dataset, as well as an analysis of the volume of research in NLP over the years, are available in Mohammad (2020c).) 3.1 ACL Anthology Data The ACL Anthology provides access to its data through its website and a github repository (Gildea et al., 2018).6 We extracted paper title, names of authors, year of publication, and venue of publication f"
2020.acl-demos.27,W18-2504,0,0.516833,"ages/nlpscholar.html 2012), and to create corpora of scientific articles (Mariani et al., 2018; Bird et al., 2008). However, none of these works provide an interactive visualization for users to explore NLP literature and their citations. 3 Data We now briefly describe how we extracted information from the ACL Anthology and Google Scholar. (Further details about the dataset, as well as an analysis of the volume of research in NLP over the years, are available in Mohammad (2020c).) 3.1 ACL Anthology Data The ACL Anthology provides access to its data through its website and a github repository (Gildea et al., 2018).6 We extracted paper title, names of authors, year of publication, and venue of publication from the repository.7 As of June 2019, AA had ∼50K entries; however, this includes forewords, schedules, etc. that are not truly research publications. After discarding them we are left with a set of 44,895 papers. 3.2 Google Scholar Data Google Scholar does not provide an API to extract information about the papers. This is likely because of its agreement with publishing companies that have scientific literature behind paywalls (Mart´ınMart´ın et al., 2018). We extracted citation information from Goog"
2020.acl-demos.27,N09-1066,1,0.768773,"a and interactive visualizations associated with this work are freely available through the project homepage.5 2 Background and Related Work Much of the work in visualizing scientific literature has focused on showing topics of research (Wu et al., 2019; Heimerl et al., 2012; Lee et al., 2005). There is also notable work on visualizing communities through citation networks (Heimerl et al., 2015; Radev et al., 2016). Various subsets of AA have been used in the past for a number of tasks, including: to study citation patterns and intent (Radev et al., 2016; Zhu et al., 2015; Nanba et al., 2011; Mohammad et al., 2009; Teufel et al., 2006; Aya et al., 2005; Pham and Hoffmann, 2003), to generate summaries of scientific articles (Qazvinian et al., 2013), to study gender disparities in NLP (Schluter, 2018), to study subtopics within NLP (Anderson et al., 233 5 http://saifmohammad.com/WebPages/nlpscholar.html 2012), and to create corpora of scientific articles (Mariani et al., 2018; Bird et al., 2008). However, none of these works provide an interactive visualization for users to explore NLP literature and their citations. 3 Data We now briefly describe how we extracted information from the ACL Anthology and G"
2020.acl-demos.27,D18-1301,0,0.0884837,"has focused on showing topics of research (Wu et al., 2019; Heimerl et al., 2012; Lee et al., 2005). There is also notable work on visualizing communities through citation networks (Heimerl et al., 2015; Radev et al., 2016). Various subsets of AA have been used in the past for a number of tasks, including: to study citation patterns and intent (Radev et al., 2016; Zhu et al., 2015; Nanba et al., 2011; Mohammad et al., 2009; Teufel et al., 2006; Aya et al., 2005; Pham and Hoffmann, 2003), to generate summaries of scientific articles (Qazvinian et al., 2013), to study gender disparities in NLP (Schluter, 2018), to study subtopics within NLP (Anderson et al., 233 5 http://saifmohammad.com/WebPages/nlpscholar.html 2012), and to create corpora of scientific articles (Mariani et al., 2018; Bird et al., 2008). However, none of these works provide an interactive visualization for users to explore NLP literature and their citations. 3 Data We now briefly describe how we extracted information from the ACL Anthology and Google Scholar. (Further details about the dataset, as well as an analysis of the volume of research in NLP over the years, are available in Mohammad (2020c).) 3.1 ACL Anthology Data The ACL"
2020.acl-demos.27,W06-1613,0,0.223671,"lizations associated with this work are freely available through the project homepage.5 2 Background and Related Work Much of the work in visualizing scientific literature has focused on showing topics of research (Wu et al., 2019; Heimerl et al., 2012; Lee et al., 2005). There is also notable work on visualizing communities through citation networks (Heimerl et al., 2015; Radev et al., 2016). Various subsets of AA have been used in the past for a number of tasks, including: to study citation patterns and intent (Radev et al., 2016; Zhu et al., 2015; Nanba et al., 2011; Mohammad et al., 2009; Teufel et al., 2006; Aya et al., 2005; Pham and Hoffmann, 2003), to generate summaries of scientific articles (Qazvinian et al., 2013), to study gender disparities in NLP (Schluter, 2018), to study subtopics within NLP (Anderson et al., 233 5 http://saifmohammad.com/WebPages/nlpscholar.html 2012), and to create corpora of scientific articles (Mariani et al., 2018; Bird et al., 2008). However, none of these works provide an interactive visualization for users to explore NLP literature and their citations. 3 Data We now briefly describe how we extracted information from the ACL Anthology and Google Scholar. (Furth"
2020.acl-demos.27,D11-1055,0,0.355188,"Missing"
2020.acl-main.464,W12-3202,0,0.586133,"Creative Commons Attribution 4.0 International License. ence). It is the largest single source of scientific literature on NLP. Various subsets of AA have been used in the past for a number of tasks including: the study of citation patterns and intent (Pham and Hoffmann, 2003; Aya et al., 2005; Teufel et al., 2006; Mohammad et al., 2009; Nanba et al., 2011; Zhu et al., 2015; Radev et al., 2016), generating summaries of scientific articles (Qazvinian et al., 2013), and creating corpora of scientific articles (Bird et al., 2008; Mariani et al., 2018). Perhaps the work closest to ours is that by Anderson et al. (2012), who examine papers from 1980 to 2008 to track the ebb and flow of topics within NLP, the influence of subfields on each other, and the influence of researchers from outside NLP. However, that work did not examine trends in the citations of NLP papers. Google Scholar is a free web search engine for academic literature.6 Through it, users can access the metadata associated with an article such as the number of citations it has received. Google Scholar does not provide information on how many articles are included in its database. However, scientometric researchers estimated that it included ab"
2020.acl-main.464,bird-etal-2008-acl,0,0.133167,"/nlpscholar.html 4 https://www.aclweb.org/anthology/ 5 ACL licenses its papers with a Creative Commons Attribution 4.0 International License. ence). It is the largest single source of scientific literature on NLP. Various subsets of AA have been used in the past for a number of tasks including: the study of citation patterns and intent (Pham and Hoffmann, 2003; Aya et al., 2005; Teufel et al., 2006; Mohammad et al., 2009; Nanba et al., 2011; Zhu et al., 2015; Radev et al., 2016), generating summaries of scientific articles (Qazvinian et al., 2013), and creating corpora of scientific articles (Bird et al., 2008; Mariani et al., 2018). Perhaps the work closest to ours is that by Anderson et al. (2012), who examine papers from 1980 to 2008 to track the ebb and flow of topics within NLP, the influence of subfields on each other, and the influence of researchers from outside NLP. However, that work did not examine trends in the citations of NLP papers. Google Scholar is a free web search engine for academic literature.6 Through it, users can access the metadata associated with an article such as the number of citations it has received. Google Scholar does not provide information on how many articles are"
2020.acl-main.464,N09-1066,1,0.869092,"mad, 2020a). We have also developed an interactive visualization tool that allows users to search for relevant related work in the ACL Anthology Mohammad (2020c). 3 http://saifmohammad.com/WebPages/nlpscholar.html 4 https://www.aclweb.org/anthology/ 5 ACL licenses its papers with a Creative Commons Attribution 4.0 International License. ence). It is the largest single source of scientific literature on NLP. Various subsets of AA have been used in the past for a number of tasks including: the study of citation patterns and intent (Pham and Hoffmann, 2003; Aya et al., 2005; Teufel et al., 2006; Mohammad et al., 2009; Nanba et al., 2011; Zhu et al., 2015; Radev et al., 2016), generating summaries of scientific articles (Qazvinian et al., 2013), and creating corpora of scientific articles (Bird et al., 2008; Mariani et al., 2018). Perhaps the work closest to ours is that by Anderson et al. (2012), who examine papers from 1980 to 2008 to track the ebb and flow of topics within NLP, the influence of subfields on each other, and the influence of researchers from outside NLP. However, that work did not examine trends in the citations of NLP papers. Google Scholar is a free web search engine for academic litera"
2020.acl-main.464,2020.acl-main.702,1,0.906919,"window into research impact. Thus citation metrics are often a factor when making decisions about funding research and hiring scientists. Citation analysis can also be used to gauge the influence of outside fields on one’s field and the influence of one’s field on other fields. Therefore, it can be used to determine the relationship of a field with the wider academic community. As part of a broader project on analyzing NLP Literature, we extracted and aligned information from the ACL Anthology (AA) and Google Scholar to create a dataset of tens of thousands of NLP papers and their citations (Mohammad, 2020b, 2019).2 In this paper, we describe work on examining the papers and their citations to identify broad trends within NLP research—overall, across paper types, across publication venues, over time, and across research areas within NLP. Notably, we explored questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers published in different time spans? how well cited are papers from different areas of research within NLP? etc. The dataset and the analyses have many uses including: understanding how the field"
2020.acl-main.464,2020.lrec-1.109,1,0.579211,"window into research impact. Thus citation metrics are often a factor when making decisions about funding research and hiring scientists. Citation analysis can also be used to gauge the influence of outside fields on one’s field and the influence of one’s field on other fields. Therefore, it can be used to determine the relationship of a field with the wider academic community. As part of a broader project on analyzing NLP Literature, we extracted and aligned information from the ACL Anthology (AA) and Google Scholar to create a dataset of tens of thousands of NLP papers and their citations (Mohammad, 2020b, 2019).2 In this paper, we describe work on examining the papers and their citations to identify broad trends within NLP research—overall, across paper types, across publication venues, over time, and across research areas within NLP. Notably, we explored questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers published in different time spans? how well cited are papers from different areas of research within NLP? etc. The dataset and the analyses have many uses including: understanding how the field"
2020.acl-main.464,W18-2504,0,0.489001,"as well as an analysis of the volume of research in NLP over the years, are available in Mohammad (2020b).) We aligned the information across AA and GS using the paper title, year of publication, and first author last name. 5200 6 https://scholar.google.com Figure 1: A timeline graph of citations received by papers published in each year. Colored segments correspond to papers; the height of a segment is proportional to the number of citations. Hovering over a paper shows metadata. 3.1 ACL Anthology Data The ACL Anthology provides access to its data through its website and a github repository (Gildea et al., 2018).7 We extracted paper title, names of authors, year of publication, and venue of publication from the repository.8 As of June 2019, AA had ∼50K entries; however, this includes forewords, schedules, etc. that are not truly research publications. After discarding them we are left with a set of 44,894 papers.9 3.2 Google Scholar Data Google Scholar does not provide an API to extract information about the papers. This is likely because of its agreement with publishing companies that have scientific literature behind paywalls (Mart´ın-Mart´ın et al., 2018). We extracted citation information from Go"
2020.acl-main.464,2020.acl-demos.27,1,0.547494,"Missing"
2020.acl-main.464,P02-1040,0,0.109842,"bigrams approach for its simplicity. Figure 9: Bar graph of median citations. Title bigrams ordered by number of papers. In parenthesis: average citations, #papers. 5206 Discussion: The graph shows, for example, that the bigram machine translation occurred in 1,659 AA0 papers that have a median citation count of 14, while the average is 68.8. The average is one of the highest among the bigrams, despite the median being more middle of the pack. This suggests the presence of heavily cited, outlier, papers. Indeed, the most cited paper in all of AA0 is an MT paper with more than 9000 citations (Papineni et al., 2002). Note that not all MT papers have machine translation in the title. Although non-random, this sample of 1,659 papers is arguably a reasonably representative sample of MT papers. Third in the list are papers with statistical machine in the title—most commonly from the phrase statistical machine translation. One expects considerable overlap across these sets of papers. However, machine translation likely covers a broader range of research including work done before statistical MT was introduced, as well as work on neural MT and MT evaluation. The bigrams with the highest median include: sentime"
2020.acl-main.464,W06-1613,0,0.262447,"and citations (Mohammad, 2020a). We have also developed an interactive visualization tool that allows users to search for relevant related work in the ACL Anthology Mohammad (2020c). 3 http://saifmohammad.com/WebPages/nlpscholar.html 4 https://www.aclweb.org/anthology/ 5 ACL licenses its papers with a Creative Commons Attribution 4.0 International License. ence). It is the largest single source of scientific literature on NLP. Various subsets of AA have been used in the past for a number of tasks including: the study of citation patterns and intent (Pham and Hoffmann, 2003; Aya et al., 2005; Teufel et al., 2006; Mohammad et al., 2009; Nanba et al., 2011; Zhu et al., 2015; Radev et al., 2016), generating summaries of scientific articles (Qazvinian et al., 2013), and creating corpora of scientific articles (Bird et al., 2008; Mariani et al., 2018). Perhaps the work closest to ours is that by Anderson et al. (2012), who examine papers from 1980 to 2008 to track the ebb and flow of topics within NLP, the influence of subfields on each other, and the influence of researchers from outside NLP. However, that work did not examine trends in the citations of NLP papers. Google Scholar is a free web search eng"
2020.acl-main.464,D11-1055,0,0.27312,"Missing"
2020.acl-main.702,W12-3202,0,0.32187,"(Mishra et al., 2018), on ∼1.5 million articles from fifty disciplines published in JSTOR (King et al., 2017), and on ∼0.5 million publications from US research universities (Duch et al., 2012). There also exists some work that shows that in fields such as linguistics (LSA, 2017) and psychology (Willyard, 2011), female and male participation is either close to parity or tilted in favor of women. In NLP research, Schluter (2018) showed that there are barriers in the paths of women researchers, delaying their attainment of mentorship status (as estimated through last author position in papers). Anderson et al. (2012) examine papers from 1980 to 2008 to track the ebb and flow of topics within NLP, and the influence of researchers from outside NLP on NLP. Vogel and Jurafsky (2012) examined about 13,000 papers from 1980 to 2008 to determine basic authorship statistics by women and men. Gender statistics were determined by a combination of automatic and manual means. The automatic method relied on lists of baby names from various languages. They found that female authorship has been steadily increasing from 1980 to 2008. Our work examines a much larger set of NLP papers (1965–2019), re-examines some of the qu"
2020.acl-main.702,2020.acl-main.464,1,0.881827,"t to disentangle. We hope that this work will increase awareness of gender gaps and inspire concrete steps to improve inclusiveness and fairness in research. It should also be noted that even though this paper focuses on female–male disparities, there are many aspects to demographic diversity including: representation from transgender people; representation from various nationalities and race; representation by people who speak a diverse set of languages; 1 https://www.aclweb.org/anthology/ Mohammad (2019) presents an overview of the many research directions pursued, using this data. Notably, Mohammad (2020a) explores questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers published in different time spans? how well cited are papers from different areas of research within NLP? etc. Mohammad (2020c) presents an interactive visualization tool that allows users to search for relevant related work in the ACL Anthology. 2 diversity by income, age, physical abilities, etc. All of these factors impact the breadth of technologies we create, how useful they are, and whether they reach those that need it most. Reso"
2020.acl-main.702,2020.lrec-1.109,1,0.937051,"t to disentangle. We hope that this work will increase awareness of gender gaps and inspire concrete steps to improve inclusiveness and fairness in research. It should also be noted that even though this paper focuses on female–male disparities, there are many aspects to demographic diversity including: representation from transgender people; representation from various nationalities and race; representation by people who speak a diverse set of languages; 1 https://www.aclweb.org/anthology/ Mohammad (2019) presents an overview of the many research directions pursued, using this data. Notably, Mohammad (2020a) explores questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers published in different time spans? how well cited are papers from different areas of research within NLP? etc. Mohammad (2020c) presents an interactive visualization tool that allows users to search for relevant related work in the ACL Anthology. 2 diversity by income, age, physical abilities, etc. All of these factors impact the breadth of technologies we create, how useful they are, and whether they reach those that need it most. Reso"
2020.acl-main.702,2020.acl-demos.27,1,0.896427,"Missing"
2020.acl-main.702,D18-1301,0,0.0644185,"al., 2016; Andersen and Nielsen, 2018), on ∼2 million mathematics articles (Mihaljevi´c-Brandt et al., 2016), on ∼1.6 million articles from PubMed life science and biomedical research (Mishra et al., 2018), on ∼1.5 million articles from fifty disciplines published in JSTOR (King et al., 2017), and on ∼0.5 million publications from US research universities (Duch et al., 2012). There also exists some work that shows that in fields such as linguistics (LSA, 2017) and psychology (Willyard, 2011), female and male participation is either close to parity or tilted in favor of women. In NLP research, Schluter (2018) showed that there are barriers in the paths of women researchers, delaying their attainment of mentorship status (as estimated through last author position in papers). Anderson et al. (2012) examine papers from 1980 to 2008 to track the ebb and flow of topics within NLP, and the influence of researchers from outside NLP on NLP. Vogel and Jurafsky (2012) examined about 13,000 papers from 1980 to 2008 to determine basic authorship statistics by women and men. Gender statistics were determined by a combination of automatic and manual means. The automatic method relied on lists of baby names from"
2020.acl-main.702,W12-3204,0,0.0730401,"niversities (Duch et al., 2012). There also exists some work that shows that in fields such as linguistics (LSA, 2017) and psychology (Willyard, 2011), female and male participation is either close to parity or tilted in favor of women. In NLP research, Schluter (2018) showed that there are barriers in the paths of women researchers, delaying their attainment of mentorship status (as estimated through last author position in papers). Anderson et al. (2012) examine papers from 1980 to 2008 to track the ebb and flow of topics within NLP, and the influence of researchers from outside NLP on NLP. Vogel and Jurafsky (2012) examined about 13,000 papers from 1980 to 2008 to determine basic authorship statistics by women and men. Gender statistics were determined by a combination of automatic and manual means. The automatic method relied on lists of baby names from various languages. They found that female authorship has been steadily increasing from 1980 to 2008. Our work examines a much larger set of NLP papers (1965–2019), re-examines some of the questions raised in Vogel and Jurafsky (2012), and explores several new questions, especially on first author gender and disparities in citation. 3 Data We extracted a"
2020.lrec-1.109,W12-3202,0,0.666571,"from different areas of research within NLP etc. Mohammad (2020b) quantifies and examines gender gap in Natural Language Processing Research; specifically, the disparities in authorship and citations across gender. Some of the analyses presented in the papers associated with this project are also available as a series of blog posts online.7 2. Related Work This work is inspired by a vast amount of past research, including that on Google Scholar (Khabsa and Giles, 2014; Howland, 2010; Ordu˜na-Malea et al., 2014; Mart´ın-Mart´ın et al., 2018), on the analysis of NLP papers (Radev et al., 2016; Anderson et al., 2012; Bird et al., 2008; Schluter, 2018; Mariani et al., 2018; Qazvinian et al., 2013; Teich, 2010; Saggion et al., 2017), on citation intent (Aya et al., 2005; Teufel et al., 2006; Pham and Hoffmann, 2003; Nanba et al., 2011; Mohammad et al., 2009; Zhu et al., 2015), and on measuring scholarly impact (Ravenscroft et al., 2017; Priem and Hemminger, 2010; Bulaitis, 2017; Bos and Nitza, 2019; Ioannidis et al., 2019; Yogatama et al., 2011; Mishra et al., 2018). 3. Data We extracted information from both the ACL Anthology and Google Scholar in June 2019.8 The three subsections below describe the infor"
2020.lrec-1.109,bird-etal-2008-acl,0,0.187873,"f research within NLP etc. Mohammad (2020b) quantifies and examines gender gap in Natural Language Processing Research; specifically, the disparities in authorship and citations across gender. Some of the analyses presented in the papers associated with this project are also available as a series of blog posts online.7 2. Related Work This work is inspired by a vast amount of past research, including that on Google Scholar (Khabsa and Giles, 2014; Howland, 2010; Ordu˜na-Malea et al., 2014; Mart´ın-Mart´ın et al., 2018), on the analysis of NLP papers (Radev et al., 2016; Anderson et al., 2012; Bird et al., 2008; Schluter, 2018; Mariani et al., 2018; Qazvinian et al., 2013; Teich, 2010; Saggion et al., 2017), on citation intent (Aya et al., 2005; Teufel et al., 2006; Pham and Hoffmann, 2003; Nanba et al., 2011; Mohammad et al., 2009; Zhu et al., 2015), and on measuring scholarly impact (Ravenscroft et al., 2017; Priem and Hemminger, 2010; Bulaitis, 2017; Bos and Nitza, 2019; Ioannidis et al., 2019; Yogatama et al., 2011; Mishra et al., 2018). 3. Data We extracted information from both the ACL Anthology and Google Scholar in June 2019.8 The three subsections below describe the information extracted fr"
2020.lrec-1.109,W18-2504,0,0.688957,"and Nitza, 2019; Ioannidis et al., 2019; Yogatama et al., 2011; Mishra et al., 2018). 3. Data We extracted information from both the ACL Anthology and Google Scholar in June 2019.8 The three subsections below describe the information extracted from AA, the information extracted from Google Scholar, and how we aligned the information. 3.1. The ACL Anthology Data AA provides access to its data through its website and a github repository.9 The code for the ACL Anthology service is open source and available under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License (Gildea et al., 2018).10 The github repository includes a “data/xml/” directory that houses individual xml files for each of the proceedings. Table 1 shows the primary information we extracted from each of these xml files. Heuristics to obtain secondary information from AA: AA does not explicitly record certain attributes of the paper such as whether it is a main conference paper, a student research paper, a system demonstration paper, a shared task paper, a workshop paper, a tutorial abstract, etc. It also does not record whether it is a long paper or a short paper. Since such attributes allow for interesting ana"
2020.lrec-1.109,N09-1066,1,0.857883,"ed in the papers associated with this project are also available as a series of blog posts online.7 2. Related Work This work is inspired by a vast amount of past research, including that on Google Scholar (Khabsa and Giles, 2014; Howland, 2010; Ordu˜na-Malea et al., 2014; Mart´ın-Mart´ın et al., 2018), on the analysis of NLP papers (Radev et al., 2016; Anderson et al., 2012; Bird et al., 2008; Schluter, 2018; Mariani et al., 2018; Qazvinian et al., 2013; Teich, 2010; Saggion et al., 2017), on citation intent (Aya et al., 2005; Teufel et al., 2006; Pham and Hoffmann, 2003; Nanba et al., 2011; Mohammad et al., 2009; Zhu et al., 2015), and on measuring scholarly impact (Ravenscroft et al., 2017; Priem and Hemminger, 2010; Bulaitis, 2017; Bos and Nitza, 2019; Ioannidis et al., 2019; Yogatama et al., 2011; Mishra et al., 2018). 3. Data We extracted information from both the ACL Anthology and Google Scholar in June 2019.8 The three subsections below describe the information extracted from AA, the information extracted from Google Scholar, and how we aligned the information. 3.1. The ACL Anthology Data AA provides access to its data through its website and a github repository.9 The code for the ACL Anthology"
2020.lrec-1.109,2020.acl-main.464,1,0.372024,"resenting concluding remarks in Section 7 Additional work that uses the NLP Scholar dataset for specific analyses is presented in separate papers. Moham6 868 http://saifmohammad.com/WebPages/nlpscholar.html Attribute paper-id paper-title mad (2020a) presents a comprehensive overview of citations in NLP Literature. Specifically, it explores questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers published in different time spans? how well cited are papers from different areas of research within NLP etc. Mohammad (2020b) quantifies and examines gender gap in Natural Language Processing Research; specifically, the disparities in authorship and citations across gender. Some of the analyses presented in the papers associated with this project are also available as a series of blog posts online.7 2. Related Work This work is inspired by a vast amount of past research, including that on Google Scholar (Khabsa and Giles, 2014; Howland, 2010; Ordu˜na-Malea et al., 2014; Mart´ın-Mart´ın et al., 2018), on the analysis of NLP papers (Radev et al., 2016; Anderson et al., 2012; Bird et al., 2008; Schluter, 2018; Marian"
2020.lrec-1.109,2020.acl-main.702,1,0.837668,"resenting concluding remarks in Section 7 Additional work that uses the NLP Scholar dataset for specific analyses is presented in separate papers. Moham6 868 http://saifmohammad.com/WebPages/nlpscholar.html Attribute paper-id paper-title mad (2020a) presents a comprehensive overview of citations in NLP Literature. Specifically, it explores questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers published in different time spans? how well cited are papers from different areas of research within NLP etc. Mohammad (2020b) quantifies and examines gender gap in Natural Language Processing Research; specifically, the disparities in authorship and citations across gender. Some of the analyses presented in the papers associated with this project are also available as a series of blog posts online.7 2. Related Work This work is inspired by a vast amount of past research, including that on Google Scholar (Khabsa and Giles, 2014; Howland, 2010; Ordu˜na-Malea et al., 2014; Mart´ın-Mart´ın et al., 2018), on the analysis of NLP papers (Radev et al., 2016; Anderson et al., 2012; Bird et al., 2008; Schluter, 2018; Marian"
2020.lrec-1.109,D18-1301,0,0.11698,"LP etc. Mohammad (2020b) quantifies and examines gender gap in Natural Language Processing Research; specifically, the disparities in authorship and citations across gender. Some of the analyses presented in the papers associated with this project are also available as a series of blog posts online.7 2. Related Work This work is inspired by a vast amount of past research, including that on Google Scholar (Khabsa and Giles, 2014; Howland, 2010; Ordu˜na-Malea et al., 2014; Mart´ın-Mart´ın et al., 2018), on the analysis of NLP papers (Radev et al., 2016; Anderson et al., 2012; Bird et al., 2008; Schluter, 2018; Mariani et al., 2018; Qazvinian et al., 2013; Teich, 2010; Saggion et al., 2017), on citation intent (Aya et al., 2005; Teufel et al., 2006; Pham and Hoffmann, 2003; Nanba et al., 2011; Mohammad et al., 2009; Zhu et al., 2015), and on measuring scholarly impact (Ravenscroft et al., 2017; Priem and Hemminger, 2010; Bulaitis, 2017; Bos and Nitza, 2019; Ioannidis et al., 2019; Yogatama et al., 2011; Mishra et al., 2018). 3. Data We extracted information from both the ACL Anthology and Google Scholar in June 2019.8 The three subsections below describe the information extracted from AA, the infor"
2020.lrec-1.109,W06-1613,0,0.235789,"thorship and citations across gender. Some of the analyses presented in the papers associated with this project are also available as a series of blog posts online.7 2. Related Work This work is inspired by a vast amount of past research, including that on Google Scholar (Khabsa and Giles, 2014; Howland, 2010; Ordu˜na-Malea et al., 2014; Mart´ın-Mart´ın et al., 2018), on the analysis of NLP papers (Radev et al., 2016; Anderson et al., 2012; Bird et al., 2008; Schluter, 2018; Mariani et al., 2018; Qazvinian et al., 2013; Teich, 2010; Saggion et al., 2017), on citation intent (Aya et al., 2005; Teufel et al., 2006; Pham and Hoffmann, 2003; Nanba et al., 2011; Mohammad et al., 2009; Zhu et al., 2015), and on measuring scholarly impact (Ravenscroft et al., 2017; Priem and Hemminger, 2010; Bulaitis, 2017; Bos and Nitza, 2019; Ioannidis et al., 2019; Yogatama et al., 2011; Mishra et al., 2018). 3. Data We extracted information from both the ACL Anthology and Google Scholar in June 2019.8 The three subsections below describe the information extracted from AA, the information extracted from Google Scholar, and how we aligned the information. 3.1. The ACL Anthology Data AA provides access to its data through"
2020.lrec-1.109,D11-1055,0,0.465263,"Missing"
2020.lrec-1.195,N16-1095,1,0.753407,"ore basic than others, and these emotions are each to be treated as separate categories. We use the NRC Valence, Arousal, and Dominance (NRC VAD) lexicon (Mohammad, 2018a) and the NRC Emotion lexicon (Mohammad and Turney, 2013; Mohammad and Turney, 2010) to determine the emotion associations of the words in SOLO. These lexicons were created by manual annotation. The NRC VAD lexicon has valence, arousal, and dominance scores for over twenty thousand English terms, and it was created using a comparative annotation technique called Best-Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016). It has been shown to have high reliability (repeated annotations produce similar association scores). The NRC Emotion lexicon has binary (associated or not associated) scores for about fourteen thousand English terms (a subset of terms in the VAD lexicon) with eight basic emotions (joy, sadness, fear, anger, surprise, anticipation, disgust, and trust) as well as positive and negative sentiment. 3. Creating the SOLO Corpus We now describe how we collected tweets related to the state of being alone and created the SOLO corpus. 3.1. Query Term Selection After consulting with psychologists on ou"
2020.lrec-1.195,W10-0204,1,0.788112,"of valence, arousal, or dominance. For example, the word party indicates more positiveness than the word crying; terrible indicates more arousal than conversation; and hike indicates more dominance than abandoned. According to the basic emotions model (aka discrete model) (Ekman, 1992; Plutchik, 1980; Frijda, 1988), some emotions, such as joy, sadness, fear, etc., are more basic than others, and these emotions are each to be treated as separate categories. We use the NRC Valence, Arousal, and Dominance (NRC VAD) lexicon (Mohammad, 2018a) and the NRC Emotion lexicon (Mohammad and Turney, 2013; Mohammad and Turney, 2010) to determine the emotion associations of the words in SOLO. These lexicons were created by manual annotation. The NRC VAD lexicon has valence, arousal, and dominance scores for over twenty thousand English terms, and it was created using a comparative annotation technique called Best-Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016). It has been shown to have high reliability (repeated annotations produce similar association scores). The NRC Emotion lexicon has binary (associated or not associated) scores for about fourteen thousand English terms (a s"
2020.lrec-1.195,P18-1017,1,0.941495,", when comparing the meanings of two words, we can compare their degrees of valence, arousal, or dominance. For example, the word party indicates more positiveness than the word crying; terrible indicates more arousal than conversation; and hike indicates more dominance than abandoned. According to the basic emotions model (aka discrete model) (Ekman, 1992; Plutchik, 1980; Frijda, 1988), some emotions, such as joy, sadness, fear, etc., are more basic than others, and these emotions are each to be treated as separate categories. We use the NRC Valence, Arousal, and Dominance (NRC VAD) lexicon (Mohammad, 2018a) and the NRC Emotion lexicon (Mohammad and Turney, 2013; Mohammad and Turney, 2010) to determine the emotion associations of the words in SOLO. These lexicons were created by manual annotation. The NRC VAD lexicon has valence, arousal, and dominance scores for over twenty thousand English terms, and it was created using a comparative annotation technique called Best-Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016). It has been shown to have high reliability (repeated annotations produce similar association scores). The NRC Emotion lexicon has binary"
2020.lrec-1.196,W09-4104,0,0.011334,"pidly from grades 8 to 11 compared to girls (Larson et al., 2002; Weinstein et al., 2007). These findings are contrasted by the mental health literature which suggests that adolescent girls are more at-risk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corporation hosts a website that publishes children’s poems.3 It provides school-age children a platform to submit poetry which becomes openly accessible to anybody"
2020.lrec-1.196,D16-1126,0,0.0298084,"ge differently as a function of gender. Previous studies have shown that boys have lower overall valence and their valence declines more rapidly from grades 8 to 11 compared to girls (Larson et al., 2002; Weinstein et al., 2007). These findings are contrasted by the mental health literature which suggests that adolescent girls are more at-risk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corporation hosts a website that"
2020.lrec-1.196,W15-3703,0,0.0243247,"isk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corporation hosts a website that publishes children’s poems.3 It provides school-age children a platform to submit poetry which becomes openly accessible to anybody on the World Wide Web. The poems are mostly in English. The exact dates of publication are not available, but we estimate that they are roughly from the year 2000 onwards (based on dates in the bodies of"
2020.lrec-1.196,W12-2502,0,0.0284685,"all valence and their valence declines more rapidly from grades 8 to 11 compared to girls (Larson et al., 2002; Weinstein et al., 2007). These findings are contrasted by the mental health literature which suggests that adolescent girls are more at-risk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corporation hosts a website that publishes children’s poems.3 It provides school-age children a platform to submit poetry"
2020.lrec-1.196,W16-5614,0,0.0577904,"Missing"
2020.lrec-1.196,W15-0702,0,0.0213732,"t boys have lower overall valence and their valence declines more rapidly from grades 8 to 11 compared to girls (Larson et al., 2002; Weinstein et al., 2007). These findings are contrasted by the mental health literature which suggests that adolescent girls are more at-risk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corporation hosts a website that publishes children’s poems.3 It provides school-age children a pl"
2020.lrec-1.196,W10-0204,1,0.642664,"t have been scored on valence (0 = maximally unpleasant, 1 = maximally pleasant), arousal (0 = maximally calm/sluggish, 1 = maximally active/intense), and dominance (0 = maximally weak, 1 = maximally powerful). As an example, the word nice has a valence of .93, an arousal of .44, and dominance of .65, whereas the word despair has a valence of .11, an arousal of .79, and dominance of .25. Table 1 (last column) shows the mean number of VAD lexicon words in poems by grade.8 The NRC EI lexicon v0.5 contains about six thousand words from the NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013; Mohammad and Turney, 2010) that were marked as being associated with anger, fear, sadness, and joy.9 Each word comes with intensity ratings for the associated emotion—scores between 0 (lowest intensity) and 1 (highest intensity). For instance, hate is rated .83 on anger intensity, scare is rated .84 on fear intensity, tragic is rated .96 on sadness intensity, and happiness is rated .98 on joy intensity. We only analyzed poems that included at least five words from the NRC VAD lexicon to ensure that each poem included a sufficient number of words for computing averages. This was the minimum cut-off and resulted in a dat"
2020.lrec-1.196,P18-1017,1,0.841488,"Our Primary Focus of Analysis: What are the developmental trends in emotions—how do the emotion associations of words in poems change from grade 1 to grade 12? Motivation: To identify and better understand stages in children’s development where they might be more or less prone to emotional distress. In order to address these questions, we needed a method to determine the emotions associated with words, a method to Emotion Words in PoKi Tokenization of PoKi resulted in about 1.1M word tokens and about 56K unique word types. We used the NRC Valence, Arousal, and Dominance (NRC VAD) lexicon v1 (Mohammad, 2018a) and the NRC Emotion Intensity (NRC EI) lexicon v0.5 (Mohammad, 2018b) to determine the emotion associations of the words.7 Although we share a lemmatized version of PoKi, we opted to analyze the nonlemmatized version because the NRC VAD and EI lexica cover most morphological forms of common words. The NRC VAD lexicon contains about twenty thousand commonly used English words that have been scored on valence (0 = maximally unpleasant, 1 = maximally pleasant), arousal (0 = maximally calm/sluggish, 1 = maximally active/intense), and dominance (0 = maximally weak, 1 = maximally powerful). As an"
2020.lrec-1.196,2020.acl-main.702,1,0.882576,"Missing"
2020.lrec-1.196,W15-5937,0,0.0197586,"lence declines more rapidly from grades 8 to 11 compared to girls (Larson et al., 2002; Weinstein et al., 2007). These findings are contrasted by the mental health literature which suggests that adolescent girls are more at-risk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corporation hosts a website that publishes children’s poems.3 It provides school-age children a platform to submit poetry which becomes openly a"
2020.lrec-1.196,K18-1024,0,0.0279158,"Emotions may change differently as a function of gender. Previous studies have shown that boys have lower overall valence and their valence declines more rapidly from grades 8 to 11 compared to girls (Larson et al., 2002; Weinstein et al., 2007). These findings are contrasted by the mental health literature which suggests that adolescent girls are more at-risk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corpo"
2020.lrec-1.196,P19-3005,0,0.0172145,"of gender. Previous studies have shown that boys have lower overall valence and their valence declines more rapidly from grades 8 to 11 compared to girls (Larson et al., 2002; Weinstein et al., 2007). These findings are contrasted by the mental health literature which suggests that adolescent girls are more at-risk for depression and anxiety relative to boys (Nolen-Hoeksema and Girgus, 1994). There is growing interest in working with poetry in the NLP research community. Much of this work can be divided into two kinds: automatic poetry generation (Yi et al., 2018; Ghazvininejad et al., 2016; Zhipeng et al., 2019) and poetry analysis (McCurdy et al., 2015; Kao and Jurafsky, 2012; Rakshit et al., 2015; Fang et al., 2009). Much of the analysis work has looked at aspects of poems such as imagery, rhyming elements, and meter, but some work has looked at sentiment in poems as well (Kao and Jurafsky, 2012; Hou and Frank, 2015). However, none of this work has examined poems written by children. We hope that the availability of PoKi will encourage more computational work on child language in poems. 3. A Dataset of Poems Written by Children The Scholastic Corporation hosts a website that publishes children’s po"
2020.lrec-1.377,D17-1118,0,0.0424164,"Missing"
2020.lrec-1.377,P16-1141,0,0.168789,"ecies over time, there is a natural selection of words over time (Darwin, 1968; Darwin, 2003). In this analogy, word type maps to a species, and word frequency maps to the species population. Thus the creation and use of a new word is akin to the birth of a new species, the wide-spread use of a word is akin to the thriving of a species, and the complete lack of use of a word is akin to the extinction of a species. Over the last few years, there has been a spurt of research on historical language change, especially on how the meaning of a word has changed over time (Mihalcea and Nastase, 2012; Hamilton et al., 2016a; Bamler and Mandt, 2017; Zimmermann, 2019; Jawahar and Seddah, 2019)— this might be considered as a mutation. Much of this work is a direct result of the availability of resources such as the Google Books Ngrams Corpus (GBNC) (Google, 2012). However, there is very little computational work on the natural selection of words; that is, how words compete to represent a meaning, and how, over time, some words become more successful, whereas others become less successful. In this paper, we present a large dataset to foster computational studies on the natural selection of English words. Specifical"
2020.lrec-1.377,D16-1229,0,0.044671,"Missing"
2020.lrec-1.377,W19-4705,0,0.0486469,"Darwin, 1968; Darwin, 2003). In this analogy, word type maps to a species, and word frequency maps to the species population. Thus the creation and use of a new word is akin to the birth of a new species, the wide-spread use of a word is akin to the thriving of a species, and the complete lack of use of a word is akin to the extinction of a species. Over the last few years, there has been a spurt of research on historical language change, especially on how the meaning of a word has changed over time (Mihalcea and Nastase, 2012; Hamilton et al., 2016a; Bamler and Mandt, 2017; Zimmermann, 2019; Jawahar and Seddah, 2019)— this might be considered as a mutation. Much of this work is a direct result of the availability of resources such as the Google Books Ngrams Corpus (GBNC) (Google, 2012). However, there is very little computational work on the natural selection of words; that is, how words compete to represent a meaning, and how, over time, some words become more successful, whereas others become less successful. In this paper, we present a large dataset to foster computational studies on the natural selection of English words. Specifically, the dataset includes sets of synonymous monosemous words and their"
2020.lrec-1.377,P12-2051,0,0.213851,"n in the plant and animal species over time, there is a natural selection of words over time (Darwin, 1968; Darwin, 2003). In this analogy, word type maps to a species, and word frequency maps to the species population. Thus the creation and use of a new word is akin to the birth of a new species, the wide-spread use of a word is akin to the thriving of a species, and the complete lack of use of a word is akin to the extinction of a species. Over the last few years, there has been a spurt of research on historical language change, especially on how the meaning of a word has changed over time (Mihalcea and Nastase, 2012; Hamilton et al., 2016a; Bamler and Mandt, 2017; Zimmermann, 2019; Jawahar and Seddah, 2019)— this might be considered as a mutation. Much of this work is a direct result of the availability of resources such as the Google Books Ngrams Corpus (GBNC) (Google, 2012). However, there is very little computational work on the natural selection of words; that is, how words compete to represent a meaning, and how, over time, some words become more successful, whereas others become less successful. In this paper, we present a large dataset to foster computational studies on the natural selection of En"
2020.lrec-1.377,W19-4703,0,0.0709172,"words over time (Darwin, 1968; Darwin, 2003). In this analogy, word type maps to a species, and word frequency maps to the species population. Thus the creation and use of a new word is akin to the birth of a new species, the wide-spread use of a word is akin to the thriving of a species, and the complete lack of use of a word is akin to the extinction of a species. Over the last few years, there has been a spurt of research on historical language change, especially on how the meaning of a word has changed over time (Mihalcea and Nastase, 2012; Hamilton et al., 2016a; Bamler and Mandt, 2017; Zimmermann, 2019; Jawahar and Seddah, 2019)— this might be considered as a mutation. Much of this work is a direct result of the availability of resources such as the Google Books Ngrams Corpus (GBNC) (Google, 2012). However, there is very little computational work on the natural selection of words; that is, how words compete to represent a meaning, and how, over time, some words become more successful, whereas others become less successful. In this paper, we present a large dataset to foster computational studies on the natural selection of English words. Specifically, the dataset includes sets of synonymous"
2021.acl-long.210,N19-1050,1,0.83103,"the first dataset of 6000 English language Reddit comments that has finegrained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive) – normative offensiveness ratings for the comments. For the first time, we use comparative annotations to detect offensive language. In its simplest form, comparative annotations involve giving the annotators two instances at a time, and asking which exhibits the property of interest to a greater extent. This alleviates several annotation biases present in standard rating scales, such as scale-region bias (Presser and Schuman, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017). However, instead of needing to annotate N instances, one now needs to annotate N 2 instance pairs—which can be prohibitive. Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). Surveys by Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Vidgen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); David"
2021.acl-long.210,S19-2007,0,0.0334653,"s), the reduced-range dataset. We discuss the models’ performance on this dataset in the next section. HateBERT HateBERT (Caselli et al., 2020b) is a version of BERT pretrained for abusive language detection in English. HateBERT was trained on RAL-E, a large dataset of English language Reddit comments from communities banned for being offensive or hateful. HateBERT has been shown to outperform the general purpose BERT model on the offensive language detection task when finetuned on popular datasets such as OffensEval 2019 (Zampieri et al., 2019), AbusEval (Caselli et al., 2020a), and HatEval (Basile et al., 2019). We fine-tuned HateBERT on Ruddit and its variants. The experimental setup for this model is the same as that described for the BERT model. 6.2 6 Computational Modeling In this section, we present benchmark experiments on Ruddit and its variants by implementing some commonly used model architectures. The task of the models was to predict the offensiveness score of a given comment. We performed 5-fold crossvalidation for each of the models.6 6.1 Models Bidirectional LSTM We fed pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014) to a 2-layered BiLSTM to obtain a sentenc"
2021.acl-long.210,S12-1047,1,0.755705,"annotation results in inequalities for 5 of the 6 item pairs. For example, a 4-tuple with items A, B, C, and D, where A is the best, and D is the worst, results in inequalities: A&gt;B, A&gt;C, A&gt;D, B&gt;D, and C&gt;D. Real-valued scores of associations are calculated between the items and the property of interest from the best–worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014). The scores can be used to rank items by the degree of association with the property of interest. Within the NLP community, BWS has thus far been used only for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Using BWS, we create the first dataset with degree of offensiveness scores for social media comments. 3 Data collection and sampling We extracted Reddit data from the Pushshift repository (Baumgartner et al., 2020) using Google BigQuery. Reddit is a social news aggregation, web content rating, and discussion website. It contains forums calle"
2021.acl-long.210,P17-2074,1,0.933368,"that has finegrained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive) – normative offensiveness ratings for the comments. For the first time, we use comparative annotations to detect offensive language. In its simplest form, comparative annotations involve giving the annotators two instances at a time, and asking which exhibits the property of interest to a greater extent. This alleviates several annotation biases present in standard rating scales, such as scale-region bias (Presser and Schuman, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017). However, instead of needing to annotate N instances, one now needs to annotate N 2 instance pairs—which can be prohibitive. Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). Surveys by Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Vidgen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); Davidson et al. (2017); Founta et al. (2018) created datasets based on Twit"
2021.acl-long.210,N16-1095,1,0.888787,"Missing"
2021.acl-long.210,2020.alw-1.17,0,0.0310683,"ffensive nor hate-speech and Founta et al. (2018) as abusive, hateful, normal, spam. Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Kiritchenko and Nejadgholi (2020) summarize the different definitions. However, these categories have significant overlaps with each other, creating ill-defined boundaries, thus introducing ambiguity and annotation inconsistency (Founta et al., 2018). A further challenge is that after encountering several highly offensive comments, an annotator might find subsequent moderately offensive comments to not be offensive (de-sensitization) (Kurrek et al., 2020; Soral et al., 2018). At the same time, existing approaches do not take into account that comments can be offensive to a different degree. Knowing the degree of offensiveness of a comment has practical implications, when taking action against inappropriate behaviour online, as it allows for a more fine-grained analysis and prioritization in moderation. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that i"
2021.acl-long.210,2020.alw-1.9,0,0.0473109,"Missing"
2021.acl-long.210,P18-1017,1,0.850033,"Emotions are highly representative of one’s mental state, which in turn are associated with their behaviour (Poria et al., 2019). For example, Jay and Janschewitz (2008) show that people tend to swear when they are angry, frustrated or anxious. Studies have shown that the primary dimensions of emotion are valence, arousal, and dominance (VAD) (Osgood et al., 1957; Russell, 1980, 2702 2003). Valence is the positive–negative or pleasure– displeasure dimension. Arousal is the excited– calm or active–passive dimension. Dominance is powerful–weak or ‘have full control’–‘have no control’ dimension (Mohammad, 2018). To boost the representation of offensive and emotional comments in our dataset, we up-sampled comments that included low-valence (highly negative) words and those that included high-arousal words (as per the NRC VAD lexicon (Mohammad, 2018)). The manually constructed NRC VAD lexicon includes 20,000 English words, each with a real-valued score between 0 and 1 in the V, A, D dimensions. In order to do this upsampling, we first defined the valence score of each comment as the average valence score of the negative words within the comment (A negative word is defined as a word with a valence scor"
2021.acl-long.210,S17-1007,1,0.938177,"negative consequences for the victim’s mental health (Munro, 2011). Automated offensive language detection has thus been gaining interest in the NLP community, as a promising direction to better understand the nature and spread of such content. There are several challenges in the automatic detection of offensive language (Wiedemann et al., 2018). The NLP community has adopted various definitions for offensive language, classifying it into specific categories. For example, Waseem and ∗ Both authors contributed equally. Hovy (2016) classified comments as racist, sexist, neither; Davidson et al. (2017) as hate-speech, offensive but not hate-speech, neither offensive nor hate-speech and Founta et al. (2018) as abusive, hateful, normal, spam. Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Kiritchenko and Nejadgholi (2020) summarize the different definitions. However, these categories have significant overlaps with each other, creating ill-defined boundaries, thus introducing ambiguity and annotation inconsistency (Founta et al., 2018). A further challenge is that after encountering several highly offensive comments, an annotator might find subsequent moderately of"
2021.acl-long.210,L18-1030,1,0.849767,"calculated between the items and the property of interest from the best–worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014). The scores can be used to rank items by the degree of association with the property of interest. Within the NLP community, BWS has thus far been used only for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Using BWS, we create the first dataset with degree of offensiveness scores for social media comments. 3 Data collection and sampling We extracted Reddit data from the Pushshift repository (Baumgartner et al., 2020) using Google BigQuery. Reddit is a social news aggregation, web content rating, and discussion website. It contains forums called subreddits dedicated to specific topics. Users can make a post on the subreddit to start a discussion. Users can comment on existing posts or comments to participate in the discussion. As users can also reply to a comment, the entire discussion has a hi"
2021.acl-long.210,D18-1302,0,0.0188152,"gen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); Davidson et al. (2017); Founta et al. (2018) created datasets based on Twitter data. Due to prevalence of the non-offensive class in naturallyoccurring data (Waseem, 2016; Founta et al., 2018), the authors devised techniques to boost the presence of the offensive class in the dataset. Waseem and Hovy (2016) used terms frequently occurring in offensive tweets, while Davidson et al. (2017) used a list of hate-related terms to extract offensive tweets from the Twitter search API. Park et al. (2018), Wiegand et al. (2019), and Davidson et al. (2019) show that the Waseem and Hovy (2016) dataset exhibits topic bias and author bias due to the employed sampling strategy. Founta et al. (2018) boosted the representation of offensive class in their dataset by analysing the sentiment of the tweets and checking for the presence of offensive terms. In our work, we employ a hybrid approach, selecting our data in three ways: specific topics, emotion-related key-words, and random sampling. Past work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words"
2021.acl-long.210,D14-1162,0,0.0864822,"busEval (Caselli et al., 2020a), and HatEval (Basile et al., 2019). We fine-tuned HateBERT on Ruddit and its variants. The experimental setup for this model is the same as that described for the BERT model. 6.2 6 Computational Modeling In this section, we present benchmark experiments on Ruddit and its variants by implementing some commonly used model architectures. The task of the models was to predict the offensiveness score of a given comment. We performed 5-fold crossvalidation for each of the models.6 6.1 Models Bidirectional LSTM We fed pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014) to a 2-layered BiLSTM to obtain a sentence representation (using a concatenation of the last hidden state from the forward and backward direction). This sentence representation was then passed to a linear layer with a tanh activation to produce a score between −1 and 1. We used Mean Squared Error (MSE) loss as the objective function, Adam with 0.001 learning rate as the optimizer, hidden dimension of 256, batch size of 32, and a dropout of 0.5. The model was trained for 7 epochs. BERT We fine-tuned BERTbase (Devlin et al., 2019). We added a regression head containing a linear layer to the pre"
2021.acl-long.210,P19-1163,0,0.049151,"Missing"
2021.acl-long.210,2020.acl-main.486,0,0.12141,"lysing the sentiment of the tweets and checking for the presence of offensive terms. In our work, we employ a hybrid approach, selecting our data in three ways: specific topics, emotion-related key-words, and random sampling. Past work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words, taboo words, or hate terms) and implicitly offensive (those that do not include profanity) (Waseem et al., 2017; Caselli et al., 2020a; Wiegand et al., 2021). Some other past work has defined explicitly and implicitly offensive instances a little differently: Sap et al. (2020) considered factors such as obviousness, intent to offend and biased implications, Breitfeller et al. (2019) considered factors such as the context and the person annotating the instance, and Razo and K¨ubler (2020) considered the kind of lexicon used. Regardless of the exact definition, implicit offensive language, due to a lack of lexical cues, is harder to classify not only for computational models, but also for humans. In our work, we consider implicitly offensive comments as those offensive comments that do not contain any swear words. By eliminating different offensiveness categories, tr"
2021.acl-long.210,W17-1101,0,0.177044,"community, as a promising direction to better understand the nature and spread of such content. There are several challenges in the automatic detection of offensive language (Wiedemann et al., 2018). The NLP community has adopted various definitions for offensive language, classifying it into specific categories. For example, Waseem and ∗ Both authors contributed equally. Hovy (2016) classified comments as racist, sexist, neither; Davidson et al. (2017) as hate-speech, offensive but not hate-speech, neither offensive nor hate-speech and Founta et al. (2018) as abusive, hateful, normal, spam. Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Kiritchenko and Nejadgholi (2020) summarize the different definitions. However, these categories have significant overlaps with each other, creating ill-defined boundaries, thus introducing ambiguity and annotation inconsistency (Founta et al., 2018). A further challenge is that after encountering several highly offensive comments, an annotator might find subsequent moderately offensive comments to not be offensive (de-sensitization) (Kurrek et al., 2020; Soral et al., 2018). At the same time, existing approaches do not take into account that c"
2021.acl-long.210,W19-3509,0,0.054317,"Missing"
2021.acl-long.210,S19-2010,0,0.0352511,"ents with scores from −0.5 to 0.5. We call this subset (of 5151 comments), the reduced-range dataset. We discuss the models’ performance on this dataset in the next section. HateBERT HateBERT (Caselli et al., 2020b) is a version of BERT pretrained for abusive language detection in English. HateBERT was trained on RAL-E, a large dataset of English language Reddit comments from communities banned for being offensive or hateful. HateBERT has been shown to outperform the general purpose BERT model on the offensive language detection task when finetuned on popular datasets such as OffensEval 2019 (Zampieri et al., 2019), AbusEval (Caselli et al., 2020a), and HatEval (Basile et al., 2019). We fine-tuned HateBERT on Ruddit and its variants. The experimental setup for this model is the same as that described for the BERT model. 6.2 6 Computational Modeling In this section, we present benchmark experiments on Ruddit and its variants by implementing some commonly used model architectures. The task of the models was to predict the offensiveness score of a given comment. We performed 5-fold crossvalidation for each of the models.6 6.1 Models Bidirectional LSTM We fed pre-trained 300 dimensional GloVe word embedding"
2021.acl-long.210,W16-5618,0,0.237183,"deration. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2700–2717 August 1–6, 2021. ©2021 Association for Computational Linguistics Lastly, existing datasets consider offensive comments in isolation from the wider conversation of which they are a"
2021.acl-long.210,W17-3012,0,0.0837158,"comments can be offensive to a different degree. Knowing the degree of offensiveness of a comment has practical implications, when taking action against inappropriate behaviour online, as it allows for a more fine-grained analysis and prioritization in moderation. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Internation"
2021.acl-long.210,N16-2013,0,0.0362638,"n, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017). However, instead of needing to annotate N instances, one now needs to annotate N 2 instance pairs—which can be prohibitive. Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). Surveys by Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Vidgen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); Davidson et al. (2017); Founta et al. (2018) created datasets based on Twitter data. Due to prevalence of the non-offensive class in naturallyoccurring data (Waseem, 2016; Founta et al., 2018), the authors devised techniques to boost the presence of the offensive class in the dataset. Waseem and Hovy (2016) used terms frequently occurring in offensive tweets, while Davidson et al. (2017) used a list of hate-related terms to extract offensive tweets from the Twitter search API. Park et al. (2018), Wiegand et al. (2019), and Davidson et al. (2019) show that the Waseem and Hovy (2016) dataset e"
2021.acl-long.210,2021.eacl-main.27,0,0.372393,"and prioritization in moderation. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2700–2717 August 1–6, 2021. ©2021 Association for Computational Linguistics Lastly, existing datasets consider offensive comments in isolation from the wider conversation of w"
2021.acl-long.210,N19-1060,0,0.22765,"representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2700–2717 August 1–6, 2021. ©2021 Association for Computational Linguistics Lastly, existing datasets consider offensive comments in isolation from the wider conversation of which they are a part. Offensive langua"
D07-1060,W02-2006,0,0.0324426,"Missing"
D07-1060,P06-1046,0,0.0283785,"ombining written text with a published thesaurus to measure distance between concepts (or word senses) using distributional measures, thereby eliminating sense-conflation and achieving results better than the simple word-distance measures and indeed also most of the WordNet-based semantic measures. We called these measures distributional measures of concept-distance. Concept-distance 2 LSA is especially expensive as singular value decomposition, a key component for dimensionality reduction, requires computationally intensive matrix operations; making it less scalable to large amounts of text (Gorman and Curran, 2006). 572 measures can be used to measure distance between a word pair by choosing the distance between their closest senses. Thus, even though ‘children’s recreation’ is the predominant sense of play, the ‘drama’ sense is much closer to actor and so their distance will be chosen. These distributional conceptdistance approaches need to create only V × C cooccurrence and C × C distance matrices, where C is the number of categories or senses (usually about 1000). It should also be noted that unlike the best WordNet-based measures, distributional measures (both word- and concept-distance ones) can be"
D07-1060,I05-1067,1,0.955571,"ke resources that these methods require do not exist for most of the 3000–6000 languages in existence today and they are costly to create. In this paper, we introduce cross-lingual distributional measures of concept-distance, or simply cross-lingual measures, that determine the distance between a word pair belonging to a resource-poor language using a knowledge source in a resourcerich language and a bilingual lexicon3 . We will use the cross-lingual measures to calculate distances between German words using an English thesaurus and a German corpus. Although German is not resourcepoor per se, Gurevych (2005) has observed that the German wordnet GermaNet (Kunze, 2004) (about 60,000 synsets) is less developed than the English WordNet (Fellbaum, 1998) (about 117,000 synsets) with respect to the coverage of lexical items and lexical semantic relations represented therein. On the other hand, substantial raw corpora are available for the German language. Crucially for our evaluation, the existence of GermaNet allows comparison of our cross-lingual approach with monolingual ones. 2 Monolingual Distributional Measures In order to set the context for cross-lingual conceptdistance measures (Section 3), we"
D07-1060,O97-1002,0,0.740037,"guistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an English thesaurus to create English– German distributional profiles of concepts, which in turn will be used to measure the semantic distance between German words. Two classes of methods have been used in determining semantic distance. Semantic measures of concept-distance, such as those of Jiang and Conrath (1997) and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it (see Budanitsky and Hirst (2006) for a survey). Distributional measures of word-distance1 , such as cosine and α-skew divergence (Lee, 2001), deem 1 Many distributional approaches represent the sets of contexts of the target words as points in multidimensional cooccurrence space or as co-occurrence distributions. A measure, such as cosine, that captures vector distance or a measure, such as α-skew divergence, that captures distance between distributions"
D07-1060,S07-1004,0,0.0788107,"Missing"
D07-1060,W04-2607,1,0.78437,"e synset and in synsets close to it in the network. 576 (2005) and Zesch et al. (2007) asked native German speakers to mark two different sets of German word pairs with distance values. Set 1 (Gur65) consists of a German translation of the English Rubenstein and Goodenough (1965) dataset. It has 65 noun– noun word pairs. Set 2 (Gur350) is a larger dataset containing 350 word pairs made up of nouns, verbs, and adjectives. The semantically close word pairs in Gur65 are mostly synonyms or hypernyms (hyponyms) of each other, whereas those in Gur350 have both classical and non-classical relations (Morris and Hirst, 2004) with each other. Details of these semantic distance benchmarks13 are summarized in Table 2. Inter-subject correlations are indicative of the degree of ease in annotating the datasets. 4.1.2 Results and Discussion Word-pair distances determined using different distance measures are compared in two ways with the two human-created benchmarks. The rank ordering of the pairs from closest to most distant is evaluated with Spearman’s rank order correlation ρ; the distance judgments themselves are evaluated with Pearson’s correlation coefficient r. The higher the correlation, the more accurate the me"
D07-1060,2003.mtsummit-papers.42,0,0.0345828,"n raw text and possibly some shallow syntactic processing. They do not require any other manually-created resource, and tend to have a higher coverage. However, by themselves they perform poorly when compared to semantic measures (Mohammad and Hirst, 2006b) because when given a target word pair we usually need the distance between their closest senses, but distributional measures of word-distance tend to conflate the distances between all possible sense pairs. Latent semantic analysis (LSA) (Landauer et al., 1998) has also been used to measure distributional distance with encouraging results (Rapp, 2003). However, it too measures the distance between words and not senses. Further, the dimensionality reduction inherent to LSA has the effect of making the predominant sense more dominant while de-emphasizing the other senses. Therefore, an LSA-based approach will also conflate information from the different senses, and even more emphasis will be placed on the predominant senses. Given the semantically close target nouns play and actor, for example, a distributional measure will give a score that is some sort of a dominance-based average of the distances between their senses. The noun play has th"
D07-1060,P98-2127,0,0.234235,"Missing"
D07-1060,P04-1036,0,0.211979,"cooccurs with any word. A statistic such as PMI can then give the strength of association between w and c. with each of its senses is summed. The sense that has the highest cumulative association is chosen as the intended sense. A new bootstrapped WCCM is created such that each cell mi j , corresponding to en word wen i and concept c j , is populated with the en number of times wi co-occurs with any word used in sense cen j . Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain nearupper-bound results in the task of determining word sense dominance. Unlike the McCarthy et al. (2004) dominance system, our approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sensedistributed text5 . In Mohammad and Hirst (2006a), the DPC-based monolingual distributional measures of concept-distance were used to rank word pairs by their semantic similarity and to correct realword spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the distributional concept-distance measures performed better than all WordNet-based measures as well, excep"
D07-1060,P06-1040,0,0.0290057,". Nachbildung (replica) b. Doppelkinn (double chin) d. Zweitschrift (copy) Our approach to evaluating distance measures fol14 In Table 3, all values are statistically significant at the 0.01 level (2-tailed), except for the one in italic (0.212), which is significant at the 0.05 level (2-tailed). 15 English translations are in parentheses. 577 lows that of Jarmasz and Szpakowicz (2003), who evaluated semantic similarity measures through their ability to solve synonym problems (80 TOEFL (Landauer and Dumais, 1997), 50 ESL (Turney, 2001), and 300 (English) Reader’s Digest Word Power questions). Turney (2006) used a similar approach to evaluate the identification of semantic relations, with 374 college-level multiple-choice word analogy questions. The Reader’s Digest Word Power (RDWP) benchmark for German consists of 1072 of these word-choice problems collected from the January 2001 to December 2005 issues of the Germanlanguage edition (Wallace and Wallace, 2005). We discarded 44 problems that had more than one correct answer, and 20 problems that used a phrase instead of a single term as the target. The remaining 1008 problems form our evaluation dataset, which is significantly larger than any of"
D07-1060,E06-1016,1,0.581452,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,W06-1605,1,0.605764,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,N07-2052,1,0.781373,"Missing"
D07-1060,C92-2070,0,\N,Missing
D07-1060,J06-1003,1,\N,Missing
D08-1103,C92-2082,0,0.0701406,"Methods in Natural Language Processing, pages 982–991, c Honolulu, October 2008. 2008 Association for Computational Linguistics Lexicons of pairs of words that native speakers consider antonyms have been created for certain languages, but their coverage has been limited. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that are commonly considered antonym pairs, and they remain unrecorded. Even though a number of computational approaches have been proposed for semantic closeness, and some for hypernymy–hyponymy (Hearst, 1992), measures of antonymy have been less successful. To some extent, this is because antonymy is not as well understood as other classical lexical-semantic relations. We first very briefly summarize insights and intuitions about this phenomenon, as proposed by linguists and lexicographers (Section 2). We discuss related work (Section 3). We describe the resources we use (Section 4) and present experiments that examine the manifestation of antonymy in text (Sections 5 and 6). We then propose a new empirical approach to determine the degree of antonymy between two words (Section 7). We compiled a d"
D08-1103,J91-1001,0,0.842435,"rgeon) are also semantically related, but terms that are semantically related may not always be semantically similar (plane–sky, surgeon–scalpel). Antonymy is unique among these relations because it simultaneously conveys both a sense of closeness and of distance (Cruse, 1986). Antonymous concepts are semantically related but not semantically similar. 3 Related work Charles and Miller (1989) proposed that antonyms occur together in a sentence more often than chance. This is known as the co-occurrence hypothesis. They also showed that this was empirically true for four adjective antonym pairs. Justeson and Katz (1991) demonstrated the co-occurrence hypothesis for 35 prototypical antonym pairs (from an original set of 39 antonym pairs compiled by Deese (1965)) and also for an additional 22 frequent antonym pairs. All of these pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb, adjective, and adverb pairs (noun–noun, noun–verb, noun–adjective, verb–adverb and so on) pertaining to 18 concepts (for example, lose(v)–gain(n) and loss(n)–gain(n), where lose(v) and loss(n) pertain to the concept of “failing to have/maintain”). However, non-antonymous semantically related words su"
D08-1103,P98-2127,0,0.292578,"c1  c2  ∑w  T  c1   T  c2  I c1  w  I c2  w   (2) ∑w  T  c1  I c1  w  ∑w  T  c2  I c2  w   Here T c  is the set of all words w that have positive pointwise mutual information with the thesaurus category c (I c  w  0). We adopt this method for use in our approach to determine word-pair antonymy. 5 The co-occurrence hypothesis of antonyms Co-occurrence statistics The distributional hypothesis of closeness states that words that occur in similar contexts tend to be semantically close (Firth, 1957). Distributional measures of distance, such as those proposed by Lin (1998), quantify how similar the two sets of contexts of a target word pair are. Equation 1 is a modified form of Lin’s measure that ignores syntactic dependencies and hence it estimates semantic relatedness rather than semantic similarity: Lin w1  w2  ∑w  T  w1   T  w2  I w1  w  I w2  w   (1) ∑w  T  w1  I w1  w  ∑w  T  w2  I w2  w   Here w1 and w2 are the target words; I x  y  is the pointwise mutual information between x and y; and T x  is the set of all words y that have positive pointwise mutual information with the word x (I x  y  0). Mohammad and Hirst (20"
D08-1103,P02-1047,0,0.084301,"ascend–descend, shout–whisper). In its broadest Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automatic creation of a thesaurus it is necessary to distinguish nearsynonyms from word pairs that are semantically contrasting. Measures of distributional similarity fail to do so. Detecting antonymous words is not sufficient to solve most of these problems, but it remains a crucial, and largely unsolved, component"
D08-1103,P08-1118,0,0.078719,"Missing"
D08-1103,H05-1067,0,0.0321323,"ining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automatic creation of a thesaurus it is necessary to distinguish nearsynonyms from word pairs that are semantically contrasting. Measures of distributional similarity fail to do so. Detecting antonymous words is not sufficient to solve most of these problems, but it remains a crucial, and largely unsolved, component. 982 Proceedings of the 2008 Conference on Empirical Methods in Natural La"
D08-1103,W06-1605,1,0.670245,"proposed by Lin (1998), quantify how similar the two sets of contexts of a target word pair are. Equation 1 is a modified form of Lin’s measure that ignores syntactic dependencies and hence it estimates semantic relatedness rather than semantic similarity: Lin w1  w2  ∑w  T  w1   T  w2  I w1  w  I w2  w   (1) ∑w  T  w1  I w1  w  ∑w  T  w2  I w2  w   Here w1 and w2 are the target words; I x  y  is the pointwise mutual information between x and y; and T x  is the set of all words y that have positive pointwise mutual information with the word x (I x  y  0). Mohammad and Hirst (2006) showed that these distributional word-distance measures perform poorly when compared with WordNet-based concept-distance measures. They argued that this is because the word-distance measures clump together the contexts of the different senses of the target words. They proposed a way to obtain distributional distance between word senses, using any of the distributional measures such as cosine or that proposed by Lin, and showed that this approach performed markedly better than the traditional worddistance approach. They used thesaurus categories 985 As a first step towards formulating our appr"
D08-1103,W04-2607,1,0.680847,"ter-thanchance co-occurrence of antonyms in sentences is because together they convey contrast well, which is rhetorically useful, and not really the reason why they are considered antonyms in the first place. 2.2 Are semantic closeness and antonymy opposites? Two words (more precisely, two lexical units) are considered to be close in meaning if there is a lexical-semantic relation between them. Lexicalsemantic relations are of two kinds: classical and non-classical. Examples of classical relations include synonymy, hyponymy, troponymy, and meronymy. Non-classical relations, as pointed out by Morris and Hirst (2004), are much more common and include concepts pertaining to another concept (kind, chivalrous, formal pertaining to gentlemanly), and commonly co-occurring words (for example, problem–solution pairs such as homeless, shelter). Semantic distance (or closeness) in this broad sense is known as semantic relatedness. Two words are considered to be semantically similar if they are associated via the synonymy, hyponymy– hypernymy, or the troponymy relation. So terms that are semantically similar (plane–glider, doctor– surgeon) are also semantically related, but terms that are semantically related may n"
D08-1103,W02-1011,0,0.0141073,"Missing"
D08-1103,C02-1061,0,0.311749,"proposed in this paper is completely unsupervised. Harabagiu et al. (2006) detected antonyms for the purpose of identifying contradictions by using WordNet chains—synsets connected by the hypernymy–hyponymy links and exactly one antonymy link. Lucerto et al. (2002) proposed detecting antonym pairs using the number of words 984 between two words in text and also cue words such as but, from, and and. Unfortunately, they evaluated their method on only 18 word pairs. Neither of these methods determines the degree of antonymy between words and they have not been shown to have substantial coverage. Schwab et al. (2002) create “antonymous vector” for a target word. The closer this vector is to the context vectors of the other target word, the more antonymous the two target words are. However, the antonymous vectors are manually created. Further, the approach is not evaluated beyond a handful of word pairs. Work in sentiment detection and opinion mining aims at determining the polarity of words. For example, Pang, Lee and Vaithyanathan (2002) detect that adjectives such as dazzling, brilliant, and gripping cast their qualifying nouns positively whereas adjectives such as bad, cliched, and boring portray the n"
D08-1103,C08-1114,0,0.0555083,"such as hypernyms, holonyms, meronyms, and nearsynonyms also tend to occur together more often than chance. Thus, separating antonyms from them has proven to be difficult. Lin et al. (2003) used patterns such as “from X to Y ” and “either X or Y ” to separate antonym word pairs from distributionally similar pairs. They evaluated their method on 80 pairs of antonyms and 80 pairs of synonyms taken from the Webster’s Collegiate Thesaurus (Kay, 1988). In this paper, we propose a method to determine the degree of antonymy between any word pair and not just those that are distributionally similar. Turney (2008) proposed a uniform method to solve word analogy problems that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. However, the Turney method is supervised whereas the method proposed in this paper is completely unsupervised. Harabagiu et al. (2006) detected antonyms for the purpose of identifying contradictions by using WordNet chains—synsets connected by the hypernymy–hyponymy links and exactly one antonymy link. Lucerto et al. (2002) proposed detecting antonym pairs using the number of words 984 between two words in text and also cue w"
D08-1103,P08-1008,0,0.0734216,"from each other in small and large respects. In its strictest sense, antonymy applies to gradable adjectives, such as hot–cold and tall–short, where the two words represent the two ends of a semantic dimension. In a broader sense, it includes other adjectives, nouns, and verbs as well (life–death, ascend–descend, shout–whisper). In its broadest Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automati"
D08-1103,C98-2122,0,\N,Missing
D09-1063,D08-1083,0,0.0214768,"Missing"
D09-1063,esuli-sebastiani-2006-sentiwordnet,0,0.318161,"e GI lexicon has orientation labels for only about 3,600 entries. The Pittsburgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words. Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resources—most of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; AnSentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has properties that support the"
D09-1063,E06-1027,0,0.560541,"Missing"
D09-1063,strapparava-valitutti-2004-wordnet,0,0.0365172,"or English—the most notable being the General Inquirer (GI) (Stone et al., 1966).1 However, the GI lexicon has orientation labels for only about 3,600 entries. The Pittsburgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words. Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resources—most of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; AnSentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like"
D09-1063,P05-1017,0,0.388491,"l., 1966).1 However, the GI lexicon has orientation labels for only about 3,600 entries. The Pittsburgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words. Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resources—most of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; AnSentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has"
D09-1063,P02-1053,0,0.0167643,"b Institute for Advanced Computer Studies† Department of Computer Science‡ , University of Maryland. Human Language Technology Center of Excellence∗ {saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.edu Abstract et al., 1997), question answering (Somasundaran et al., 2007; Lita et al., 2005), and summarizing multiple view points (Seki et al., 2004) and opinions (Mohammad et al., 2008a). A crucial sub-problem is to determine whether positive or negative sentiment is expressed. Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008). A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion. Some example words that have positive semantic orientation are excellent, happy, honest, and so on. Similarly, a word is said to have negative semantic orientation if it is often used to convey unfavorable sentiment or evaluation of the target. Examples include poor, sad, and dishonest. Certain semantic orientation lexicons have been manually compiled for English—the most notable bein"
D09-1063,J94-2004,0,0.0392561,"gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above. 1 Introduction Sentiment analysis involves determining the opinions and private states (beliefs, emotions, speculations, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette"
D09-1063,H05-1044,0,0.109497,"or Advanced Computer Studies† Department of Computer Science‡ , University of Maryland. Human Language Technology Center of Excellence∗ {saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.edu Abstract et al., 1997), question answering (Somasundaran et al., 2007; Lita et al., 2005), and summarizing multiple view points (Seki et al., 2004) and opinions (Mohammad et al., 2008a). A crucial sub-problem is to determine whether positive or negative sentiment is expressed. Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008). A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion. Some example words that have positive semantic orientation are excellent, happy, honest, and so on. Similarly, a word is said to have negative semantic orientation if it is often used to convey unfavorable sentiment or evaluation of the target. Examples include poor, sad, and dishonest. Certain semantic orientation lexicons have been manually compiled for English—the most notable being the General Inquire"
D09-1063,W03-1017,0,0.0452558,"ns, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004). Many of these lexicons, such as SentiWordNet (SWN) (Esuli and Sebastiani, 2006) were created using supervised classifiers and significant manual annotation, whereas others such as the Turney and Littman lexicon (TLL) (2003) were created from very large corpora (more than 100 billion words). Related Work Pang and Lee (2008) provide an excellent survey of the literature on sentiment analysis. Here we briefly describe the work closest to ours. Hatzivassiloglou and McKeown (1997) proposed a supervised algorithm to determine the semantic orientation of adjectives. They"
D09-1063,P97-1023,0,0.952146,"the opinions and private states (beliefs, emotions, speculations, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004). Many of these lexicons, such as SentiWordNet (SWN) (Esuli and Sebastiani, 2006) were created using supervised classifiers and significant manual annotation, whereas others such as the Turney and Littman lexicon (TLL) (2003) were created from very large corpora (more than 100 billion words). Related Work Pang and Lee (2008) provide an excellent survey of the literature on sentiment analysis. Here we briefly describe the work closest to ours. Hatzivassiloglou and McKeown (1997) proposed a supervised algorithm"
D09-1063,kamps-etal-2004-using,0,0.61341,"Missing"
D09-1063,W06-1642,0,0.276944,"entioned above. 1 Introduction Sentiment analysis involves determining the opinions and private states (beliefs, emotions, speculations, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004). Many of these lexicons, such as SentiWordNet (SWN) (Esuli and Sebastiani, 2006) were created using supervised classifiers and significant manual annotation, whereas others such as the Turney and Littman lexicon (TLL) (2003) were created from very large corpora (more than 100 billion words). Related Work Pang and Lee (2008) provide an excellent survey of the literature on sentiment analysis. Here we briefly describe the work closest"
D09-1063,W06-1605,1,0.751491,"ly inexpensive method to compile a high-coverage semantic orientation lexicon without the use of any text corpora or manually annotated semantic orientation labels. Both of these resources may be used, if available, to further improve results. The lexicon has about twenty times the number of entries in the GI lexicon, and it includes entries for both individual words and common multiword expressions. The method makes use of a Roget-like thesaurus and a handful of antonymgenerating affix patterns. Whereas thesauri have long been used to estimate semantic distance (Jarmasz and Szpakowicz, 2003; Mohammad and Hirst, 2006), the closest thesaurus-based work on sentiment analysis is by Aman and Szpakowicz (2007) on detecting emotions such as happiness, sadness, and anger. We evaluated our thesaurusbased algorithm both intrinsically and extrinsically and show that the semantic orientation lexicon it generates has significantly more correct entries than the state-of-the-art high-coverage lexicon SentiWordNet, and that it has a significantly higher coverage than the General Inquirer and Turney–Littman lexicons. In Section 2 we examine related work. Section 3 presents our algorithm for creating semantic orientation l"
D09-1063,D08-1103,1,0.416462,"h-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus Saif Mohammadφ†∗ , Cody Dunne‡ , and Bonnie Dorrφ†‡∗ Laboratory for Computational Linguistics and Information Processingφ Human-Computer Interaction Lab Institute for Advanced Computer Studies† Department of Computer Science‡ , University of Maryland. Human Language Technology Center of Excellence∗ {saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.edu Abstract et al., 1997), question answering (Somasundaran et al., 2007; Lita et al., 2005), and summarizing multiple view points (Seki et al., 2004) and opinions (Mohammad et al., 2008a). A crucial sub-problem is to determine whether positive or negative sentiment is expressed. Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008). A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion. Some example words that have positive semantic orientation are excellent, happy, honest, and so on. Similarly, a word is said to have negative semantic orientati"
D09-1081,W06-2503,0,0.0367625,"Missing"
D09-1081,W06-1605,1,0.0644995,"pplicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to estimate the semantic distance between two terms only if both terms exist in the lexical resource they rely on. We know lexical resources tend to have limited vocabulary and a large number of domainStrictly co"
D09-1081,D07-1060,1,0.870678,"Missing"
D09-1081,W04-2607,0,0.0135592,"make use of their cross-lingual DPCs, to compute semantic distance in a resource-poor language, just as they did. We leave that for future work. 2 Background and Related Work Strictly speaking, semantic distance/closeness is a property of lexical units—a combination of the surface form and word sense.3 Two terms are considered to be semantically close if there is a lexical semantic relation between them. Such a relation may be a classical relation such as hypernymy, troponymy, meronymy, and antonymy, or it may be what have been called an ad-hoc nonclassical relation, such as cause-and-effect (Morris and Hirst, 2004). If the closeness in meaning is due to certain specific classical relations such as hypernymy and troponymy, then the terms are said to be semantically similar. Semantic relatedness is the term used to describe the more general form of semantic closeness caused by any semantic relation (Hirst and Budanitsky, 2005). So the nouns liquid and water are both semantically similar and semantically related, whereas the nouns boat and rudder are semantically related, but not similar. The next three sub-sections describe three kinds of automatic distance measures: (1) lexicalresource-based measures tha"
D09-1081,P06-1014,0,0.0132628,"can be computed from corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet sense information has been criticized to be too fine grained (Agirre and Lopez de Lacalle Lekuona, 2003; Navigli, 2006). See Hirst and Budanitsky (2005) for a comprehensive survey of WordNet-based measures. sider the following. The noun bank has two senses “river bank” and “financial institution”. Assume that bank, when used in the “financial institution” sense, co-occurred with the noun money 100 times in a corpus. Similarly, assume that bank, when used in the “river bank” sense, co-occurred with the noun boat 80 times. So the DP of bank will have co-occurrence information with money as well as boat: 2.2 Both Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) proposed measures that are not only dist"
D09-1081,W06-2501,0,0.0942612,"s for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to estimate the semantic distance between two terms only if both terms exist in the lexical resource they rely on. We know lexical resources tend to have limited vocabulary and a large number of domainStrictly corpus-based measures of semantic distance conflate co-occurrenc"
D09-1081,N06-1025,0,0.0870394,"Missing"
D09-1081,J93-1003,0,0.0753405,"pping method proposed in Mohammad and Hirst (2006) has the effect of resetting to 0 the small co-occurrence counts. The noise from these small co-occurrence counts affects the sense-filtered-counts method more adversely (since any non-zero value will cause the inclusion of the corresponding collocate’s full cooccurrence count) and so the bootstrapped matrix is more suitable for this method. The results also show that the cosine of loglikelihood ratios method mostly performs better than cosine of conditional probabilities and the pmi methods on the noun sets. This further supports the claim by Dunning (1993) that loglikelihood ratio is much less sensitive than pmi to low counts. Interestingly, on the verb set, the pmi methods, and especially hybrid*-prop-cospmi, did extremely well. Further investigation is needed in order to determine if pmi is indeed more suitable for verb semantic similarity, and why. Acknowledgments We thank Mona Diab for her help with her verb test set, Raluca Budiu for her help and clarifications regarding the GLSA method and its implementation details, and the anonymous reviewers for their valuable feedback. This work was supported, in part, by the National Science Foundati"
D09-1081,D08-1094,0,0.153404,"Missing"
D09-1081,C04-1146,0,0.166627,"Missing"
D14-2001,S13-2053,1,\N,Missing
D14-2001,W11-0705,0,\N,Missing
D14-2001,H05-1044,0,\N,Missing
D14-2001,J11-2001,0,\N,Missing
D14-2001,S14-2077,1,\N,Missing
D14-2001,P02-1053,0,\N,Missing
D14-2001,D08-1083,0,\N,Missing
D14-2001,W10-0204,1,\N,Missing
D14-2001,P97-1023,0,\N,Missing
D14-2001,S14-2009,0,\N,Missing
D14-2001,P14-1029,1,\N,Missing
D14-2001,P14-1146,0,\N,Missing
D14-2001,S14-2004,0,\N,Missing
D14-2001,D13-1170,0,\N,Missing
D14-2001,S14-2076,1,\N,Missing
D14-2001,S12-1033,1,\N,Missing
D14-2001,W10-3111,0,\N,Missing
D14-2001,S13-2052,0,\N,Missing
E06-1016,J98-1006,0,0.337939,"Missing"
E06-1016,P98-2127,0,0.896817,"unsupervised algorithm that discriminates instances into different usages can use word sense dominance to assign senses to the different clusters generated. Sense dominance may be determined by simple counting in sense-tagged data. However, dominance varies with domain, and existing sensetagged data is largely insufficient. McCarthy et al. (2004) automatically determine domainspecific predominant senses of words, where the domain may be specified in the form of an untagged target text or simply by name (for example, financial domain). The system (Figure 1) automatically generates a thesaurus (Lin, 1998) using a measure of distributional similarity and an untagged corpus. The target text is used for this purpose, provided it is large enough to learn a thesaurus from. Otherwise a large corpus with sense distribution similar to the target text (text pertaining to the specified domain) must be used. The thesaurus has an entry for each word type, which lists a limited number of words (neighbors) that are distributionally most similar to it. Since Lin’s distributional measure overestimates the distributional similarity of more-frequent word pairs (Mohammad and Hirst, Submitted), the neighbors of a"
E06-1016,P04-1036,0,0.633615,"he sense to the total occurrences of the target word. The sense with the highest dominance in the target text is called the predominant sense of the target word. Determination of word sense dominance has many uses. An unsupervised system will benefit by backing off to the predominant sense in case of insufficient evidence. The dominance values may be used as prior probabilities for the different senses, obviating the need for labeled training data in a sense disambiguation task. Natural language systems can choose to ignore infrequent senses of words or consider only the most dominant senses (McCarthy et al., 2004). An unsupervised algorithm that discriminates instances into different usages can use word sense dominance to assign senses to the different clusters generated. Sense dominance may be determined by simple counting in sense-tagged data. However, dominance varies with domain, and existing sensetagged data is largely insufficient. McCarthy et al. (2004) automatically determine domainspecific predominant senses of words, where the domain may be specified in the form of an untagged target text or simply by name (for example, financial domain). The system (Figure 1) automatically generates a thesau"
E06-1016,P05-1016,0,0.0272457,"Missing"
E06-1016,C92-2070,0,0.297198,"are better indicators of a word’s characteristics than second-order co-occurrences (distributionally similar words). 2 Thesauri Published thesauri, such as Roget’s and Macquarie, divide the English vocabulary into around a thousand categories. Each category has a list of semantically related words, which we will call category terms or c-terms for short. Words with multiple meanings may be listed in more than one category. For every word type in the vocabulary of the thesaurus, the index lists the categories that include it as a c-term. Categories roughly correspond to coarse senses of a word (Yarowsky, 1992), and the two terms will be used interchangeably. For example, in the Macquarie Thesaurus, bark is a c-term in the categories ‘animal noises’ and ‘membrane’. These categories represent the coarse senses of bark. Note that published thesauri are structurally quite different from the “thesaurus” automatically generated by Lin (1998), wherein a word has exactly one entry, and its neighbors may be semantically related to it in any of its senses. All future mentions of thesaurus will refer to a published thesaurus. While other sense inventories such as WordNet exist, use of a published thesaurus ha"
E06-1016,C98-2122,0,\N,Missing
E06-1016,kilgarriff-yallop-2000-whats,0,\N,Missing
J13-3004,J90-1003,0,0.118002,"onyms set. We will refer to the common terms (agitation in this example) as the focus words. Because we also wanted to compare occurrence statistics of the high-contrast set with the random pairs set, we created the control set of random pairs by taking each of the focus words and pairing them with another word in WordNet that has a frequency of occurrence in BNC closest to the term contrasting with the focus word. This is to ensure that members of the pairs across the high-contrast set and the control set have similar unigram frequencies. We calculated the pointwise mutual information (PMI) (Church and Hanks 1990) for each of the word pairs in the high-contrast set, the random pairs set, and the synonyms set using unigram and co-occurrence frequencies in the BNC. If two words occurred within a window of five adjacent words in a sentence, they were marked as co-occurring (same window as Church and Hanks [1990] used in their seminal work on word–word associations). Table 7 shows the average and standard deviation in each set. 7 If both members of a pair have WordNet synonyms, then one is chosen at random, and its synonym is taken. 8 WordNet lists synonyms in order of decreasing frequency in the SemCor co"
J13-3004,P08-1118,0,0.0607305,"Missing"
J13-3004,P97-1023,0,0.224847,"tors for each using dictionary definitions. The approach was evaluated on only a handful of word pairs. There is a large amount of work on sentiment analysis and opinion mining aimed at determining the polarity of words (Pang and Lee 2008). For example, Pang, Lee, and Vaithyanathan (2002) detected that adjectives such as dazzling, brilliant, and gripping cast their qualifying nouns positively whereas adjectives such as bad, cliched, and boring portray the noun negatively. Many of these gradable adjectives have opposites, but these approaches, with the exception of that of Hatzivassiloglou and McKeown (1997), did not attempt to determine pairs of positive and negative polarity words that are opposites. Hatzivassiloglou and McKeown proposed a supervised algorithm that uses word usage patterns to generate a graph with adjectives as nodes. An edge between two nodes indicates either that the two adjectives have the same or opposite polarity. A clustering algorithm then partitions the graph into two subgraphs such that the nodes in a subgraph have the same polarity. They used this method to create a lexicon of positive and negative words, and argued that the method could also be used to detect opposit"
J13-3004,C92-2082,0,0.0872931,"in GRE “most contrasting word” questions are not listed as antonyms in WordNet. We should not infer from this that WordNet or any other lexicographic resource is a poor source for detecting opposites, but rather that identifying the large number of contrasting word pairs requires further computation, possibly relying on other semantic relations stored in the lexicographic resource. Even though a number of computational approaches have been proposed for semantic closeness (Curran 2004; Budanitsky and Hirst 2006), and some for hypernymy– 556 Mohammad et al. Computing Lexical Contrast hyponymy (Hearst 1992), measures of lexical contrast have been less successful. To some extent, this is because lexical contrast is not as well understood as other classical lexical– semantic relations. Over the years, many definitions of semantic contrast and opposites have been proposed by linguists (Lehrer and Lehrer 1982; Cruse 1986), cognitive scientists (Kagan 1984), psycholinguists (Deese 1965), and lexicographers (Egan 1984), which differ from each other in various respects. Cruse (1986, page 197) observes that even though people have a robust intuition of opposites, “the overall class is not a well-defined"
J13-3004,J91-1001,0,0.858055,"entary pairs, but poorly on disjoint opposite pairs. Among different parts of speech, the method performs best on noun pairs, and relatively worse on verb pairs. All of the data created and compiled as part of this research are summarized in Table 18 (Section 8), and is available for download.3 2. Related Work Charles and Miller (1989) proposed that opposites occur together in a sentence more often than chance. This is known as the co-occurrence hypothesis. Paradis, Willners, and Jones (2009) describe further experiments to show how canonical opposites tend to have high textual co-occurrence. Justeson and Katz (1991) gave evidence in support of the hypothesis using 35 prototypical opposites (from an original set of 39 opposites compiled by Deese [1965]) and also with an additional 22 frequent opposites. They also showed that opposites tend to occur in parallel syntactic constructions. All of these pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb, adjective, and adverb pairs (noun–noun, noun–verb, noun–adjective, verb–adverb, etc.) pertaining to 18 concepts (for example, lose(v)–gain(n) and loss(n)–gain(n), where lose(v) and loss(n) pertain to the concept of “failing to"
J13-3004,P98-2127,0,0.762129,"Word Pairs in Text As pointed out earlier, there is work on a small set of opposites showing that opposites co-occur more often than chance (Charles and Miller 1989; Fellbaum 1995). Section 5.1 describes experiments on a larger scale to determine whether highly contrasting word pairs (including opposites) occur together more often than randomly chosen word pairs of similar frequency. The section also compares co-occurrence associations with synonyms. Research in distributional similarity has found that entries in distributional thesauri tend to also contain terms that are opposite in meaning (Lin 1998; Lin et al. 2003). Section 5.2 describes experiments to determine whether highly contrasting word pairs (including opposites) occur in similar contexts as often as randomly chosen pairs of words with similar frequencies, and whether highly contrasting words occur in similar contexts as often as synonyms. 5.1 Co-Occurrence In order to compare the tendencies of highly contrasting word pairs, synonyms, and random word pairs to co-occur in text, we created three sets of word pairs: the highcontrast set, the synonyms set, and the control set of random word pairs. The high-contrast set was created"
J13-3004,P02-1047,0,0.0671634,"Missing"
J13-3004,W11-2128,0,0.0852512,"Missing"
J13-3004,H05-1067,0,0.027602,"Missing"
J13-3004,D09-1063,1,0.520022,"Missing"
J13-3004,D07-1060,1,0.643146,"r, that as of February 2012, most of the Mechanical Turk participants are native speakers of English, certain Indian languages, and some European languages. Our future goals include porting this approach to a cross-lingual framework to determine lexical contrast in a resource-poor language by using a bilingual lexicon to connect the words in that language with words in another resource-rich language. We can then use the structure of the thesaurus from the resource-rich language as described in this article to detect contrasting categories of terms. This is similar to the approach described by Mohammad et al. (2007), who compute semantic distance in a resourcepoor language by using a bilingual lexicon and a sense disambiguation algorithm to connect text in the resource-poor language with a thesaurus in a different language. This enables automatic discovery of lexical contrast in a language even if it does not have a Roget-like thesaurus. The cross-lingual method still requires a bilingual lexicon to map words between the target language and the language with the thesaurus, however. 587 Computational Linguistics Volume 39, Number 3 Our method used only one Roget-like published thesaurus, but even more gai"
J13-3004,W02-1011,0,0.021522,"Missing"
J13-3004,C02-1061,0,0.33566,"Missing"
J13-3004,C08-1114,1,0.717504,"sites and synonyms? How does the proposed method perform when compared with other automatic methods? Experiments: We conduct three experiments (described in Sections 7.1, 7.2, and 7.3) involving three different data sets and two tasks to answer 2 Note that though linguists have classified opposites into different kinds, we know of no work doing so for contrasts more generally. Thus this particular analysis must be restricted to opposites alone. 559 Computational Linguistics Volume 39, Number 3 these questions. We compare performance of our method with methods proposed by Lin et al. (2003) and Turney (2008). We automatically generate a new set of 1,296 “most contrasting word” questions to evaluate performance of our method on five different kinds of opposites and across four parts of speech. (The evaluation described in Section 7.1 was first described in Mohammad, Dorr, and Hirst [2008].) Findings: We find that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods. Our method performs best on gradable pairs, antipodal pairs, and complementary pairs, but poorly on disjoint opposite pairs. Among different parts of speech, the method perf"
J13-3004,P08-1008,0,0.0266766,"Missing"
J13-3004,D08-1103,1,\N,Missing
J13-3004,J06-1003,1,\N,Missing
J13-3004,C98-2122,0,\N,Missing
L16-1006,S16-1003,1,0.194214,"Missing"
L16-1006,S14-2004,0,0.0100309,"Missing"
L16-1006,refaee-rieser-2014-arabic,0,0.0137623,"ranslations provided, we chose words that were less ambiguous and tended to have strong sentiment in Arabic texts. We polled the Twitter API to collect tweets that included these seed words as hashtags. For the purposes of generating a sentiment lexicon, a positive seed hashtag was considered a positive label (pos) and a negative seed hashtag was considered a negative label (neg). For each word w that occurred at least five times in these tweets, we calculated a sentiment score using Equation 1. We will refer to this lexicon as the Arabic Hashtag Lexicon. • Arabic Hashtag Lexicon (Dialectal): Refaee and Rieser (2014) manually created a small sentiment lexicon of 483 dialectal Arabic sentiment words from tweets. We used these words as seeds to collect tweets that contain them, and generated a PMI-based sentiment lexicon just as described above. We refer to this lexicon as the Dialectal Arabic Hashtag Lexicon or Arabic Hashtag Lexicon (dialectal). In Section 3, we show how we used these lexicons for sentiment analysis. 2.2. Generating Arabic Translations of English Sentiment Lexicons We used Google Translate to translate into Arabic the words in each of the following English sentiment lexicons: AFINN (Niels"
L16-1006,S15-2078,1,0.631633,"Missing"
L16-1006,N15-1078,1,0.841503,"Missing"
L16-1006,H05-1044,0,0.0573138,"Missing"
L16-1006,S13-2052,0,0.0189209,"Missing"
L16-1006,N12-1006,0,0.0334929,"Missing"
L16-1006,S14-2077,1,0.392482,"ed, we chose words that were less ambiguous and tended to have strong sentiment in Arabic texts. We polled the Twitter API to collect tweets that included these seed words as hashtags. For the purposes of generating a sentiment lexicon, a positive seed hashtag was considered a positive label (pos) and a negative seed hashtag was considered a negative label (neg). For each word w that occurred at least five times in these tweets, we calculated a sentiment score using Equation 1. We will refer to this lexicon as the Arabic Hashtag Lexicon. • Arabic Hashtag Lexicon (Dialectal): Refaee and Rieser (2014) manually created a small sentiment lexicon of 483 dialectal Arabic sentiment words from tweets. We used these words as seeds to collect tweets that contain them, and generated a PMI-based sentiment lexicon just as described above. We refer to this lexicon as the Dialectal Arabic Hashtag Lexicon or Arabic Hashtag Lexicon (dialectal). In Section 3, we show how we used these lexicons for sentiment analysis. 2.2. Generating Arabic Translations of English Sentiment Lexicons We used Google Translate to translate into Arabic the words in each of the following English sentiment lexicons: AFINN (Niels"
L16-1006,P11-2103,0,0.0173389,"Missing"
L16-1006,W14-3623,0,0.089505,"Missing"
L16-1006,S14-2076,1,0.0915351,"et al., 2009), to generate three Arabic sentiment lexicons: 1 2 33 http://www.saifmohammad.com/WebPages/ArabicSA.html Google Translate: https://translate.google.com 3. • Arabic Emoticon Lexicon: We collected close to one million Arabic tweets that had emoticons (“:)” or “:(”). For the purposes of generating a sentiment lexicon, “:)” was considered a positive label (pos) and “:(” was considered a negative label (neg). For each word w, that occurred at least five times in these tweets, a sentiment score was calculated using the formula shown below (proposed earlier in Mohammad et al. (2013) and Kiritchenko et al. (2014b)): SentimentScore(w) = P M I(w, pos)−P M I(w, neg) (1) where PMI stands for Pointwise Mutual Information. We refer to the resulting entries as the Arabic Emoticon Lexicon. • Arabic Hashtag Lexicon: The NRC-Canada system used 77 positive and negative seed words to generate the English NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013; Kiritchenko et al., 2014b). We translated these English seeds into Arabic using Google Translate. Among the translations provided, we chose words that were less ambiguous and tended to have strong sentiment in Arabic texts. We polled the Twitter API to collec"
L16-1006,S16-1004,1,0.847686,"vements (Wilson et al., 2013; Pontiki et al., 2014; Rosenthal et al., 2015; Mohammad et al., 2016a). However, much of the past work has focused on English texts and English sentiment lexicons. Arabic sentiment analysis has benefited from recent work (Farra et al., 2010; Abdul-Mageed et al., 2011; Badaro et al., 2014; Refaee and Rieser, 2014), but a resource that is particularly lacking is a large sentiment lexicon. Existing lexicons contain only a few hundred to a few thousand entries. Further, most do not indicate the degree of association between a word and positive (or negative) sentiment. Kiritchenko et al. (2016) created a manually annotated Arabic sentiment lexicon with real-valued sentiment scores; however, that lexicon too has only around one thousand entries. In this paper, we describe how we created Arabic sentiment lexicons with tens of thousands of entries. These include new automatically generated ones as well as translations of existing English lexicons. We use the lexicons (old and new) for sentiment classification of Arabic social media posts. Our baseline system uses numerous surface form features. Nonetheless, the system benefits from using additional features drawn from sentiment lexicon"
L16-1006,W10-0204,1,0.693616,"Missing"
L16-1006,S13-2053,1,0.791797,"as distant supervision (Go et al., 2009), to generate three Arabic sentiment lexicons: 1 2 33 http://www.saifmohammad.com/WebPages/ArabicSA.html Google Translate: https://translate.google.com 3. • Arabic Emoticon Lexicon: We collected close to one million Arabic tweets that had emoticons (“:)” or “:(”). For the purposes of generating a sentiment lexicon, “:)” was considered a positive label (pos) and “:(” was considered a negative label (neg). For each word w, that occurred at least five times in these tweets, a sentiment score was calculated using the formula shown below (proposed earlier in Mohammad et al. (2013) and Kiritchenko et al. (2014b)): SentimentScore(w) = P M I(w, pos)−P M I(w, neg) (1) where PMI stands for Pointwise Mutual Information. We refer to the resulting entries as the Arabic Emoticon Lexicon. • Arabic Hashtag Lexicon: The NRC-Canada system used 77 positive and negative seed words to generate the English NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013; Kiritchenko et al., 2014b). We translated these English seeds into Arabic using Google Translate. Among the translations provided, we chose words that were less ambiguous and tended to have strong sentiment in Arabic texts. We pol"
L16-1184,esuli-sebastiani-2006-sentiwordnet,0,0.151414,"al. (2013) extended this list to include 13,915 English lemmas, again manually rated for valence, arousal, and dominance on a 9-point scale. In a similar way, the LabMT lexicon was created (Dodds et al., 2011). It provides real-valued estimates of association of English single words with happiness. Later, the lexicon was extended to include frequently used words from ten languages (Dodds et al., 2015). None of these lexicons, however, contain multi-word phrases. Manually created sentiment lexicons can be used to automatically generate larger sentiment lexicons using semisupervised techniques (Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2013). Automatically collected lexicons often have real-valued sentiment association scores, are larger in scale, and can easily be collected for a specific domain; therefore, they are often more beneficial in downstream applications, such as sentencelevel sentiment prediction (Kiritchenko et al., 2014). Yet, their intrinsic evaluation has been limited due to the lack of manually created real-valued sentiment lexicons. Further, any analysis of the relationship between the sentiment of a phrase and its constituents is less reliable when made from an"
L16-1184,P11-2008,0,0.0702203,"Missing"
L16-1184,N15-1124,0,0.0167017,"While easy to understand, rating items on a scale is not natural for people. Different people may assign different scores to the same target item, and it is hard for annotators to remain consistent when annotating a large number of items. Also, respondents can mark many terms as equally positive making the annotations less useful. Furthermore, respondents often use just a limited part of the scale providing a bias and reducing the discrimination among items. To obtain reliable annotations, the rating scale methods require a high number of responses, typically 15 to 20 (Warriner et al., 2013; Graham et al., 2015). A more natural annotation task for humans is to compare items (e.g., whether one word is more positive than the other). Most commonly, the items are compared in pairs (Thurstone, 1927; David, 1963). In this work, we use Best–Worst Scaling (described in Section 3.2.), which is another annotation technique that exploits the comparative approach to annotation. Best–Worst Scaling forces the respondent to indicate a choice (Best and Worst), while still producing real-valued scores reflective of relative importance. In a small marketing study that compared three scaling techniques (9-point rating"
L16-1184,N16-1095,1,0.897115,", where N is the number of terms to be annotated). Here, in contrast to most previous work on sentiment annotation, we create a lexicon that provides real-valued scores of association of a phrase with positive sentiment. For this, we employ the Best–Worst Scaling method of annotation, which is commonly used in marketing research (Louviere and Woodworth, 1990). It exploits the comparative approach to annotation while keeping the number of annotations small. When applied on the task of sentiment annotation, Best–Worst Scaling has been shown to produce remarkably consistent annotations of terms (Kiritchenko and Mohammad, 2016a). In this paper, we describe how we compiled real-valued sentiment association scores for opposing polarity phrases and their constituents through Best-Worst Scaling. We refer to this resource as the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP). The lexicon includes entries for 265 trigrams, 311 bigrams, and 602 unigrams. We show that re-doing the annotation with different sets of annotators produces consistent rankings of terms by sentiment, proving that the obtained sentiment scores are reliable. We explore the entries in SCL-OPP in search for linguistic regulariti"
L16-1184,W16-0410,1,0.887962,", where N is the number of terms to be annotated). Here, in contrast to most previous work on sentiment annotation, we create a lexicon that provides real-valued scores of association of a phrase with positive sentiment. For this, we employ the Best–Worst Scaling method of annotation, which is commonly used in marketing research (Louviere and Woodworth, 1990). It exploits the comparative approach to annotation while keeping the number of annotations small. When applied on the task of sentiment annotation, Best–Worst Scaling has been shown to produce remarkably consistent annotations of terms (Kiritchenko and Mohammad, 2016a). In this paper, we describe how we compiled real-valued sentiment association scores for opposing polarity phrases and their constituents through Best-Worst Scaling. We refer to this resource as the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP). The lexicon includes entries for 265 trigrams, 311 bigrams, and 602 unigrams. We show that re-doing the annotation with different sets of annotators produces consistent rankings of terms by sentiment, proving that the obtained sentiment scores are reliable. We explore the entries in SCL-OPP in search for linguistic regulariti"
L16-1184,N16-1128,1,0.92209,", where N is the number of terms to be annotated). Here, in contrast to most previous work on sentiment annotation, we create a lexicon that provides real-valued scores of association of a phrase with positive sentiment. For this, we employ the Best–Worst Scaling method of annotation, which is commonly used in marketing research (Louviere and Woodworth, 1990). It exploits the comparative approach to annotation while keeping the number of annotations small. When applied on the task of sentiment annotation, Best–Worst Scaling has been shown to produce remarkably consistent annotations of terms (Kiritchenko and Mohammad, 2016a). In this paper, we describe how we compiled real-valued sentiment association scores for opposing polarity phrases and their constituents through Best-Worst Scaling. We refer to this resource as the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP). The lexicon includes entries for 265 trigrams, 311 bigrams, and 602 unigrams. We show that re-doing the annotation with different sets of annotators produces consistent rankings of terms by sentiment, proving that the obtained sentiment scores are reliable. We explore the entries in SCL-OPP in search for linguistic regulariti"
L16-1184,S16-1004,1,0.889075,"Missing"
L16-1184,S13-2053,1,0.895904,"h lemmas, again manually rated for valence, arousal, and dominance on a 9-point scale. In a similar way, the LabMT lexicon was created (Dodds et al., 2011). It provides real-valued estimates of association of English single words with happiness. Later, the lexicon was extended to include frequently used words from ten languages (Dodds et al., 2015). None of these lexicons, however, contain multi-word phrases. Manually created sentiment lexicons can be used to automatically generate larger sentiment lexicons using semisupervised techniques (Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2013). Automatically collected lexicons often have real-valued sentiment association scores, are larger in scale, and can easily be collected for a specific domain; therefore, they are often more beneficial in downstream applications, such as sentencelevel sentiment prediction (Kiritchenko et al., 2014). Yet, their intrinsic evaluation has been limited due to the lack of manually created real-valued sentiment lexicons. Further, any analysis of the relationship between the sentiment of a phrase and its constituents is less reliable when made from an automatically generated resource as opposed to whe"
L16-1184,L16-1623,1,0.90312,"sentiment, whereas dishonest and dull are associated with negative sentiment. Further, the degree of positivity (or negativity), also referred to as sentiment intensity, can vary. For example, most people will agree that succeed is more positive (or less negative) than improve, and failure is more negative (or less positive) than decline. Sentiment associations are commonly captured in sentiment lexicons—lists of associated word–sentiment pairs (optionally with a score indicating the degree of association). They are mostly used in sentiment analysis, but are also valuable in stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011), and other applications. Manually created sentiment lexicons usually include only single words. However, the sentiment of a phrase can differ significantly from the sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, based on its constituents. Lexicons that include sentiment associations for phrases as well as their constituent words can be very useful in studying sentiment composition. We will refer to them as sentim"
L16-1184,J11-2001,0,0.122076,"ses. We describe how we created a sentiment composition lexicon for opposing polarity phrases and their constituent words. Most existing manually created sentiment lexicons provide only lists of positive and negative words with very coarse levels of sentiment (Stone et al., 1966; Wilson et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less useful in downstream applications than 1 Observe that lazy is associated with negative sentiment whereas sundays is associated with positive sentiment. having access to fine-grained (real-valued) sentiment association scores (Taboada et al., 2011). However, obtaining real-valued sentiment annotations is challenging for several reasons. Respondents are faced with a higher cognitive load when asked for real-valued sentiment scores for terms as opposed to simply classifying terms as either positive or negative. Further, it is difficult for an annotator to remain consistent with his/her annotations. One could overcome these problems by providing annotators with pairs of terms and asking which is more positive (a comparative approach), however that requires a much larger set of annotations (order of N 2 , where N is the number of terms to b"
L16-1184,H05-1044,0,0.721351,"hem as sentiment composition lexicons (SCLs). In this work, we focus on sentiment composition in phrases that include at least one positive and at least one negative word—for example, phrases such as happy accident, best winter break, couldn’t stop smiling, and lazy sundays.1 We refer to them as opposing polarity phrases. We describe how we created a sentiment composition lexicon for opposing polarity phrases and their constituent words. Most existing manually created sentiment lexicons provide only lists of positive and negative words with very coarse levels of sentiment (Stone et al., 1966; Wilson et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less useful in downstream applications than 1 Observe that lazy is associated with negative sentiment whereas sundays is associated with positive sentiment. having access to fine-grained (real-valued) sentiment association scores (Taboada et al., 2011). However, obtaining real-valued sentiment annotations is challenging for several reasons. Respondents are faced with a higher cognitive load when asked for real-valued sentiment scores for terms as opposed to simply classifying terms as either positive or negative. Further, it"
L16-1623,W11-1701,0,0.0742288,"Missing"
L16-1623,W14-2107,0,0.042192,"Missing"
L16-1623,W12-3810,0,0.0175543,"understand how stance can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet."
L16-1623,P13-2142,0,0.00626643,"can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amou"
L16-1623,I13-1191,0,0.0256128,"can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amou"
L16-1623,S14-2076,1,0.86144,"ce from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, ‘Legalization of Abortion’, and ‘Donald Trump’. The annotations were performed by crowdsourcing. Several techniques were employed to encourage high-quality annotations and to identify and discard poor annotations. We analyzed the dataset to sho"
L16-1623,S13-2053,1,0.0870714,"tion that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen tar"
L16-1623,S16-1003,1,0.853828,"at one can be in favor of Jeb Bush and yet also be in favor of Donald Trump. However, the goal in stance detection, is to determine which is more probable: that the author is in favor of, against, or neutral towards the target. In this case, most annotators will agree that the tweeter is likely against Donald Trump. To aid further analysis, the tweets in the Stance Dataset are also annotated for whether target of interest is the target of opinion in the tweet. Partitions of the Stance Dataset were used to create training and test sets for the SemEval-2016 Task 6: Detecting Stance from Tweets (Mohammad et al., 2016a).1 Mohammad et al. (2016b) subsequently annotated the Stance Dataset for sentiment and quantitatively explored the relationship between stance and sentiment. The rest of the paper is structured as follows. In Section 2, we describe how we created the Stance Dataset. Section 3 presents a detailed analysis of the stance annotations. Section 4 presents an online interactive visualization of the Stance Dataset. Section 5 discusses how the dataset can be (and is being) used by the research community. Finally we present concluding remarks in Section 6. All of the data created as part of this proje"
L16-1623,C10-2100,0,0.0609499,"Missing"
L16-1623,S14-2004,0,0.174273,"Missing"
L16-1623,S15-2082,0,0.0208171,"mpts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, ‘Legalization of Abortion’, and ‘Donald Trump’. The annotations were performed by crowdsourcing. Several techniques were employed to encourage high-quality annotations and to identify and discard poor annotations. We a"
L16-1623,S15-2078,1,0.0922389,"ers retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheis"
L16-1623,W15-0509,1,0.0660808,"f interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surv"
L16-1623,P09-1026,0,0.0238553,"Missing"
L16-1623,W10-0214,0,0.0476215,"Missing"
L16-1623,W14-2715,0,0.0421924,"icitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and"
L16-1623,W06-1639,0,0.0852829,"Missing"
L16-1623,N12-1072,0,0.0225157,"Missing"
L16-1623,S13-2052,0,0.0239726,"l based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances correspondin"
L18-1027,S12-1047,1,0.86021,"gion bias. Despite this, a key question is whether humans are able to distinguish affect at only four or five coarse levels, or whether we can discriminate across much smaller affect intensity differences. Best–Worst Scaling (BWS) was developed by Louviere (1991), building on some ground-breaking research in the 1960’s in mathematical psychology and psychophysics by Anthony A. J. Marley and Duncan Luce. However, it is not well known outside the areas of choice modeling and marketing research. Within the NLP community, BWS has thus far been used for creating datasets for relational similarity (Jurgens et al., 2012) and word-sense disambiguation (Jurgens, 2013). Mohammad (2018) used best–worst scaling to annotate about 20K words for valence, arousal, and dominance. In this work, we use BWS to annotate words for intensity (or degree) of basic emotions. With BWS we address the challenges of direct scoring, and produce more reliable emotion intensity scores. Further, this will be the first dataset that will also include emotion scores for words common in social media. There is growing work on automatically determining word–emotion associations (Mohammad and Kiritchenko, 2015; Mohammad, 2012; Strapparava and"
L18-1027,N13-1062,0,0.0677209,"mans are able to distinguish affect at only four or five coarse levels, or whether we can discriminate across much smaller affect intensity differences. Best–Worst Scaling (BWS) was developed by Louviere (1991), building on some ground-breaking research in the 1960’s in mathematical psychology and psychophysics by Anthony A. J. Marley and Duncan Luce. However, it is not well known outside the areas of choice modeling and marketing research. Within the NLP community, BWS has thus far been used for creating datasets for relational similarity (Jurgens et al., 2012) and word-sense disambiguation (Jurgens, 2013). Mohammad (2018) used best–worst scaling to annotate about 20K words for valence, arousal, and dominance. In this work, we use BWS to annotate words for intensity (or degree) of basic emotions. With BWS we address the challenges of direct scoring, and produce more reliable emotion intensity scores. Further, this will be the first dataset that will also include emotion scores for words common in social media. There is growing work on automatically determining word–emotion associations (Mohammad and Kiritchenko, 2015; Mohammad, 2012; Strapparava and Valitutti, 2004). These automatic methods oft"
L18-1027,N16-1095,1,0.907605,"ix item pairs (i.e., for a 4tuple with items A, B, C, and D, if A is the best, and D is the worst, then A &gt; B, A &gt; C, A &gt; D, B &gt; D, and C &gt; D). We can calculate real-valued scores of association between the items and the property of interest from the best– worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014). The scores can be used to rank items by the degree of association with the property of interest. It has been empirically shown that three annotations each for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016).1 Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. Here, for the first time, we create an affect intensity lexicon with real-valued scores of association for four basic emotions (anger, fear, joy, and sadness) using best–worst scaling. For a given word and emotion X, the scores range from 0 to 1. A score of 1 means that the word conveys the highest intensity (amount) of emotion X. A score of 0 means that the word conveys the lowest intensity (amount) of emotion X. We wil"
L18-1027,P17-2074,1,0.85579,"ver, existing manually created affect lexicons for basic emotions such as anger and fear do not provide scores for the intensity of the emotion. Annotating instances for finegrained intensity of affect is a substantially more difficult undertaking than categorical annotation. It is particularly hard to ensure consistency (both across responses by different annotators and within the responses produced by the same annotator). Best–Worst Scaling (BWS) is an annotation scheme that addresses these limitations by employing comparative annotations (Louviere, 1991; Cohen, 2003; Louviere et al., 2015; Kiritchenko and Mohammad, 2017). Annotators are given n items at a time (an n-tuple, where n &gt; 1 and commonly n = 4). They are asked which item is the best (highest in terms of the property of interest) and which is the worst (least in terms of the property of interest). When working on 4-tuples, best–worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six item pairs (i.e., for a 4tuple with items A, B, C, and D, if A is the best, and D is the worst, then A &gt; B, A &gt; C, A &gt; D, B &gt; D, and C &gt; D). We can calculate real-valued scores of association between the"
L18-1027,S17-1007,1,0.8488,"only half the number of annotations; the correlations obtained when repeating the experiment with four annotations for each 4-tuple is expected to be higher than 0.91. Thus 0.91 is a lower bound on the quality of annotations obtained with four annotations per 4-tuple. 6. Applications and Future Work The NRC Affect Intensity Lexicon has many applications including automatic emotion analysis in a number of domains such as commerce, education, intelligence, and public health. The AIL was already used by several teams that participated in the WASSA-2017 shared task on Emotion Intensity in Tweet (Mohammad and Bravo-Marquez, 2017b) as well as the SemEval-2018 Task 1: Affect in Tweets (Mohammad et al., 2018) (including the teams that came first in both shared tasks). AIL is also useful in the building of natural language generation systems. We are currently using the NRC Affect Intensity Lexicon along with tweets datasets that were annotated for emotion intensity (Tweet Emotion Intensity Dataset (Mohammad and Bravo-Marquez, 2017a)), to test the extent to which people convey strong emotions in tweets using highintensity emotion words. We will also use the lexicon to identify syllables that consistently tend to occur in"
L18-1027,W17-5205,1,0.869893,"Missing"
L18-1027,L18-1030,1,0.819364,"issive). Both the basic emotions model and the VAD model offer different perspectives that help our understanding of emotions. However, there is little work relating the two models of emotion with each other. Much of the past work on textual utterances such as sentences and tweets, is based on exactly one or the other model (not both). For example, corpora annotated for emotions are either annotated only for the basic emotions (Strapparava and Mihalcea, 2007; Mohammad and BravoMarquez, 2017b) or only for valence, arousal, and dominance (Yu et al., ; Mohammad et al., 2017; Nakov et al., 2016). Mohammad and Kiritchenko (2018) created the first dataset of tweets manually annotated for multiple affect dimensions from both the basic emotion model and the VAD model. For each emotion dimension, they annotated the data for coarse classes (such as no anger, low anger, moderate anger, and high anger) and also for fine real-valued scores indicating the intensity of emotion (anger, sadness, valence, etc.). They present an analysis of emotion intensities of tweets and their relationship with valence. Similar to the situation for textual corpora, words have been annotated largely either just for valence, arousal, and dominanc"
L18-1027,W10-0204,1,0.934954,"n by (Mohammad, 2018), and the lexicon by Warriner et al. (2013). The work on creating lexicons for categorical emotions such as joy, sadness, fear, etc, is comparatively small. WordNet Affect Lexicon (Strapparava and Valitutti, 2004) has a few hundred words annotated with the emotions they evoke.4 It was created by manually identifying the emotions of a few seed words and then marking all their WordNet synonyms as having the same emotion. The NRC Emotion Lexicon was created by crowdsourcing and it includes entries for about 14,000 words and eight Plutchik emotions (Mohammad and Turney, 2013; Mohammad and Turney, 2010).5 It also includes entries for positive and negative sentiment. Most prior work in sentiment analysis describes machine learning systems trained and tested on data with coarse categorical annotations. This is not surprising, because it is difficult for humans to directly provide valence (sentiment) scores at a fine granularity. A common problem is inconsistencies in annotations among different annotators. One annotator might assign a score of 7.9 to a word, whereas another annotator may assign a score of 6.2 to the same word. It is also common that the same annotator assigns different scores"
L18-1027,S18-1001,1,0.846666,"ment with four annotations for each 4-tuple is expected to be higher than 0.91. Thus 0.91 is a lower bound on the quality of annotations obtained with four annotations per 4-tuple. 6. Applications and Future Work The NRC Affect Intensity Lexicon has many applications including automatic emotion analysis in a number of domains such as commerce, education, intelligence, and public health. The AIL was already used by several teams that participated in the WASSA-2017 shared task on Emotion Intensity in Tweet (Mohammad and Bravo-Marquez, 2017b) as well as the SemEval-2018 Task 1: Affect in Tweets (Mohammad et al., 2018) (including the teams that came first in both shared tasks). AIL is also useful in the building of natural language generation systems. We are currently using the NRC Affect Intensity Lexicon along with tweets datasets that were annotated for emotion intensity (Tweet Emotion Intensity Dataset (Mohammad and Bravo-Marquez, 2017a)), to test the extent to which people convey strong emotions in tweets using highintensity emotion words. We will also use the lexicon to identify syllables that consistently tend to occur in words with strong affect associations. This has implications in understanding h"
L18-1027,S12-1033,1,0.952858,"milarity (Jurgens et al., 2012) and word-sense disambiguation (Jurgens, 2013). Mohammad (2018) used best–worst scaling to annotate about 20K words for valence, arousal, and dominance. In this work, we use BWS to annotate words for intensity (or degree) of basic emotions. With BWS we address the challenges of direct scoring, and produce more reliable emotion intensity scores. Further, this will be the first dataset that will also include emotion scores for words common in social media. There is growing work on automatically determining word–emotion associations (Mohammad and Kiritchenko, 2015; Mohammad, 2012; Strapparava and Valitutti, 2004). These automatic methods often assign a real-valued score representing the degree of association. Further, these association scores are likely to be somewhat correlated with the intensity of the emotion. The Affect Intensity Lexicon can be used to judge the quality of the automatic lexicons, and also to explore the extent of correlation between emotion association and emotion intensity. 2 www.saifmohammad.com/WebPages/AffectIntensity.htm However, they disagree on which emotions (and how many) should be classified as basic emotions—some propose 6, some 8, some"
L18-1027,P18-1017,1,0.885655,"1980; Frijda, 1988; Parrot, 2001).3 Thus, most work on capturing word– emotion associations has focused on a handful of emotions, especially since manually annotating for a large number of emotions is arduous. In this project, we focus on four emotions common among the many proposals for basic emotions (Plutchik, 1980; Ekman, 1992; Parrot, 2001): anger, fear, joy, and sadness. There is a large body of work on creating valence or sentiment lexicons, including the General Inquirer (Stone et al., 1966), ANEW (Nielsen, 2011; Bradley and Lang, 1999), MPQA (Wiebe et al., 2005), NRC VAD Lexicon by (Mohammad, 2018), and the lexicon by Warriner et al. (2013). The work on creating lexicons for categorical emotions such as joy, sadness, fear, etc, is comparatively small. WordNet Affect Lexicon (Strapparava and Valitutti, 2004) has a few hundred words annotated with the emotions they evoke.4 It was created by manually identifying the emotions of a few seed words and then marking all their WordNet synonyms as having the same emotion. The NRC Emotion Lexicon was created by crowdsourcing and it includes entries for about 14,000 words and eight Plutchik emotions (Mohammad and Turney, 2013; Mohammad and Turney,"
L18-1027,S07-1013,0,0.199037,"el of affect, individual emotions are points in a three-dimensional space of valence (positiveness–negativeness), arousal (active–passive), and dominance (dominant–submissive). Both the basic emotions model and the VAD model offer different perspectives that help our understanding of emotions. However, there is little work relating the two models of emotion with each other. Much of the past work on textual utterances such as sentences and tweets, is based on exactly one or the other model (not both). For example, corpora annotated for emotions are either annotated only for the basic emotions (Strapparava and Mihalcea, 2007; Mohammad and BravoMarquez, 2017b) or only for valence, arousal, and dominance (Yu et al., ; Mohammad et al., 2017; Nakov et al., 2016). Mohammad and Kiritchenko (2018) created the first dataset of tweets manually annotated for multiple affect dimensions from both the basic emotion model and the VAD model. For each emotion dimension, they annotated the data for coarse classes (such as no anger, low anger, moderate anger, and high anger) and also for fine real-valued scores indicating the intensity of emotion (anger, sadness, valence, etc.). They present an analysis of emotion intensities of t"
L18-1027,strapparava-valitutti-2004-wordnet,0,0.887359,"ords are associated with affect even though they do not denotate affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they connotate some amount of sadness. Lexicons of word–affect association have numerous applications, including: tracking brand and product perception, tracking support for issues and policies, tracking public health and well-being, literary analysis, and developing more natural dialogue systems. Past work on manually compiling affect lexicons has focused on denotative words (Wiebe et al., 2005; Francisco and Gerv´as, 2006; Strapparava and Valitutti, 2004). A notable exception to this is the NRC Emotion Lexicon, which includes words that are associated with (or connotate) an emotion (Mohammad and Turney, 2013). Words can be associated with different intensities (or degrees) of an emotion. For example, most people will agree that the word outrage is associated with a greater degree of anger (or more anger) than the word irritate. However, existing manually created affect lexicons for basic emotions such as anger and fear do not provide scores for the intensity of the emotion. Annotating instances for finegrained intensity of affect is a substant"
L18-1030,H05-1073,0,0.0369192,"e), and dominance (dominant–submissive). Both the categorical model and the dimensional model of emotions have a large body of work supporting them, and offer different perspectives that help our understanding of emotions. However, there is very little work relating the two models of emotion with each other. Much of the past work on textual utterances such as sentences and tweets, is based on exactly one or the other model (not both).2 For example, corpora annotated for emotions are either annotated only for the basic emotions (Mohammad and BravoMarquez, 2017b; Strapparava and Mihalcea, 2007; Alm et al., 2005) or only for valence, arousal, and dominance (Yu et al., ; Mohammad et al., 2017; Nakov et al., 2016). 1 Intensity is different from arousal, which refers to the extent to which an emotion is calming or exciting. 2 There is some work on words that are annotated both for association to basic emotions as well as for valence, arousal, and dominance (Mohammad, 2018). Within Natural Language Processing, an overwhelming majority of the work has focused on classifying text into positive, negative, and neutral classes (valence classification), and a much smaller amount on classifying text into basic e"
L18-1030,N16-1095,1,0.913409,"ntensities. Given a focus emotion, each tweet is annotated for intensity of the emotion felt by the speaker using a technique called Best–Worst Scaling (BWS). 198 Dataset E-c EI-reg, EI-oc V-reg, V-oc Source of Tweets Tweets-2016 Tweets-2017 Tweets-2016 Tweets-2017 Tweets-2016 Tweets-2017 Annotated In 2016 2017 X X X X X X Table 1: The data and annotations in the AIT Dataset. BWS is an annotation scheme that addresses the limitations of traditional rating scale methods, such as interand intra-annotator inconsistency, by employing comparative annotations (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017). Annotators are given n items (an n-tuple, where n &gt; 1 and commonly n = 4). They are asked which item is the best (highest in terms of the property of interest) and which is the worst (lowest in terms of the property of interest). When working on 4-tuples, best–worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six item pairs. For example, for a 4-tuple with items A, B, C, and D, if A is the best, and D is the worst, then A &gt; B, A &gt; C, A &gt; D, B &gt; D, and C &gt; D. Real-valued scores of associati"
L18-1030,P17-2074,1,0.861291,"n, each tweet is annotated for intensity of the emotion felt by the speaker using a technique called Best–Worst Scaling (BWS). 198 Dataset E-c EI-reg, EI-oc V-reg, V-oc Source of Tweets Tweets-2016 Tweets-2017 Tweets-2016 Tweets-2017 Tweets-2016 Tweets-2017 Annotated In 2016 2017 X X X X X X Table 1: The data and annotations in the AIT Dataset. BWS is an annotation scheme that addresses the limitations of traditional rating scale methods, such as interand intra-annotator inconsistency, by employing comparative annotations (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017). Annotators are given n items (an n-tuple, where n &gt; 1 and commonly n = 4). They are asked which item is the best (highest in terms of the property of interest) and which is the worst (lowest in terms of the property of interest). When working on 4-tuples, best–worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six item pairs. For example, for a 4-tuple with items A, B, C, and D, if A is the best, and D is the worst, then A &gt; B, A &gt; C, A &gt; D, B &gt; D, and C &gt; D. Real-valued scores of association between the items and the prop"
L18-1030,S17-1007,1,0.85519,"a large single textual dataset annotated for many emotion (or affect) dimensions (from both the basic emotion model and the VAD model). Specifically, we annotate tweets for the emotions of people that posted the tweets—emotions that can be inferred solely from the text of the tweet. For each emotion dimension, we annotate the data for not just coarse classes (such as anger or no anger) but also for finegrained real-valued scores indicating the intensity of emotion (anger, sadness, valence, etc.). The datasets can be used to train many different kinds of emotion analysis systems. Further, as (Mohammad and Bravo-Marquez, 2017a) showed, correlations across emotions means that training data for one emotion can be used to supplement training data for another emotion. We choose Twitter as the source of the textual data we annotate because tweets are selfcontained, widely used, public posts, and tend to be rich in emotions. However, other choices such as weblogs, forum posts, and comments on newspaper articles are also suitable avenues for future work. Similarly, annotating for the emotions of the reader or emotions of those mentioned in the tweets are also suitable avenues for future work. Mohammad and Bravo-Marquez ("
L18-1030,W17-5205,1,0.895392,"Missing"
L18-1030,S18-1001,1,0.899936,"ication, or E-c, systems. Table 1 shows the two stages in which the annotations were done: in 2016 as described in the work by Mohammad and Bravo-Marquez (2017b) and in 2017 as described in this paper. Together, we well refer to the joint set of tweets from Tweets-2016 and Tweets-2017 along with all the emotion-related annotations described above as the SemEval-2018 Affect in Tweets Dataset (or AIT Dataset for short), since this data was used to create the training, development, and test sets in the SemEval-2018 shared task of the same name – SemEval-2018 Task 1: Affect in Tweets shared task (Mohammad et al., 2018).5 The shared task evaluates automatic systems for EI-reg, EI-oc, V-reg, V-oc, and E-c in three languages: English, Arabic, and Spanish. We show that the intensity annotations in the AIT dataset have a high split-half reliability (between 0.82 and 0.92), indicating a high quality of annotation. (Split half reliability measures the average correlation between scores produced by two halves of the annotations—higher correlations indicate stable and consistent outputs.) The annotator agreement on the multi-label emotion annotations (E-c) is also well above the random agreement. We show that certai"
L18-1030,L18-1027,1,0.904482,"ences and tweets, is based on exactly one or the other model (not both).2 For example, corpora annotated for emotions are either annotated only for the basic emotions (Mohammad and BravoMarquez, 2017b; Strapparava and Mihalcea, 2007; Alm et al., 2005) or only for valence, arousal, and dominance (Yu et al., ; Mohammad et al., 2017; Nakov et al., 2016). 1 Intensity is different from arousal, which refers to the extent to which an emotion is calming or exciting. 2 There is some work on words that are annotated both for association to basic emotions as well as for valence, arousal, and dominance (Mohammad, 2018). Within Natural Language Processing, an overwhelming majority of the work has focused on classifying text into positive, negative, and neutral classes (valence classification), and a much smaller amount on classifying text into basic emotion categories such as joy, sadness, and fear. A key obstacle in developing algorithms for other emotionrelated tasks, especially those involving fine-grained intensity scores, is the lack of large reliably labeled datasets. The goal of this work is to create, for the first time, a large single textual dataset annotated for many emotion (or affect) dimensions"
L18-1030,S07-1013,0,0.211903,"iveness), arousal (active–passive), and dominance (dominant–submissive). Both the categorical model and the dimensional model of emotions have a large body of work supporting them, and offer different perspectives that help our understanding of emotions. However, there is very little work relating the two models of emotion with each other. Much of the past work on textual utterances such as sentences and tweets, is based on exactly one or the other model (not both).2 For example, corpora annotated for emotions are either annotated only for the basic emotions (Mohammad and BravoMarquez, 2017b; Strapparava and Mihalcea, 2007; Alm et al., 2005) or only for valence, arousal, and dominance (Yu et al., ; Mohammad et al., 2017; Nakov et al., 2016). 1 Intensity is different from arousal, which refers to the extent to which an emotion is calming or exciting. 2 There is some work on words that are annotated both for association to basic emotions as well as for valence, arousal, and dominance (Mohammad, 2018). Within Natural Language Processing, an overwhelming majority of the work has focused on classifying text into positive, negative, and neutral classes (valence classification), and a much smaller amount on classifyin"
L18-1197,L18-1030,1,0.840521,"Missing"
L18-1197,S18-1001,1,0.818323,"n et al., 2014) are of particular interest for developing algorithms at the intersection of computer vision and natural language processing. ImageNet has thousands of images each for many WordNet noun concepts, whereas MS COCO has hundreds of thousands of images, many of which have English captions (descriptions). Automatically detecting emotions has also gained considerable attention over recent years, especially from text (Mohammad, 2012b; Mohammad, 2012a; Zhu et al., 2014; Kiritchenko et al., 2014; Yang et al., 2007; Bollen et al., 2009; Wang et al., 2016; Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018) but also from images (Fasel and Luettin, 2003; De Silva et al., 1997; Zheng et al., 2010). However, image annotations for emotions have largely been limited to small datasets of facial expressions (Lucey et al., 2010; Susskind et al., 2007). Ours is the first dataset we know of that includes emotions annotations for thousands of pieces of art. 3 WikiArt Emotions Project webpage: http://saifmohammad.com/WebPages/wikiartemotions.html Figure 1: WikiArt.org’s page for the Mona Lisa. In the WikiArt Emotions Dataset, the Mona Lisa is labeled as evoking happiness, love, and trust; its average rating"
L18-1197,S12-1033,1,0.464211,"ision datasets have been developed and made available. (See Appendix.) Resources such as ImageNet (Deng et al., 2009) and Microsoft Common Objects in Context (MS COCO) (Lin et al., 2014) are of particular interest for developing algorithms at the intersection of computer vision and natural language processing. ImageNet has thousands of images each for many WordNet noun concepts, whereas MS COCO has hundreds of thousands of images, many of which have English captions (descriptions). Automatically detecting emotions has also gained considerable attention over recent years, especially from text (Mohammad, 2012b; Mohammad, 2012a; Zhu et al., 2014; Kiritchenko et al., 2014; Yang et al., 2007; Bollen et al., 2009; Wang et al., 2016; Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018) but also from images (Fasel and Luettin, 2003; De Silva et al., 1997; Zheng et al., 2010). However, image annotations for emotions have largely been limited to small datasets of facial expressions (Lucey et al., 2010; Susskind et al., 2007). Ours is the first dataset we know of that includes emotions annotations for thousands of pieces of art. 3 WikiArt Emotions Project webpage: http://saifmohammad.com/WebPages/wikia"
L18-1224,S12-1051,0,0.0609539,"w similarity between assertions (why people tend to agree with one assertion and disagree with the other) include: the two assertions are contradictory or contrasting (e.g., ‘It is safe to use vaccines.’ and ‘Vaccines cause autism.’), and underlying socio-cultural and political factors cause people to vote dissimilarly on two (sometimes seemingly unrelated) assertions (e.g., ‘Congress should immediately fund Trump’s wall.’ and ‘All immigrants should have the right to vote in the American elections.’). To further explore the relation between judgment similarity and semantic textual similarity (Agirre et al., 2012), we compute the textual overlap between assertions using the Jaccard index (Lyon et al., 2001) and examine the agreement scores of textually similar assertions. We observe that assertions with high text similarity often have very similar 1410 (a) cosine > 0.0 (b) cosine > 0.1 (c) cosine > 0.2 (d) cosine > 0.3 (e) cosine > 0.4 Figure 4: Similarity of participants visualized in an undirected graph for the issue Black Lives Matter. In the sub figures, we draw edges between two persons if their voting similarity is above a certain threshold. agreement scores. An example for this case are the foll"
L18-1224,W16-2802,0,0.023675,"proaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on a controversial issue can be obtained by having a large number of people vote on a large number of relevant assertions. As the manual creation of assertions"
L18-1224,W15-0514,0,0.0477572,"Missing"
L18-1224,P15-2072,0,0.0212045,"we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative element"
L18-1224,L16-1591,0,0.014112,"ce containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et"
L18-1224,C10-1039,0,0.0311501,"d Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable"
L18-1224,D14-1168,0,0.0133673,"l and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on"
L18-1224,W01-0515,0,0.487407,"e other) include: the two assertions are contradictory or contrasting (e.g., ‘It is safe to use vaccines.’ and ‘Vaccines cause autism.’), and underlying socio-cultural and political factors cause people to vote dissimilarly on two (sometimes seemingly unrelated) assertions (e.g., ‘Congress should immediately fund Trump’s wall.’ and ‘All immigrants should have the right to vote in the American elections.’). To further explore the relation between judgment similarity and semantic textual similarity (Agirre et al., 2012), we compute the textual overlap between assertions using the Jaccard index (Lyon et al., 2001) and examine the agreement scores of textually similar assertions. We observe that assertions with high text similarity often have very similar 1410 (a) cosine > 0.0 (b) cosine > 0.1 (c) cosine > 0.2 (d) cosine > 0.3 (e) cosine > 0.4 Figure 4: Similarity of participants visualized in an undirected graph for the issue Black Lives Matter. In the sub figures, we draw edges between two persons if their voting similarity is above a certain threshold. agreement scores. An example for this case are the following assertions: ‘Women should have the same rights than men.’ (ags = 0.84) ‘Women should have"
L18-1224,N15-1046,0,0.0265583,"al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on a controversial issue can be obtained by having a large number of people vote on a large number"
L18-1224,S16-1003,1,0.841664,"ned questions such as: whether these applications have an effect on voting behavior (Ladner and Pianzola, 2010), what characteristics their users have (Wall et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall u"
L18-1224,S18-1001,1,0.808268,"ience, where researchers have examined questions such as: whether these applications have an effect on voting behavior (Ladner and Pianzola, 2010), what characteristics their users have (Wall et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utteran"
L18-1224,D14-1006,0,0.0309952,"all et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McD"
L18-1224,P08-1036,0,0.0507931,"McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3."
L18-1224,W14-1305,0,0.0300253,"sur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on a controversial issue can be obtained by having a large number of people v"
L18-1224,P15-1157,0,0.0174511,"g a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaı"
L18-1224,walker-etal-2012-corpus,0,0.0366734,"a, 2010), what characteristics their users have (Wall et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang"
N09-1066,kan-etal-2002-using,0,0.273112,"citing article. Elkiss et al. (2008b) conducted several experiments on a set of 2, 497 articles from the free PubMed Central (PMC) repository.1 Results from this experiment confirmed that the cohesion of a citation text of an article is consistently higher than the that of its abstract. They also concluded that citation texts contain additional information are more focused than abstracts. Nakov et al. (2004) use sentences surrounding citations to create training and testing data for semantic analysis, synonym set creation, database curation, document summarization, and information retrieval. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www"
N09-1066,P03-1054,0,0.00253884,"rization system by generating multiple alternative sentence compressions of the most important sentences in target documents (Zajic et al., 2007). Trimmer compressions are generated by applying linguistically-motivated rules to mask syntactic components of a parse of a source sentence. The rules can be applied iteratively to compress sentences below a configurable length threshold, or can be applied in all combinations to generate the full space of compressions. Trimmer can leverage the output of any constituency parser that uses the Penn Treebank conventions. At present, the Stanford Parser (Klein and Manning, 2003) is used. The set of compressions is ranked according to a set of features that may include metadata about the source sentences, details of the compression process that generated the compression, and externally calculated features of the compression. Summaries are constructed from the highest scoring compressions, using the metadata and maximal marginal relevance (Carbonell and Goldstein, 1998) to avoid redundancy and over-representation of a single source. 587 LexRank We also used LexRank (Erkan and Radev, 2004), a state-of-the-art multidocument summarization system, to generate summaries. Le"
N09-1066,W04-1013,0,0.0666638,"mentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004). 4.2 4 The salience of a node is recursively defined on the salience of adjacent nodes. This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows. However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006)). Summarization systems We used four summarization systems for our survey-creation approach: Trimmer, LexRank, CLexRank, and C-RR. Trimmer is a syntac"
N09-1066,P08-1093,0,0.371637,"ribing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input. 3 Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine. Citation 586 texts have also been used to create summaries of single scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008). However, there is no previous work that uses the text of the citations to produce a multi-document survey of scientific articles. Furthermore, there is no study contrasting the quality of surveys generated from citation summaries— both automatically and manually produced—to surveys generated from other forms of input such as the abstracts or full texts of the source articles. Nanba and Okumura (1999) discuss citation categorization to support a system for writing a survey. Nanba et al. (2004a) automatically categorize citation sentences into three groups using pre-defined phrase-based rules."
N09-1066,N04-1019,0,0.386146,"zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004). 4.2 4 The salience of a node is recursively defined on the salience of adjacent nodes. This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows. However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006)). Summarization systems We used four summarization systems for our survey-creation approach: Trimmer, LexRank, CLexRank, and C-RR. Trimmer"
N09-1066,C08-1087,1,0.846023,"as, 584 some of which may be unfamiliar to panelists. Thus, they must learn about a new discipline “on the fly” in order to relate their own expertise to the proposal. Our goal is to effectively serve these needs by combining two currently available technologies: (1) bibliometric lexical link mining that exploits the structure of citations and relations among citations; and (2) summarization techniques that exploit the content of the material in both the citing and cited papers. It is generally agreed upon that manually written abstracts are good summaries of individual papers. More recently, Qazvinian and Radev (2008) argue that citation texts are useful in creating a summary of the important contributions of a research paper. The citation text of a target paper is the set of sentences in other technical papers that explicitly refer to it (Elkiss et al., 2008a). However, Teufel (2005) argues that using citation text directly is not suitable for document summarization. In this paper, we compare and contrast the usefulness of abstracts and of citation text in automatically generating a technical survey on a given topic from multiple research papers. The next section provides the background for this work, inc"
N09-1066,N07-1040,0,0.503915,"e focused than abstracts. Nakov et al. (2004) use sentences surrounding citations to create training and testing data for semantic analysis, synonym set creation, database curation, document summarization, and information retrieval. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the"
N09-1066,J02-4002,0,0.390372,"ons of the target paper. Our goal is to test the hypothesis that an effective technical survey will reflect information on research not only from the perspective of its authors but also from the perspective of others who use/commend/discredit/add to it. Before describing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input. 3 Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine. Citation 586 texts have also been used to create summaries of single scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008). However, there is no previous work that uses the text of the citations to produce a multi-document survey of scientific articles. Furthermore, there is no study contrasting the quality of surveys generated from citation summaries— both automatically and manually produced—to surveys generated from other forms of input such as the abstra"
N09-1066,W06-1613,0,0.814172,"eering and Computer Scienceφ School of Information§ , University of Michigan. {hassanam,mpradeep,vahed,radev}@umich.edu Abstract The number of research publications in various disciplines is growing exponentially. Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material. In this paper we present the first steps in producing an automatically generated, readily consumable, technical survey. Specifically we explore the combination of citation information and summarization techniques. Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role. 1 Introduction In today’s rapidly expanding disciplines, scientists and scholars are constantly faced with the daunting task of keeping up with knowledge in their field. In addition, the increasingly interconnected nature of real-world tasks often requires experts in one discipline to rapidly learn about other areas in a short amount of time. Cross-disciplinary research requires scientists in areas such as linguistics, biology, and"
N12-1071,H05-1073,0,0.204166,"xpressed in text has a number of applications, including tracking customer satisfaction (Bougie et al., 2003), determining popularity of politicians and government policies (Mohammad and Yang, 2011), depression detection (Osgood and Walker, 1959; Pestian et al., 2008; Matykiewicz et al., 2009; Cherry et al., 2012), affect-based search (Mohammad, 2011), and improving human-computer interaction (Vel´asquez, 1997; Ravaja et al., 2006). Supervised methods for classifying emotions expressed in a sentence tend to perform better than unsupervised ones. They use features such as unigrams and bigrams (Alm et al., 2005; Aman and Szpakowicz, 2007; Neviarouskaya et al., 2009; Chaffar and Inkpen, 2011). For example, a system can learn that the word excruciating tends to occur in sentences labeled with sadness, and use this word as a feature in classifying new sentences. Approaches that do not rely on supervised training with sentence-level annotations often use affect lexicons. An affect lexicon, in its simplest form, is a list of words and associated emotions and sentiments. For example, the word excruciating may be associated with the emotions of sadness and fear. Note that such lexicons are at best indicato"
N12-1071,W09-1323,0,0.0104108,"icon can provide significant gains. We further show that while ngram features tend to be accurate, they are often unsuitable for use in new domains. On the other hand, affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain. 1 Introduction Automatically identifying emotions expressed in text has a number of applications, including tracking customer satisfaction (Bougie et al., 2003), determining popularity of politicians and government policies (Mohammad and Yang, 2011), depression detection (Osgood and Walker, 1959; Pestian et al., 2008; Matykiewicz et al., 2009; Cherry et al., 2012), affect-based search (Mohammad, 2011), and improving human-computer interaction (Vel´asquez, 1997; Ravaja et al., 2006). Supervised methods for classifying emotions expressed in a sentence tend to perform better than unsupervised ones. They use features such as unigrams and bigrams (Alm et al., 2005; Aman and Szpakowicz, 2007; Neviarouskaya et al., 2009; Chaffar and Inkpen, 2011). For example, a system can learn that the word excruciating tends to occur in sentences labeled with sadness, and use this word as a feature in classifying new sentences. Approaches that do not"
N12-1071,W10-0204,1,0.827404,"erican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 587–591, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics with six emotions considered to be the most basic— joy, sadness, fear, disgust, anger, and surprise (Ekman, 1992).1 It was created by manually identifying the emotions of a few seed words and then labeling all their WordNet synonyms with the same emotion. Affective Norms for English Words has pleasure (happy–unhappy), arousal (excited–calm), and dominance (controlled–in control) ratings for 1034 words.2 Mohammad and Turney (2010; 2012) compiled manual annotations for eight emotions (the six of Ekman, plus trust and anticipation) as well as for positive and negative sentiment.3 The lexicon was created by crowdsourcing to Mechanical Turk. This lexicon, referred to as the NRC word-emotion lexicon (NRC-10) version 0.91, has annotations for about 14,000 words.4 We evaluate the affect lexicons that have annotations for the Ekman emotions—the WordNet Affect Lexicon and the NRC-10. We also experimented with a subset of NRC-10, which we will call NRC6, that has annotations for only the six Ekman emotions (no trust and anticip"
N12-1071,W11-1709,1,0.533204,"th two very different emotion lexicons and show that even in supervised settings, an affect lexicon can provide significant gains. We further show that while ngram features tend to be accurate, they are often unsuitable for use in new domains. On the other hand, affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain. 1 Introduction Automatically identifying emotions expressed in text has a number of applications, including tracking customer satisfaction (Bougie et al., 2003), determining popularity of politicians and government policies (Mohammad and Yang, 2011), depression detection (Osgood and Walker, 1959; Pestian et al., 2008; Matykiewicz et al., 2009; Cherry et al., 2012), affect-based search (Mohammad, 2011), and improving human-computer interaction (Vel´asquez, 1997; Ravaja et al., 2006). Supervised methods for classifying emotions expressed in a sentence tend to perform better than unsupervised ones. They use features such as unigrams and bigrams (Alm et al., 2005; Aman and Szpakowicz, 2007; Neviarouskaya et al., 2009; Chaffar and Inkpen, 2011). For example, a system can learn that the word excruciating tends to occur in sentences labeled wit"
N12-1071,W11-1514,1,0.758834,"features tend to be accurate, they are often unsuitable for use in new domains. On the other hand, affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain. 1 Introduction Automatically identifying emotions expressed in text has a number of applications, including tracking customer satisfaction (Bougie et al., 2003), determining popularity of politicians and government policies (Mohammad and Yang, 2011), depression detection (Osgood and Walker, 1959; Pestian et al., 2008; Matykiewicz et al., 2009; Cherry et al., 2012), affect-based search (Mohammad, 2011), and improving human-computer interaction (Vel´asquez, 1997; Ravaja et al., 2006). Supervised methods for classifying emotions expressed in a sentence tend to perform better than unsupervised ones. They use features such as unigrams and bigrams (Alm et al., 2005; Aman and Szpakowicz, 2007; Neviarouskaya et al., 2009; Chaffar and Inkpen, 2011). For example, a system can learn that the word excruciating tends to occur in sentences labeled with sadness, and use this word as a feature in classifying new sentences. Approaches that do not rely on supervised training with sentence-level annotations"
N12-1071,W08-0616,0,0.0116054,"ettings, an affect lexicon can provide significant gains. We further show that while ngram features tend to be accurate, they are often unsuitable for use in new domains. On the other hand, affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain. 1 Introduction Automatically identifying emotions expressed in text has a number of applications, including tracking customer satisfaction (Bougie et al., 2003), determining popularity of politicians and government policies (Mohammad and Yang, 2011), depression detection (Osgood and Walker, 1959; Pestian et al., 2008; Matykiewicz et al., 2009; Cherry et al., 2012), affect-based search (Mohammad, 2011), and improving human-computer interaction (Vel´asquez, 1997; Ravaja et al., 2006). Supervised methods for classifying emotions expressed in a sentence tend to perform better than unsupervised ones. They use features such as unigrams and bigrams (Alm et al., 2005; Aman and Szpakowicz, 2007; Neviarouskaya et al., 2009; Chaffar and Inkpen, 2011). For example, a system can learn that the word excruciating tends to occur in sentences labeled with sadness, and use this word as a feature in classifying new sentence"
N12-1071,S07-1013,0,0.666612,"Missing"
N12-1071,strapparava-valitutti-2004-wordnet,0,0.584805,"sults that are hard to surpass. For example, it is possible that classifiers can learn from unigram features alone that excruciating is associated with sadness and fear. In this paper, we investigate whether word– emotion association lexicons can provide gains in addition to those already provided by ngram features. We conduct experiments with different affect lexicons and determine their usefulness in this extrinsic task. We also conduct experiments to determine how portable the ngram features and the emotion lexicon features are to a new domain. 2 Affect Lexicons The WordNet Affect Lexicon (Strapparava and Valitutti, 2004) has a few thousand words annotated for associations with a number of affect categories. This includes 1536 words annotated for associations 587 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 587–591, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics with six emotions considered to be the most basic— joy, sadness, fear, disgust, anger, and surprise (Ekman, 1992).1 It was created by manually identifying the emotions of a few seed words and then labeling all their WordNet synonym"
N15-1078,W11-0705,0,0.0436407,"lysis of English Social Media Sentiment analysis systems have been applied to many different kinds of texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas, 2002; Mohammad and Yang, 2011), emails (Liu et al., 2003; Mohammad and Yang, 2011), blogs (Neviarouskaya et al., 2011), and tweets (Mohammad, 2012). Often these systems have to cater to the specific needs of the text such as formality versus informality, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by Go et al. (2009), Pak and Paroubek (2010), Agarwal et al. (2011), and Thelwall et al. (2011). A survey by Mart´ınez-C´amara et al. (2012) provides an overview of the research on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment analysis were organized by the Conference on Semantic Evaluation Exercises (SemEval), which allowed for comparison of different approaches on common datasets from different domains (Wilson et al., 2013; Rosenthal et al., 2014; Pontiki et al., 2014). The NRC-Canada system (Kiritchenko et al., 2014) ranked first in these competitions, and we use it in our experiments. Details of the system are desc"
N15-1078,W14-3623,0,0.0773688,"languages other than Arabic and multiple scripts may be used to express Arabic and foreign words. In addition, Arabic is a morphologically complex language, thus having a lexicon of word-sentiment associations that covers all different surface forms becomes a cumbersome task. Negation in MSA is expressed through negation particles, but in some dialects (Egyptian) it is expressed using suffixes at the end of the word. We refer the reader to Mourad and Darwish (2013) for more details on these issues. There have been a few studies tackling sentiment analysis of Arabic texts (Ahmad et al., 2006; Badaro et al., 2014). The ones most closely related to our work are the studies of sentiment analysis of Arabic social media (Al-Kabi et al., 2013; ElBeltagy and Ali, 2013; Mourad and Darwish, 2013; Abdul-Mageed et al., 2014). Here we review existing Arabic sentiment analysis systems that were designed specifically for Arabic social media datasets. Abdul-Mageed et al. (2014) trained an SVM classifier on a manually labeled dataset and applied a two-stage classification that first separates subjective from objective sentences and then classifies the subjective into positive or negative instances. The authors have c"
N15-1078,W10-0201,0,0.0284986,"to Arabic. We also created a substantial amount of sentiment labeled data pertaining to Arabic social media texts and their English translations which is made freely available.1 1 http://www.purl.com/net/ArabicSentiment 768 This is the first such resource where text in one language and its translations into another language (both manually and automatically produced) are each manually labeled for sentiment. 2 2.1 Related Work Sentiment Analysis of English Social Media Sentiment analysis systems have been applied to many different kinds of texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas, 2002; Mohammad and Yang, 2011), emails (Liu et al., 2003; Mohammad and Yang, 2011), blogs (Neviarouskaya et al., 2011), and tweets (Mohammad, 2012). Often these systems have to cater to the specific needs of the text such as formality versus informality, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by Go et al. (2009), Pak and Paroubek (2010), Agarwal et al. (2011), and Thelwall et al. (2011). A survey by Mart´ınez-C´amara et al. (2012) provides an overview of the research on sentiment analysis of tweets. In the la"
N15-1078,E14-1064,0,0.0187301,"German, Spanish, and French. The authors concluded that the quality of machine translation is sufficient for sentiment analysis to be performed on automatically translated texts without a substantial loss in accuracy. Contrary to that work, our study uses both manual and automatic translations as well as both manual and automatic sentiment assignments to systematically examine the effect of translation on sentiment. Additionally, we deal with noisy social media texts as opposed to more polished news media texts. There exists research on using sentiment analysis to improve machine translation (Chen and Zhu, 2014), but that is beyond the scope of this paper. 3 Method for Determining Sentiment Predictability on Translation In order to systematically study the impact of translation on sentiment analysis, we propose the following experimental setup: • Identify or compile an Arabic social media dataset. We will refer to it as Ar. (Ar comes from the first two letter of Arabic.) • Manually translate Ar into English. We will refer to these English translations as En(Manl.Trans.) [Manl. is for manual, and Trans. is for translations.] • Automatically translate Ar into English. We will refer to these English tra"
N15-1078,N12-1047,0,0.0104449,"asks: generating translations manually and automatically (Section 4), manually annotating Arabic and English texts for sentiment (Section 5), automatic sentiment analysis of English texts (Section 6), and automatic sentiment analysis of Arabic texts (Section 7). 4 Generating English Translations The BBN dialectal Arabic dataset comes with manual translations into English. We generate automatic translations of the Arabic BBN posts and the Syrian tweets, by training a multi-stack phrase-based machine translation system to translate from Arabic to English. Our in-house system is quite similar to Cherry and Foster (2012). This statistical machine translation (SMT) system is trained on data from OpenMT 2012. We preprocess the training data by 2 https://catalog.ldc.upenn.edu/LDC2012T09 segmenting the Arabic source side of the training data with MADA 3.2 (Habash et al., 2009), using Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012). The Arabic script is further normalized by convert  ing different forms of Alif @ @ @ @ and Ya ø ø to bare Alif @ and dotless Ya ø. The different forms are used interchangeably, and normalization decreases the sparcity of Arabic tokens and"
N15-1078,D08-1024,0,0.0162359,"). The Arabic script is further normalized by convert  ing different forms of Alif @ @ @ @ and Ya ø ø to bare Alif @ and dotless Ya ø. The different forms are used interchangeably, and normalization decreases the sparcity of Arabic tokens and improves translation. The English side of the training data is lowercased and tokenized by stripping punctuation marks. We set the decoder’s stack size to 10000 and distortion limit to 7. We replace the out-of-vocabulary words in the translated text with UNKNOWN token (which is shown to the annotators). The decoder’s log-linear model is tuned with MIRA (Chiang et al., 2008; Cherry and Foster, 2012). A KN-smoothed 5gram language model is trained on the English Gigaword and the target side of the parallel data. 5 Creating sentiment labeled data in Arabic and English Manual sentiment annotations were performed on the crowdsourcing platform CrowdFlower3 for three BBN datasets and two Syrian datasets: 1. Original Arabic posts (BBN and Syria datasets), annotated by Arabic speakers. 2. Manual English translations of Arabic posts, annotated by English speakers (only for BBN dataset). 3. Automatic English translations of Arabic posts (BBN and Syria datasets), annotated"
N15-1078,P11-2008,0,0.0285766,"Missing"
N15-1078,P07-1123,0,0.0393262,"two native Arabic speakers. They used an SVM to classify tweets in a twostage approach, polar vs neutral, then positive vs. negative. The authors shared their data with us and we test our system on their dataset. However, the dataset they provided us is a larger superset than the one they had originally used (Refaee and Rieser, 2014a). Thus, the results of sentiment systems on the two sets are not directly comparable. 2.3 Multilingual Sentiment Analysis Work on multilingual sentiment analysis has mainly addressed mapping sentiment resources from English into morphologically complex languages. Mihalcea et al. (2007) used English resources to automatically generate a Romanian subjectivity lexicon using an English–Romanian dictionary. The generated lexicon is then used to classify Romanian text. Wan (2008) translated Chinese customer reviews to English using a machine trans769 lation system. The translated reviews are then classified with a rule-based system that relies on English lexicons. A higher accuracy is achieved by using ensemble methods and combining knowledge from Chinese and English resources. Balahur and Turchi (2014) conducted a study to assess the performance of statistical sentiment analysis"
N15-1078,W10-0204,1,0.275041,"international competitions on sentiment analysis of tweets –SemEval2013 Task 2 and SemEval-2014 Task 9 (Wilson et al., 2013; Rosenthal et al., 2014). We briefly describe the system below; for more details, we refer the reader to Kiritchenko et al. (2014). A linear-kernel Support Vector Machine (Chang and Lin, 2011) classifier is trained on the available training data. The classifier leverages a variety of surface-form, semantic, and sentiment lexicon features described below. The sentiment lexicon features are derived from existing, generalpurpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013), Bing Liu’s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically generated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014).4 6.1 Ablation experiments in Mohammad et al. (2013) showed that their sentiment system benefited most from the use of the Hashtag Sentiment Lexicon. The lexicon was created as follows. A list of 77 seed words, which are synonyms of positive and negative, was compiled from the Roget’s Thesaurus. Then, the Twitter API was polled to collec"
N15-1078,W11-1709,1,0.499825,"nt of sentiment labeled data pertaining to Arabic social media texts and their English translations which is made freely available.1 1 http://www.purl.com/net/ArabicSentiment 768 This is the first such resource where text in one language and its translations into another language (both manually and automatically produced) are each manually labeled for sentiment. 2 2.1 Related Work Sentiment Analysis of English Social Media Sentiment analysis systems have been applied to many different kinds of texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas, 2002; Mohammad and Yang, 2011), emails (Liu et al., 2003; Mohammad and Yang, 2011), blogs (Neviarouskaya et al., 2011), and tweets (Mohammad, 2012). Often these systems have to cater to the specific needs of the text such as formality versus informality, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by Go et al. (2009), Pak and Paroubek (2010), Agarwal et al. (2011), and Thelwall et al. (2011). A survey by Mart´ınez-C´amara et al. (2012) provides an overview of the research on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment analy"
N15-1078,S13-2053,1,0.596913,"classifier is trained on the available training data. The classifier leverages a variety of surface-form, semantic, and sentiment lexicon features described below. The sentiment lexicon features are derived from existing, generalpurpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013), Bing Liu’s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically generated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014).4 6.1 Ablation experiments in Mohammad et al. (2013) showed that their sentiment system benefited most from the use of the Hashtag Sentiment Lexicon. The lexicon was created as follows. A list of 77 seed words, which are synonyms of positive and negative, was compiled from the Roget’s Thesaurus. Then, the Twitter API was polled to collect tweets that had these words as hashtags. A tweet is considered positive if it has a positive hashtag and negative if it 4 http://www.crowdflower.com 771 Generating English Sentiment Lexicon http://www.purl.com/net/lexicons BBN data a. Ar(Manl.Sent) b. En(Manl.Trans., Manl.Sent) c. En(Auto.Trans., Manl.Sent) Sy"
N15-1078,S12-1033,1,0.801929,"able.1 1 http://www.purl.com/net/ArabicSentiment 768 This is the first such resource where text in one language and its translations into another language (both manually and automatically produced) are each manually labeled for sentiment. 2 2.1 Related Work Sentiment Analysis of English Social Media Sentiment analysis systems have been applied to many different kinds of texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas, 2002; Mohammad and Yang, 2011), emails (Liu et al., 2003; Mohammad and Yang, 2011), blogs (Neviarouskaya et al., 2011), and tweets (Mohammad, 2012). Often these systems have to cater to the specific needs of the text such as formality versus informality, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by Go et al. (2009), Pak and Paroubek (2010), Agarwal et al. (2011), and Thelwall et al. (2011). A survey by Mart´ınez-C´amara et al. (2012) provides an overview of the research on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment analysis were organized by the Conference on Semantic Evaluation Exercises (SemEval), which allowed for comparison of diff"
N15-1078,W13-1608,0,0.45957,"hich is a standardized form of Arabic, dialectal Arabic is the spoken form of Arabic and lacks strict writing standards. The text often includes words from languages other than Arabic and multiple scripts may be used to express Arabic and foreign words. In addition, Arabic is a morphologically complex language, thus having a lexicon of word-sentiment associations that covers all different surface forms becomes a cumbersome task. Negation in MSA is expressed through negation particles, but in some dialects (Egyptian) it is expressed using suffixes at the end of the word. We refer the reader to Mourad and Darwish (2013) for more details on these issues. There have been a few studies tackling sentiment analysis of Arabic texts (Ahmad et al., 2006; Badaro et al., 2014). The ones most closely related to our work are the studies of sentiment analysis of Arabic social media (Al-Kabi et al., 2013; ElBeltagy and Ali, 2013; Mourad and Darwish, 2013; Abdul-Mageed et al., 2014). Here we review existing Arabic sentiment analysis systems that were designed specifically for Arabic social media datasets. Abdul-Mageed et al. (2014) trained an SVM classifier on a manually labeled dataset and applied a two-stage classificati"
N15-1078,pak-paroubek-2010-twitter,0,0.0342938,"elated Work Sentiment Analysis of English Social Media Sentiment analysis systems have been applied to many different kinds of texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas, 2002; Mohammad and Yang, 2011), emails (Liu et al., 2003; Mohammad and Yang, 2011), blogs (Neviarouskaya et al., 2011), and tweets (Mohammad, 2012). Often these systems have to cater to the specific needs of the text such as formality versus informality, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by Go et al. (2009), Pak and Paroubek (2010), Agarwal et al. (2011), and Thelwall et al. (2011). A survey by Mart´ınez-C´amara et al. (2012) provides an overview of the research on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment analysis were organized by the Conference on Semantic Evaluation Exercises (SemEval), which allowed for comparison of different approaches on common datasets from different domains (Wilson et al., 2013; Rosenthal et al., 2014; Pontiki et al., 2014). The NRC-Canada system (Kiritchenko et al., 2014) ranked first in these competitions, and we use it in our experiments. Details"
N15-1078,S14-2004,0,0.0062074,"y, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by Go et al. (2009), Pak and Paroubek (2010), Agarwal et al. (2011), and Thelwall et al. (2011). A survey by Mart´ınez-C´amara et al. (2012) provides an overview of the research on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment analysis were organized by the Conference on Semantic Evaluation Exercises (SemEval), which allowed for comparison of different approaches on common datasets from different domains (Wilson et al., 2013; Rosenthal et al., 2014; Pontiki et al., 2014). The NRC-Canada system (Kiritchenko et al., 2014) ranked first in these competitions, and we use it in our experiments. Details of the system are described in Section 6. 2.2 Sentiment Analysis of Arabic Social Media Sentiment analysis of Arabic social media texts has several challenges. The text is often in a regional Arabic dialect rather than Modern Standard Arabic (MSA). Unlike MSA which is a standardized form of Arabic, dialectal Arabic is the spoken form of Arabic and lacks strict writing standards. The text often includes words from languages other than Arabic and multiple scripts may b"
N15-1078,refaee-rieser-2014-arabic,0,0.318575,"ally labeled dataset and applied a two-stage classification that first separates subjective from objective sentences and then classifies the subjective into positive or negative instances. The authors have compiled several datasets from multiple social media resources that include chatroom messages, tweets, forum posts, and Wikipedia Talk pages. However, these resources have not been made publicly available yet. Mourad and Darwish (2013) trained SVM and Naive Bayes classifiers on Arabic tweets annotated by two native Arabic speakers. We compare our system’s performance to theirs in Section 7. Refaee and Rieser (2014b) manually annotated tweets for sentiment by two native Arabic speakers. They used an SVM to classify tweets in a twostage approach, polar vs neutral, then positive vs. negative. The authors shared their data with us and we test our system on their dataset. However, the dataset they provided us is a larger superset than the one they had originally used (Refaee and Rieser, 2014a). Thus, the results of sentiment systems on the two sets are not directly comparable. 2.3 Multilingual Sentiment Analysis Work on multilingual sentiment analysis has mainly addressed mapping sentiment resources from En"
N15-1078,S14-2009,0,0.0547714,"mality versus informality, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by Go et al. (2009), Pak and Paroubek (2010), Agarwal et al. (2011), and Thelwall et al. (2011). A survey by Mart´ınez-C´amara et al. (2012) provides an overview of the research on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment analysis were organized by the Conference on Semantic Evaluation Exercises (SemEval), which allowed for comparison of different approaches on common datasets from different domains (Wilson et al., 2013; Rosenthal et al., 2014; Pontiki et al., 2014). The NRC-Canada system (Kiritchenko et al., 2014) ranked first in these competitions, and we use it in our experiments. Details of the system are described in Section 6. 2.2 Sentiment Analysis of Arabic Social Media Sentiment analysis of Arabic social media texts has several challenges. The text is often in a regional Arabic dialect rather than Modern Standard Arabic (MSA). Unlike MSA which is a standardized form of Arabic, dialectal Arabic is the spoken form of Arabic and lacks strict writing standards. The text often includes words from languages other than Arabic and"
N15-1078,D08-1058,0,0.354477,"dataset. However, the dataset they provided us is a larger superset than the one they had originally used (Refaee and Rieser, 2014a). Thus, the results of sentiment systems on the two sets are not directly comparable. 2.3 Multilingual Sentiment Analysis Work on multilingual sentiment analysis has mainly addressed mapping sentiment resources from English into morphologically complex languages. Mihalcea et al. (2007) used English resources to automatically generate a Romanian subjectivity lexicon using an English–Romanian dictionary. The generated lexicon is then used to classify Romanian text. Wan (2008) translated Chinese customer reviews to English using a machine trans769 lation system. The translated reviews are then classified with a rule-based system that relies on English lexicons. A higher accuracy is achieved by using ensemble methods and combining knowledge from Chinese and English resources. Balahur and Turchi (2014) conducted a study to assess the performance of statistical sentiment analysis techniques on machine-translated texts. Opinionbearing phrases from the New York Times text corpus (2002–2005) were automatically translated using publicly available machine-translation engin"
N15-1078,H05-1044,0,0.0653843,"Rosenthal et al., 2014). We briefly describe the system below; for more details, we refer the reader to Kiritchenko et al. (2014). A linear-kernel Support Vector Machine (Chang and Lin, 2011) classifier is trained on the available training data. The classifier leverages a variety of surface-form, semantic, and sentiment lexicon features described below. The sentiment lexicon features are derived from existing, generalpurpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013), Bing Liu’s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically generated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014).4 6.1 Ablation experiments in Mohammad et al. (2013) showed that their sentiment system benefited most from the use of the Hashtag Sentiment Lexicon. The lexicon was created as follows. A list of 77 seed words, which are synonyms of positive and negative, was compiled from the Roget’s Thesaurus. Then, the Twitter API was polled to collect tweets that had these words as hashtags. A tweet is considered positive if it has a positive hashtag and negative if i"
N15-1078,S13-2052,0,0.0213633,"Missing"
N15-1078,N12-1006,0,0.0137989,"d to often be translated into neutral tweets, and to what extent. The results will also show how feasible it is to first translate Arabic text into English and then use automatic sentiment analysis (Ar(Manl.Sent.) vs. En(Auto.Trans., Auto.Sent.)). In Section 8, we provide an analysis of several such comparisons for two different Arabic social media datasets. DATA: Since manual translation of text from Arabic to English is a costly exercise, we chose, for our experiments, an existing Arabic social media dataset that has already been translated – the BBN Arabic770 Dialect/English Parallel Text (Zbib et al., 2012).2 It contains about 3.5 million tokens of Arabic dialect sentences and their English translations. We use a randomly chosen subset of 1200 Levantine dialectal sentences, which we will refer to as the BBN posts or BBN dataset, in our experiments. Additionally, we also conduct experiments on a dataset of 2000 tweets originating from Syria (a country where Levantine dialectal Arabic is commonly spoken). These tweets were collected in May 2014 by polling the Twitter API. We will refer to this dataset as the Syrian tweets or Syrian dataset. Note, however, that manual translations of the Syrian dat"
N16-1095,esuli-sebastiani-2006-sentiwordnet,0,0.0292215,"ons, commonly captured in sentiment lexicons, are useful in automatic sentiment prediction (Pontiki et al., 2014; Rosenthal et al., 2014), stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons are especially useful because they tend to be more accurate than automatically generated lexicons; they can be used to automatically generate large-scale lexicons (Tang et al., 2014; Esuli and Sebastiani, 2006); The sentiment of a phrase can differ significantly from the sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, from its constituents. Lexicons that include sentiment associations for phrases as well as for their constituent words are useful in studying sentiment composition. We refer to them as sentiment composition lexicons (SCLs). We created SCLs for three domains, and all three were used in recent SemEval shared tasks. We refer to the lexicon created for the English Twitter domain as th"
N16-1095,N15-1124,0,0.0225569,"Missing"
N16-1095,W16-0410,1,0.818295,"Missing"
N16-1095,S16-1004,1,0.652983,"Missing"
N16-1095,L16-1623,1,0.529786,"s in three different domains: general English, English Twitter, and Arabic Twitter. We show that on all three domains the ranking of words by sentiment remains remarkably consistent even when the annotation process is repeated with a different set of annotators. We also, for the first time, determine the minimum difference in sentiment association that is perceptible to native speakers of a language. 1 Introduction Word–sentiment associations, commonly captured in sentiment lexicons, are useful in automatic sentiment prediction (Pontiki et al., 2014; Rosenthal et al., 2014), stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons are especially useful because they tend to be more accurate than automatically generated lexicons; they can be used to automatically generate large-scale lexicons (Tang et al., 2014; Esuli and Sebastiani, 2006); The sentiment of a phrase can differ significantly from the sentiment of its constituent words. Sentiment composition is the determining of sentiment"
N16-1095,S14-2004,0,0.0250713,"ain real-valued sentiment association scores for words and phrases in three different domains: general English, English Twitter, and Arabic Twitter. We show that on all three domains the ranking of words by sentiment remains remarkably consistent even when the annotation process is repeated with a different set of annotators. We also, for the first time, determine the minimum difference in sentiment association that is perceptible to native speakers of a language. 1 Introduction Word–sentiment associations, commonly captured in sentiment lexicons, are useful in automatic sentiment prediction (Pontiki et al., 2014; Rosenthal et al., 2014), stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons are especially useful because they tend to be more accurate than automatically generated lexicons; they can be used to automatically generate large-scale lexicons (Tang et al., 2014; Esuli and Sebastiani, 2006); The sentiment of a phrase can differ significantly from the sentiment of its consti"
N16-1095,S14-2009,0,0.0132692,"ent association scores for words and phrases in three different domains: general English, English Twitter, and Arabic Twitter. We show that on all three domains the ranking of words by sentiment remains remarkably consistent even when the annotation process is repeated with a different set of annotators. We also, for the first time, determine the minimum difference in sentiment association that is perceptible to native speakers of a language. 1 Introduction Word–sentiment associations, commonly captured in sentiment lexicons, are useful in automatic sentiment prediction (Pontiki et al., 2014; Rosenthal et al., 2014), stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons are especially useful because they tend to be more accurate than automatically generated lexicons; they can be used to automatically generate large-scale lexicons (Tang et al., 2014; Esuli and Sebastiani, 2006); The sentiment of a phrase can differ significantly from the sentiment of its constituent words. Sentiment co"
N16-1095,S15-2078,1,0.593506,"Missing"
N16-1095,C14-1018,0,0.011583,"sentiment associations, commonly captured in sentiment lexicons, are useful in automatic sentiment prediction (Pontiki et al., 2014; Rosenthal et al., 2014), stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons are especially useful because they tend to be more accurate than automatically generated lexicons; they can be used to automatically generate large-scale lexicons (Tang et al., 2014; Esuli and Sebastiani, 2006); The sentiment of a phrase can differ significantly from the sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, from its constituents. Lexicons that include sentiment associations for phrases as well as for their constituent words are useful in studying sentiment composition. We refer to them as sentiment composition lexicons (SCLs). We created SCLs for three domains, and all three were used in recent SemEval shared tasks. We refer to the lexicon created for the"
N16-1095,H05-1044,0,0.0386704,"sh Twitter domain as the SemEval2015 English Twitter Sentiment Lexicon; for the general English domain as the SemEval-2016 General English Sentiment Modifiers Lexicon; and for the Arabic Twitter domain as the SemEval-2016 Arabic Twitter Sentiment Lexicon. Note that the English Twitter lexicon was first described in (Kiritchenko et al., 2014), whereas the other two are novel contributions presented in this paper. Most existing manually created sentiment lexicons tend to provide only lists of positive and negative words with very coarse levels of sentiment (Stone et al., 1966; Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less useful in downstream applications than having access to fine-grained (real-valued) sentiment association scores. Only a small number of manual lexicons 811 Proceedings of NAACL-HLT 2016, pages 811–817, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics capture sentiment associations at a fine-grained level (Bradley and Lang, 1999; Warriner et al., 2013). This is not surprising because obtaining real-valued sentiment annotations has several challenges. Respondents are faced with a hi"
N16-1128,D08-1083,0,0.0816623,"Missing"
N16-1128,J15-2004,0,0.062773,"Missing"
N16-1128,P11-2008,0,0.00686058,"Missing"
N16-1128,N16-1095,1,0.686479,"ch phrases present a particular challenge for automatic sentiment analysis systems that often rely on bag-ofword features. Word–sentiment associations are commonly captured in sentiment lexicons. However, most existing manually created sentiment lexicons include only single words. Lexicons that include sentiment associations for multi-word phrases as well as their constituent words can be very useful in studying sentiment composition. We refer to them as sentiment composition lexicons (SCLs). We created a sentiment composition lexicon for opposing polarity phrases and their constituent words (Kiritchenko and Mohammad, 2016c).1 Both phrases and single words were manually annotated with real-valued sentiment association scores using an annotation scheme known as Best–Worst Scaling.2 We refer to the created resource as the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP). The lexicon includes entries for 265 trigrams, 311 bigrams, and 602 unigrams. In this paper, we use SCL-OPP to analyze regularities present in different kinds of opposing polarity phrases. We calculate the extent to which different part-of-speech combinations result in phrases of positive and negative polarity. We also show t"
N16-1128,W16-0410,1,0.873484,"ch phrases present a particular challenge for automatic sentiment analysis systems that often rely on bag-ofword features. Word–sentiment associations are commonly captured in sentiment lexicons. However, most existing manually created sentiment lexicons include only single words. Lexicons that include sentiment associations for multi-word phrases as well as their constituent words can be very useful in studying sentiment composition. We refer to them as sentiment composition lexicons (SCLs). We created a sentiment composition lexicon for opposing polarity phrases and their constituent words (Kiritchenko and Mohammad, 2016c).1 Both phrases and single words were manually annotated with real-valued sentiment association scores using an annotation scheme known as Best–Worst Scaling.2 We refer to the created resource as the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP). The lexicon includes entries for 265 trigrams, 311 bigrams, and 602 unigrams. In this paper, we use SCL-OPP to analyze regularities present in different kinds of opposing polarity phrases. We calculate the extent to which different part-of-speech combinations result in phrases of positive and negative polarity. We also show t"
N16-1128,L16-1184,1,0.82923,"Missing"
N16-1128,S16-1004,1,0.215962,"oy Best–Worst Scaling for sentiment annotation by providing four (single-word or multiword) terms at a time and asking which term is the most positive (or least negative) and which is the least positive (or most negative). Each question was answered by eight annotators through a crowdsourcing platform, CrowdFlower.4 We refer to the resulting lexicon as the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP). Portions of the created lexicon have been used as development and evaluation sets in SemEval-2016 Task 7 ‘Determining Sentiment Intensity of English and Arabic Phrases’ (Kiritchenko et al., 2016).5 The objective of that task was to test different methods of automatically predicting sentiment association scores for multi-word phrases. 4 Sentiment Composition Patterns SCL-OPP allows us to explore sentiment composition patterns in opposing polarity phrases. We define a Sentiment Composition Pattern (SCP) as a rule that includes on the left-hand side the parts of speech (POS) and the sentiment associations of the constituent unigrams (in the order they appear in the phrase), and on the right-hand side the sentiment association of the phrase. Table 2 shows examples. SCPs that have a positi"
N16-1128,D09-1017,0,0.0402378,"proaches have been proposed to address sentiment composition, which include manually derived syntactic rules (Moilanen and Pulman, 2007; Neviarouskaya et al., 2010), combination of hand-written rules and statistical learning (Choi and Cardie, 2008), and machine learning approaches (Nakagawa et al., 2010; Yessenalina and Cardie, 2011; Dong et al., 2015). Much work has been devoted to model the impact of negators and (to a lesser degree) intensifiers, words commonly referred to as contextual valence shifters, on sentiment of words they modify (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Liu and Seneff, 2009; Wiegand et al., 2010; Taboada et al., 2011; Kiritchenko et al., 2014). Kiritchenko and Mohammad (2016b) created a sentiment composition lexicon for negators, modals, and adverbs (SCL-NMA) through manual annotation and analyzed the effect of these groups of modifiers on sentiment in short phrases. Recently, recursive deep model approaches have been proposed for handling sentiment of syntactic phrases through sentiment composition over parse trees (Socher et al., 2013; Zhu et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015). In this work, we apply several unsupervised and supervised techni"
N16-1128,W10-0204,1,0.748555,"in SCL-OPP. to (Kiritchenko and Mohammad, 2016c). Table 1 shows a few example entries from the lexicon. Term selection: We polled the Twitter API (from 2013 to 2015) to collect about 11 million tweets that contain emoticons: ‘:)’ or ‘:(’. We will refer to this corpus as the Emoticon Tweets Corpus. From this corpus, we selected bigrams and trigrams that had at least one positive word and at least one negative word. The polarity labels (positive or negative) of the words were determined by simple look-up in existing sentiment lexicons: Hu and Liu lexicon (Hu and Liu, 2004), NRC Emotion lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013), MPQA lexicon (Wilson et al., 2005), and NRC’s Twitter-specific lexicon (Kiritchenko et al., 2014; Mohammad et al., 2013).3 In total, 576 opposing polarity n-grams (bigrams and trigrams) were selected. We also chose for annotation all unigrams that appeared in the selected set of bigrams and trigrams. There were 602 such unigrams. Note that even though the multi-word phrases and single-word terms were drawn from a corpus of tweets, most of the terms are used in everyday English. Best–Worst Scaling Method of Annotation: Best– Worst Scaling (BWS), also sometimes refe"
N16-1128,S13-2053,1,0.155934,"013 to 2015) to collect about 11 million tweets that contain emoticons: ‘:)’ or ‘:(’. We will refer to this corpus as the Emoticon Tweets Corpus. From this corpus, we selected bigrams and trigrams that had at least one positive word and at least one negative word. The polarity labels (positive or negative) of the words were determined by simple look-up in existing sentiment lexicons: Hu and Liu lexicon (Hu and Liu, 2004), NRC Emotion lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013), MPQA lexicon (Wilson et al., 2005), and NRC’s Twitter-specific lexicon (Kiritchenko et al., 2014; Mohammad et al., 2013).3 In total, 576 opposing polarity n-grams (bigrams and trigrams) were selected. We also chose for annotation all unigrams that appeared in the selected set of bigrams and trigrams. There were 602 such unigrams. Note that even though the multi-word phrases and single-word terms were drawn from a corpus of tweets, most of the terms are used in everyday English. Best–Worst Scaling Method of Annotation: Best– Worst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff), is an annotation scheme that exploits the comparative approach to annotation (Louviere and Woodworth,"
N16-1128,N10-1120,0,0.0848781,"Missing"
N16-1128,C10-1091,0,0.0302683,"Missing"
N16-1128,D13-1170,0,0.0432066,"rred to as contextual valence shifters, on sentiment of words they modify (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Liu and Seneff, 2009; Wiegand et al., 2010; Taboada et al., 2011; Kiritchenko et al., 2014). Kiritchenko and Mohammad (2016b) created a sentiment composition lexicon for negators, modals, and adverbs (SCL-NMA) through manual annotation and analyzed the effect of these groups of modifiers on sentiment in short phrases. Recently, recursive deep model approaches have been proposed for handling sentiment of syntactic phrases through sentiment composition over parse trees (Socher et al., 2013; Zhu et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015). In this work, we apply several unsupervised and supervised techniques of sentiment composition for a specific type of phrases—opposing polarity phrases. 3 Creating a Sentiment Lexicon for Opposing Polarity Phrases This section summarizes how we created a sentiment composition lexicon for opposing polarity phrases using the Best–Worst Scaling annotation technique. For more details we refer the reader 1103 Term best winter break breaking free isn’t long enough breaking heart breaking moment Sentiment score 0.844 0.172 -0.188 -0.500 -"
N16-1128,J11-2001,0,0.0405272,"ment composition, which include manually derived syntactic rules (Moilanen and Pulman, 2007; Neviarouskaya et al., 2010), combination of hand-written rules and statistical learning (Choi and Cardie, 2008), and machine learning approaches (Nakagawa et al., 2010; Yessenalina and Cardie, 2011; Dong et al., 2015). Much work has been devoted to model the impact of negators and (to a lesser degree) intensifiers, words commonly referred to as contextual valence shifters, on sentiment of words they modify (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Liu and Seneff, 2009; Wiegand et al., 2010; Taboada et al., 2011; Kiritchenko et al., 2014). Kiritchenko and Mohammad (2016b) created a sentiment composition lexicon for negators, modals, and adverbs (SCL-NMA) through manual annotation and analyzed the effect of these groups of modifiers on sentiment in short phrases. Recently, recursive deep model approaches have been proposed for handling sentiment of syntactic phrases through sentiment composition over parse trees (Socher et al., 2013; Zhu et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015). In this work, we apply several unsupervised and supervised techniques of sentiment composition for a specific"
N16-1128,P15-1150,0,0.0359228,"hey modify (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Liu and Seneff, 2009; Wiegand et al., 2010; Taboada et al., 2011; Kiritchenko et al., 2014). Kiritchenko and Mohammad (2016b) created a sentiment composition lexicon for negators, modals, and adverbs (SCL-NMA) through manual annotation and analyzed the effect of these groups of modifiers on sentiment in short phrases. Recently, recursive deep model approaches have been proposed for handling sentiment of syntactic phrases through sentiment composition over parse trees (Socher et al., 2013; Zhu et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015). In this work, we apply several unsupervised and supervised techniques of sentiment composition for a specific type of phrases—opposing polarity phrases. 3 Creating a Sentiment Lexicon for Opposing Polarity Phrases This section summarizes how we created a sentiment composition lexicon for opposing polarity phrases using the Best–Worst Scaling annotation technique. For more details we refer the reader 1103 Term best winter break breaking free isn’t long enough breaking heart breaking moment Sentiment score 0.844 0.172 -0.188 -0.500 -0.797 Table 1: Example entries in SCL-OPP. to (Kiritchenko an"
N16-1128,W10-3111,0,0.0313631,"posed to address sentiment composition, which include manually derived syntactic rules (Moilanen and Pulman, 2007; Neviarouskaya et al., 2010), combination of hand-written rules and statistical learning (Choi and Cardie, 2008), and machine learning approaches (Nakagawa et al., 2010; Yessenalina and Cardie, 2011; Dong et al., 2015). Much work has been devoted to model the impact of negators and (to a lesser degree) intensifiers, words commonly referred to as contextual valence shifters, on sentiment of words they modify (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Liu and Seneff, 2009; Wiegand et al., 2010; Taboada et al., 2011; Kiritchenko et al., 2014). Kiritchenko and Mohammad (2016b) created a sentiment composition lexicon for negators, modals, and adverbs (SCL-NMA) through manual annotation and analyzed the effect of these groups of modifiers on sentiment in short phrases. Recently, recursive deep model approaches have been proposed for handling sentiment of syntactic phrases through sentiment composition over parse trees (Socher et al., 2013; Zhu et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015). In this work, we apply several unsupervised and supervised techniques of sentiment comp"
N16-1128,H05-1044,0,0.141956,"example entries from the lexicon. Term selection: We polled the Twitter API (from 2013 to 2015) to collect about 11 million tweets that contain emoticons: ‘:)’ or ‘:(’. We will refer to this corpus as the Emoticon Tweets Corpus. From this corpus, we selected bigrams and trigrams that had at least one positive word and at least one negative word. The polarity labels (positive or negative) of the words were determined by simple look-up in existing sentiment lexicons: Hu and Liu lexicon (Hu and Liu, 2004), NRC Emotion lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013), MPQA lexicon (Wilson et al., 2005), and NRC’s Twitter-specific lexicon (Kiritchenko et al., 2014; Mohammad et al., 2013).3 In total, 576 opposing polarity n-grams (bigrams and trigrams) were selected. We also chose for annotation all unigrams that appeared in the selected set of bigrams and trigrams. There were 602 such unigrams. Note that even though the multi-word phrases and single-word terms were drawn from a corpus of tweets, most of the terms are used in everyday English. Best–Worst Scaling Method of Annotation: Best– Worst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff), is an annotatio"
N16-1128,D11-1016,0,0.140006,"Missing"
N16-1128,P14-1029,1,0.260302,"valence shifters, on sentiment of words they modify (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Liu and Seneff, 2009; Wiegand et al., 2010; Taboada et al., 2011; Kiritchenko et al., 2014). Kiritchenko and Mohammad (2016b) created a sentiment composition lexicon for negators, modals, and adverbs (SCL-NMA) through manual annotation and analyzed the effect of these groups of modifiers on sentiment in short phrases. Recently, recursive deep model approaches have been proposed for handling sentiment of syntactic phrases through sentiment composition over parse trees (Socher et al., 2013; Zhu et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015). In this work, we apply several unsupervised and supervised techniques of sentiment composition for a specific type of phrases—opposing polarity phrases. 3 Creating a Sentiment Lexicon for Opposing Polarity Phrases This section summarizes how we created a sentiment composition lexicon for opposing polarity phrases using the Best–Worst Scaling annotation technique. For more details we refer the reader 1103 Term best winter break breaking free isn’t long enough breaking heart breaking moment Sentiment score 0.844 0.172 -0.188 -0.500 -0.797 Table 1: Exa"
N19-1050,S14-2010,0,0.018053,") compiled a dataset of 2,180 bigram–unigram synonym pairs from WordNet synsets. (The bigrams are either noun–noun or adjective–noun phrases.) Other pairs were created taking bigrams and words that do not exist in the same synsets. 507 He thus created a dataset of synonyms and non-synonyms. In contrast to these datasets, BiRD has fine-grained relatedness scores. Motivation: This is useful for testing sensitivity of semantic composition models to word order. Other Similarity Datasets: There exist datasets on the semantic similarity between sentences and between documents (Marelli et al., 2014; Agirre et al., 2014; Cera et al., 2017). Those are outside the scope of this work. • The unigrams and bigrams should be commonly used English terms. Motivation: Data annotation of common terms is expected to be more reliable. Also, common terms are more likely to occur in application datasets. Other Natural Language Datasets Created Using BWS: BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018b), and tweet–emotion intensity (Mohammad a"
N19-1050,D10-1115,0,0.040685,"ghest score terms were often synonymous. Thus co-aligned pairs from phrase tables are indeed a good source of term pairs for a semantic relatedness dataset, since they include pairs with a wide variety of relatedness values. 6 Evaluating Methods of Semantic Composition on BiRD A popular approach to represent word meaning in natural language systems is through vectors that capture the contexts in which the word occurs. An area of active research is how these word vectors can be composed to create representations for larger units of text such as phrases and sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Tai et al., 2015). Even though there is a large body of work on how to represent the meanings of sentences (Le and Mikolov, 2014; Kiros et al., 2015; Lin et al., 2017), there is relatively less work on how best to compose the meanings of two words to represent the meaning of a bigram. One reason for this is a lack of suitable evaluation resources. A common approach to evaluate representations of unigrams is through their ability to rank pairs of words by closeness in meaning (Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui and Dyer, 2014). BiRD allows for the e"
N19-1050,H92-1086,0,0.613225,"nd its use in NLP applications. 3 In a separate project, the second author is developing a semantic relatedness dataset for unigrams using BWS (an order of magnitude larger than existing ones). Project page: http://saifmohammad.com/WebPages/Relatedness.html simple arithmetic on the counts of how often an item is chosen best and worst (Orme, 2009; Flynn and Marley, 2014). (Details in Section 3.) To evaluate the quality of BiRD we determine the consistency of the BWS annotations. A commonly used approach to determine consistency in dimensional annotations is to calculate split-half reliability (Cronbach, 1951). We show that our semantic relatedness annotations have a split-half reliability score of r = 0.937, indicating high reliability, that is, if the annotations were repeated then similar scores and rankings would be obtained. (Details in Section 4.) We use BiRD to (a) obtain insights into bigram semantic relatedness, and (b) to evaluate automatic semantic composition methods. Examining Bigram Semantic Relatedness: Since very little work exists on the semantic relatedness of bigrams, several research questions remain unanswered, including: What is the distribution and mean of the semantic relate"
N19-1050,N19-1423,0,0.0386984,"Missing"
N19-1050,E14-1049,0,0.180492,"What is the average semantic relatedness between a bigram and its hypernym?; Are co-aligned terms from a phrase table a good source of term pairs to be included in a semantic relatedness dataset (specifically, do they cover a wide range of semantic relatedness values)?; etc. In Section 5, we present an analysis of BiRD to obtain insights into these questions. Evaluating Semantic Composition: A common approach to evaluate different methods of representing words via vectors is through their ability to rank pairs of words by closeness in meaning (Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui and Dyer, 2014). BiRD allows for the evaluation of semantic composition methods through their ability to rank pairs involving bigrams, by semantic relatedness. In Section 6, we present benchmark experiments on using BiRD as a testbed to evaluate various common semantic composition methods using various pre-trained word representations. Specifically, we conduct experiments to gain insights into research questions such as: Which common mathematical operations for vector composition (e.g., vector addition, vector multiplication, etc.) capture the semantics of a bigram more accurately?; Which of the two words in"
N19-1050,L18-1550,0,0.0230881,"elatedness of that bigram with other terms. 511 We focus on unsupervised approaches as we wanted to identify how well basic composition operations perform. The applicability of BiRD is much broader though, and it can be used: (1) for evaluating the large number of proposed supervised methods of semantic composition; (2) for evaluating the large number of measures of semantic relatedness; (3) to study the mechanisms underpinning semantic composition; etc. We leave those for future work. We test three vector space models to obtain word representations: GloVe (Pennington et al., 2014), fastText (Grave et al., 2018), and a traditional model based on matrix factorization of a word–context co-occurrence matrix (Turney et al., 2011). We test four mathematical composition operations: (1) vector addition, (2) element-wise vector multiplication, (3) tensor product with circular convolution (Widdows, 2008), and (4) dilation (Mitchell and Lapata, 2010). In adjective–noun and noun–noun bigrams, the second word usually plays a role of a head noun, and the first word is a modifier. We test the performance of two baseline methods that do not employ vector composition: one that represents a bigram with the vector for"
N19-1050,J15-4004,0,0.0702136,"antic priming and fMRI studies that show that the human brain stores information in a thematic manner (based on relatedness) rather than based on similarity (Hutchison, 2003; Huth et al., 2016). Word-Pair Datasets: Several semantic similarity and relatedness datasets involving unigram pairs (word pairs) exist. Rubenstein and Goodenough (1965) and Miller and Charles (1991) provided influential but small English word–pair datasets with fine–grained semantic similarity scores. More recent larger datasets including hundreds of pairs were provided by Finkelstein et al. (2002) (for relatedness) and Hill et al. (2015) (for similarity). Similar datasets exist in some other languages as well, such as the one by Gurevych (2006) and Panchenko et al. (2016) for relatedness. However, none of these datasets include items that are bigrams. Bigram Semantic Similarity Datasets: Mitchell and Lapata (2010) created a semantic similarity dataset for 324 bigram pairs. The terms include adjective–noun, noun–noun, and verb–object bigrams. Annotators were asked to choose an integer between one and seven, indicating a coarse semantic similarity rating. Turney (2012) compiled a dataset of 2,180 bigram–unigram synonym pairs fr"
N19-1050,N13-1062,0,0.0213718,"ther Similarity Datasets: There exist datasets on the semantic similarity between sentences and between documents (Marelli et al., 2014; Agirre et al., 2014; Cera et al., 2017). Those are outside the scope of this work. • The unigrams and bigrams should be commonly used English terms. Motivation: Data annotation of common terms is expected to be more reliable. Also, common terms are more likely to occur in application datasets. Other Natural Language Datasets Created Using BWS: BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018b), and tweet–emotion intensity (Mohammad and Kiritchenko, 2018). The largest BWS dataset is the NRC Valence, Arousal, and Dominance Lexicon, which has valence, arousal, and dominance scores for over 20,000 English words (Mohammad, 2018a). 3 English Bigram Relatedness Dataset We first describe how we selected the term pairs to include in the bigram relatedness dataset, followed by how they were annotated using BWS. 3.1 Term Pair Selection Randomly selecting term pairs will result in most pairs be"
N19-1050,S12-1047,1,0.811076,"ty of semantic composition models to word order. Other Similarity Datasets: There exist datasets on the semantic similarity between sentences and between documents (Marelli et al., 2014; Agirre et al., 2014; Cera et al., 2017). Those are outside the scope of this work. • The unigrams and bigrams should be commonly used English terms. Motivation: Data annotation of common terms is expected to be more reliable. Also, common terms are more likely to occur in application datasets. Other Natural Language Datasets Created Using BWS: BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018b), and tweet–emotion intensity (Mohammad and Kiritchenko, 2018). The largest BWS dataset is the NRC Valence, Arousal, and Dominance Lexicon, which has valence, arousal, and dominance scores for over 20,000 English words (Mohammad, 2018a). 3 English Bigram Relatedness Dataset We first describe how we selected the term pairs to include in the bigram relatedness dataset, followed by how they were annotated using BWS. 3.1 Term Pair Selection Randomly select"
N19-1050,N16-1095,1,0.875465,"oser to N 2 ). 505 Proceedings of NAACL-HLT 2019, pages 505–516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics working on 4-tuples, best–worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six items (i.e., for a 4-tuple with items A, B, C, and D, if A is the best, and D is the worst, then A > B, A > C, A > D, B > D, and C > D). It has been empirically shown that annotating 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016). Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales.2 In this paper, we describe how we obtained fine-grained human ratings of semantic relatedness for English term pairs involving at least one bigram.3 The other term in the pair is either another bigram or a unigram. We first selected a set of target bigrams AB (A represents the first word in the bigram and B represents the second word). For each AB, we created several pairs of the form AB–X, where X is a unigram or bigram"
N19-1050,P17-2074,1,0.925779,"(Annotators were asked to give scores for each pair; usually on a discrete 0 to 5 scale.) Rating scales suffer from significant known limitations, including: inconsistencies in annotations by different annotators, inconsistencies in annotations by the same annotator, scale region bias (annotators often have a bias towards a portion of the scale), and problems associated with a fixed granularity (Presser and Schuman, 1996). Best–Worst Scaling (BWS) is an annotation scheme that addresses these limitations by employing comparative annotations (Louviere, 1991; Cohen, 2003; Louviere et al., 2015; Kiritchenko and Mohammad, 2017). Annotators are given n items at a time (an n-tuple, where n > 1 and commonly n = 4). They are asked which item is the best (highest in terms of the property of interest) and which is the worst (least in terms of the property of interest).1 When 1 At its limit, when n = 2, BWS becomes a paired comparison (Thurstone, 1927; David, 1963), but then a much larger set of tuples need to be annotated (closer to N 2 ). 505 Proceedings of NAACL-HLT 2019, pages 505–516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics working on 4-tuples, best–worst annotati"
N19-1050,W10-1717,0,0.012859,"anguage being mapped to a common word/phrase in the other language. We will refer to such terms as being co-aligned. Due to the nature of languages and the various forms that the same text can be translated to, co-aligned terms tend to include not just synonyms but also other semantically related terms, and sometimes even unrelated terms. Thus, we hypothesize that it is beneficial to include pairs of co-aligned terms in a semantic relatedness dataset as they pertain to varying degrees of semantic relatedness. We used an English–French phrase table from the Portage Machine Translation Toolkit (Larkin et al., 2010) to determine additional pairs AB–X.8 Specifically, for each AB–F entry in the phrase table (where F is a French term) we keep the five most frequent English unigrams and the five most frequent English bigrams (other than AB) that are also aligned to F. Among the 4,095 ABs, 454 occurred in the phrase table. This resulted in 3,255 AB–X pairs in total (1,897 where X is a unigram, and 1,358 where X is a bigram). Finally, we chose to filter the term pairs, keeping only those ABs that occurred in at least three unique pairs. (So for a given AB, apart from the AB–BA entry, there should be at least t"
N19-1050,S14-2001,0,0.0275407,"y rating. Turney (2012) compiled a dataset of 2,180 bigram–unigram synonym pairs from WordNet synsets. (The bigrams are either noun–noun or adjective–noun phrases.) Other pairs were created taking bigrams and words that do not exist in the same synsets. 507 He thus created a dataset of synonyms and non-synonyms. In contrast to these datasets, BiRD has fine-grained relatedness scores. Motivation: This is useful for testing sensitivity of semantic composition models to word order. Other Similarity Datasets: There exist datasets on the semantic similarity between sentences and between documents (Marelli et al., 2014; Agirre et al., 2014; Cera et al., 2017). Those are outside the scope of this work. • The unigrams and bigrams should be commonly used English terms. Motivation: Data annotation of common terms is expected to be more reliable. Also, common terms are more likely to occur in application datasets. Other Natural Language Datasets Created Using BWS: BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018b), and tweet–emotion"
N19-1050,L18-1030,1,0.866193,"al., 2014; Cera et al., 2017). Those are outside the scope of this work. • The unigrams and bigrams should be commonly used English terms. Motivation: Data annotation of common terms is expected to be more reliable. Also, common terms are more likely to occur in application datasets. Other Natural Language Datasets Created Using BWS: BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018b), and tweet–emotion intensity (Mohammad and Kiritchenko, 2018). The largest BWS dataset is the NRC Valence, Arousal, and Dominance Lexicon, which has valence, arousal, and dominance scores for over 20,000 English words (Mohammad, 2018a). 3 English Bigram Relatedness Dataset We first describe how we selected the term pairs to include in the bigram relatedness dataset, followed by how they were annotated using BWS. 3.1 Term Pair Selection Randomly selecting term pairs will result in most pairs being unrelated. This is sub-optimal in terms of the human annotation effort that is to follow. Further, since our goal is to create a gold standard relatedness data"
N19-1050,P18-1017,1,0.91592,"ween documents (Marelli et al., 2014; Agirre et al., 2014; Cera et al., 2017). Those are outside the scope of this work. • The unigrams and bigrams should be commonly used English terms. Motivation: Data annotation of common terms is expected to be more reliable. Also, common terms are more likely to occur in application datasets. Other Natural Language Datasets Created Using BWS: BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018b), and tweet–emotion intensity (Mohammad and Kiritchenko, 2018). The largest BWS dataset is the NRC Valence, Arousal, and Dominance Lexicon, which has valence, arousal, and dominance scores for over 20,000 English words (Mohammad, 2018a). 3 English Bigram Relatedness Dataset We first describe how we selected the term pairs to include in the bigram relatedness dataset, followed by how they were annotated using BWS. 3.1 Term Pair Selection Randomly selecting term pairs will result in most pairs being unrelated. This is sub-optimal in terms of the human annotation effort that is to follow. Furth"
N19-1050,L18-1027,1,0.837261,"ween documents (Marelli et al., 2014; Agirre et al., 2014; Cera et al., 2017). Those are outside the scope of this work. • The unigrams and bigrams should be commonly used English terms. Motivation: Data annotation of common terms is expected to be more reliable. Also, common terms are more likely to occur in application datasets. Other Natural Language Datasets Created Using BWS: BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018b), and tweet–emotion intensity (Mohammad and Kiritchenko, 2018). The largest BWS dataset is the NRC Valence, Arousal, and Dominance Lexicon, which has valence, arousal, and dominance scores for over 20,000 English words (Mohammad, 2018a). 3 English Bigram Relatedness Dataset We first describe how we selected the term pairs to include in the bigram relatedness dataset, followed by how they were annotated using BWS. 3.1 Term Pair Selection Randomly selecting term pairs will result in most pairs being unrelated. This is sub-optimal in terms of the human annotation effort that is to follow. Furth"
N19-1050,D14-1162,0,0.0852814,"Missing"
N19-1050,N18-1202,0,0.0488129,"Missing"
N19-1050,D12-1110,0,0.0442703,"synonymous. Thus co-aligned pairs from phrase tables are indeed a good source of term pairs for a semantic relatedness dataset, since they include pairs with a wide variety of relatedness values. 6 Evaluating Methods of Semantic Composition on BiRD A popular approach to represent word meaning in natural language systems is through vectors that capture the contexts in which the word occurs. An area of active research is how these word vectors can be composed to create representations for larger units of text such as phrases and sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Tai et al., 2015). Even though there is a large body of work on how to represent the meanings of sentences (Le and Mikolov, 2014; Kiros et al., 2015; Lin et al., 2017), there is relatively less work on how best to compose the meanings of two words to represent the meaning of a bigram. One reason for this is a lack of suitable evaluation resources. A common approach to evaluate representations of unigrams is through their ability to rank pairs of words by closeness in meaning (Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui and Dyer, 2014). BiRD allows for the evaluation of semantic"
N19-1050,P15-1150,0,0.0720201,"Missing"
N19-1050,D11-1063,0,0.0515854,"ll basic composition operations perform. The applicability of BiRD is much broader though, and it can be used: (1) for evaluating the large number of proposed supervised methods of semantic composition; (2) for evaluating the large number of measures of semantic relatedness; (3) to study the mechanisms underpinning semantic composition; etc. We leave those for future work. We test three vector space models to obtain word representations: GloVe (Pennington et al., 2014), fastText (Grave et al., 2018), and a traditional model based on matrix factorization of a word–context co-occurrence matrix (Turney et al., 2011). We test four mathematical composition operations: (1) vector addition, (2) element-wise vector multiplication, (3) tensor product with circular convolution (Widdows, 2008), and (4) dilation (Mitchell and Lapata, 2010). In adjective–noun and noun–noun bigrams, the second word usually plays a role of a head noun, and the first word is a modifier. We test the performance of two baseline methods that do not employ vector composition: one that represents a bigram with the vector for the first word and one that represents a bigram with the vector for the second word. Word representations: We use G"
P11-2064,W06-1605,1,0.832876,"ating is a score given by human judges that reflects how easy it is to visualize the concept. It is a scale from 100 (very hard to visualize) to 700 (very easy to visualize). We use the ratings in our experiments to determine whether there is a correlation between imageability and strength of colour association. 3 Crowdsourcing We used the Macquarie Thesaurus (Bernard, 1986) as the source for terms to be annotated by people on Mechanical Turk.5 Thesauri, such as the Roget’s and Macquarie, group related words into categories. These categories can be thought of as coarse senses (Yarowsky, 1992; Mohammad and Hirst, 2006). If a word is ambiguous, then it is listed in more than one category. Since we were additionally interested in determining colour signatures for emotions (Section 7), we chose to annotate all of the 10,170 word– sense pairs that Mohammad and Turney (2010) used to create their word–emotion lexicon. Below is an example questionnaire: Q1. Which word is closest in meaning to sleep? • car • nap • olive • tree Q2. What colour is associated with sleep? • green • black • purple • white • grey • blue • pink • yellow • orange • brown • red Q1 is a word choice question generated automatically by taking"
P11-2064,W10-0204,1,0.889542,"n between imageability and strength of colour association. 3 Crowdsourcing We used the Macquarie Thesaurus (Bernard, 1986) as the source for terms to be annotated by people on Mechanical Turk.5 Thesauri, such as the Roget’s and Macquarie, group related words into categories. These categories can be thought of as coarse senses (Yarowsky, 1992; Mohammad and Hirst, 2006). If a word is ambiguous, then it is listed in more than one category. Since we were additionally interested in determining colour signatures for emotions (Section 7), we chose to annotate all of the 10,170 word– sense pairs that Mohammad and Turney (2010) used to create their word–emotion lexicon. Below is an example questionnaire: Q1. Which word is closest in meaning to sleep? • car • nap • olive • tree Q2. What colour is associated with sleep? • green • black • purple • white • grey • blue • pink • yellow • orange • brown • red Q1 is a word choice question generated automatically by taking a near-synonym from the thesaurus and random distractors. If an annotator answers this question incorrectly, then we discard information from both Q1 and Q2. The near-synonym also guides the annotator to the desired sense of the word. Further, it encourage"
P11-2064,W10-3405,0,0.0836802,"did not find any other academic work that gathered large word–colour associations. There is, however, a commercial endeavor—Cymbolism1 . Child et al. (1968), Ou et al. (2011), and others show that people of different ages and genders have different colour preferences. (See also the online study by Joe Hallock2 .) In this work, we are interested in identifying words that have a strong association with a colour due to their meaning; associations that are not affected by age and gender preferences. There is substantial work on inferring the emotions evoked by colour (Luscher, 1969; Kaya, 2004). Strapparava and Ozbal (2010) compute corpusbased semantic similarity between emotions and colours. We combine a word–colour and a word– emotion lexicon to determine the association between emotion words and colours. Berlin and Kay (1969), and later Kay and Maffi (1999), showed that often colour terms appeared in languages in certain groups. If a language has only two colour terms, then they are white and black. If a language has three colour terms, then they tend to be white, black, and red. Such groupings are seen for up to eleven colours, and based on these groupings, colours can be ranked as follows: 1 http://www.cymb"
P11-2064,C92-2070,0,0.0652101,"e imageability rating is a score given by human judges that reflects how easy it is to visualize the concept. It is a scale from 100 (very hard to visualize) to 700 (very easy to visualize). We use the ratings in our experiments to determine whether there is a correlation between imageability and strength of colour association. 3 Crowdsourcing We used the Macquarie Thesaurus (Bernard, 1986) as the source for terms to be annotated by people on Mechanical Turk.5 Thesauri, such as the Roget’s and Macquarie, group related words into categories. These categories can be thought of as coarse senses (Yarowsky, 1992; Mohammad and Hirst, 2006). If a word is ambiguous, then it is listed in more than one category. Since we were additionally interested in determining colour signatures for emotions (Section 7), we chose to annotate all of the 10,170 word– sense pairs that Mohammad and Turney (2010) used to create their word–emotion lexicon. Below is an example questionnaire: Q1. Which word is closest in meaning to sleep? • car • nap • olive • tree Q2. What colour is associated with sleep? • green • black • purple • white • grey • blue • pink • yellow • orange • brown • red Q1 is a word choice question generat"
P14-1029,W12-3802,0,0.0345519,"and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. The approaches of modeling this include bag-of-wordbased models. For example, in the work of (Kennedy and Inkpen, 2006), a feature not good will be created if the word good is encountered We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather roughly. Moreover, the figure shows that same or similar s(w) ~ scores (x-axis) can correspond to very different s(wn , w) ~ scores (y-axis), which,"
P14-1029,D08-1083,0,0.362631,"positive value corresponds to a negative or a positive sentiment respectively; zero means neutral. The negator list will be discussed later in the paper. 2 Similar distribution is observed in other data such as Tweets (Kiritchenko et al., 2014). 305 a negator simply reverses the sentiment score s(w) ~ to be −s(w); ~ i.e., f (s(w)) ~ = −s(w). ~ within a predefined range after a negator. There exist different ways of incorporating more complicated syntactic and semantic information. Much recent work considers sentiment analysis from a semantic-composition perspective (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013), which achieved the state-of-the-art performance. Moilanen and Pulman (2007) used a collection of hand-written compositional rules to assign sentiment values to different granularities of text spans. Choi and Cardie (2008) proposed a learning-based framework. The more recent work of (Socher et al., 2012; Socher et al., 2013) proposed models based on recursive neural networks that do not rely on any heuristic rules. Such models work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition o"
P14-1029,P97-1023,0,0.183368,"ional Linguistics on the Stanford Sentiment Treebank.1 Each dot in the figure corresponds to a negated phrase in the treebank. The x-axis is the sentiment score of its argument s(w) ~ and y-axis the sentiment score of the entire negated phrase s(wn , w). ~ 2 Related work Automatic sentiment analysis The expression of sentiment is an integral component of human language. In written text, sentiment is conveyed with word senses and their composition, and in speech also via prosody such as pitch (Mairesse et al., 2012). Early work on automatic sentiment analysis includes the widely cited work of (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002), among others. Since then, there has been an explosion of research addressing various aspects of the problem, including detecting subjectivity, rating and classifying sentiment, labeling sentiment-related semantic roles (e.g., target of sentiment), and visualizing sentiment (see surveys by Pang and Lee (2008) and Liu and Zhang (2012)). Negation modeling Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection sys"
P14-1029,D09-1017,0,0.0334571,"dead, however, not tall does not always mean short. Some automatic methods to detect opposites were proposed by Hatzivassiloglou and McKeown (1997) and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. The approaches of modeling this include bag-of-wordbased models. For example, in the work of (Kennedy and Inkpen, 2006), a feature not good will be created if the word good is encountered We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather r"
P14-1029,J13-3004,1,0.0615959,"Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection systems rely on detecting negated expressions and opposites (Harabagiu et al., 2006). In general, a negated expression and the opposite of the expression may or may not convey the same meaning. For example, not alive has the same meaning as dead, however, not tall does not always mean short. Some automatic methods to detect opposites were proposed by Hatzivassiloglou and McKeown (1997) and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In"
P14-1029,J12-2001,0,0.0468002,"dified by and composed with a negator as the argument (e.g., very good), and entire phrase (e.g., isn’t very good) as the negated phrase. The recently available Stanford Sentiment Treebank (Socher et al., 2013) renders manually annotated, real-valued sentiment scores for all phrases in parse trees. This corpus provides us with the data to further understand the quantitative behavior of negators, as the effect of negators can now be studied with arguments of rich syntactic and semantic variety. Figure 1 illustrates the effect of a common list of negators on sentiment as observed 1 Introduction Morante and Sporleder (2012) define negation to be “a grammatical category that allows the changing of the truth value of a proposition”. Negation is often expressed through the use of negative signals or negators–words like isn’t and never, and it can significantly affect the sentiment of its scope. Understanding the impact of negation on sentiment is essential in automatic analysis of sentiment. The literature contains interesting research attempting to model and understand the behavior (reviewed in Section 2). For example, 304 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pag"
P14-1029,P05-1015,0,0.0210681,"ly δp2 ,down [1 : d] and δp2 ,down [d + 1 : 2d], respectively. Following this notation, we have the error message for the two children of p2 , provided that we have the δp2 ,down : Data As described earlier, the Stanford Sentiment Treebank (Socher et al., 2013) has manually annotated, real-valued sentiment values for all phrases in parse trees. This provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting. The data contain around 11,800 sentences from movie reviews that were originally collected by Pang and Lee (2005). The sentences were parsed with the Stanford parser (Klein and Manning, 2003). The phrases at all tree nodes were manually annotated with one of 25 sentiment values that uniformly span between the positive and negative poles. The values are normalized to the range of [0, 1]. In this paper, we use a list of most frequent negators that include the words not, no, never, and their combinations with auxiliaries (e.g., didn’t). We search these negators in the Stanford Sentiment Treebank and normalize the same negators to a single form; e.g., “is n’t”, “isn’t”, and “is not” are all normalized to “is"
P14-1029,W02-1011,0,0.0270894,"ntiment Treebank.1 Each dot in the figure corresponds to a negated phrase in the treebank. The x-axis is the sentiment score of its argument s(w) ~ and y-axis the sentiment score of the entire negated phrase s(wn , w). ~ 2 Related work Automatic sentiment analysis The expression of sentiment is an integral component of human language. In written text, sentiment is conveyed with word senses and their composition, and in speech also via prosody such as pitch (Mairesse et al., 2012). Early work on automatic sentiment analysis includes the widely cited work of (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002), among others. Since then, there has been an explosion of research addressing various aspects of the problem, including detecting subjectivity, rating and classifying sentiment, labeling sentiment-related semantic roles (e.g., target of sentiment), and visualizing sentiment (see surveys by Pang and Lee (2008) and Liu and Zhang (2012)). Negation modeling Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection systems rely on detect"
P14-1029,D12-1110,0,0.11819,"nds to a negative or a positive sentiment respectively; zero means neutral. The negator list will be discussed later in the paper. 2 Similar distribution is observed in other data such as Tweets (Kiritchenko et al., 2014). 305 a negator simply reverses the sentiment score s(w) ~ to be −s(w); ~ i.e., f (s(w)) ~ = −s(w). ~ within a predefined range after a negator. There exist different ways of incorporating more complicated syntactic and semantic information. Much recent work considers sentiment analysis from a semantic-composition perspective (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013), which achieved the state-of-the-art performance. Moilanen and Pulman (2007) used a collection of hand-written compositional rules to assign sentiment values to different granularities of text spans. Choi and Cardie (2008) proposed a learning-based framework. The more recent work of (Socher et al., 2012; Socher et al., 2013) proposed models based on recursive neural networks that do not rely on any heuristic rules. Such models work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expre"
P14-1029,D13-1170,0,0.206229,"mental role in modifying sentiment of textual expressions. We will refer to a negation word as the negator and the text span within the scope of the negator as the argument. Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument (and not on the negator or the argument itself). We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment. We then modify these heuristics to be dependent on the negators and show that this improves prediction. Next, we evaluate a recently proposed composition model (Socher et al., 2013) that relies on both the negator and the argument. This model learns the syntax and semantics of the negator’s argument with a recursive neural network. We show that this approach performs better than those mentioned above. In addition, we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors. Figure 1: Effect of a list of common negators in modifying sentiment values in Stanford Sentiment Treebank. The x-axis is s(w), ~ and y-axis is s(wn , w). ~ Each dot in the figure corresponds to a text span being modified by (composed"
P14-1029,J11-2001,0,0.606524,"g errors. Figure 1: Effect of a list of common negators in modifying sentiment values in Stanford Sentiment Treebank. The x-axis is s(w), ~ and y-axis is s(wn , w). ~ Each dot in the figure corresponds to a text span being modified by (composed with) a negator in the treebank. The red diagonal line corresponds to the sentiment-reversing hypothesis that simply reverses the sign of sentiment values. a simple yet influential hypothesis posits that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). The shifting hypothesis (Taboada et al., 2011), however, assumes that negators change sentiment values by a constant amount. In this paper, we refer to a negation word as the negator (e.g., isn’t), a text span being modified by and composed with a negator as the argument (e.g., very good), and entire phrase (e.g., isn’t very good) as the negated phrase. The recently available Stanford Sentiment Treebank (Socher et al., 2013) renders manually annotated, real-valued sentiment scores for all phrases in parse trees. This corpus provides us with the data to further understand the quantitative behavior of negators, as the effect of negators can"
P14-1029,P02-1053,0,0.010742,"Each dot in the figure corresponds to a negated phrase in the treebank. The x-axis is the sentiment score of its argument s(w) ~ and y-axis the sentiment score of the entire negated phrase s(wn , w). ~ 2 Related work Automatic sentiment analysis The expression of sentiment is an integral component of human language. In written text, sentiment is conveyed with word senses and their composition, and in speech also via prosody such as pitch (Mairesse et al., 2012). Early work on automatic sentiment analysis includes the widely cited work of (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002), among others. Since then, there has been an explosion of research addressing various aspects of the problem, including detecting subjectivity, rating and classifying sentiment, labeling sentiment-related semantic roles (e.g., target of sentiment), and visualizing sentiment (see surveys by Pang and Lee (2008) and Liu and Zhang (2012)). Negation modeling Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection systems rely on detecting negated exp"
P14-1029,W10-3111,0,0.190685,"posed by Hatzivassiloglou and McKeown (1997) and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. The approaches of modeling this include bag-of-wordbased models. For example, in the work of (Kennedy and Inkpen, 2006), a feature not good will be created if the word good is encountered We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather roughly. Moreover, the figure shows that same or similar s(w) ~ scores (x-axis) can correspond to very"
P14-1029,H05-1044,0,0.131616,"cted distribution y i ∈ Rm×1 at node i and the target distribution ti ∈ Rm×1 at that node. That is, the error for a sentence is calculated as:      T a a [1:d] a (6) +W V p2 = tanh( p1 p1 p1  T     a a a sen[1:d] sen + sen V +W ) p1 psen psen 1 1 As shown in Equation 6, for the node vector p1 ∈ Rd×1 , we employ a matrix, namely W sen ∈ Rd×(d+m) and a tensor, V sen ∈ R(d+m)×(d+m)×d , aiming at explicitly capturing the interplays between the sentiment class of p1 , denoted as psen 1 (∈ Rm×1 ), and the negator a. Here, we assume the sentiment task has m classes. Following the idea of Wilson et al. (2005), we regard the sentiment of p1 as a prior sentiment as it has not been affected by the specific context (negators), so we denote our method as prior sentiment-enriched tensor network (PSTN). In Figure 2, the red portion shows the added components of PSTN. Note that depending on different purposes, psen 1 can take the value of the automatically predicted sentiment distribution obtained in forward propagation, the gold sentiment annotation of node p1 , or even other normalized prior sentiment value or confidence score from external sources (e.g., sentiment lexicons or external training data). T"
P14-1029,P03-1054,0,\N,Missing
P17-2074,D13-1170,0,0.00189295,"Missing"
P17-2074,N13-1062,0,0.0709146,"‘strongly agree’ (Likert, 1932). The annotations for an item from multiple respondents are usually averaged to obtain a real-valued score for that item. Thus, for an N 465 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 465–470 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2074 The sparse prior work in natural language annotations that uses BWS involves the creation of datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), and word–sentiment intensity (Kiritchenko and Mohammad, 2016a). However, none of these works has systematically compared BWS with the rating scale method. We hope that our findings will encourage the use of BWS more widely to obtain high-quality NLP annotations. All data from our experiments as well as scripts to generate BWS tuples, to generate item scores from BWS annotations, and for assessing reliability of the annotations are made freely available.1 Best–Worst Scaling (BWS) is a less-known, and more recently introduced, variant of comparative annotation. It was developed by Louviere (19"
P17-2074,S07-1013,0,0.547446,"Missing"
P17-2074,S12-1047,1,0.628005,"agree–disagree scale, from ‘strongly disagree’ to ‘strongly agree’ (Likert, 1932). The annotations for an item from multiple respondents are usually averaged to obtain a real-valued score for that item. Thus, for an N 465 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 465–470 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2074 The sparse prior work in natural language annotations that uses BWS involves the creation of datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), and word–sentiment intensity (Kiritchenko and Mohammad, 2016a). However, none of these works has systematically compared BWS with the rating scale method. We hope that our findings will encourage the use of BWS more widely to obtain high-quality NLP annotations. All data from our experiments as well as scripts to generate BWS tuples, to generate item scores from BWS annotations, and for assessing reliability of the annotations are made freely available.1 Best–Worst Scaling (BWS) is a less-known, and more recently introduced, variant of comparative a"
P17-2074,N16-1095,1,0.62182,"ns for an item from multiple respondents are usually averaged to obtain a real-valued score for that item. Thus, for an N 465 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 465–470 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2074 The sparse prior work in natural language annotations that uses BWS involves the creation of datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), and word–sentiment intensity (Kiritchenko and Mohammad, 2016a). However, none of these works has systematically compared BWS with the rating scale method. We hope that our findings will encourage the use of BWS more widely to obtain high-quality NLP annotations. All data from our experiments as well as scripts to generate BWS tuples, to generate item scores from BWS annotations, and for assessing reliability of the annotations are made freely available.1 Best–Worst Scaling (BWS) is a less-known, and more recently introduced, variant of comparative annotation. It was developed by Louviere (1991), building on some groundbreaking research in the 1960s in"
P17-2074,W16-0410,1,0.580484,"ns for an item from multiple respondents are usually averaged to obtain a real-valued score for that item. Thus, for an N 465 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 465–470 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2074 The sparse prior work in natural language annotations that uses BWS involves the creation of datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), and word–sentiment intensity (Kiritchenko and Mohammad, 2016a). However, none of these works has systematically compared BWS with the rating scale method. We hope that our findings will encourage the use of BWS more widely to obtain high-quality NLP annotations. All data from our experiments as well as scripts to generate BWS tuples, to generate item scores from BWS annotations, and for assessing reliability of the annotations are made freely available.1 Best–Worst Scaling (BWS) is a less-known, and more recently introduced, variant of comparative annotation. It was developed by Louviere (1991), building on some groundbreaking research in the 1960s in"
P18-1017,N16-1095,1,0.801053,"ear, joy, sadness, surprise, and trust (Plutchik, 1980). A, B, C, and D, if A is the best, and D is the worst, then A &gt; B, A &gt; C, A &gt; D, B &gt; D, and C &gt; D). Real-valued scores of association between the items and the property of interest can be determined using simple arithmetic on the number of times an item was chosen best and number of times it was chosen worst (as described in Section 3) (Orme, 2009; Flynn and Marley, 2014). It has been empirically shown that three annotations each for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016). Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. (See Kiritchenko and Mohammad (2016, 2017) for further details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad"
P18-1017,P17-2074,1,0.869969,"trust (Plutchik, 1980). A, B, C, and D, if A is the best, and D is the worst, then A &gt; B, A &gt; C, A &gt; D, B &gt; D, and C &gt; D). Real-valued scores of association between the items and the property of interest can be determined using simple arithmetic on the number of times an item was chosen best and number of times it was chosen worst (as described in Section 3) (Orme, 2009; Flynn and Marley, 2014). It has been empirically shown that three annotations each for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016). Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. (See Kiritchenko and Mohammad (2016, 2017) for further details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018). Automati"
P18-1017,S12-1033,1,0.848837,"2015; Yu et al., 2015; Staiano and Guerini, 2014). The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal, and dominance. 3 • All 4,206 terms in the positive and negative lists of the General Inquirer (Stone et al., 1966). • All 1,061 terms listed in ANEW (Bradley and Lang, 1999). • All 13,915 terms listed in the Warriner et al. (2013) lexicon. • 520 words from the Roget’s Thesaurus categories corresponding to the eight basic Plutchik emotions.6 • About 1000 high-frequency content terms, including emoticons, from the Hashtag Emotion Corpus (HEC) (Mohammad, 2012).7 The union of the above sets resulted in 20,007 terms that were then annotated for valence, arousal, and dominance. 3.2 We describe below how we annotated words for valence. The same approach is followed for arousal and dominance. The annotators were presented with four words at a time (4-tuples) and asked to select the word with the highest valence and the word with the lowest valence. The questionnaire uses a set of paradigm words that signify the two ends of the valence dimension. The paradigm words were taken from past literature on VAD (Bradley and Lang, 1999; Osgood et al., 1957; Russe"
P18-1017,L18-1027,1,0.858238,"for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016). Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. (See Kiritchenko and Mohammad (2016, 2017) for further details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018). Automatically Creating Affect Lexicons: There is growing work on automatically determining word–sentiment and word–emotion associations (Yang et al., 2007; Mohammad and Kiritchenko, 2015; Yu et al., 2015; Staiano and Guerini, 2014). The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal, and dominance. 3 • All 4,206 terms in the positive and negative lists of the General Inquirer (Stone et al., 1966). • All 1,061 terms listed in"
P18-1017,W17-5205,1,0.892934,"Missing"
P18-1017,S18-1001,1,0.880329,"Missing"
P18-1017,N13-1062,0,0.0536839,"ey, 2014). It has been empirically shown that three annotations each for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016). Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. (See Kiritchenko and Mohammad (2016, 2017) for further details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018). Automatically Creating Affect Lexicons: There is growing work on automatically determining word–sentiment and word–emotion associations (Yang et al., 2007; Mohammad and Kiritchenko, 2015; Yu et al., 2015; Staiano and Guerini, 2014). The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal, and dominance. 3 • All 4,206 terms in the p"
P18-1017,L18-1030,1,0.882637,"Missing"
P18-1017,S12-1047,1,0.828126,"escribed in Section 3) (Orme, 2009; Flynn and Marley, 2014). It has been empirically shown that three annotations each for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016). Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. (See Kiritchenko and Mohammad (2016, 2017) for further details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018). Automatically Creating Affect Lexicons: There is growing work on automatically determining word–sentiment and word–emotion associations (Yang et al., 2007; Mohammad and Kiritchenko, 2015; Yu et al., 2015; Staiano and Guerini, 2014). The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal,"
P18-1017,P07-2034,0,0.0504642,"ting scales. (See Kiritchenko and Mohammad (2016, 2017) for further details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018). Automatically Creating Affect Lexicons: There is growing work on automatically determining word–sentiment and word–emotion associations (Yang et al., 2007; Mohammad and Kiritchenko, 2015; Yu et al., 2015; Staiano and Guerini, 2014). The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal, and dominance. 3 • All 4,206 terms in the positive and negative lists of the General Inquirer (Stone et al., 1966). • All 1,061 terms listed in ANEW (Bradley and Lang, 1999). • All 13,915 terms listed in the Warriner et al. (2013) lexicon. • 520 words from the Roget’s Thesaurus categories corresponding to the eight basic Plutchik emotions.6 • About 1000 high-frequency content terms, including emoticons, from the Ha"
P18-1017,P15-2129,0,0.0268734,"017) for further details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018). Automatically Creating Affect Lexicons: There is growing work on automatically determining word–sentiment and word–emotion associations (Yang et al., 2007; Mohammad and Kiritchenko, 2015; Yu et al., 2015; Staiano and Guerini, 2014). The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal, and dominance. 3 • All 4,206 terms in the positive and negative lists of the General Inquirer (Stone et al., 1966). • All 1,061 terms listed in ANEW (Bradley and Lang, 1999). • All 13,915 terms listed in the Warriner et al. (2013) lexicon. • 520 words from the Roget’s Thesaurus categories corresponding to the eight basic Plutchik emotions.6 • About 1000 high-frequency content terms, including emoticons, from the Hashtag Emotion Corpus (HEC) (Mohammad, 2012).7 The"
P18-1017,P14-2070,0,0.0229522,"details on BWS.) Within the NLP community, BWS has been used for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko and Mohammad, 2016), word–emotion intensity (Mohammad, 2018), and tweet–emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; Mohammad and Kiritchenko, 2018). Automatically Creating Affect Lexicons: There is growing work on automatically determining word–sentiment and word–emotion associations (Yang et al., 2007; Mohammad and Kiritchenko, 2015; Yu et al., 2015; Staiano and Guerini, 2014). The VAD Lexicon can be used to evaluate how accurately the automatic methods capture valence, arousal, and dominance. 3 • All 4,206 terms in the positive and negative lists of the General Inquirer (Stone et al., 1966). • All 1,061 terms listed in ANEW (Bradley and Lang, 1999). • All 13,915 terms listed in the Warriner et al. (2013) lexicon. • 520 words from the Roget’s Thesaurus categories corresponding to the eight basic Plutchik emotions.6 • About 1000 high-frequency content terms, including emoticons, from the Hashtag Emotion Corpus (HEC) (Mohammad, 2012).7 The union of the above sets res"
S07-1071,N06-2015,0,0.0357647,"or manually-annotated data. ∑i m i j ∑i, j mi j mi j P(wi |c j ) = ∑i m i j P(c j ) = (2) (3) 329 c pmi = argmax c j ∈C ∑ PMI(wi , c j ) (4) (5) (6) (7) wi ∈W Note that this PMI-based classifier does not capitalize on prior probabilities of the different senses. 4 Data 4.1 English Lexical Sample Task The English Lexical Sample Task training and test data (Pradhan et al., 2007) have 22281 and 4851 instances respectively for 100 target words (50 nouns and 50 verbs). WordNet 2.1 is used as the sense inventory for most of the target words, but certain words have one or more senses from OntoNotes (Hovy et al., 2006). Many of the finegrained senses are grouped into coarser senses. Our approach relies on representing a sense with a number of near-synonymous words, for which a thesaurus is a natural source. Even though the approach can be ported to WordNet4 , there was no easy 4 The synonyms within a synset, along with its one-hop neighbors and all its hyponyms, can represent that sense. W ORDS all nouns only verbs only BASELINE 27.8 25.6 29.2 T RAINING DATA PMI- BASED NA¨I VE BAYES 41.4 50.8 43.4 53.6 38.4 44.5 P RIOR 37.4 18.1 58.9 T EST DATA L IKELIHOOD NA¨I VE BAYES 49.4 52.1 49.6 49.7 49.1 54.7 Table 1"
S07-1071,O97-1002,0,0.0672605,"to correct real-word spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the 1 The McCarthy et al. (2004) system needs to first generate a distributional thesaurus from the target text (if it is large enough—a few million words) or from another large text with a distribution of senses similar to the target text. ... .. . 327 Figure 1: The cross-lingual candidate senses of Chiand . nese words distributional concept-distance measures performed better than all WordNet-based measures as well, except for the Jiang and Conrath (1997) measure. 2.2 Cross-lingual word–category co-occurrence matrix Given a Chinese word wch in context, we use a Chinese–English bilingual lexicon to determine its different possible English translations. Each English translation wen may have one or more possible coarse senses, as listed in an English thesaurus. These English thesaurus concepts (cen ) will be referred to as cross-lingual candidate senses of the Chinese word wch .2 Figure 1 depicts examples. We create a cross-lingual word–category cooccurrence matrix (CL-WCCM) with Chinese word types wch as one dimension and English thesaurus conce"
S07-1071,S07-1004,0,0.323552,"e a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical Sample Task (Jin et al., 2007) is to select from a given list a suitable English translation of a Chinese target word in context. Mohammad et al. (2007) proposed a way to create cross-lingual distributional profiles of a concepts (CL-DPCs)— the strengths of association between the concepts of one language and words of another. For this task, we mapped the list of English translations to appropriate thesaurus categories and used an implementation of a CL-DPC–based unsupervised na¨ıve Bayes classifier to identify the intended senses (and thereby the English translations) of target Chinese words. 2 Distributional profiles of"
S07-1071,S07-1009,0,0.189256,"the target in each of its senses. However, only limited amounts of senseannotated data exist and it is expensive to create. In our previous work (Mohammad and Hirst, 2006a), The English Lexical Sample Task (Pradhan et al., 2007) is a traditional word sense disambiguation task wherein the intended (WordNet) sense of a target word is to be determined from its context. We manually mapped the WordNet senses to the categories in a thesaurus and the DPC-based na¨ıve Bayes classifier was used to identify the intended sense (category) of the target words. The object of the Lexical Substitution Task (McCarthy and Navigli, 2007) is to replace a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical"
S07-1071,P04-1036,0,0.0308951,"he words in it. For each word, the strength of association of each of the words in its context (±5 words) with each of its senses is summed. The sense that has the highest cumulative association is chosen as the intended sense. A new bootstrapped WCCM is created such that each cell mi j , corresponding to word wen i and concept cen , is populated with the number of times wen j i en co-occurs with any word used in sense c j . Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain near-upper-bound results in the task of determining word sense dominance. Unlike the McCarthy et al. (2004) dominance system, this approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sense-distributed text1 . Mohammad and Hirst (2006b) used the DPC-based monolingual distributional measures of concept-distance to rank word pairs by their semantic similarity and to correct real-word spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the 1 The McCarthy et al. (2004) system needs to first generate a distributional thesaurus from the target text (i"
S07-1071,E06-1016,1,0.928947,"stributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words—the distributional profile of a concept (DPC)—without the use of manually annotated data. We implemented an unsupervised na¨ıve Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese– English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its perfo"
S07-1071,W06-1605,1,0.921315,"stributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words—the distributional profile of a concept (DPC)—without the use of manually annotated data. We implemented an unsupervised na¨ıve Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese– English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its perfo"
S07-1071,D07-1060,1,0.850175,"sible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical Sample Task (Jin et al., 2007) is to select from a given list a suitable English translation of a Chinese target word in context. Mohammad et al. (2007) proposed a way to create cross-lingual distributional profiles of a concepts (CL-DPCs)— the strengths of association between the concepts of one language and words of another. For this task, we mapped the list of English translations to appropriate thesaurus categories and used an implementation of a CL-DPC–based unsupervised na¨ıve Bayes classifier to identify the intended senses (and thereby the English translations) of target Chinese words. 2 Distributional profiles of concepts In order to determine the strength of association between a sense of the target word and its co-occurring words,"
S07-1071,S07-1016,0,0.332879,"poor. 1 Introduction Determining the intended sense of a word is potentially useful in many natural language tasks including machine translation and information retrieval. The best approaches for word sense disambiguation are supervised and they use words that co-occur with the target as features. These systems rely on senseannotated data to identify words that are indicative of the use of the target in each of its senses. However, only limited amounts of senseannotated data exist and it is expensive to create. In our previous work (Mohammad and Hirst, 2006a), The English Lexical Sample Task (Pradhan et al., 2007) is a traditional word sense disambiguation task wherein the intended (WordNet) sense of a target word is to be determined from its context. We manually mapped the WordNet senses to the categories in a thesaurus and the DPC-based na¨ıve Bayes classifier was used to identify the intended sense (category) of the target words. The object of the Lexical Substitution Task (McCarthy and Navigli, 2007) is to replace a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its i"
S07-1071,C92-2070,0,0.588737,"Toronto Toronto, ON M5S 3G4 Canada smm@cs.toronto.edu Philip Resnik Graeme Hirst Dept. of Computer Science Dept. of Linguistics and UMIACS University of Maryland University of Toronto College Park, MD 20742 Toronto, ON M5S 3G4 USA Canada gh@cs.toronto.edu resnik@umiacs.umd.edu Abstract we proposed an unsupervised approach to determine the strength of association between a sense or concept and its co-occurring words—the distributional profile of a concept (DPC)—relying simply on raw text and a published thesaurus. The categories in a published thesaurus were used as coarse senses or concepts (Yarowsky, 1992). We now show how distributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word"
S12-1033,H05-1073,0,0.67807,"s emotion lexicon can be used in many applications, including highlighting words and phrases in a piece of text to quickly convey regions of affect. We show that the lexicon leads to significantly better results than that obtained using the manually crafted WordNet Affect lexicon in an emotion classification task. 247 2 Related Work Emotion analysis can be applied to all kinds of text, but certain domains and modes of communication tend to have more overt expressions of emotions than others. Genereux and Evans (2006), Mihalcea and Liu (2006), and Neviarouskaya et al. (2009) analyzed web-logs. Alm et al. (2005) and Francisco and Gerv´as (2006) worked on fairy tales. Boucouvalas (2002), John et al. (2006), and Mohammad (2012a) explored emotions in novels. Zhe and Boucouvalas (2002), Holzman and Pottenger (2003), and Ma et al. (2005) annotated chat messages for emotions. Liu et al. (2003) and Mohammad and Yang (2011) worked on email data. Kim et al. (2009) analyzed sadness in posts reacting to news of Michael Jackson’s death. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Much of this work focuses on six Ekman emotions. There is less work on complex emotions, for example,"
S12-1033,P07-1033,0,0.032706,"Missing"
S12-1033,D10-1124,0,0.0760591,"Missing"
S12-1033,P11-2102,0,0.0990822,"death. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Much of this work focuses on six Ekman emotions. There is less work on complex emotions, for example, work by Pearl and Steyvers (2010) that focuses on politeness, rudeness, embarrassment, formality, persuasion, deception, confidence, and disbelief. Bolen et al. (2009) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. One of the advantages of our work is that we can easily collect tweets with hashtags for many emotions, well beyond the basic six. Go et al. (2009) and Gonz´alez-Ib´an˜ ez et al. (2011) noted that sometimes people use the hashtag #sarcasm to indicate that their tweet is sarcastic. They collected tweets with hashtags of #sarcasm and #sarcastic to create a dataset of sarcastic tweets. We follow their ideas and collect tweets with hashtags pertaining to different emotions. Additionally, we present several experiments to validate that the emotion labels in the corpus are consistent and match intuitions of trained judges. 3 Existing Emotion-Labeled Text The SemEval-2007 Affective Text corpus has newspaper headlines labeled with the six Ekman emotions by six annotators (Strapparav"
S12-1033,W11-1709,1,0.606073,"cation task. 247 2 Related Work Emotion analysis can be applied to all kinds of text, but certain domains and modes of communication tend to have more overt expressions of emotions than others. Genereux and Evans (2006), Mihalcea and Liu (2006), and Neviarouskaya et al. (2009) analyzed web-logs. Alm et al. (2005) and Francisco and Gerv´as (2006) worked on fairy tales. Boucouvalas (2002), John et al. (2006), and Mohammad (2012a) explored emotions in novels. Zhe and Boucouvalas (2002), Holzman and Pottenger (2003), and Ma et al. (2005) annotated chat messages for emotions. Liu et al. (2003) and Mohammad and Yang (2011) worked on email data. Kim et al. (2009) analyzed sadness in posts reacting to news of Michael Jackson’s death. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Much of this work focuses on six Ekman emotions. There is less work on complex emotions, for example, work by Pearl and Steyvers (2010) that focuses on politeness, rudeness, embarrassment, formality, persuasion, deception, confidence, and disbelief. Bolen et al. (2009) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. One of the advantages of our work is that we can easily collect"
S12-1033,W10-0209,0,0.00727878,"ncisco and Gerv´as (2006) worked on fairy tales. Boucouvalas (2002), John et al. (2006), and Mohammad (2012a) explored emotions in novels. Zhe and Boucouvalas (2002), Holzman and Pottenger (2003), and Ma et al. (2005) annotated chat messages for emotions. Liu et al. (2003) and Mohammad and Yang (2011) worked on email data. Kim et al. (2009) analyzed sadness in posts reacting to news of Michael Jackson’s death. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Much of this work focuses on six Ekman emotions. There is less work on complex emotions, for example, work by Pearl and Steyvers (2010) that focuses on politeness, rudeness, embarrassment, formality, persuasion, deception, confidence, and disbelief. Bolen et al. (2009) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. One of the advantages of our work is that we can easily collect tweets with hashtags for many emotions, well beyond the basic six. Go et al. (2009) and Gonz´alez-Ib´an˜ ez et al. (2011) noted that sometimes people use the hashtag #sarcasm to indicate that their tweet is sarcastic. They collected tweets with hashtags of #sarcasm and #sarcastic to create a dataset of sarcastic tweets. W"
S12-1033,S07-1013,0,0.868757,"Missing"
S12-1033,strapparava-valitutti-2004-wordnet,0,0.644577,"e) = PMI (n, e) − PMI (n, ¬e) (3) where PMI is the pointwise mutual information. Creating the TEC Emotion Lexicon Word–emotion association lexicons are lists of words and associated emotions. For example, the word victory may be associated with the emotions of joy and relief. These emotion lexicons have many applications, including automatically highlighting words and phrases to quickly convey regions of affect in a piece of text. Mohammad (2012b) shows that these lexicon features can significantly improve classifier performance over and above that obtained using ngrams alone. WordNet Affect (Strapparava and Valitutti, 2004) includes 1536 words with associations to the six Ekman emotions.12 Mohammad and colleagues compiled emotion annotations for about 14,000 words by crowdsourcing to Mechanical Turk (Mohammad PMI (n, e) = log PMI (n, ¬e) = log 252 (4) freq(n, ¬e) freq(n) ∗ freq(¬e) (5) where freq(n, ¬e) is the number of times n occurs in a sentence that does not have the label e. freq(¬e) is the number of sentences that do not have the label e. Thus, equation 4 is simplified to: SoA(n, e) = log 14 http://wndomains.fbk.eu/wnaffect.html freq(n, e) freq(n) ∗ freq(e) where freq(n, e) is the number of times n occurs"
S12-1033,N12-1071,1,\N,Missing
S12-1047,W09-2416,0,0.0120009,"Missing"
S12-1047,J90-1003,0,0.250057,"d Pedersen V0 V1 V2 WordNet is used to build the set of concepts connected by WordNet relations to the pairs’ words. Prototypicality is estimated using the vector similarity of the concatenated glosses. Same procedure as V0, with one further expansion to related concepts. Same procedure as V0, with two further expansions to related concepts. Table 2: Descriptions of the participating teams and systems. as the most illustrative and the least associated as the least illustrative. Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words. For this baseline, the prototypicality rating given to a word pair is simply the PMI score for the pair. For  x and y,  two terms p(x,y) PMI(x, y) is defined as log2 p(x)p(y) where p(·) denotes the probability of a term or pair of terms. The PMI score was calculated using the method of Turney (2001) on a corpus of approximately 50 billion tokens, indexed by the Wumpus search engine.4 To calculate p(x, y), we recorded all co-occurrences of both terms within a ten-word window. 5 Systems Three teams submitted six systems for evalua"
S12-1047,S07-1003,1,0.712268,"Missing"
S12-1047,S10-1006,0,0.134072,"Missing"
S12-1047,W10-0204,1,0.0803248,". Give four additional word pairs that illustrate the same relation, in the same order (X on the left, Y on the right). Please do not use phrases composed of two or more words in your examples (e.g., “racing car”). Please do not use names of people, places, or things in your examples (e.g., “Europe”, “Kleenex”). (1) (2) (3) (4) : : : : Figure 1: An example of the two questions for Phase 1. annotations needed, we used Amazon Mechanical Turk (MTurk),2 which is a popular choice in computational linguistics for gathering large numbers of human responses to linguistic questions (Snow et al., 2008; Mohammad and Turney, 2010). We refer to the MTurk workers as Turkers. The data set was built in two phases. In the first phase, Turkers were given three paradigmatic examples of a subcategory and asked to create new pairs that instantiate the same relation as the paradigms. In the second phase, people were asked to distinguish the new pairs from the first phase according to the degree to which they are good representatives of the given subcategory. Phase 1 In the first phase, we built upon the paradigmatic examples of Bejar et al. (1991), who provided one to ten examples for each subcategory. From these examples, we ma"
S12-1047,P08-1052,0,0.00730346,"om the paradigmatic examples and identify what relational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples. 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE - PURPOSE and SPACE - TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE - PURPOSE includes CAUSE:EFFECT and ACTION :GOAL), for a total of 79 distinct subcategories. Although these c"
S12-1047,P06-1015,0,0.00741066,"shared by the paradigmatic examples. 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE - PURPOSE and SPACE - TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE - PURPOSE includes CAUSE:EFFECT and ACTION :GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based SemEval tasks (Girju et al., 2007; Hendrickx et al., 2010),"
S12-1047,D08-1027,0,0.0383199,"Missing"
S12-1047,J06-3003,1,0.643254,"t variability in how characteristic they are of that class. We present a new SemEval task based on identifying the degree of prototypicality for instances within a given class. As a part of the task, we have assembled the first dataset of graded relational similarity ratings across 79 relation categories. Three teams submitted six systems, which were evaluated using two methods. 1 Introduction Relational similarity measures the degree of correspondence between two relations, where instance pairs that have high relational similarity are said to be analogous, i.e., to express the same relation (Turney, 2006). However, a class of analogous relations may still have significant variability in the degree of relational similarity of its members. Consider the four word pairs dog:bark, cat:meow, floor:squeak, and car:honk. We could say that these four X:Y pairs are all instances of the semantic relation EN TITY :SOUND ; that is, X is an entity that characteristically makes the sound Y . Within a class of analogous pairs, certain pairs are more characteristic of the relation. For example, many would agree that dog:bark and cat:meow are better prototypes of the ENTITY :SOUND relation than floor:squeak. Ou"
S12-1047,S10-1007,0,\N,Missing
S13-2053,P11-2008,0,0.0483453,"Missing"
S13-2053,W12-2104,0,0.0657813,"ntiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated using freely available resources.1 1 Introduction Hundreds of millions of people around the world actively use microblogging websites such as Twitter. Thus there is tremendous interest in sentiment analysis of tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salath´e and Khandelwal, 2011), and disaster management (Verma et al., 2011; Mandel et al., 2012). 1 The three authors contributed equally to this paper. Svetlana Kiritchenko developed the system for the message-level task, Xiaodan Zhu developed the system for the term-level task, and Saif Mohammad led the overall effort, co-ordinated both tasks, and contributed to feature development. In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task). The sentiment can be one out of three possibilities: positive, neg"
S13-2053,W10-0204,1,0.796442,"Missing"
S13-2053,W11-1709,1,0.242544,"from tweets with sentiment-word hashtags, and one from tweets with emoticons. The automatically generated lexicons were particularly useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3 www.purl.com/net/sentimentoftweets 322 hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) is expressing the same"
S13-2053,S12-1033,1,0.415659,"Missing"
S13-2053,W02-1011,0,0.0227834,"emoticons at any position in the tweet; – whether the last token is a positive or negative emoticon; • elongated words: the number of words with one character repeated more than two times, for example, ‘soooo’; • clusters: The CMU pos-tagging tool provides the token clusters produced with the Brown clustering algorithm on 56 million Englishlanguage tweets. These 1,000 clusters serve as alternative representation of tweet content, reducing the sparcity of the token space. – the presence or absence of tokens from each of the 1000 clusters; • negation: the number of negated contexts. Following (Pang et al., 2002), we defined a negated context as a segment of a tweet that starts with a negation word (e.g., no, shouldn’t) and ends with one of the punctuation marks: ‘,’, ‘.’, ‘:’, ‘;’, ‘!’, ‘?’. A negated context affects the ngram and lexicon features: we add ‘ NEG’ suffix to each word following the negation word (‘perfect’ becomes ‘perfect NEG’). The ‘ NEG’ suffix is also added to polarity and emotion features (‘POLARITY positive’ becomes ‘POLARITY positive NEG’). The list of negation words was adopted from Christopher Potts’ sentiment tutorial.5 4 5 http://sentiment.christopherpotts.net/tokenizing.html"
S13-2053,H05-1044,0,0.893982,"th emoticons. The automatically generated lexicons were particularly useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3 www.purl.com/net/sentimentoftweets 322 hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) is expressing the same emotion. We adapted that idea to create a large corpus of po"
S13-2053,S13-2052,0,0.604218,"Missing"
S14-2076,J92-4003,0,0.121213,"quency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w , neg) was calculated in a similar way. Since PMI is known to be a poor estimator of association for low-frequency events, we ignored terms that occurred less than five times in each (positive and negative) groups of reviews. 1 score (w , c) = PMI (w , c) − PMI (w , ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text, significantly reducing the sparsity of the token space. Using Brown clustering algorithm (Brown et al., 1992), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available http://www.yelp.com/dataset_challenge 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was"
S14-2076,W10-0204,1,0.338337,"ts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: F"
S14-2076,W02-1001,0,0.046524,"i+2 )), and prefixes and suffixes of wi (up to 3 characters in length). There are only two phrase-level emission feature templates: the cased and uncased identity of the entire phrase covered by a tag, which allow the system to memorize complete terms such as, “getting a table” or “fish and chips.” Transition features couple tags with tags. Let the current tag be yj . Its transition feature templates are short n-grams of tag identities: yj ; yj , yj−1 ; and yj , yj−1 , yj−2 . During development, we experimented with the training algorithm, trying both PA and the simpler structured perceptron (Collins, 2002). We also added the lowercased back-off features. In Table 2, we re-test these design decisions on the test set, revealing that lower-cased back-off features made a strong contribution, while PA training was perhaps not as important. Our complete system achieved an F1-score of 80.19 on the restaurant domain and 68.57 on the laptop domain, ranking third among 24 teams in both. System NRC-Canada (All) All − lower-casing All − PA + percep Restaurants P R F1 84.41 76.37 80.19 83.68 75.49 79.37 83.37 76.45 79.76 System NRC-Canada (All) All − lower-casing All − PA + percep Laptops P R F1 78.77 60.70"
S14-2076,S13-2053,1,0.543874,"elp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon for restaurants. Following Turney and Littman (2003) and Mohammad et al. (2013), we calculated a sentiment score for each term w in the corpus: score (w ) = PMI (w , pos) − PMI (w , neg) (1) where pos denotes positive reviews and neg denotes negative reviews. PMI stands for pointwise mutual information: freq (w , pos) ∗ N PMI (w , pos) = log2 (2) freq (w ) ∗ freq (pos) where freq (w, pos) is the number of times a term w occurs in positive reviews, freq (w) is the total frequency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w , neg) was calculated in a similar way. Since PMI"
S14-2076,N13-1039,0,0.0130894,"iation for low-frequency events, we ignored terms that occurred less than five times in each (positive and negative) groups of reviews. 1 score (w , c) = PMI (w , c) − PMI (w , ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text, significantly reducing the sparsity of the token space. Using Brown clustering algorithm (Brown et al., 1992), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available http://www.yelp.com/dataset_challenge 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was tagged using a semi-Markov tagger (Sarawagi and Cohen, 2004). The tagger had two possible tags: O for outside, and T for aspect term, where an aspect term could tag a phrase of up to 5 consecutive tokens. The tagger was trained using the struc"
S14-2076,S14-2004,0,0.53975,"ect category of ‘food’. In Task 4, customer reviews are provided for two domains: restaurants and laptops. A fixed set of five aspect categories is defined for the restaurant domain: food, service, price, ambiance, and anecdotes. Automatic systems are to determine if any of those aspect categories are described in a review. The example sentence above describes the aspect categories of food (positive sentiment) and service (negative sentiment). For the laptop reviews, there is no aspect category detection subtask. Further details of the task and data can be found in the task description paper (Pontiki et al., 2014). Introduction Automatically identifying sentiment expressed in text has a number of applications, including tracking sentiment towards products, movies, politicians, etc.; improving customer relation models; and detecting happiness and well-being. In many applications, it is important to associate sentiment with a particular entity or an aspect of an entity. For example, in reviews, customers might express different sentiment towards various aspects of a product or service they have availed. Consider: The lasagna was great, but the service was a bit slow. The review is for a restaurant, and w"
S14-2076,P07-1033,0,0.0159279,"Missing"
S14-2076,de-marneffe-etal-2006-generating,0,0.0335632,"Missing"
S14-2076,H05-1044,0,0.0642609,"t two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon f"
S14-2076,P11-2008,0,0.0134045,"Missing"
S14-2076,P14-1029,1,0.61803,"n of 124,712 reviews as the Amazon laptop reviews corpus. Both the Yelp and the Amazon reviews have one to five star ratings associated with each review. We treated the one- and two-star reviews as negative reviews, and the four- and five-star reviews as positive reviews. 2.2 A positive sentiment score indicates a greater overall association with positive sentiment, whereas a negative score indicates a greater association with negative sentiment. The magnitude is indicative of the degree of association. Negation words (e.g., not, never) can significantly affect the sentiment of an expression (Zhu et al., 2014). Therefore, when generating the sentiment lexicons we distinguished terms appearing in negated contexts (defined as text spans between a negation word and a punctuation mark) and affirmative (non-negated) contexts. The sentiment scores were then calculated separately for the two types of contexts. For example, the term good in affirmative contexts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we gener"
S14-2077,S13-2053,1,0.8504,"c-cnrc.gc.ca Abstract Evaluation Set Twt14 Twt13 Sarc14 LvJn14 SMS13 This paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets. In a Semeval-2014 shared task (Task 9), our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets. In the message-level sentiment classification task, our submissions obtained highest scores on the LiveJournal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set. These systems build on our SemEval-2013 sentiment analysis systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts. 1 Term-level Task 1 1 3 2 2 Message-level Task 4 2 1 1 1 Table 1: Overall rank of NRC-Canada sentiment analysis models in Semeval-2014 Task 9 under the constrained condition. The rows are five evaluation datasets and the columns are the two subtasks. tem and the subsequent submissions to the 2014 shared task (Rosenthal et al., 2014). The trai"
S14-2077,J12-2001,0,0.0124923,"e sparsity of the token space. Encodings The encoding features are derived from hashtags, punctuation marks, emoticons, elongated words, and uppercased words. For the term-level task, all the above features are extracted for target terms and their context, where a context is a window of words surrounding a target term. For the message-level task, the features are extracted from the whole tweet. Our systems are based on supervised SVMs and a number of surface-form, semantic, and sentiment features. The major improvement in our 2014 system over the 2013 system is in the way it handles negation. Morante and Sporleder (2012) define negation to be “a grammatical category that allows the changing of the truth value of a proposition”. Negation is often expressed through the use of negative signals or negators, words such as isnt and never, and it can significantly affect the sentiment of its scope. We create separate tweetspecific sentiment lexicons for terms in affirmative contexts and in negated contexts. That is, we automatically determine the average sentiment of a term when occurring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our"
S14-2077,W02-1011,0,0.0177185,"iment scores of the text span; (5) the sentiment score of the last token in the text span. Note that all these features are generated, when applicable, by using each of the sentiment lexicons mentioned above. Ngrams We employed two types of ngram features: word ngrams and character ngrams. The former reflect the presence or absence of contiguous or non-contiguous sequences of words, and the latter are sequences of prefix/suffix characters in each word. These features are same as in our last year’s submission. Negation The number of negated contexts. Our definition of a negated context follows Pang et al. (2002), which will be described in more details below in Section 2.1. POS The number of occurrences of each partof-speech tag. We tokenized and part-of-speech tagged the tweets with the Carnegie Mellon University (CMU) Twitter NLP tool (Gimpel et al., 2011). Cluster features The CMU POS-tagging tool provides the token clusters produced with the Brown clustering algorithm from 56 million Englishlanguage tweets. These 1,000 clusters serve as an alternative representation of tweet content, reducing the sparsity of the token space. Encodings The encoding features are derived from hashtags, punctuation m"
S14-2077,P11-2008,0,0.0187367,"Missing"
S14-2077,S14-2009,0,0.0724322,"s systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts. 1 Term-level Task 1 1 3 2 2 Message-level Task 4 2 1 1 1 Table 1: Overall rank of NRC-Canada sentiment analysis models in Semeval-2014 Task 9 under the constrained condition. The rows are five evaluation datasets and the columns are the two subtasks. tem and the subsequent submissions to the 2014 shared task (Rosenthal et al., 2014). The training data for the SemEval-2014 shared task is same as that of SemEval-2013 (about 10,000 tweets). The 2014 test set has five subcategories: a tweet set provided newly in 2014 (Twt14), the tweet set used for testing in the 2013 shared task (Twt13), a set of tweets that are sarcastic (Sarc14), a set of sentences from the blogging website LiveJournal (LvJn14), and the set of SMS messages used for testing in the 2013 shared task (SMS13). Instances from these categories were interspersed in the provided test set. The participants were not told about the source of the individual messages."
S14-2077,J11-2001,0,0.0477166,"negated context. We reconstructed the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon with this approach and used them in our SemEval-2014 systems. Improving Lexicons and Negation Models An important advantage of our SemEval-2013 systems comes from the use of the two highcoverage tweet-specific sentiment lexicons. In the SemEval-2014 submissions, we improve these lexicons by incorporating negation modeling into the lexicon generation process. 2.1.2 Discriminating Negation Words Different negation words, e.g., never and didn’t, can have different effects on sentiment (Zhu et al., 2014; Taboada et al., 2011). In our SemEval-2014 submission, we discriminate negation words in the term-level models. For example, the word acceptable appearing in a sentence this is never acceptable is marked as acceptable beNever, while in the sentence this is not acceptable, it is marked as acceptable beNot. In this way, different negators (e.g., be not and be never) are treated differently. Note that we do not differentiate the tense and person of auxiliaries in order to reduce sparseness (e.g., was not and am not are treated in the same way). This new representation is used to extract ngrams and lexicon-based featu"
S14-2077,H05-1044,0,0.310616,"a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = P M I(w, pos) − P M I(w, neg) (1) where w is a term in the lexicons. P M I(w, pos) is the PMI score between w and the positi"
S14-2077,W10-0204,1,0.801641,"ring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = P M I(w, pos) − P M I(w, neg) (1) where w is a term in the"
S14-2077,P14-1029,1,0.444802,"nd one for distant negated context. We reconstructed the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon with this approach and used them in our SemEval-2014 systems. Improving Lexicons and Negation Models An important advantage of our SemEval-2013 systems comes from the use of the two highcoverage tweet-specific sentiment lexicons. In the SemEval-2014 submissions, we improve these lexicons by incorporating negation modeling into the lexicon generation process. 2.1.2 Discriminating Negation Words Different negation words, e.g., never and didn’t, can have different effects on sentiment (Zhu et al., 2014; Taboada et al., 2011). In our SemEval-2014 submission, we discriminate negation words in the term-level models. For example, the word acceptable appearing in a sentence this is never acceptable is marked as acceptable beNever, while in the sentence this is not acceptable, it is marked as acceptable beNot. In this way, different negators (e.g., be not and be never) are treated differently. Note that we do not differentiate the tense and person of auxiliaries in order to reduce sparseness (e.g., was not and am not are treated in the same way). This new representation is used to extract ngrams"
S14-2077,W11-1709,1,\N,Missing
S15-2078,baccianella-etal-2010-sentiwordnet,0,0.572805,"rs to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other"
S15-2078,C10-2005,0,0.0833505,"ion are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we dis"
S15-2078,W10-2914,0,0.0273404,"iu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of"
S15-2078,P11-2102,0,0.0395462,"dia such as Weblogs, microblogs, and discussion forums are used daily to express personal thoughts, which allows researchers to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, speci"
S15-2078,N03-2012,0,0.0438636,"of prior polarity of a phrase. 1 Svetlana Kiritchenko Introduction Social media such as Weblogs, microblogs, and discussion forums are used daily to express personal thoughts, which allows researchers to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et"
S15-2078,S12-1047,1,0.517008,"ator agreement is low, and annotators struggle even to remain self-consistent. In contrast, it is much easier to make relative judgments, e.g., to say whether one word is more positive than another. Moreover, it is possible to derive an absolute score from pairwise judgments, but this requires a much larger number of annotations. Fortunately, there are schemes that allow to infer more pairwise annotations from less judgments. 455 One such annotation scheme is MaxDiff (Louviere, 1991), which is widely used in market surveys (Almquist and Lee, 2009); it was also used in a previous SemEval task (Jurgens et al., 2012). In MaxDiff, the annotator is presented with four terms and asked which term is most positive and which is least positive. By answering just these two questions, five out of six pairwise rankings become known. Consider a set in which a judge evaluates A, B, C, and D. If she says that A and D are the most and the least positive, we can infer the following: A &gt; B, A &gt; C, A &gt; D, B &gt; D, C &gt; D. The responses to the MaxDiff questions can then be easily translated into a ranking for all the terms and also into a real-valued score for each term. We crowdsourced the MaxDiff questions on CrowdFlower, r"
S15-2078,W13-1605,0,0.0482134,"Missing"
S15-2078,S13-2053,1,0.407706,"of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five subtasks, with most teams participating in more than one subtask. This year the task included reruns of two legacy subtasks, which asked to detect the sentiment expressed in a tweet or by a particular phrase in a tweet. The task further added three new subtasks. The first two focused on the sentiment towards a given topic in a single tweet or in a set of tweets, respect"
S15-2078,S13-2052,1,0.689604,"ing sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five sub"
S15-2078,S10-1097,0,0.0524882,"ion (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter ha"
S15-2078,W02-1011,0,0.0472385,"t into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled en"
S15-2078,D11-1141,1,0.0366066,"ive. • Subtask E. Determining Strength of Association of Twitter Terms with Positive Sentiment (Degree of Prior Polarity): Given a word/phrase, propose a score between 0 (lowest) and 1 (highest) that is indicative of the strength of association of that word/phrase with positive sentiment. If a word/phrase is more positive than another one, it should be assigned a relatively higher score. 452 Data Collection Subtasks A–D First, we gathered tweets that express sentiment about popular topics. For this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned NER system (Ritter et al., 2011). Our initial training set was collected over a one-year period spanning from January 2012 to January 2013. Each subsequent Twitter test set was collected a few months prior to the corresponding evaluation. We used the public streaming Twitter API to download the tweets. We then identified popular topics as those named entities that are frequently mentioned in association with a specific date (Ritter et al., 2012). Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from tr"
S15-2078,S14-2009,1,0.518377,"d on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five subtasks, with most teams pa"
S15-2078,H05-1044,0,0.415097,"s much smaller: 30.3 for B vs. 26.7 for C. Finally, the last column in the table reports the results for the 75 sarcastic 2015 tweets. The winner here is KLUEless with an F1 of 39.26, followed by TwitterHawk with F1 =31.30, and then by UMDuluth-CS8761 with F1 =29.91. 460 Subtask D: Trend Towards a Topic Subtask E: Degree of Prior Polarity Ten teams participated in subtask E. Many chose an unsupervised approach and leveraged newlycreated and pre-existing sentiment lexicons such as the Hashtag Sentiment Lexicon, the Sentiment140 Lexicon (Kiritchenko et al., 2014), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and SentiWordNet (Baccianella et al., 2010), among others. Several participants further automatically created their own sentiment lexicons from large collections of tweets. Three teams, including the winner INESC-ID, adopted a supervised approach and used word embeddings (supplemented with lexicon features) to train a regression model. The results are presented in Table 14. The last row shows the performance of a lexicon-based baseline. For this baseline, we chose the two most frequently used existing, publicly available, and automatically generated sentiment lexicons: Hashtag Sentiment Lexi"
S15-2078,P14-1029,1,0.129095,"tag Sentiment tweet corpora (Kiritchenko et al., 2014). In order to reduce the skewness towards the neutral class, we selected terms from different ranges of automatically determined sentiment values as provided by the corresponding Sentiment140 and Hashtag Sentiment lexicons. The term set comprised regular English words, hashtagged words (e.g., #loveumom), misspelled or creatively spelled words (e.g., parlament or happeeee), abbreviations, shortenings, and slang. Some terms were negated expressions such as no fun. (It is known that negation impacts the sentiment of its scope in complex ways (Zhu et al., 2014).) We annotated these terms for degree of sentiment manually. Further details about the data collection and the annotation process can be found in Section 3.2.2 as well as in (Kiritchenko et al., 2014). The trial dataset consisted of 200 instances, and no training dataset was provided. Note, however, that the trial data was large enough to be used as a development set, or even as a training set. Moreover, the participants were free to use any additional manually or automatically generated resources when building their systems for subtask E. The testset included 1,315 instances. 453 Annotation"
S16-1003,W11-1701,0,0.686791,"labels, with pkudblab using keyword rules, LitisMind using hashtag rules on external data, and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based"
S16-1003,S16-1063,0,0.18439,"25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3 Discussion Some teams did very well detecting tweets in favor of Trump (ltl.uni-due), with most of the othe"
S16-1003,W14-2107,0,0.0579715,"and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as sp"
S16-1003,W12-3810,0,0.00926055,"tion of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and s"
S16-1003,S16-1061,0,0.0301153,"lt of 56.28 actually beating the best result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other ent"
S16-1003,S16-1070,0,0.0161479,"ht have expected, with the best result of 56.28 actually beating the best result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of"
S16-1003,S14-2076,1,0.512441,"s these results. It also shows results on the complete test set (All), for easy reference. Observe that the stance task is markedly more difficult when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). This is not surprising because it is a more challenging task, and because there has been very little work on this in the past. 5.3 Discussion Most teams used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features such as those drawn from sentiment lexicons (Kiritchenko et al., 2014b). Some teams polled Twitter for stancebearing hashtags, creating additional noisy stance data. Three teams tried variants of this strategy: MITRE, DeepStance and nldsucsc. These teams are distributed somewhat evenly throughout the standings, and although MITRE did use extra data in its top-placing entry, pkudblab achieved nearly the same score with only the provided data. Another possible differentiator would be the use of continuous word representations, derived either from extremely large sources such as Google News, directly from Twitter corpora, or as a by-product of training a neural ne"
S16-1003,S16-1068,0,0.0138727,"Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and n"
S16-1003,W10-0204,1,0.110514,"ier. Nine of the nineteen entries used some form of word embedding, including the top three entries, but PKULCWM’s fourth place result shows that it is possible to do well with a more traditional approach that relies instead on Twitter-specific linguistic pre-processing. Along these lines, it is worth noting that both MITRE and pkudblab reflect knowledge-light approaches to the problem, each relying minimally on linguistic processing and external lexicons. Seven of the nineteen submissions made extensive use of publicly-available sentiment and emotion lexicons such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Lexicons (Kiritchenko et al., 2014b). Recall that the SVM-ngrams baseline also performed very well, using only word and character ngrams in its classifiers. This helps emphasize the fact that for this young task, the community is still a long way from an established set of best practices. 6 Systems and Results for Task B The sub-sections below discuss baselines and official submissions to Task B. Recall, that the test data for Task B is for the target ‘Donald Trump’, and no training data f"
S16-1003,S13-2053,1,0.777604,"task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formul"
S16-1003,L16-1623,1,0.884636,"d submissions from 9 teams wherein the highest classification F-score obtained was 56.28. The best performing systems used standard text classification features such as those drawn from n-grams, word vectors, and sentiment lexicons. Some teams drew additional gains from noisy stance-labeled data created using distant supervision techniques. A large number of teams used word embeddings and some used deep neural networks such as RNNs and convolutional neural nets. Nonetheless, for Task A, none of these systems surpassed a baseline SVM classifier that uses word and character n-grams as features (Mohammad et al., 2016b). Further, results are markedly worse for instances where the target of interest is not the target of opinion. More gains can be expected in the future on both tasks, as researchers better understand this new task and data. All of the data, an interactive visualization of the data, and the evaluation scripts are available on the task website as well as the homepage for this Stance project.2 2 Subtleties of Stance Detection In the sub-sections below we discuss some of the nuances of stance detection, including a discussion on neutral stance and the relationship between stance and sentiment. 2"
S16-1003,S16-1071,0,0.0205908,"result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results"
S16-1003,S14-2004,0,0.0916108,"d 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formulation where no labeled data pertaining to the test d"
S16-1003,S15-2082,0,0.099003,"Missing"
S16-1003,S15-2078,1,0.755235,"n the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formulation where no labeled da"
S16-1003,W15-0509,1,0.694399,"to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a"
S16-1003,W10-0214,0,0.524808,"the approach of producing noisy labels, with pkudblab using keyword rules, LitisMind using hashtag rules on external data, and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and sys"
S16-1003,W14-2715,0,0.0663582,"d-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a c"
S16-1003,S16-1075,0,0.119664,"Missing"
S16-1003,S16-1062,0,0.246544,"Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3"
S16-1003,H05-1044,0,0.231407,"tries, but PKULCWM’s fourth place result shows that it is possible to do well with a more traditional approach that relies instead on Twitter-specific linguistic pre-processing. Along these lines, it is worth noting that both MITRE and pkudblab reflect knowledge-light approaches to the problem, each relying minimally on linguistic processing and external lexicons. Seven of the nineteen submissions made extensive use of publicly-available sentiment and emotion lexicons such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Lexicons (Kiritchenko et al., 2014b). Recall that the SVM-ngrams baseline also performed very well, using only word and character ngrams in its classifiers. This helps emphasize the fact that for this young task, the community is still a long way from an established set of best practices. 6 Systems and Results for Task B The sub-sections below discuss baselines and official submissions to Task B. Recall, that the test data for Task B is for the target ‘Donald Trump’, and no training data for this target was provided. 6.1 Task B Baselines We calculated two baselines listed bel"
S16-1003,S13-2052,0,0.0609198,"Missing"
S16-1003,S16-1069,0,0.0184379,"ate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to"
S16-1003,S16-1065,0,0.0288745,"ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3 Discussion Some teams did very well detecting tweets in favor of T"
S16-1003,S16-1074,0,0.155861,"jority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion"
S16-1004,abdul-mageed-diab-2014-sana,0,0.0348074,"ords and phrases are taken from three domains: general English, English Twitter, and Arabic Twitter. Existing manually created sentiment lexicons tend to provide only lists of positive and negative words (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less useful in downstream applications than having access to fine-grained (realvalued) sentiment association scores. Most of the existing sentiment resources are available only for English. Non-English resources are scarce and often based on automatic translation of the English lexicons (Abdul-Mageed and Diab, 2014; Eskander and Rambow, 2015). Manually created sentiment lexicons usually include only single words. Yet, the sentiment of a phrase can differ markedly from the 42 Proceedings of SemEval-2016, pages 42–51, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multiword linguistic unit, such as a phrase or a sentence, from its constituents. Lexicons that include sentiment associations for phrases and their constituents are useful in studying sentiment composition. W"
S16-1004,P11-2103,0,0.276847,"Missing"
S16-1004,S16-1079,0,0.0196102,"Missing"
S16-1004,L16-1463,0,0.052136,"Missing"
S16-1004,D15-1304,0,0.0488044,"om three domains: general English, English Twitter, and Arabic Twitter. Existing manually created sentiment lexicons tend to provide only lists of positive and negative words (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less useful in downstream applications than having access to fine-grained (realvalued) sentiment association scores. Most of the existing sentiment resources are available only for English. Non-English resources are scarce and often based on automatic translation of the English lexicons (Abdul-Mageed and Diab, 2014; Eskander and Rambow, 2015). Manually created sentiment lexicons usually include only single words. Yet, the sentiment of a phrase can differ markedly from the 42 Proceedings of SemEval-2016, pages 42–51, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multiword linguistic unit, such as a phrase or a sentence, from its constituents. Lexicons that include sentiment associations for phrases and their constituents are useful in studying sentiment composition. We refer to them as sentiment"
S16-1004,esuli-sebastiani-2006-sentiwordnet,0,0.0313905,"icons, PMI scores between terms and sentiment classes computed on sentiment-annotated corpora, and PMI scores between terms and words poor and excellent computed on Google search results. All teams heavily relied on existing sentiment lexicons: AFINN (Nielsen, 2011), ArabSenti (AbdulMageed et al., 2011), Hu and Liu (Hu and Liu, 2004), Dialectal Arabic Lexicon (Refaee and Rieser, 2014), JRC (Steinberger et al., 2012), MPQA (Wilson et al., 2005), NRC Emoticon (a.k.a. Sentiment140) (Kiritchenko et al., 2014), NRC Emotion (Mohammad and Turney, 2013), NileULex (ElBeltagy, 2016b), and SentiWordNet (Esuli and Sebastiani, 2006). (Note that even though the NRC Emotion Lexicon was created for English terms, its translations in close to 40 languages, including Arabic, are available.12 ) 12 http://saifmohammad.com/WebPages/NRC-EmotionLexicon.htm 48 6 Results The results for the three tasks are presented in Tables 6, 7, and 8. Team ECNU showed the best performance in both English-language tasks. In the Arabic task, the best performing system was developed by iLab-Edinburgh. A few observations can be made from the results: • On all three datasets, the team rankings based on the two metrics, Kendall’s τ and Spearman’s ρ, a"
S16-1004,S16-1076,0,0.063892,"Missing"
S16-1004,N16-1095,1,0.647618,"scores, have a high coverage, can include longer phrases, and can easily be collected for a specific domain. However, due to the lack of manually annotated real-valued sentiment lexicons the quality of automatic lexicons are often assessed only extrinsically through their use in sentence-level sentiment prediction. In this shared task, we intrinsically evaluate automatic methods that estimate sentiment association scores for terms in English and Arabic. For this, we assembled three datasets of phrases and their constituent single words manually annotated for sentiment with realvalued scores (Kiritchenko and Mohammad, 2016a; Kiritchenko and Mohammad, 2016c). We first introduced this task as part of the SemEval-2015 Task 10 ‘Sentiment Analysis in Twitter’ Subtask E (Rosenthal et al., 2015). The 2015 test set was restricted to English single words and simple two-word negated expressions commonly found in tweets. This year (2016), we broadened the scope of the task and included three different domains. Furthermore, we shifted the focus from single words to longer, more complex phrases to explore sentiment composition. Five teams submitted nine system outputs for the three tasks. All submitted outputs correlated st"
S16-1004,W16-0410,1,0.824951,"scores, have a high coverage, can include longer phrases, and can easily be collected for a specific domain. However, due to the lack of manually annotated real-valued sentiment lexicons the quality of automatic lexicons are often assessed only extrinsically through their use in sentence-level sentiment prediction. In this shared task, we intrinsically evaluate automatic methods that estimate sentiment association scores for terms in English and Arabic. For this, we assembled three datasets of phrases and their constituent single words manually annotated for sentiment with realvalued scores (Kiritchenko and Mohammad, 2016a; Kiritchenko and Mohammad, 2016c). We first introduced this task as part of the SemEval-2015 Task 10 ‘Sentiment Analysis in Twitter’ Subtask E (Rosenthal et al., 2015). The 2015 test set was restricted to English single words and simple two-word negated expressions commonly found in tweets. This year (2016), we broadened the scope of the task and included three different domains. Furthermore, we shifted the focus from single words to longer, more complex phrases to explore sentiment composition. Five teams submitted nine system outputs for the three tasks. All submitted outputs correlated st"
S16-1004,L16-1184,1,0.889926,"ask, we intrinsically evaluate automatic methods that estimate sentiment association scores for terms in English and Arabic. For this, we assembled three datasets of phrases and their constituent single words manually annotated for sentiment with realvalued scores (Kiritchenko and Mohammad, 2016a; Kiritchenko and Mohammad, 2016c). We first introduced this task as part of the SemEval-2015 Task 10 ‘Sentiment Analysis in Twitter’ Subtask E (Rosenthal et al., 2015). The 2015 test set was restricted to English single words and simple two-word negated expressions commonly found in tweets. This year (2016), we broadened the scope of the task and included three different domains. Furthermore, we shifted the focus from single words to longer, more complex phrases to explore sentiment composition. Five teams submitted nine system outputs for the three tasks. All submitted outputs correlated strongly with the gold term rankings (Kendall’s rank correlation above 0.35). The best results on all tasks were achieved with supervised methods by exploiting a variety of sentiment resources. The highest rank correlation was obtained by team ECNU on the General English test set (τ = 0.7). On the other two dom"
S16-1004,N16-1128,1,0.27211,"scores, have a high coverage, can include longer phrases, and can easily be collected for a specific domain. However, due to the lack of manually annotated real-valued sentiment lexicons the quality of automatic lexicons are often assessed only extrinsically through their use in sentence-level sentiment prediction. In this shared task, we intrinsically evaluate automatic methods that estimate sentiment association scores for terms in English and Arabic. For this, we assembled three datasets of phrases and their constituent single words manually annotated for sentiment with realvalued scores (Kiritchenko and Mohammad, 2016a; Kiritchenko and Mohammad, 2016c). We first introduced this task as part of the SemEval-2015 Task 10 ‘Sentiment Analysis in Twitter’ Subtask E (Rosenthal et al., 2015). The 2015 test set was restricted to English single words and simple two-word negated expressions commonly found in tweets. This year (2016), we broadened the scope of the task and included three different domains. Furthermore, we shifted the focus from single words to longer, more complex phrases to explore sentiment composition. Five teams submitted nine system outputs for the three tasks. All submitted outputs correlated st"
S16-1004,S16-1078,0,0.0307242,"Missing"
S16-1004,S13-2053,1,0.0679304,"esented as the test sets for three separate tasks (each focusing on a specific domain). Five teams submitted nine system outputs for the three tasks. All datasets created for this shared task are freely available to the research community. 1 Lists of words and their associated sentiment are commonly referred to as sentiment lexicons. They are used in sentiment analysis. For example, a number of unsupervised classifiers rely primarily on sentiment lexicons to determine whether a piece of text is positive or negative. Supervised classifiers also often use features drawn from sentiment lexicons (Mohammad et al., 2013; Pontiki et al., 2014). Sentiment lexicons are also beneficial in stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), and for detecting personality traits (Minamikawa and Yokoyama, 2011; Mohammad and Kiritchenko, 2015). Introduction Words have prior associations with sentiment. For example, honest and competent are associated with positive sentiment, whereas dishonest and dull are associated with negative sentiment. Further, the degree of positivity (or negativity), also referred to as intensity, can vary. For exa"
S16-1004,L16-1623,1,0.041937,"ed nine system outputs for the three tasks. All datasets created for this shared task are freely available to the research community. 1 Lists of words and their associated sentiment are commonly referred to as sentiment lexicons. They are used in sentiment analysis. For example, a number of unsupervised classifiers rely primarily on sentiment lexicons to determine whether a piece of text is positive or negative. Supervised classifiers also often use features drawn from sentiment lexicons (Mohammad et al., 2013; Pontiki et al., 2014). Sentiment lexicons are also beneficial in stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), and for detecting personality traits (Minamikawa and Yokoyama, 2011; Mohammad and Kiritchenko, 2015). Introduction Words have prior associations with sentiment. For example, honest and competent are associated with positive sentiment, whereas dishonest and dull are associated with negative sentiment. Further, the degree of positivity (or negativity), also referred to as intensity, can vary. For example, most people will agree that succeed is more positive (or less negative) than improve, and fail is mo"
S16-1004,S14-2004,0,0.146758,"s for three separate tasks (each focusing on a specific domain). Five teams submitted nine system outputs for the three tasks. All datasets created for this shared task are freely available to the research community. 1 Lists of words and their associated sentiment are commonly referred to as sentiment lexicons. They are used in sentiment analysis. For example, a number of unsupervised classifiers rely primarily on sentiment lexicons to determine whether a piece of text is positive or negative. Supervised classifiers also often use features drawn from sentiment lexicons (Mohammad et al., 2013; Pontiki et al., 2014). Sentiment lexicons are also beneficial in stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), and for detecting personality traits (Minamikawa and Yokoyama, 2011; Mohammad and Kiritchenko, 2015). Introduction Words have prior associations with sentiment. For example, honest and competent are associated with positive sentiment, whereas dishonest and dull are associated with negative sentiment. Further, the degree of positivity (or negativity), also referred to as intensity, can vary. For example, most people will"
S16-1004,refaee-rieser-2014-arabic,0,0.183212,"m Forest algorithm. The development data available for each task was used as the training data by some teams. However, these data were limited (200 instances per task); therefore, other manually labeled resources were also explored. One commonly used resource was the LabMT lexicon—a set of over 100,000 frequent single words from 10 languages, including English and Arabic (about 10,000 words in each language), manually annotated for happiness through Mechanical Turk (Dodds et al., 2011; Dodds et al., 2015). For the Arabic task, two teams took advantage of the Arabic Twitter corpus collected by Refaee and Rieser (2014). The features employed include sentiment scores obtained from different sentiment lexicons, general and sentiment-specific word embeddings, pointwise mutual information (PMI) scores between terms (single words and multi-word phrases) and sentiment classes, as well as lists of negators, intensifiers, and diminishers. Team name ECNU Supervision Algorithm Random Forest Training data LabMT, dev. data Sentiment lexicons used Hu and Liu, MPQA External corpora and other resources used 1.6M tweets (with emoticons) supervised LSIS unsupervised PMI - NRC Emoticon, SentiWordNet, MPQA 10K tweets (with ma"
S16-1004,S16-1077,0,0.186755,"Missing"
S16-1004,S15-2078,1,0.128586,"nt lexicons the quality of automatic lexicons are often assessed only extrinsically through their use in sentence-level sentiment prediction. In this shared task, we intrinsically evaluate automatic methods that estimate sentiment association scores for terms in English and Arabic. For this, we assembled three datasets of phrases and their constituent single words manually annotated for sentiment with realvalued scores (Kiritchenko and Mohammad, 2016a; Kiritchenko and Mohammad, 2016c). We first introduced this task as part of the SemEval-2015 Task 10 ‘Sentiment Analysis in Twitter’ Subtask E (Rosenthal et al., 2015). The 2015 test set was restricted to English single words and simple two-word negated expressions commonly found in tweets. This year (2016), we broadened the scope of the task and included three different domains. Furthermore, we shifted the focus from single words to longer, more complex phrases to explore sentiment composition. Five teams submitted nine system outputs for the three tasks. All submitted outputs correlated strongly with the gold term rankings (Kendall’s rank correlation above 0.35). The best results on all tasks were achieved with supervised methods by exploiting a variety o"
S16-1004,S16-1080,0,0.16993,"Missing"
S16-1004,H05-1044,0,0.179479,"r, the degree of positivity (or negativity), also referred to as intensity, can vary. For example, most people will agree that succeed is more positive (or less negative) than improve, and fail is more negative (or less positive) than setback. We present a shared task where automatic systems are asked to predict a prior sentiment intensity score for a word or a phrase. The words and phrases are taken from three domains: general English, English Twitter, and Arabic Twitter. Existing manually created sentiment lexicons tend to provide only lists of positive and negative words (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less useful in downstream applications than having access to fine-grained (realvalued) sentiment association scores. Most of the existing sentiment resources are available only for English. Non-English resources are scarce and often based on automatic translation of the English lexicons (Abdul-Mageed and Diab, 2014; Eskander and Rambow, 2015). Manually created sentiment lexicons usually include only single words. Yet, the sentiment of a phrase can differ markedly from the 42 Proceedings of SemEval-2016, pages 42–51, c San Die"
S16-1004,W11-1704,0,\N,Missing
S16-2003,H05-1073,0,0.0250423,"extracted the following types of instances from WordNet: Emotion annotation: Sentiment analysis is defined as detecting the evaluative or affective attitude in text. A vast majority of work in sentiment analysis has focused on developing classifiers for valence prediction (Kiritchenko et al., 2014; Dong et al., 2014; Socher et al., 2013; Mohammad et al., 2013), i.e., determining whether a piece of text expresses positive, negative, or neutral attitude. However, there is a growing interest in detecting a wider range of emotions such as joy, sadness, optimism, etc. (Holzman and Pottenger, 2003; Alm et al., 2005; Brooks et al., 2013; Mohammad, 2012). Much of the this work has been influenced by the idea that some emotions are more basic than others (Ekman, 1992; Ekman and Friesen, 2003; Plutchik, 1980; Plutchik, 1991). Mohammad (2012) polled the Twitter API for tweets that have hashtag words such as #anger and #sadness corresponding to the eight Plutchik basic emo4 Experimental Setup Instance 1 Target verb: erase Sentence: The Turks erased the Armenians. Here, erase is used metaphorically. We will refer to such instances as metaphorical instances. Now consider an instance similar to the one above, bu"
S16-2003,W13-0902,0,0.0858739,"analysis thereof. The majority of corpus-linguistic studies were concerned with metaphorical expressions and mappings within a limited domain, e.g., WAR , BUSINESS , FOOD or www.crowdflower.com http://saifmohammad.com/WebPages/metaphor.html 3 24 Words used in different senses convey different affect. PLANT metaphors (Santa Ana, 1999; Izwaini, 2003; Koller, 2004; Skorczynska Sznajder and Pique-Angordans, 2004; Lu and Ahrens, 2008; Low et al., 2010; Hardie et al., 2007), in a particular genre or type of discourse (Charteris-Black, 2000; Cameron, 2003; Lu and Ahrens, 2008; Martin, 2006; Beigman Klebanov and Flor, 2013). Two recent studies (Steen et al., 2010; Shutova and Teufel, 2010) moved away from investigating particular domains to a more general study of how metaphor behaves in unrestricted continuous text. Steen and colleagues (Pragglejaz Group, 2007; Steen et al., 2010) proposed a metaphor identification procedure (MIP), in which every word is tagged as literal or metaphorical, based on whether it has a “more basic meaning” in other contexts than the current one. The basic meaning was defined as “more concrete; related to bodily action; more precise (as opposed to vague); historically older” and its"
S16-2003,S12-1023,0,0.0343494,"Missing"
S16-2003,P13-1067,0,0.0419462,"a distinct viewpoint on governmental regulation of business, as opposed to a more neutral factual statement expressed in (1b). The interplay of metaphor and emotion has been an object of interest in fields such as linguistics (Blanchette et al., 2001; Kovecses, 2003), political science (Lakoff and Wehling, 2012), cognitive psychology (Crawford, 2009; Thibodeau and Boroditsky, 2011) and neuroscience (Aziz-Zadeh and Damasio, 2008; Jabbi et al., 2008). A number of computational approaches for sentiment polarity classification of metaphorical language have also been proposed (Veale and Li, 2012; Kozareva, 2013; Strzalkowski et al., 2014). However, there is no quantitative study establishing the extent to which metaphorical language is used to express emotion nor a data-supported account of the mechanisms by which this happens. Our study addresses two questions: (i) whether a metaphorical statement is likely to convey a stronger emotional content than its literal counterpart; and (ii) how this emotional content arises in the metaphor, i.e. whether it comes from the source domain, or from the target domain, or rather arises compositionally through interaction of the source and the target. To answer t"
S16-2003,P14-2009,0,0.00457945,"he usage of one or more of the near-synonyms. We will refer to each of these sentences as the verb-sense sentence, or just sentence. The portion of the sentence excluding the target verb will be called the context. We will refer to each pair of target verb and verb-sense sentence as an instance. We extracted the following types of instances from WordNet: Emotion annotation: Sentiment analysis is defined as detecting the evaluative or affective attitude in text. A vast majority of work in sentiment analysis has focused on developing classifiers for valence prediction (Kiritchenko et al., 2014; Dong et al., 2014; Socher et al., 2013; Mohammad et al., 2013), i.e., determining whether a piece of text expresses positive, negative, or neutral attitude. However, there is a growing interest in detecting a wider range of emotions such as joy, sadness, optimism, etc. (Holzman and Pottenger, 2003; Alm et al., 2005; Brooks et al., 2013; Mohammad, 2012). Much of the this work has been influenced by the idea that some emotions are more basic than others (Ekman, 1992; Ekman and Friesen, 2003; Plutchik, 1980; Plutchik, 1991). Mohammad (2012) polled the Twitter API for tweets that have hashtag words such as #anger"
S16-2003,shutova-teufel-2010-metaphor,1,0.581634,"Missing"
S16-2003,D13-1170,0,0.00351019,"more of the near-synonyms. We will refer to each of these sentences as the verb-sense sentence, or just sentence. The portion of the sentence excluding the target verb will be called the context. We will refer to each pair of target verb and verb-sense sentence as an instance. We extracted the following types of instances from WordNet: Emotion annotation: Sentiment analysis is defined as detecting the evaluative or affective attitude in text. A vast majority of work in sentiment analysis has focused on developing classifiers for valence prediction (Kiritchenko et al., 2014; Dong et al., 2014; Socher et al., 2013; Mohammad et al., 2013), i.e., determining whether a piece of text expresses positive, negative, or neutral attitude. However, there is a growing interest in detecting a wider range of emotions such as joy, sadness, optimism, etc. (Holzman and Pottenger, 2003; Alm et al., 2005; Brooks et al., 2013; Mohammad, 2012). Much of the this work has been influenced by the idea that some emotions are more basic than others (Ekman, 1992; Ekman and Friesen, 2003; Plutchik, 1980; Plutchik, 1991). Mohammad (2012) polled the Twitter API for tweets that have hashtag words such as #anger and #sadness correspo"
S16-2003,strapparava-valitutti-2004-wordnet,0,0.0378304,"or a word predicts the intended sense, based on context. A problem with this approach to WSD is that good coverage of common polysemous English words would require about 3,200 distinct models. Kilgarriff (1997) has argued there are systematic relations among word senses across different words, focusing in particular on metaphor as a ubiquitous source of polysemy. This area of research is known as regular polysemy. Thus, there is a systematic relation between metaphor and word sense (Kilgarriff, 1997; Turney et al., 2011) and the emotion associated with a word depends on the sense of the word (Strapparava and Valitutti, 2004; Mohammad and Turney, 2013).3 This raises the question of whether there is a systematic relation between presence of metaphor and the emotional content of words. As far as we know, this is the first paper to quantitatively explore this question. Gibbs et al. (2002) conducted a study that looked at how listeners respond to metaphor and irony when they are played audio tapes describing emotional experiences. They found that on average metaphors were rated as being more emotional than non-metaphoric expressions. However, that work did not compare paraphrase pairs that differed in just one word ("
S16-2003,S13-2053,1,0.355924,"nyms. We will refer to each of these sentences as the verb-sense sentence, or just sentence. The portion of the sentence excluding the target verb will be called the context. We will refer to each pair of target verb and verb-sense sentence as an instance. We extracted the following types of instances from WordNet: Emotion annotation: Sentiment analysis is defined as detecting the evaluative or affective attitude in text. A vast majority of work in sentiment analysis has focused on developing classifiers for valence prediction (Kiritchenko et al., 2014; Dong et al., 2014; Socher et al., 2013; Mohammad et al., 2013), i.e., determining whether a piece of text expresses positive, negative, or neutral attitude. However, there is a growing interest in detecting a wider range of emotions such as joy, sadness, optimism, etc. (Holzman and Pottenger, 2003; Alm et al., 2005; Brooks et al., 2013; Mohammad, 2012). Much of the this work has been influenced by the idea that some emotions are more basic than others (Ekman, 1992; Ekman and Friesen, 2003; Plutchik, 1980; Plutchik, 1991). Mohammad (2012) polled the Twitter API for tweets that have hashtag words such as #anger and #sadness corresponding to the eight Plutc"
S16-2003,W14-2306,0,0.0297923,"point on governmental regulation of business, as opposed to a more neutral factual statement expressed in (1b). The interplay of metaphor and emotion has been an object of interest in fields such as linguistics (Blanchette et al., 2001; Kovecses, 2003), political science (Lakoff and Wehling, 2012), cognitive psychology (Crawford, 2009; Thibodeau and Boroditsky, 2011) and neuroscience (Aziz-Zadeh and Damasio, 2008; Jabbi et al., 2008). A number of computational approaches for sentiment polarity classification of metaphorical language have also been proposed (Veale and Li, 2012; Kozareva, 2013; Strzalkowski et al., 2014). However, there is no quantitative study establishing the extent to which metaphorical language is used to express emotion nor a data-supported account of the mechanisms by which this happens. Our study addresses two questions: (i) whether a metaphorical statement is likely to convey a stronger emotional content than its literal counterpart; and (ii) how this emotional content arises in the metaphor, i.e. whether it comes from the source domain, or from the target domain, or rather arises compositionally through interaction of the source and the target. To answer these questions, we conduct a"
S16-2003,S12-1033,1,0.46576,"ces from WordNet: Emotion annotation: Sentiment analysis is defined as detecting the evaluative or affective attitude in text. A vast majority of work in sentiment analysis has focused on developing classifiers for valence prediction (Kiritchenko et al., 2014; Dong et al., 2014; Socher et al., 2013; Mohammad et al., 2013), i.e., determining whether a piece of text expresses positive, negative, or neutral attitude. However, there is a growing interest in detecting a wider range of emotions such as joy, sadness, optimism, etc. (Holzman and Pottenger, 2003; Alm et al., 2005; Brooks et al., 2013; Mohammad, 2012). Much of the this work has been influenced by the idea that some emotions are more basic than others (Ekman, 1992; Ekman and Friesen, 2003; Plutchik, 1980; Plutchik, 1991). Mohammad (2012) polled the Twitter API for tweets that have hashtag words such as #anger and #sadness corresponding to the eight Plutchik basic emo4 Experimental Setup Instance 1 Target verb: erase Sentence: The Turks erased the Armenians. Here, erase is used metaphorically. We will refer to such instances as metaphorical instances. Now consider an instance similar to the one above, but where the target verb is replaced by"
S16-2003,D11-1063,1,0.2517,"sambiguation (WSD) is to develop a model for each polysemous word (Navigli, 2009). The model for a word predicts the intended sense, based on context. A problem with this approach to WSD is that good coverage of common polysemous English words would require about 3,200 distinct models. Kilgarriff (1997) has argued there are systematic relations among word senses across different words, focusing in particular on metaphor as a ubiquitous source of polysemy. This area of research is known as regular polysemy. Thus, there is a systematic relation between metaphor and word sense (Kilgarriff, 1997; Turney et al., 2011) and the emotion associated with a word depends on the sense of the word (Strapparava and Valitutti, 2004; Mohammad and Turney, 2013).3 This raises the question of whether there is a systematic relation between presence of metaphor and the emotional content of words. As far as we know, this is the first paper to quantitatively explore this question. Gibbs et al. (2002) conducted a study that looked at how listeners respond to metaphor and irony when they are played audio tapes describing emotional experiences. They found that on average metaphors were rated as being more emotional than non-met"
S16-2003,P12-3002,0,0.0197562,"” in (1a) we express a distinct viewpoint on governmental regulation of business, as opposed to a more neutral factual statement expressed in (1b). The interplay of metaphor and emotion has been an object of interest in fields such as linguistics (Blanchette et al., 2001; Kovecses, 2003), political science (Lakoff and Wehling, 2012), cognitive psychology (Crawford, 2009; Thibodeau and Boroditsky, 2011) and neuroscience (Aziz-Zadeh and Damasio, 2008; Jabbi et al., 2008). A number of computational approaches for sentiment polarity classification of metaphorical language have also been proposed (Veale and Li, 2012; Kozareva, 2013; Strzalkowski et al., 2014). However, there is no quantitative study establishing the extent to which metaphorical language is used to express emotion nor a data-supported account of the mechanisms by which this happens. Our study addresses two questions: (i) whether a metaphorical statement is likely to convey a stronger emotional content than its literal counterpart; and (ii) how this emotional content arises in the metaphor, i.e. whether it comes from the source domain, or from the target domain, or rather arises compositionally through interaction of the source and the tar"
S16-2021,W11-1701,0,0.198813,"abeled data created using distant supervision techniques. A large number of teams used word embeddings and some used deep neural networks such as RNNs and convolutional neural nets. Nonetheless, none of these systems surpassed our results presented here. Other Stance Detection Work. In work by Somasundaran and Wiebe (2010), a lexicon for detecting argument trigger expressions was created and subsequently leveraged to identify arguments. These extracted arguments, together with sentiment expressions and their targets, were employed in a supervised learner as features for stance classification. Anand et al. (2011) deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. Here, we did not explore dependency features since dependency parsers are not as accurate on tweets. Additionally, Anand et al. (2011) showed that there is no significant difference in performance between systems that use only unigrams and systems that also use other features such as LIWC and opinion or POS generalized dependencies in stance classification. Some of these features were used by the teams participating in the SemEval"
S16-2021,E14-1040,0,0.0104657,"ance (unlike stance prediction performance) is similar on the two sets of tweets. This shows that the two sets of tweets are not qualitatively different in how they express opinion. However, since one set expresses opinion about an entity other than the target of interest, detecting stance towards the target of interest from them is notably more challenging. 6 Related Work SemEval-2016 Task #6. The SemEval-2016 Task ‘Detecting Stance in Tweets’ received submissions from 19 teams, wherein the highest classification 166 two sets of features that are supposed to represent stance-taking language. Deng and Wiebe (2014) investigated the relationships and interactions among entities and events explicitly mentioned in the text with the goal of improving sentiment classification. In stance classification, however, the predetermined target of interest may not be mentioned in the text, or may not be the target of opinion in the text. Rajadesingan and Liu (2014) determined stance at user level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. In this work, we focus on detecting stance, as well as possible,"
S16-2021,W10-0204,1,0.265336,"aging the Favg calculated for each target separately, will be called F-macro-across-targets or F-macroT (for short). F-microT was used as the bottom-line evaluation metric in the SemEval2016 shared task on stance detection. Note that systems that perform relatively better on the more frequent target classes will obtain higher F-microT scores. On the other hand, to obtain a high FmacroT score a system has to perform well on all target classes. • sentiment features: features drawn from sentiment lexicons as suggested in (Kiritchenko et al., 2014b). The lexicons used include NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Sentiment and Emoticon Lexicons (Kiritchenko et al., 2014b). Some other feature sets that we experimented with, via cross-validation on the training set, included word embeddings trained on a generic Twitter corpus (not the domain corpus), the number of occurrences of each part-of-speech tag, the number of repeated sequences of exclamation or question marks, and the number of words with one character repeated more than two times (for example, yessss). However, they did not improve results"
S16-2021,S13-2053,1,0.75242,"ysis and Related Tasks. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Pontiki et al., 2014). Mohammad et al. (2013) and Kiritchenko et al. (2014a) came first in the SemEval-2013 Sentiment in Twitter and SemEval-2014 ABSA shared tasks. We use most of the features they proposed in our classifier. There are other subtasks in opinion mining related to stance classification, such as biased language detection (Recasens et al., 2013; Yano et al., 2010), perspective identification (Lin et al., 2006) and user classification based on their views (Kato et al., 2008). Perspective identification was defined as the subjective evaluation of points of view (Lin et al., 2006). None of the prior work has created a dataset a"
S16-2021,P11-2008,0,0.0300524,"Missing"
S16-2021,L16-1623,1,0.666366,"her one can deduce favorable or unfavorable stance towards one of five targets ‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, and ‘Legalization of Abortion’. Each of these tweets is also annotated for whether the target of opinion expressed in the tweet is the same as the given target of interest. Finally, each tweet is annotated for whether it conveys positive, negative, or neutral sentiment. (2) Partitions of this stance-annotated data were used as training and test sets in the SemEval2016 shared task competition ‘Task #6: Detecting Stance in Tweets’ (Mohammad et al., 2016b). Participants were provided with 2,914 training instances labeled for stance for the five targets. The test data included 1,249 instances. The task received submissions from 19 teams. The best per(4) We conduct experiments to better understand the interaction between stance and sentiment and the factors influencing their interaction. We use the gold labels to determine the extent to which stance can be determined simply from sentiment. We apply the stance detection system (mentioned above in (3)), as a common text classification framework, to determine both stance and sentiment. Results sho"
S16-2021,I13-1191,0,0.0307539,"Missing"
S16-2021,S16-1003,1,0.361383,"Missing"
S16-2021,C10-2100,0,0.0153802,"Missing"
S16-2021,S14-2076,1,0.650403,"apply it to the Stance Dataset for detecting both stance and sentiment. There is one exception to the common machine learning framework. The words and concepts used in tweets corresponding to the three stance categories are not expected to generalize across the targets. Thus, the stance system learns a separate model from training data pertaining to each of the targets.8 Positive and negative language tend to have sufficient amount of commonality regardless of topic of discussion, and hence sentiment analysis systems traditionally learn a single model from all of the training data (Liu, 2015; Kiritchenko et al., 2014b; Rosenthal et al., 2015). Thus our sentiment experiments are also based on a single model trained on all of the Stance Training set.9 Tweets are tokenized and part-of-speech tagged with the CMU Twitter NLP tool (Gimpel et al., 2011). We train a linear-kernel Support Vector Machine (SVM) classifier on the Stance training set. SVM is a state-of-the-art learning algorithm proved to be effective on text categorization tasks and robust on large feature spaces. The SVM parameters are tuned using 5-fold cross-validation on Stance Training set. We used the implementation provided in Scikit-learn Mac"
S16-2021,S14-2004,0,0.011508,"tweeter. Sentiment Analysis and Related Tasks. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Pontiki et al., 2014). Mohammad et al. (2013) and Kiritchenko et al. (2014a) came first in the SemEval-2013 Sentiment in Twitter and SemEval-2014 ABSA shared tasks. We use most of the features they proposed in our classifier. There are other subtasks in opinion mining related to stance classification, such as biased language detection (Recasens et al., 2013; Yano et al., 2010), perspective identification (Lin et al., 2006) and user classification based on their views (Kato et al., 2008). Perspective identification was defined as the subjective evaluation of points of view (Lin et al., 2006). None of the prior work"
S16-2021,W06-2915,0,0.01443,"ment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Pontiki et al., 2014). Mohammad et al. (2013) and Kiritchenko et al. (2014a) came first in the SemEval-2013 Sentiment in Twitter and SemEval-2014 ABSA shared tasks. We use most of the features they proposed in our classifier. There are other subtasks in opinion mining related to stance classification, such as biased language detection (Recasens et al., 2013; Yano et al., 2010), perspective identification (Lin et al., 2006) and user classification based on their views (Kato et al., 2008). Perspective identification was defined as the subjective evaluation of points of view (Lin et al., 2006). None of the prior work has created a dataset annotated for both stance and sentiment. ‘Task #6: Detecting Stance in Tweets’ that received submissions from 19 teams. We proposed a simple, but effective stance detection system that obtained an F-score (70.32) higher than the one obtained by the more complex, best-performing system in the competition. We used a linear-kernel SVM classifier that leveraged word and character n-g"
S16-2021,S15-2082,0,0.0607965,"Missing"
S16-2021,P13-1162,0,0.0104168,"t based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Pontiki et al., 2014). Mohammad et al. (2013) and Kiritchenko et al. (2014a) came first in the SemEval-2013 Sentiment in Twitter and SemEval-2014 ABSA shared tasks. We use most of the features they proposed in our classifier. There are other subtasks in opinion mining related to stance classification, such as biased language detection (Recasens et al., 2013; Yano et al., 2010), perspective identification (Lin et al., 2006) and user classification based on their views (Kato et al., 2008). Perspective identification was defined as the subjective evaluation of points of view (Lin et al., 2006). None of the prior work has created a dataset annotated for both stance and sentiment. ‘Task #6: Detecting Stance in Tweets’ that received submissions from 19 teams. We proposed a simple, but effective stance detection system that obtained an F-score (70.32) higher than the one obtained by the more complex, best-performing system in the competition. We used a"
S16-2021,W10-0723,0,0.0457156,"Missing"
S16-2021,S15-2078,1,0.887797,"set for detecting both stance and sentiment. There is one exception to the common machine learning framework. The words and concepts used in tweets corresponding to the three stance categories are not expected to generalize across the targets. Thus, the stance system learns a separate model from training data pertaining to each of the targets.8 Positive and negative language tend to have sufficient amount of commonality regardless of topic of discussion, and hence sentiment analysis systems traditionally learn a single model from all of the training data (Liu, 2015; Kiritchenko et al., 2014b; Rosenthal et al., 2015). Thus our sentiment experiments are also based on a single model trained on all of the Stance Training set.9 Tweets are tokenized and part-of-speech tagged with the CMU Twitter NLP tool (Gimpel et al., 2011). We train a linear-kernel Support Vector Machine (SVM) classifier on the Stance training set. SVM is a state-of-the-art learning algorithm proved to be effective on text categorization tasks and robust on large feature spaces. The SVM parameters are tuned using 5-fold cross-validation on Stance Training set. We used the implementation provided in Scikit-learn Machine Learning library (Ped"
S16-2021,W15-0509,1,0.872196,"owed that there is no significant difference in performance between systems that use only unigrams and systems that also use other features such as LIWC and opinion or POS generalized dependencies in stance classification. Some of these features were used by the teams participating in the SemEval task over this dataset; however, their systems’ performances were lower than the performance showed by our stance detection system. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. (2012). These relationships are not provided for our Stance dataset. Sobhani et al. (2015) extracted arguments used in online news comments to leverage them as extra features for detecting stance. Faulkner (2014) investigated the problem of detecting documentlevel stance in student essays by making use of Table 8: Stance Classification: F-scores obtained for tweets with opinion towards the target and tweets with opinion towards another entity. Classifier Majority classifier Our classifiers a. n-grams b. n-grams, sentiment lex. c. n-grams, embeddings d. all three feature sets FP os 44.22 FN eg 78.35 F-microT 61.28 64.78 72.21 68.85 71.90 81.75 85.52 84.00 85.21 73.27 78.87 76.43 78."
S16-2021,P09-1026,0,0.0245325,"Missing"
S16-2021,W10-0214,0,0.0148936,"hree features F-macroT Target Other 63.51 38.14 F-microT Target Other 75.31 44.15 F-score obtained was 67.82. The best performing systems used standard text classification features such as those drawn from n-grams, word vectors, and sentiment lexicons. Some teams drew additional gains from noisy stance-labeled data created using distant supervision techniques. A large number of teams used word embeddings and some used deep neural networks such as RNNs and convolutional neural nets. Nonetheless, none of these systems surpassed our results presented here. Other Stance Detection Work. In work by Somasundaran and Wiebe (2010), a lexicon for detecting argument trigger expressions was created and subsequently leveraged to identify arguments. These extracted arguments, together with sentiment expressions and their targets, were employed in a supervised learner as features for stance classification. Anand et al. (2011) deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. Here, we did not explore dependency features since dependency parsers are not as accurate on tweets. Additionally, Anand et al. (2011) show"
S16-2021,P14-1146,0,0.0256889,"targets, except ‘Atheism’, are predominantly negative. 4 A Common Text Classification Framework for Stance and Sentiment Past work has shown that the most useful features for sentiment analysis are word and character n-grams and sentiment lexicons, whereas others such as negation features, part-of-speech features, and punctuation have a smaller impact (Wilson et al., 2013; Mohammad et al., 2013; Kiritchenko et al., 2014b; Rosenthal et al., 2015). More recently, features drawn from word embeddings have been shown to be effective in various text classification tasks such as sentiment analysis (Tang et al., 2014) and named entity recognition (Turian et al., 2010). All of these features are expected to be useful in stance classification as well. However, it is unclear which features will be more useful (and to what extent) for detecting stance as opposed to sentiment. Since we now have a dataset annotated for both stance and sentiment, we create a com• n-grams: presence or absence of contiguous sequences of 1, 2 and 3 tokens (word n-grams); presence or absence of contiguous sequences of 2, 3, 4, and 5 characters (character n-grams); • word embeddings: the average of the word vectors for words appearing"
S16-2021,W06-1639,0,0.058159,"Missing"
S16-2021,P10-1040,0,0.0244277,"ative. 4 A Common Text Classification Framework for Stance and Sentiment Past work has shown that the most useful features for sentiment analysis are word and character n-grams and sentiment lexicons, whereas others such as negation features, part-of-speech features, and punctuation have a smaller impact (Wilson et al., 2013; Mohammad et al., 2013; Kiritchenko et al., 2014b; Rosenthal et al., 2015). More recently, features drawn from word embeddings have been shown to be effective in various text classification tasks such as sentiment analysis (Tang et al., 2014) and named entity recognition (Turian et al., 2010). All of these features are expected to be useful in stance classification as well. However, it is unclear which features will be more useful (and to what extent) for detecting stance as opposed to sentiment. Since we now have a dataset annotated for both stance and sentiment, we create a com• n-grams: presence or absence of contiguous sequences of 1, 2 and 3 tokens (word n-grams); presence or absence of contiguous sequences of 2, 3, 4, and 5 characters (character n-grams); • word embeddings: the average of the word vectors for words appearing in a given 8 We built a stance system that learns"
S16-2021,N12-1072,0,0.0174849,"endency parsers are not as accurate on tweets. Additionally, Anand et al. (2011) showed that there is no significant difference in performance between systems that use only unigrams and systems that also use other features such as LIWC and opinion or POS generalized dependencies in stance classification. Some of these features were used by the teams participating in the SemEval task over this dataset; however, their systems’ performances were lower than the performance showed by our stance detection system. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. (2012). These relationships are not provided for our Stance dataset. Sobhani et al. (2015) extracted arguments used in online news comments to leverage them as extra features for detecting stance. Faulkner (2014) investigated the problem of detecting documentlevel stance in student essays by making use of Table 8: Stance Classification: F-scores obtained for tweets with opinion towards the target and tweets with opinion towards another entity. Classifier Majority classifier Our classifiers a. n-grams b. n-grams, sentiment lex. c. n-grams, embeddings d. all three feature sets FP os 44.22 FN eg 78.35"
S16-2021,H05-1044,0,0.0490796,"F-macroT (for short). F-microT was used as the bottom-line evaluation metric in the SemEval2016 shared task on stance detection. Note that systems that perform relatively better on the more frequent target classes will obtain higher F-microT scores. On the other hand, to obtain a high FmacroT score a system has to perform well on all target classes. • sentiment features: features drawn from sentiment lexicons as suggested in (Kiritchenko et al., 2014b). The lexicons used include NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Sentiment and Emoticon Lexicons (Kiritchenko et al., 2014b). Some other feature sets that we experimented with, via cross-validation on the training set, included word embeddings trained on a generic Twitter corpus (not the domain corpus), the number of occurrences of each part-of-speech tag, the number of repeated sequences of exclamation or question marks, and the number of words with one character repeated more than two times (for example, yessss). However, they did not improve results there, and so we did not include them for the test set experiments. We evaluate the lear"
S16-2021,S13-2052,0,\N,Missing
S17-1007,H05-1073,0,0.681342,"tain dimensional models of emotion have also been proposed. The most popular among them, Russell’s circumplex model, asserts that all emotions are made up of two core dimensions: valence and arousal (Russell, 2003). In this paper, we describe work on four emotions that are the most common amongst the many proposals for basic emotions: anger, fear, joy, and sadness. However, we have also begun work on other affect categories, as well as on valence and arousal. The vast majority of emotion annotation work provides discrete binary labels to the text instances (joy–nojoy, fear–nofear, and so on) (Alm et al., 2005; Aman and Szpakowicz, 2007; Brooks et al., 2013; Neviarouskaya et al., 2009; Bollen et al., 2009). The only annotation effort that provided scores for degree of emotion is by Strapparava and Mihalcea (2007) as part of one of the SemEval2007 shared task. Annotators were given newspaper headlines and asked to provide scores between 3 Identifying intensity of emotion evoked in the reader, or intensity of emotion felt by an entity mentioned in the tweet, are also useful, and left for future work. 4 We have also begun work on creating similar datasets annotated for other emotion categories. We are"
S17-1007,N13-1062,0,0.0482579,", known as scale region bias. Best–Worst Scaling (BWS) was developed by Louviere (1991), building on some groundbreaking research in the 1960s in mathematical psychology and psychophysics by Anthony A. J. Marley and Duncan Luce. Kiritchenko and Mohammad (2017) show through empirical experiments that BWS produces more reliable fine-grained scores than scores obtained using rating scales. Within the NLP community, Best–Worst Scaling (BWS) has thus far been used only to annotate words: for example, for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), and phrase sentiment composition (Kiritchenko and Mohammad, 2016). However, in this work we use BWS to annotate whole tweets for degree of emotion. With BWS we address the challenges of direct scoring, and produce more reliable emotion intensity scores. Further, this will be the first dataset with emotion scores for tweets. Automatic emotion classification has been proposed for many different kinds of texts, including tweets (Summa et al., 2016; Mohammad, 2012; Bollen et al., 2009; Aman and Szpakowicz, 2007; Brooks et al., 2013). However,"
S17-1007,S12-1047,1,0.922046,"em pairs. For example, for a 4-tuple with items A, B, C, and D, if A is the best, and D is the worst, then A > B, A > C, A > D, B > D, and C > D. BWS annotations for a set of 4-tuples can be easily converted into real-valued scores of association between the items and the property of interest (Orme, 2009; Flynn and Marley, 2014). It has been empirically shown that annotations for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016).2 The little work using BWS in computational linguistics has focused on words (Jurgens et al., 2012; Kiritchenko and Mohammad, 2016). It is unclear whether the approach can be scaled up to larger textual units such as sentences. Twitter has a large and diverse user base, which entails rich textual content, including nonstandard language such as emoticons, emojis, creThis paper examines the task of detecting intensity of emotion from text. We create the first datasets of tweets annotated for anger, fear, joy, and sadness intensities. We use a technique called best–worst scaling (BWS) that improves annotation consistency and obtains reliable fine-grained scores. We show that emotion-word hash"
S17-1007,N16-1095,1,0.865411,"Zealand fbravoma@waikato.ac.nz Saif M. Mohammad Information and Communications Technologies National Research Council Canada Ottawa, Canada saif.mohammad@nrc-cnrc.gc.ca Abstract tating instances for degrees of affect is a substantially more difficult undertaking: respondents are presented with greater cognitive load and it is particularly hard to ensure consistency (both across responses by different annotators and within the responses produced by an individual annotator). Best–Worst Scaling (BWS) is an annotation scheme that addresses these limitations (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). Annotators are given n items (an n-tuple, where n > 1 and commonly n = 4). They are asked which item is the best (highest in terms of the property of interest) and which is the worst (lowest in terms of the property of interest). When working on 4tuples, best–worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six item pairs. For example, for a 4-tuple with items A, B, C, and D, if A is the best, and D is the worst, then A > B, A > C, A > D, B > D, and C > D. BWS annotations for a set of 4-tuples can be easily conver"
S17-1007,P17-2074,1,0.85304,"mon problem is inconsistency in annotations. One annotator might assign a score of 79 to a piece of text, whereas another annotator may assign a score of 62 to the same text. It is also common that the same annotator assigns different scores to the same text instance at different points in time. Further, annotators often have a bias towards different parts of the scale, known as scale region bias. Best–Worst Scaling (BWS) was developed by Louviere (1991), building on some groundbreaking research in the 1960s in mathematical psychology and psychophysics by Anthony A. J. Marley and Duncan Luce. Kiritchenko and Mohammad (2017) show through empirical experiments that BWS produces more reliable fine-grained scores than scores obtained using rating scales. Within the NLP community, Best–Worst Scaling (BWS) has thus far been used only to annotate words: for example, for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), and phrase sentiment composition (Kiritchenko and Mohammad, 2016). However, in this work we use BWS to annotate whole tweets for degree of emotion. With BWS we address the challenges of direc"
S17-1007,esuli-sebastiani-2006-sentiwordnet,0,0.0221677,"Missing"
S17-1007,W14-1304,0,0.0334158,"Missing"
S17-1007,P11-2008,0,0.270513,"Missing"
S17-1007,W10-0513,0,0.512416,"Missing"
S17-1007,S12-1033,1,0.952238,"ets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), and phrase sentiment composition (Kiritchenko and Mohammad, 2016). However, in this work we use BWS to annotate whole tweets for degree of emotion. With BWS we address the challenges of direct scoring, and produce more reliable emotion intensity scores. Further, this will be the first dataset with emotion scores for tweets. Automatic emotion classification has been proposed for many different kinds of texts, including tweets (Summa et al., 2016; Mohammad, 2012; Bollen et al., 2009; Aman and Szpakowicz, 2007; Brooks et al., 2013). However, there is little work on emotion regression other than the three submissions to the 2007 SemEval task (Strapparava and Mihalcea, 2007). 3 • selecting at most 50 tweets per query term. • selecting at most 1 tweet for every tweeter– query term combination. Thus, the master set of tweets is not heavily skewed towards some tweeters or query terms. To study the impact of emotion word hashtags on the intensity of the whole tweet, we identified tweets that had a query term in hashtag form towards the end of the tweet—spec"
S17-1007,W17-5205,1,0.606823,"Missing"
S17-1007,W13-1602,0,0.0434489,"Missing"
S17-1007,D14-1127,0,0.0570517,"Missing"
S17-1007,S13-2053,1,0.951922,"ore difficult to ascertain the degrees of anger of speakers from their tweets. Note that SHR indicates the quality of annotations obtained when using only half the number of annotations. The correlations obtained when repeating the experiment with three annotations for each 4-tuple is expected to be even higher. Thus the numbers shown in Table 2 are a lower bound on the quality of annotations obtained with three annotations per 4-tuple. 5 Impact of Emotion Word Hashtags on Emotion Intensity Some studies have shown that emoticons tend to be redundant in terms of the sentiment (Go et al., 2009; Mohammad et al., 2013). That is, if we remove a smiley face, ‘:)’, from a tweet, we find that the rest of the tweet still conveys a positive sentiment. Similarly, it has been shown that hashtag emotion words are also somewhat redundant in terms of the class of emotion being conveyed by the rest of the tweet (Mohammad, 2012). For example, removal of ‘#angry’ from the tweet below leaves a tweet that still conveys anger. This mindless support of a demagogue needs to stop. #racism #grrr #angry However, it is unclear what impact such emotion word hashtags have on the intensity of emotion. In fact, there exists no prior"
S17-1007,roberts-etal-2012-empatweet,0,0.0684394,"Missing"
S17-1007,S07-1013,0,0.920915,"rousal (Russell, 2003). In this paper, we describe work on four emotions that are the most common amongst the many proposals for basic emotions: anger, fear, joy, and sadness. However, we have also begun work on other affect categories, as well as on valence and arousal. The vast majority of emotion annotation work provides discrete binary labels to the text instances (joy–nojoy, fear–nofear, and so on) (Alm et al., 2005; Aman and Szpakowicz, 2007; Brooks et al., 2013; Neviarouskaya et al., 2009; Bollen et al., 2009). The only annotation effort that provided scores for degree of emotion is by Strapparava and Mihalcea (2007) as part of one of the SemEval2007 shared task. Annotators were given newspaper headlines and asked to provide scores between 3 Identifying intensity of emotion evoked in the reader, or intensity of emotion felt by an entity mentioned in the tweet, are also useful, and left for future work. 4 We have also begun work on creating similar datasets annotated for other emotion categories. We are also creating a dataset annotated for valence, arousal, and dominance. 5 https://github.com/felipebravom/AffectiveTweets 6 http://saifmohammad.com/WebPages/EmotionIntensitySharedTask.html 7 https://competit"
S17-1007,W16-4317,0,0.0386639,", for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), and phrase sentiment composition (Kiritchenko and Mohammad, 2016). However, in this work we use BWS to annotate whole tweets for degree of emotion. With BWS we address the challenges of direct scoring, and produce more reliable emotion intensity scores. Further, this will be the first dataset with emotion scores for tweets. Automatic emotion classification has been proposed for many different kinds of texts, including tweets (Summa et al., 2016; Mohammad, 2012; Bollen et al., 2009; Aman and Szpakowicz, 2007; Brooks et al., 2013). However, there is little work on emotion regression other than the three submissions to the 2007 SemEval task (Strapparava and Mihalcea, 2007). 3 • selecting at most 50 tweets per query term. • selecting at most 1 tweet for every tweeter– query term combination. Thus, the master set of tweets is not heavily skewed towards some tweeters or query terms. To study the impact of emotion word hashtags on the intensity of the whole tweet, we identified tweets that had a query term in hashtag form towards the end o"
S17-1007,H05-1044,0,0.14208,"Missing"
S17-1007,pak-paroubek-2010-twitter,0,0.296526,"Missing"
S18-1001,N16-1095,1,0.484927,"or work on sentiment and emotion classification (Mohammad, 2016). There is also growing work on related tasks such as stance detection (Mohammad et al., 2017) and argumentation mining (Wojatzki et al., 2018; Palau and Moens, 2009). However, there is little work on detecting the intensity of affect in text. Mohammad and Bravo-Marquez (2017) created the first datasets of tweets annotated for anger, fear, joy, and sadness intensities. Given a focus emotion, each tweet was annotated for intensity of the emotion felt by the speaker using a technique called Best–Worst Scaling (BWS) (Louviere, 1991; Kiritchenko and Mohammad, 2016, 2017). BWS is an annotation scheme that addresses the limitations of traditional rating scale methods, such as inter- and intra-annotator inconsistency, by employing comparative annotations. Note that at its simplest, comparative annotations involve giving people pairs of items and asking which item is greater in terms of the property of interest. However, such a method requires annotations for N 2 items, which can be prohibitively large. In BWS, annotators are given n items (an ntuple, where n > 1 and commonly n = 4). They are asked which item is the best (highest in terms of the property o"
S18-1001,W14-3623,0,0.0866787,"Missing"
S18-1001,P17-2074,1,0.840747,"ample, for a 4-tuple with items A, B, C, and D, if A is the best, and D is the worst, then A > B, A > C, A > D, B > D, and C > D. Real-valued scores of association between the items and the property of interest can be determined using simple arithmetic on the number of times an item was chosen best and number of times it was chosen worst (as described in Section 3.4.2) (Orme, 2009; Flynn and Marley, 2014). It has been empirically shown that annotations for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016). Kiritchenko and Mohammad (2017) showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. (See (Kiritchenko and Mohammad, 2016, 2017) for further details on BWS.) 4. Valence Ordinal Classification (V-oc): Given a tweet, classify it into one of seven ordinal classes, corresponding to various levels of positive and negative sentiment intensity, that best represents the mental state of the tweeter; 5. Emotion Classification (E-c): Given a tweet, classify it as ‘neutral or no emotion’ or as one, or more, of eleven given emotions that best represe"
S18-1001,S18-2005,1,0.871138,"Missing"
S18-1001,D17-1169,0,0.127329,"Missing"
S18-1001,L16-1006,1,0.0832642,"Missing"
S18-1001,P18-1017,1,0.871298,"Missing"
S18-1001,L18-1027,1,0.905245,"Missing"
S18-1001,W17-5205,1,0.548047,"Missing"
S18-1001,L18-1030,1,0.858617,"Missing"
S18-1001,L18-1224,1,0.768569,"3 https://competitions.codalab.org/competitions/17751 Determined through pilot annotations. 1 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1–17 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 2 of the tweeter—a real-valued score between 0 (most negative) and 1 (most positive); Building on Past Work There is a large body of prior work on sentiment and emotion classification (Mohammad, 2016). There is also growing work on related tasks such as stance detection (Mohammad et al., 2017) and argumentation mining (Wojatzki et al., 2018; Palau and Moens, 2009). However, there is little work on detecting the intensity of affect in text. Mohammad and Bravo-Marquez (2017) created the first datasets of tweets annotated for anger, fear, joy, and sadness intensities. Given a focus emotion, each tweet was annotated for intensity of the emotion felt by the speaker using a technique called Best–Worst Scaling (BWS) (Louviere, 1991; Kiritchenko and Mohammad, 2016, 2017). BWS is an annotation scheme that addresses the limitations of traditional rating scale methods, such as inter- and intra-annotator inconsistency, by employing comparat"
S18-1001,N16-1091,0,0.0173255,"Missing"
S18-1045,S13-2053,1,0.854221,"Missing"
S18-1045,W02-1011,0,0.0196442,"n-derived features, compiled using the AffectiveTweets package (Mohammad and BravoMarquez, 2017).3 The lexicons used include those created by Nielsen (2011); Mohammad and Turney (2013); Kiritchenko et al. (2014); Hu and Liu (2004); Bravo-Marquez et al. (2016); Thelwall et al. (2012); Wilson et al. (2005). • Embedding-based features: – Average word embedding vector; – Representation of a tweet learned by an LSTM neural network on the provided training data; – Emotion-polarized representation of a tweet learned by an LSTM neural network on a distant supervision corpus; We use bag-of-word (BOW) (Pang et al., 2002) and term frequency-inverse document frequency (tf-idf) methods to extract different word and character n-grams. We train word embeddings on a large corpus of tweets that contain emotion words. Then, we reﬁne our learned word embeddings to build emotion-speciﬁc word embeddings for every emotion. Speciﬁcally, we assign emotionspeciﬁc weights to every word in our learned word embeddings and multiply each word vector by weights. These emotion-speciﬁc weights are obtained by calculating the Pearson correlation between the extracted unigram features and intensity labels of the training and developm"
S18-1045,D15-1168,0,0.049781,"Missing"
S18-1045,D13-1170,0,0.0215474,"Missing"
S18-1045,S17-1007,1,0.895384,"Missing"
S18-1045,P14-1146,0,0.0620948,"Missing"
S18-1045,S18-1001,1,0.851121,"Missing"
S18-1045,D16-1169,0,0.0278322,"Missing"
S18-1045,H05-1044,0,0.102102,"second model incorporates both embedding-based representations and linguistic knowledge in a uniﬁed architecture (see Figure 1). We train a Support Vector Regressor (SVR) over the following two categories of features: • Lexicon-based and n-gram features: – Word and character n-gram features; – Vector of 43 lexicon-derived features, compiled using the AffectiveTweets package (Mohammad and BravoMarquez, 2017).3 The lexicons used include those created by Nielsen (2011); Mohammad and Turney (2013); Kiritchenko et al. (2014); Hu and Liu (2004); Bravo-Marquez et al. (2016); Thelwall et al. (2012); Wilson et al. (2005). • Embedding-based features: – Average word embedding vector; – Representation of a tweet learned by an LSTM neural network on the provided training data; – Emotion-polarized representation of a tweet learned by an LSTM neural network on a distant supervision corpus; We use bag-of-word (BOW) (Pang et al., 2002) and term frequency-inverse document frequency (tf-idf) methods to extract different word and character n-grams. We train word embeddings on a large corpus of tweets that contain emotion words. Then, we reﬁne our learned word embeddings to build emotion-speciﬁc word embeddings for every"
S18-1045,P14-1029,1,0.842136,". https://affectivetweets.cms.waikato.ac.nz/ 307 Emotionintensity SupportVectorRegressor Finalrepresentation Concatenation Polarizedrepresentation LSTMrepresentation Averageembedding Lexiconfeatures Handcraftedfeatures LSTMtrained onDSD LSTMintensity regressor Wordvectors averaging Tweet2Lexicon ExtractnͲgrams andextrafeatures SemEval 2018dataset Figure 1: The architecture of our model (System II). 3.3 Word embedding layer tuation mark. Due to the signiﬁcant impact of negation words in changing the sentiment polarity of a sentence, we treat the negated tokens differently (Zhu et al., 2014; Kiritchenko et al., 2014). By adding a ‘NEG’ preﬁx to them, we consider the negated tokens as different entities and learn separate word representations for them. Since the input of our model is a sequence of tokens {w1 , w2 , ..., wn }, it is crucial to learn an effective word representation for automatic emotion analysis. A word embedding is a dense, low-dimensional and real-valued vector associated with each word wi . We used word2vec (Mikolov et al., 2013a) and SVD-NS (Soleimani and Matwin, 2018) to learn word embeddings and trained it on an unlabeled corpus of 21M tweets provided as par"
S18-2005,D16-1120,0,0.090465,"Missing"
S18-2005,P17-2009,0,0.103508,"pport systems that prioritize a call from an angry male over a call from the equally angry female. Predictions of machine learning systems have also been shown to be of higher quality when dealing with information from some groups of people as opposed to other groups of people. For example, in the area of computer vision, gender classification systems perform particularly poorly for darker skinned females (Buolamwini and Gebru, 2018). Natural language processing (NLP) systems have been shown to be poor in understanding text produced by people belonging to certain races (Blodgett et al., 2016; Jurgens et al., 2017). For NLP systems, the sources of the bias often include the training data, other corpora, lexicons, and word embeddings that the machine learning algorithm may leverage to build its prediction model. Even though there is some recent work highlighting such inappropriate biases (such as the work mentioned above), each such past work has largely focused on just one or two systems and resources. Further, there is no benchmark dataset for examining inappropriate biases in natural language systems. In this paper, we describe how we compiled a dataset of 8,640 English sentences carefully chosen to t"
S18-2005,L18-1027,1,0.77124,"used machine learning algorithms. The sentence embeddings were obtained by training a neural network on the provided training data, a distant supervision corpus (e.g., AIT2018 Distant Supervision Corpus that has tweets with emotionrelated query terms), sentiment-labeled tweet corpora (e.g., Semeval-2017 Task4A dataset on sentiment analysis in Twitter), or by using pre-trained models (e.g., DeepMoji (Felbo et al., 2017), Skip thoughts (Kiros et al., 2015)). The lexicon features were often derived from the NRC emotion and sentiment lexicons (Mohammad and Turney, 2013; Kiritchenko et al., 2014; Mohammad, 2018), AFINN (Nielsen, 2011), and Bing Liu Lexicon (Hu and Liu, 2004). depressed devastated disappointed miserable sad depressing gloomy grim heartbreaking serious Table 4: Emotion words used in this study. ined the sentences to make sure they were grammatically well-formed.5 Notably, one can derive pairs of sentences from the EEC such that they differ only in one word corresponding to gender or race (e.g., ‘My daughter feels devastated’ and ‘My son feels devastated’). We refer to the full set of 8,640 sentences as Equity Evaluation Corpus. 4 Measuring Race and Gender Bias in Automatic Sentiment An"
S18-2005,D17-1169,0,0.043517,"d to learn a model using either traditional machine learning algorithms (such as SVM/SVR and Logistic Regression) or deep neural networks. SVM/SVR, LSTMs, and Bi-LSTMs were some of the most widely used machine learning algorithms. The sentence embeddings were obtained by training a neural network on the provided training data, a distant supervision corpus (e.g., AIT2018 Distant Supervision Corpus that has tweets with emotionrelated query terms), sentiment-labeled tweet corpora (e.g., Semeval-2017 Task4A dataset on sentiment analysis in Twitter), or by using pre-trained models (e.g., DeepMoji (Felbo et al., 2017), Skip thoughts (Kiros et al., 2015)). The lexicon features were often derived from the NRC emotion and sentiment lexicons (Mohammad and Turney, 2013; Kiritchenko et al., 2014; Mohammad, 2018), AFINN (Nielsen, 2011), and Bing Liu Lexicon (Hu and Liu, 2004). depressed devastated disappointed miserable sad depressing gloomy grim heartbreaking serious Table 4: Emotion words used in this study. ined the sentences to make sure they were grammatically well-formed.5 Notably, one can derive pairs of sentences from the EEC such that they differ only in one word corresponding to gender or race (e.g., ‘M"
S18-2005,W17-5205,1,0.864477,"Missing"
S18-2005,S18-1001,1,0.879857,"Missing"
S18-2005,P15-1073,0,0.0596567,"amework to represent words as low-dimensional dense vectors. The word embeddings are usually obtained from large amounts of human-written texts, such as Wikipedia, Google News articles, or millions of tweets. Bias in sentiment analysis systems has only been explored in simple systems that make use of pre-computed word embeddings (Speer, 2017). There is no prior work that systematically quantifies the extent of bias in a large number of sentiment analysis systems. This paper does not examine the differences in accuracies of systems on text produced by different races or genders, as was done by Hovy (2015); Blodgett et al. (2016); Jurgens et al. (2017); Buolamwini and Gebru (2018). Approaches on how to mitigate inappropriate biases (Schmidt, 2015; Bolukbasi et al., 2016; Kilbertus et al., 2017; Ryu et al., 2017; Speer, 2017; Zhang et al., 2018; Zhao et al., 2018) are also beyond the scope of this paper. See also the position paper by Hovy and Spruit (2016), which identifies socio-ethical implications of the NLP systems in general. 3 The Equity Evaluation Corpus We now describe how we compiled a dataset of thousands of sentences to determine whether automatic systems consistently give higher (or"
S18-2005,P16-2096,0,0.0473835,"There is no prior work that systematically quantifies the extent of bias in a large number of sentiment analysis systems. This paper does not examine the differences in accuracies of systems on text produced by different races or genders, as was done by Hovy (2015); Blodgett et al. (2016); Jurgens et al. (2017); Buolamwini and Gebru (2018). Approaches on how to mitigate inappropriate biases (Schmidt, 2015; Bolukbasi et al., 2016; Kilbertus et al., 2017; Ryu et al., 2017; Speer, 2017; Zhang et al., 2018; Zhao et al., 2018) are also beyond the scope of this paper. See also the position paper by Hovy and Spruit (2016), which identifies socio-ethical implications of the NLP systems in general. 3 The Equity Evaluation Corpus We now describe how we compiled a dataset of thousands of sentences to determine whether automatic systems consistently give higher (or lower) sentiment intensity scores to sentences involving a particular race or gender. There are several ways in which such a dataset may be compiled. We present below the choices that we made.3 3 Even though the emotion intensity task motivated some of the choices in creating the dataset, the dataset can be used to examine bias in other NLP systems as we"
S18-2005,N18-2003,0,0.0717736,"en explored in simple systems that make use of pre-computed word embeddings (Speer, 2017). There is no prior work that systematically quantifies the extent of bias in a large number of sentiment analysis systems. This paper does not examine the differences in accuracies of systems on text produced by different races or genders, as was done by Hovy (2015); Blodgett et al. (2016); Jurgens et al. (2017); Buolamwini and Gebru (2018). Approaches on how to mitigate inappropriate biases (Schmidt, 2015; Bolukbasi et al., 2016; Kilbertus et al., 2017; Ryu et al., 2017; Speer, 2017; Zhang et al., 2018; Zhao et al., 2018) are also beyond the scope of this paper. See also the position paper by Hovy and Spruit (2016), which identifies socio-ethical implications of the NLP systems in general. 3 The Equity Evaluation Corpus We now describe how we compiled a dataset of thousands of sentences to determine whether automatic systems consistently give higher (or lower) sentiment intensity scores to sentences involving a particular race or gender. There are several ways in which such a dataset may be compiled. We present below the choices that we made.3 3 Even though the emotion intensity task motivated some of the choi"
S18-2026,P14-5011,1,0.84317,"lculate overlap between the surface forms of assertions. We use the following methods as implemented by DKPro Similarity (B¨ar et al., 2013)3 : (i) unigram overlap expressed by the Jaccard coefficient (Lyon et al., 2001), (ii) greedy string tiling (Wise, 1996), (iii) longest common sub string (Gusfield, 1997). Additionally, we use averaged word embeddings (Bojanowski et al., 2017). Beyond the baselines, we apply two machine learning approaches: a conventional SVM-based classifier and a neural network. The SVM classifier is implemented using LibSVM (Chang and Lin, 2011) as provided by DKProTC (Daxenberger et al., 2014).4 We use a combination of various ngram features, sentiment features (derived from the system by Kiritchenko et al. (2014)5 ), embedding features (averaged embeddings by Bojanowski et al. (2017)) and negation features. We used a linear kernel with C=100 and the nu-SVR 0 Number of Assertions Measuring Judgment Similarity Between Assertions 150 # of Judgments 100 Black Lives Matter Climate Change Creationism in School Foreign Aid Gender Equality Gun Rights Marijuana Mandatory Vaccination Media Bias Obama Care Same-sex Marriage US Electoral System US in the Middle East US Immigration Vegetarian"
S18-2026,S12-1051,0,0.0161181,"ational Semantics (*SEM), pages 214–224 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics are not easily available – e.g. if we want to predict a judgment on a new, unseen assertion. To overcome this limitation, we propose to use methods that consider the texts of the assertions to mimic judgment similarity and have thus the ability to generalize from existing data collections. 3 Figure 1: Overview on the two prediction tasks. Measuring the judgment similarity of two assertions is related to several NLP tasks such as the detection of semantic text similarity (STS) (Agirre et al., 2012), paraphrase recognition (Bhagat and Hovy, 2013), and textual entailment (Dagan et al., 2009). Unlike semantic text similarity, we do not use a notation of similarity based on the intuition of humans, but one that derives from the context of judgments. Hence, we define that the judgment similarity of two assertions is 1 if two assertions are consistently judged the same and are thus interchangeable in the context of our task. There are several reasons why assertions are judged similarly: their text may convey similar semantics such as in the assertions ‘Marijuana alleviates the suffering of ch"
S18-2026,P13-4021,1,0.885096,"Missing"
S18-2026,P16-1150,0,0.071505,"Missing"
S18-2026,E17-1024,0,0.0592228,"Missing"
S18-2026,L18-1403,1,0.831316,"t merges these branches. SNNs have been successfully used to predict text similarity (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) and match pairs of sentences (e.g. a tweet to reply) (Hu et al., 2014). In our SNN, a branch consists of a layer that translates the assertions into sequences of word embeddings, which is followed by a convolution layer with a filter size of two, max pooling over time layer, and a dense layer. To merge the branches, we calculate the cosine similarity of the extracted vector representations. The SNN was implemented using the deep learning framework deepTC (Horsmann and Zesch, 2018) in conjunction with Keras6 and Tensorflow (Abadi et al., 2016). In order to ensure full reproducibility of our results, the source code for both approaches is publicly available.7 We evaluate all approaches using 10-fold cross validation and calculate Pearson correlation between the prediction and the gold similarity. 5.2 Method SNN SVM Embedding distance Jaccard Greedy string tiling Longest common sub string 7 .61 .58 .07 .07 .06 .05 Table 2: Pearson correlation (averaged over all issues) of text-based approaches for approximating similarity of assertion judgments.5 Issue SVM SNN Climate Cha"
S18-2026,Q17-1010,0,0.0593545,"er self-pairing. 5.1 Total 200 Table 1: Issues and number of crowdsourced assertions and judgments. 50 −1.0 −0.5 0.0 0.5 Experimental Setup As baselines for this task, we utilize wellestablished semantic text similarity (STS) methods that calculate overlap between the surface forms of assertions. We use the following methods as implemented by DKPro Similarity (B¨ar et al., 2013)3 : (i) unigram overlap expressed by the Jaccard coefficient (Lyon et al., 2001), (ii) greedy string tiling (Wise, 1996), (iii) longest common sub string (Gusfield, 1997). Additionally, we use averaged word embeddings (Bojanowski et al., 2017). Beyond the baselines, we apply two machine learning approaches: a conventional SVM-based classifier and a neural network. The SVM classifier is implemented using LibSVM (Chang and Lin, 2011) as provided by DKProTC (Daxenberger et al., 2014).4 We use a combination of various ngram features, sentiment features (derived from the system by Kiritchenko et al. (2014)5 ), embedding features (averaged embeddings by Bojanowski et al. (2017)) and negation features. We used a linear kernel with C=100 and the nu-SVR 0 Number of Assertions Measuring Judgment Similarity Between Assertions 150 # of Judgmen"
S18-2026,W16-2815,0,0.025114,"there are effects such as the author’s followers affecting the visibility of posts and thereby the likelihood of a like or a retweet (Suh et al., 2010). In addition, we relate to works that aim at predicting whether two texts (Menini and Tonelli, 2016) or sequences of utterances (Wang and Cardie, 2014; Celli et al., 2016) express agreement or disagreement with each other. More broadly, we also relate to works that analyze stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016), or arguments (Habernal and ˇ Gurevych, 2016; Boltuzic and Snajder, 2016; BarHaim et al., 2017) that are expressed via text. In contrast to these works, we do not examine what judgment, sentiment, or claim is expressed by a text, but whether we can infer agreement or disagreement based on judgments which were made on other assertions. Finally, we relate to work on analyzing and predicting outcomes of congressional roll-call voting. These works constantly find that votes of politicians can be explained by a low number of underlying, ideological dimensions such as being left or right (Heckman and Snyder, 1996; Poole and Rosenthal, 1997, 2001). Our work is different"
S18-2026,W16-4312,0,0.0214649,"iffers significantly from our task: even if one agrees with a text, one might decide not to retweet or like it for any number of reasons. There are also cases in which one may retweet a post with which one disagrees in order to flag someone or something from the opposing community. Furthermore, there are effects such as the author’s followers affecting the visibility of posts and thereby the likelihood of a like or a retweet (Suh et al., 2010). In addition, we relate to works that aim at predicting whether two texts (Menini and Tonelli, 2016) or sequences of utterances (Wang and Cardie, 2014; Celli et al., 2016) express agreement or disagreement with each other. More broadly, we also relate to works that analyze stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016), or arguments (Habernal and ˇ Gurevych, 2016; Boltuzic and Snajder, 2016; BarHaim et al., 2017) that are expressed via text. In contrast to these works, we do not examine what judgment, sentiment, or claim is expressed by a text, but whether we can infer agreement or disagreement based on judgments which were made on other assertions. Finally, we relate to work on an"
S18-2026,W01-0515,0,0.174909,"two assertions. We calculate the gold similarity between all unique pairs (e.g. we do not use both a1 with a2 and a2 with a1 ) in our data and do not consider self-pairing. 5.1 Total 200 Table 1: Issues and number of crowdsourced assertions and judgments. 50 −1.0 −0.5 0.0 0.5 Experimental Setup As baselines for this task, we utilize wellestablished semantic text similarity (STS) methods that calculate overlap between the surface forms of assertions. We use the following methods as implemented by DKPro Similarity (B¨ar et al., 2013)3 : (i) unigram overlap expressed by the Jaccard coefficient (Lyon et al., 2001), (ii) greedy string tiling (Wise, 1996), (iii) longest common sub string (Gusfield, 1997). Additionally, we use averaged word embeddings (Bojanowski et al., 2017). Beyond the baselines, we apply two machine learning approaches: a conventional SVM-based classifier and a neural network. The SVM classifier is implemented using LibSVM (Chang and Lin, 2011) as provided by DKProTC (Daxenberger et al., 2014).4 We use a combination of various ngram features, sentiment features (derived from the system by Kiritchenko et al. (2014)5 ), embedding features (averaged embeddings by Bojanowski et al. (2017)"
S18-2026,S16-1003,1,0.884562,"Missing"
S18-2026,P14-1017,0,0.0275364,"s low semantic text similarity. In addition, two assertions can also have a strong judgment similarity because of underlying socio-cultural, political, or personal factors. For instance, the assertions ‘Consuming Marijuana has no impact on your success at work’ and ‘Marijuana is not addictive’ describe different arguments for legalizing marijuana, but judgments made on these assertions are often correlated. Our work also relates to other attempts on predicting reactions to text, such as predicting the number of retweets (Suh et al., 2010; Petrovic et al., 2011), the number of likes on tweets (Tan et al., 2014), the number of karma points of reddit posts (Wei et al., 2016), or sales from product descriptions (Pryzant et al., 2017). What those dicted using a Siamese neural network, which outperforms all other approaches by a wide margin. 2 Related Work Predicting Judgments In order to predict if someone will agree with an assertion, we need knowledge about that person. Ideally, we would have access to a large set of other assertions which the person has already judged. We could then measure the similarity between previous assertions and the new assertion and hypothesize that the judgment on the new a"
S18-2026,W16-1617,0,0.0334474,"0 version 1.0 5 The NRC-Canada system ranked first in the SemEval 2013 (Nakov et al., 2013) and 2014 (Rosenthal et al., 2014) tasks on sentiment analysis. 4 217 regression model. Iterative experiments showed that this configuration gave the most stable results across the issues. For the neural approach, we adapt Siamese neural networks (SNN), which consist of two identical branches or sub-networks that try to extract useful representations of the assertions and a final layer that merges these branches. SNNs have been successfully used to predict text similarity (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) and match pairs of sentences (e.g. a tweet to reply) (Hu et al., 2014). In our SNN, a branch consists of a layer that translates the assertions into sequences of word embeddings, which is followed by a convolution layer with a filter size of two, max pooling over time layer, and a dense layer. To merge the branches, we calculate the cosine similarity of the extracted vector representations. The SNN was implemented using the deep learning framework deepTC (Horsmann and Zesch, 2018) in conjunction with Keras6 and Tensorflow (Abadi et al., 2016). In order to ensure full reproducibility of our re"
S18-2026,W14-2617,0,0.0249449,"of popularity, which differs significantly from our task: even if one agrees with a text, one might decide not to retweet or like it for any number of reasons. There are also cases in which one may retweet a post with which one disagrees in order to flag someone or something from the opposing community. Furthermore, there are effects such as the author’s followers affecting the visibility of posts and thereby the likelihood of a like or a retweet (Suh et al., 2010). In addition, we relate to works that aim at predicting whether two texts (Menini and Tonelli, 2016) or sequences of utterances (Wang and Cardie, 2014; Celli et al., 2016) express agreement or disagreement with each other. More broadly, we also relate to works that analyze stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016), or arguments (Habernal and ˇ Gurevych, 2016; Boltuzic and Snajder, 2016; BarHaim et al., 2017) that are expressed via text. In contrast to these works, we do not examine what judgment, sentiment, or claim is expressed by a text, but whether we can infer agreement or disagreement based on judgments which were made on other assertions. Finally, we"
S18-2026,P16-2032,0,0.0248283,"also have a strong judgment similarity because of underlying socio-cultural, political, or personal factors. For instance, the assertions ‘Consuming Marijuana has no impact on your success at work’ and ‘Marijuana is not addictive’ describe different arguments for legalizing marijuana, but judgments made on these assertions are often correlated. Our work also relates to other attempts on predicting reactions to text, such as predicting the number of retweets (Suh et al., 2010; Petrovic et al., 2011), the number of likes on tweets (Tan et al., 2014), the number of karma points of reddit posts (Wei et al., 2016), or sales from product descriptions (Pryzant et al., 2017). What those dicted using a Siamese neural network, which outperforms all other approaches by a wide margin. 2 Related Work Predicting Judgments In order to predict if someone will agree with an assertion, we need knowledge about that person. Ideally, we would have access to a large set of other assertions which the person has already judged. We could then measure the similarity between previous assertions and the new assertion and hypothesize that the judgment on the new assertion should be the same as for a highly similar one. In Fig"
S18-2026,L18-1224,1,0.842854,"they wanted. On average each assertion is judged by about 45 persons and each participant judged over 400 assertions. For each person, agreement is encoded with 1, disagreement with −1, and missing values with 0 (as not all subjects judged all assertions). Additionally, we can also compute the aggregated agreement score for each assertion by simply subtracting the percentage of participants that disagreed with the assertion from the Data Collection For exploring how well the two tasks can be solved automatically, we use the dataset Nuanced Assertions on Controversial Issues (NAoCI) created by Wojatzki et al. (2018). The dataset contains assertions judged on a wide range of controversial issues.2 The NAoCI dataset mimics a common situation in many social media sites, where people e.g. up- or downvote social media posts. However, it does not have the experimental problems 2 The dataset is accessible from https://sites. google.com/view/you-on-issues/ 216 Issue 135 142 129 150 130 145 138 134 133 154 148 175 138 130 128 134 6 154 6 473 5 747 6 866 5 969 6 423 6 200 5 962 5 877 6 940 6 899 7 695 6 280 5 950 5 806 5 892 2 243 101 133 5 As mentioned above, we want to predict judgments on a previously unseen as"
S18-2026,S14-2009,0,0.028532,"icates that the participants more often agree with the assertions than they disagree. Consequently, baselines accounting for this imbalance perform strongly in predicting judgments on assertions. However, the distribution corresponds to what we observe in many social network sites, where e.g. the ratio of likes to dislikes is also clearly skewed towards likes. All data, the used questionnaires along with the directions and examples are publicly available on the project website.2 3 version 2.2.0 version 1.0 5 The NRC-Canada system ranked first in the SemEval 2013 (Nakov et al., 2013) and 2014 (Rosenthal et al., 2014) tasks on sentiment analysis. 4 217 regression model. Iterative experiments showed that this configuration gave the most stable results across the issues. For the neural approach, we adapt Siamese neural networks (SNN), which consist of two identical branches or sub-networks that try to extract useful representations of the assertions and a final layer that merges these branches. SNNs have been successfully used to predict text similarity (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) and match pairs of sentences (e.g. a tweet to reply) (Hu et al., 2014). In our SNN, a branch consists"
W04-0839,W02-1006,0,0.0787142,"Missing"
W04-0839,J92-1001,0,0.128176,"Missing"
W04-0839,W04-2404,1,0.635174,"Missing"
W04-0839,P96-1006,0,0.270991,"Missing"
W04-0839,N01-1011,1,0.926133,"Missing"
W04-0839,S01-1034,1,0.85835,"Missing"
W04-0839,W02-0806,1,0.903012,"Missing"
W04-0839,J01-3001,0,0.14163,"Missing"
W04-2404,P94-1020,0,0.075095,"Missing"
W04-2404,J98-1006,0,0.0653756,"Missing"
W04-2404,H93-1051,0,0.0642397,"Missing"
W04-2404,W02-1006,0,0.0554747,"etter by letter, and a period of time. The intended sense, a charm or incantation, can be identified based on the context, which in this case includes bewitching and a reference to a famous young wizard. Word sense disambiguation is often approached by supervised learning techniques. The training data consists of sentences which have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundan"
W04-2404,P97-1009,0,0.0647713,"Missing"
W04-2404,J92-1001,0,0.14749,"ich have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble. We find that simple combinations of lexical and syntactic features can result in very high disam"
W04-2404,P96-1006,0,0.060125,"ial target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble. We find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy,"
W04-2404,N01-1011,1,0.94804,"ntation, to read out letter by letter, and a period of time. The intended sense, a charm or incantation, can be identified based on the context, which in this case includes bewitching and a reference to a famous young wizard. Word sense disambiguation is often approached by supervised learning techniques. The training data consists of sentences which have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments"
W04-2404,S01-1034,1,0.905324,"ntation, to read out letter by letter, and a period of time. The intended sense, a charm or incantation, can be identified based on the context, which in this case includes bewitching and a reference to a famous young wizard. Word sense disambiguation is often approached by supervised learning techniques. The training data consists of sentences which have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments"
W04-2404,W02-0806,1,0.911048,"Missing"
W04-2404,J01-3001,0,0.0867841,"ged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble. We find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of exper"
W04-2404,P95-1026,0,0.444294,"Missing"
W06-1605,E06-1016,1,0.904743,"concepts or senses (Yarowsky, 1992). Words with more than one sense are listed in more than one category. A published thesaurus thus provides us with a very coarse human-developed set or inventory of word senses or concepts2 that are more intuitive and discernible than the “concepts” generated by dimensionality-reduction methods such as latent semantic analysis. Using coarse senses from a known inventory means that the senses can be represented unambiguously by a large number of possibly ambiguous words (conveniently available in the thesaurus)—a feature that we exploited in our earlier work (Mohammad and Hirst, 2006) to determine useful estimates of the strength of association between a concept and co-occurring words. In this paper, we go one step further and use the idea of a very coarse sense inventory to develop a framework for distributional measures of concepts that can more naturally and more accurately be used in place of semantic measures of word senses. We use the Macquarie Thesaurus (Bernard, 1986) as a sense inventory and repository of words pertaining to each sense. It has 812 categories with around 176,000 word tokens and 98,000 word types. This allows us to have much smaller concept–concept"
W06-1605,P05-1016,0,0.010195,"cosine of the an41 gle between their aggregate vectors. However, as we pointed out in Mohammad and Hirst (2005), such aggregate co-occurrence vectors are expected to be noisy because they are created from data that is not sense-annotated. Therefore, we employed simple word sense disambiguation and bootstrapping techniques on our base WCCM to create more-accurate co-occurrence vectors, which gave markedly higher accuracies in the task of determining word sense dominance. In the experiments described in this paper, we used these bootstrapped co-occurrence vectors to determine concept-distance. Pantel (2005) also provides a way to create co-occurrence vectors for WordNet senses. The lexical co-occurrence vectors of words in a leaf node are propagated up the WordNet hierarchy. A parent node inherits those co-occurrences that are shared by its children. Lastly, co-occurrences not pertaining to the leaf nodes are removed from its vector. Even though the methodology attempts at associating a WordNet node or sense with only those co-occurrences that pertain to it, no attempt is made at correcting the frequency counts. After all, word1–word2 co-occurrence frequency (or association) is likely not the sa"
W06-1605,W06-2501,0,0.0499162,"Moreover, if we consider correction ratio to be the bottom-line statistic, then the Distribconcept measures outperform all WNetconcept measures except the Jiang–Conrath measure. If we consider correction performance to be the bottom-line statistic, then again we see that the distributional concept-distance measures outperform the worddistance measures, except in the case of Linpmi , which gives slightly poorer results with conceptdistance. Also, in contrast to correction ratio values, using the Leacock–Chodorow measure results in relatively higher correction performance values 6 Related Work Patwardhan and Pedersen (2006) create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss. The distance between two senses is then determined by the cosine of the an41 gle between their aggregate vectors. However, as we pointed out in Mohammad and Hirst (2005), such aggregate co-occurrence vectors are expected to be noisy because they are created from data that is not sense-annotated. Therefore, we employed simple word sense disambiguation and bootstrapping techniques on our base WCCM to create more-accurate co-occurrence vectors, which gave markedly hig"
W06-1605,J90-1003,0,0.271448,"a semantic network, and hence they measure the distance between the concepts or word-senses that the nodes of the resource represent. Examples include the measure for MeSH proposed by Rada et al. (1989) and those for WordNet proposed by Leacock and Chodorow (1998) and Jiang and Conrath (1997). (Some of the more successful measures, such as Jiang–Conrath, also use information content derived from word frequency.) Typically, these measures rely on an extensive hierarchy of hyponymy relationships for nouns. Therefore, these measures 1 In our experiments, we set negative PMI values to 0, because Church and Hanks (1990), in their seminal paper on word association ratio, show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus. 35 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 35–43, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 The distributional hypothesis and its limitations Table 1: Measures of DP distance and measures of strength of association. DP distance α-skew divergence cosine Jensen–Shannon divergence Lin The distributional hypothesis (Firth, 1"
W06-1605,J05-2004,0,0.0234816,":: .. . A cell mi j , corresponding to word wi and category c j , contains the number of times wi co-occurs (in a window of 5 words in the corpus) with any of the words listed under category c j in the thesaurus. Intuitively, the cell mi j captures the number of times c j and wi co-occur. A contingency table for a single word and single category can be created by simply collapsing all other rows and columns into one and summing their frequencies. Applying a suitable statistic, such as odds 2 We use the terms senses and concepts interchangeably. This is in contrast to studies, such as that of Cooper (2005), that attempt to make a principled distinction between them. 37 BNC BNC Thesaurus word–word co-occurrence counting word–category co-occurrence counting word–word co-occurrence matrix word–category co-occurrence matrix bootstrapping and sense disambiguation distributional measures distributional measures distributional relatedness of words distributional relatedness of concepts Figure 1: Distributional word-distance. Figure 2: Distributional concept-distance. ratio, on the contingency table gives the strength of association between a concept (category) and co-occurring word. Therefore, the WCC"
W06-1605,C92-2070,0,0.766966,"Missing"
W06-1605,O97-1002,0,0.434151,"nt–theme) relation with the target word. We will refer to the former kind of DPs as relation-free. Usually in 1 Semantic and distributional measures Measures of distance of meaning are of two kinds. The first kind, which we will refer to as semantic measures, rely on the structure of a resource such as WordNet or, in some cases, a semantic network, and hence they measure the distance between the concepts or word-senses that the nodes of the resource represent. Examples include the measure for MeSH proposed by Rada et al. (1989) and those for WordNet proposed by Leacock and Chodorow (1998) and Jiang and Conrath (1997). (Some of the more successful measures, such as Jiang–Conrath, also use information content derived from word frequency.) Typically, these measures rely on an extensive hierarchy of hyponymy relationships for nouns. Therefore, these measures 1 In our experiments, we set negative PMI values to 0, because Church and Hanks (1990), in their seminal paper on word association ratio, show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus. 35 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processin"
W06-1605,J06-1003,1,\N,Missing
W06-1605,P98-2127,0,\N,Missing
W06-1605,C98-2122,0,\N,Missing
W10-0204,H05-1073,0,0.0923319,"t, disgust, surprise, and anticipation— argued by many to be the basic and prototypical emotions (Plutchik, 1980). Complex emotions can be viewed as combinations of these basic emotions. community. Work in emotion detection can be roughly classified into that which looks for specific emotion denoting words (Elliott, 1992), that which determines tendency of terms to co-occur with seed words whose emotions are known (Read, 2004), that which uses hand-coded rules (Neviarouskaya et al., 2009), and that which uses machine learning and a number of emotion features, including emotion denoting words (Alm et al., 2005). Much of this recent work focuses on six emotions studied by Ekman (1992). These emotions— joy, sadness, anger, fear, disgust, and surprise— are a subset of the eight proposed in Plutchik (1980). We focus on the Plutchik emotions because the emotions can be naturally paired into opposites—joy– sadness, anger–fear, trust–disgust, and anticipation– surprise. Natural symmetry apart, we believe that prior work on automatically computing word–pair antonymy (Lin et al., 2003; Mohammad et al., 2008; Lobanova et al., 2010) can now be leveraged in automatic emotion detection. 2 Related work Words used"
W10-0204,D09-1030,0,0.011819,"As of now, high-quality high-coverage emotion lexicons do not exist for any language, although there are a few limited-coverage lexicons for a handful of languages, for example, the WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) for six basic emotions and the General Inquirer (GI) (Stone et al., 1966), which categorizes words into a number of categories, including positive and negative semantic orientation. Amazon has an online service called Mechanical Turk that can be used to obtain a large amount of human annotation in an efficient and inexpensive manner (Snow et al., 2008; Callison-Burch, 2009).1 However, one must define the task carefully to obtain annotations of high quality. Several checks must be placed to ensure that random and erroneous annotations are discouraged, rejected, and re-annotated. In this paper, we show how we compiled a moderate-sized English emotion lexicon by manual 1 https://www.mturk.com/mturk/welcome Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 26–34, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics annotation through Amazon’s Mechanical Turk ser"
W10-0204,D08-1103,1,0.224279,"Missing"
W10-0204,D09-1063,1,0.570517,"Missing"
W10-0204,D08-1027,0,0.0337472,"Missing"
W10-0204,strapparava-valitutti-2004-wordnet,0,0.898699,"simply the sum of emotions conveyed by the words in it, but the emotion lexicon will be a useful component for any sophisticated emotion detecting algorithm. The lexicon will also be useful for evaluating automatic methods that identify the emotions evoked by a word. Such algorithms may then be used to automatically generate emotion lexicons in languages where no such lexicons exist. As of now, high-quality high-coverage emotion lexicons do not exist for any language, although there are a few limited-coverage lexicons for a handful of languages, for example, the WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) for six basic emotions and the General Inquirer (GI) (Stone et al., 1966), which categorizes words into a number of categories, including positive and negative semantic orientation. Amazon has an online service called Mechanical Turk that can be used to obtain a large amount of human annotation in an efficient and inexpensive manner (Snow et al., 2008; Callison-Burch, 2009).1 However, one must define the task carefully to obtain annotations of high quality. Several checks must be placed to ensure that random and erroneous annotations are discouraged, rejected, and re-annotated. In this paper,"
W11-0611,D09-1030,0,0.0227406,"wered by the same person. We requested annotations from five different Turkers for each HIT. (A Turker cannot attempt multiple assignments for the same term.) A complete HIT is shown below: Q1. Which word is closest in meaning to sleep? • car 3 Crowdsourcing 7 • nap • olive Q2. What colour is associated with sleep? Amazon’s Mechanical Turk (AMT) is an online crowdsourcing platform that is especially well suited for tasks that can be done over the Internet through a computer or a mobile device.7 It is already being used to obtain human annotation on various linguistic tasks (Snow et al., 2008; Callison-Burch, 2009). However, one must define the task carefully to obtain annotations of high quality. Several checks must be placed to ensure that random and erroneous annotations are discouraged, rejected, and re-annotated. We used Mechanical Turk to obtain word–colour association annotations on a large-scale. Each task is broken into small independently solvable units called HITs (Human Intelligence Tasks) and uploaded on the Mechanical Turk website. The people who provide responses to these HITs are called Turkers. The annotation provided by a Turker for a HIT is called an assignment. We used the Macquarie"
W11-0611,O97-1002,0,0.256276,"Missing"
W11-0611,P97-1009,0,0.045823,"Missing"
W11-0611,W10-0204,1,0.775432,"ncy to have a colour association, then we would see most of the points along the diagonal moving up from left to right. Instead, we observe that the strongly associated categories (points in the shaded region) are spread across the imageability axis, implying that there is only weak, if any, correlation between imageability and strength of association with colour. Imageability and colour association have a Pearson’s product moment correlation of 0.116, and a Spearman rank order correlation of 0.102. 6 Emotions such as joy and anger are abstract concepts dealing with one’s psychological state. Mohammad and Turney (2010) created a crowdsourced term– emotion association lexicon consisting of associations of over 10,000 word-sense pairs with eight emotions—joy, sadness, anger, fear, trust, disgust, surprise, and anticipation—argued to be the basic and prototypical emotions (Plutchik, 1980). We combine their term–emotion association lexicon and our term–colour lexicon to determine the colour signature of different emotions—the rows in Table 3. The top two most frequently associated colours with each of the eight emotions are shown in bold. For example, the “anger” row shows the percentage of Imageability and Col"
W11-0611,D09-1063,1,0.645204,"Missing"
W11-0611,P11-2064,1,0.775634,"s.1 1 Introduction Colour is a vital component in the successful delivery of information, whether it is in marketing a commercial product (Sable and Akcay, 2010), designing webpages (Meier, 1988; Pribadi et al., 1990), or visualizing information (Christ, 1975; Card et al., 1999). Since real-world concepts have associations with certain colour categories (for example, danger with red, and softness with pink), complimenting linguistic and non-linguistic information with appropriate colours has a number of benefits, including: 1 This paper is an extended, non-archival, version of the short paper—Mohammad (2011). It provides additional details on the analysis of crowdsourced data, and experiments on the manifestations of word–colour associations in WordNet and in text. It also proposes a polarity-based automatic method. (1) strengthening the message (improving semantic coherence), (2) easing cognitive load on the receiver, (3) conveying the message quickly, and (4) evoking the desired emotional response. Consider, for example, the use of red in stop signs. Drivers are able to recognize the sign faster, and it evokes a subliminal emotion pertaining to danger, which is entirely appropriate in the conte"
W11-0611,N04-3012,0,0.0857149,"Missing"
W11-0611,D08-1027,0,0.0298867,"Missing"
W11-0611,W10-3405,0,0.0958704,"other academic work that gathered large word–colour associations. There is, however, a commercial endeavor—Cymbolism3 . Child et al. (1968), Ou et al. (2011), and others show that people of different ages and genders have different colour preferences. (See also the online study by Joe Hallock4 .) In this work, we are interested in identifying words that have a strong association with a colour due to their meaning; associations that are not affected by age and gender preferences. There is substantial work on inferring the emotions evoked by colour (Luscher, 1969; Xin et al., 2004; Kaya, 2004). Strapparava and Ozbal (2010) compute corpus-based semantic similarity between emotions and colours. We combine the word–colour and word–emotion association lexicons to determine the correlation between emotion-associated words and colours. Berlin and Kay (1969), and later Kay and Maffi (1999), showed that often colour terms appeared in languages in certain groups. If a language has only two colour terms, then they are white and black. If a language has three colour terms, then they are white, black, and red. If a language has four colour terms, then they are white, black, red, and green, and so on up to eleven colours. F"
W11-0611,C92-2070,0,0.0336256,"ntelligence Tasks) and uploaded on the Mechanical Turk website. The people who provide responses to these HITs are called Turkers. The annotation provided by a Turker for a HIT is called an assignment. We used the Macquarie Thesaurus (Bernard, 1986) as the source for terms to be annotated. Thesauri, such as the Roget’s and Macquarie, group related words into categories. The Macquarie has about a thousand categories, each having about a hundred or so related terms. Each category has a head word that best represents the words in it. The categories can be thought of as coarse senses or concepts (Yarowsky, 1992). If a word is ambiguous, then it is listed in more than one category. Since a word may have different colour associations when used in different senses, we obtained annotations at word-sense level. We chose to annotate words that 6 • tree http://www.psy.uwa.edu.au/mrcdatabase/uwa mrc.htm Mechanical Turk: www.mturk.com 99 • black • blue • brown • green • grey • orange • purple • pink • red • white • yellow Q1 is a word-choice question generated automatically by taking a near-synonym from the thesaurus and random distractors. The near-synonym also guides the annotator to the desired sense of th"
W11-1514,H05-1073,0,0.273876,"rovide users with relevant plots presented in this paper, as well as ability to search for text snippets from multiple texts that have high emotion word densities. 2 Related work Over the last decade, there has been considerable work in sentiment analysis, especially in determining whether a term has a positive or negative polarity (Lehrer, 1974; Turney and Littman, 2003; Mohammad et al., 2009). There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). 106 Automatic analysis of emotions in text has so far had to rely on small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) has a few hundred words annotated with associations to a number of affect categories including the six Ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 General Inqui"
W11-1514,W10-0201,0,0.0344776,"nberg. Given a search query, the goal is to provide users with relevant plots presented in this paper, as well as ability to search for text snippets from multiple texts that have high emotion word densities. 2 Related work Over the last decade, there has been considerable work in sentiment analysis, especially in determining whether a term has a positive or negative polarity (Lehrer, 1974; Turney and Littman, 2003; Mohammad et al., 2009). There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). 106 Automatic analysis of emotions in text has so far had to rely on small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) has a few hundred words annotated with associations to a number of affect categories including the six Ekman emotions (joy, sadness, anger,"
W11-1514,lobo-de-matos-2010-fairy,0,0.037947,"Missing"
W11-1514,W10-0204,1,0.926588,"rch query, the goal is to provide users with relevant plots presented in this paper, as well as ability to search for text snippets from multiple texts that have high emotion word densities. 2 Related work Over the last decade, there has been considerable work in sentiment analysis, especially in determining whether a term has a positive or negative polarity (Lehrer, 1974; Turney and Littman, 2003; Mohammad et al., 2009). There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). 106 Automatic analysis of emotions in text has so far had to rely on small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) has a few hundred words annotated with associations to a number of affect categories including the six Ekman emotions (joy, sadness, anger, fear, disgust, and surpris"
W11-1514,W11-1709,1,0.575433,"Missing"
W11-1514,D09-1063,1,0.789921,"tion, which are argued to be the eight basic and prototypical emotions (Plutchik, 1980). This work is part of a broader project to provide an affect-based interface to Project Gutenberg. Given a search query, the goal is to provide users with relevant plots presented in this paper, as well as ability to search for text snippets from multiple texts that have high emotion word densities. 2 Related work Over the last decade, there has been considerable work in sentiment analysis, especially in determining whether a term has a positive or negative polarity (Lehrer, 1974; Turney and Littman, 2003; Mohammad et al., 2009). There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). 106 Automatic analysis of emotions in text has so far had to rely on small emotion lexicons. The WordNet Affect Lexicon (WAL)"
W11-1514,P11-2064,1,0.841378,"emotion words in Shakespeare’s famous tragedy, Hamlet, and his comedy, As you like it, respectively. Figure 3 conveys the difference between the two novels even more explicitly by showing only the difference in percentage scores for each of the emotions. EmoFigure 6: Timeline of the emotions in As You Like It. Figure 4: Hamlet - As You Like It: relative-salience word cloud for trust words. Figure 7: Timeline of the emotions in Hamlet. Figure 5: Hamlet - As You Like It: relative-salience word cloud for sadness words. tions are represented by colours as per a study on word–colour associations (Mohammad, 2011). Observe how one can clearly see that Hamlet has more fear, sadness, disgust, and anger, and less joy, trust, and anticipation. The bar graph is effective at conveying the extent to which an emotion is more prominent in one text than another, but it does not convey the source of these emotions. Therefore, we calculate the relative salience of an emotion word w across two target texts T1 and T2 : RelativeSalience(w|T1 , T2 ) = f1 f2 − N1 N2 Figure 8: Timeline of the emotions in Frankenstein. 4.2 Flow of Emotions Literary researchers as well as casual readers may be interested in noting how the"
W11-1514,strapparava-valitutti-2004-wordnet,0,0.0841234,"There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). 106 Automatic analysis of emotions in text has so far had to rely on small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) has a few hundred words annotated with associations to a number of affect categories including the six Ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 General Inquirer (GI) (Stone et al., 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negative polarity.4 We use the NRC Emotion Lexicon (Mohammad and Yang, 2011; Mohammad and Turney, 2010), a large set of human-provided word–emotion association ratings, in our experiments.5 Empirical assessment of emotions in literary texts has sometimes relied on human annotation of the texts, but thi"
W11-1514,C92-2070,0,0.0160134,"es in the Google ngram corpus were annotated for version 0.92 of the lexicon which we use for the experiments described in this paper.7 The Roget’s Thesaurus groups related words into about a thousand categories, which can be thought of 3 WAL: http://wndomains.fbk.eu/wnaffect.html GI: http://www.wjh.harvard.edu/∼inquirer 5 Please send an e-mail to saif.mohammad@nrc-cnrc.gc.ca to obtain the latest version of the NRC Emotion Lexicon. 6 Roget’s Thesaurus: www.gutenberg.org/ebooks/10681 7 The Google N-gram Corpus is available through the Linguistic Data Consortium. 4 as coarse senses or concepts (Yarowsky, 1992). If a word is ambiguous, then it is listed in more than one category. Since a word may have different emotion associations when used in different senses, wordsense level annotations were obtained by first asking an automatically generated word-choice question pertaining to the target: Q1. Which word is closest in meaning to shark (target)? • car • tree • fish • olive Figure 1: Emotions pie chart of Shakespeare’s tragedy Hamlet. (Text from Project Gutenberg.) The near-synonym for Q1 is taken from the thesaurus, and the distractors are randomly chosen words. This question guides the annotator t"
W11-1709,H05-1073,0,0.0607063,"es words with joy, sadness, anger, fear, trust, disgust, surprise, anticipation, which are argued to be the eight basic and prototypical emotions (Plutchik, 1980). 2 Related work Over the last decade, there has been considerable work in sentiment analysis, especially in determining whether a term has a positive or negative polarity (Lehrer, 1974; Turney and Littman, 2003; Mohammad et al., 2009). There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). Automatically analyzing affect in emails has primarily been done for automatic gender identification (Cheng et al., 2009; Corney et al., 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) has a few hundred words anno71"
W11-1709,W10-0201,0,0.147759,", and no polarity (neutral). It also associates words with joy, sadness, anger, fear, trust, disgust, surprise, anticipation, which are argued to be the eight basic and prototypical emotions (Plutchik, 1980). 2 Related work Over the last decade, there has been considerable work in sentiment analysis, especially in determining whether a term has a positive or negative polarity (Lehrer, 1974; Turney and Littman, 2003; Mohammad et al., 2009). There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). Automatically analyzing affect in emails has primarily been done for automatic gender identification (Cheng et al., 2009; Corney et al., 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Val"
W11-1709,W09-1323,0,0.038146,"is hard to be objective of one’s own emotional state. Letters have long been a channel to convey emotions, explicitly and implicitly, and now with the widespread usage of email, people have access to unprecedented amounts of text that they themselves have written and received. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to track emotions in letters and emails. Automatic analysis and tracking of emotions in emails has a number of benefits including: 1. Determining risk of repeat attempts by analyzing suicide notes (Osgood and Walker, 1959; Matykiewicz et al., 2009; Pestian et al., 2008).1 2. Understanding how genders communicate through work-place and personal email (Boneva et al., 2001). 3. Tracking emotions towards people and entities, over time. For example, did a certain managerial course bring about a measurable change in one’s inter-personal communication? 4. Determining if there is a correlation between the emotional content of letters and changes in a person’s social, economic, or physiological state. Sudden and persistent changes in the amount of emotion words in mail may be a sign of psychological disorder. 5. Enabling affect-based search. Fo"
W11-1709,W10-0204,1,0.925158,"(neutral). It also associates words with joy, sadness, anger, fear, trust, disgust, surprise, anticipation, which are argued to be the eight basic and prototypical emotions (Plutchik, 1980). 2 Related work Over the last decade, there has been considerable work in sentiment analysis, especially in determining whether a term has a positive or negative polarity (Lehrer, 1974; Turney and Littman, 2003; Mohammad et al., 2009). There is also work in more sophisticated aspects of sentiment, for example, in detecting emotions such as anger, joy, sadness, fear, surprise, and disgust (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). Automatically analyzing affect in emails has primarily been done for automatic gender identification (Cheng et al., 2009; Corney et al., 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) has a few hun"
W11-1709,D09-1063,1,0.54824,"Missing"
W11-1709,P11-2064,1,0.88423,"Missing"
W11-1709,W11-1514,1,0.577226,"Missing"
W11-1709,W08-0616,0,0.0328794,"f one’s own emotional state. Letters have long been a channel to convey emotions, explicitly and implicitly, and now with the widespread usage of email, people have access to unprecedented amounts of text that they themselves have written and received. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to track emotions in letters and emails. Automatic analysis and tracking of emotions in emails has a number of benefits including: 1. Determining risk of repeat attempts by analyzing suicide notes (Osgood and Walker, 1959; Matykiewicz et al., 2009; Pestian et al., 2008).1 2. Understanding how genders communicate through work-place and personal email (Boneva et al., 2001). 3. Tracking emotions towards people and entities, over time. For example, did a certain managerial course bring about a measurable change in one’s inter-personal communication? 4. Determining if there is a correlation between the emotional content of letters and changes in a person’s social, economic, or physiological state. Sudden and persistent changes in the amount of emotion words in mail may be a sign of psychological disorder. 5. Enabling affect-based search. For example, efforts to i"
W11-1709,strapparava-valitutti-2004-wordnet,0,0.511084,"t (Bellegarda, 2010; Mohammad and Turney, 2010; Alm et al., 2005; Alm et al., 2005). The technology is still developing and it can be unpredictable when dealing with short sentences, but it has been shown to be reliable when drawing conclusions from large amounts of text (Dodds and Danforth, 2010; Pang and Lee, 2008). Automatically analyzing affect in emails has primarily been done for automatic gender identification (Cheng et al., 2009; Corney et al., 2002), but it has relied on mostly on surface features such as exclamations and very small emotion lexicons. The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004) has a few hundred words anno71 tated with associations to a number of affect categories including the six Ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 General Inquirer (GI) (Stone et al., 1966) has 11,788 words labeled with 182 categories of word tags, including positive and negative polarity.4 Affective Norms for English Words (ANEW) has pleasure (happy– unhappy), arousal (excited–calm), and dominance (controlled–in control) ratings for 1034 words.5 Mohammad and Turney (2010) compiled emotion annotations for about 4000 words with eight emotions (six of Ekman, trust, an"
W11-1709,C92-2070,0,0.0246942,"6 We follow the method outlined in Mohammad and Turney (2010). Unlike Mohammad and Turney, who used the Macquarie Thesaurus (Bernard, 1986), we use the Roget Thesaurus as the source for target terms.7 Since the 1911 US edition of Roget’s is available freely in the public domain, it allows us to distribute our emotion lexicon without the burden of restrictive licenses. We annotated only those words that occurred more than 120,000 times in the Google n-gram corpus.8 The Roget’s Thesaurus groups related words into about a thousand categories, which can be thought of as coarse senses or concepts (Yarowsky, 1992). If a word is ambiguous, then it is listed in more than one category. Since a word may have different emotion associations when used in different senses, we obtained annotations at word-sense level by first asking an automatically generated word-choice question pertaining to the target: Q1. Which word is closest in meaning to shark (target)? • car • tree • fish • olive The near-synonym is taken from the thesaurus, and the distractors are randomly chosen words. This 3 WAL: http://wndomains.fbk.eu/wnaffect.html GI: http://www.wjh.harvard.edu/∼inquirer 5 ANEW: http://csea.phhp.ufl.edu/media/anew"
W14-0901,W11-0705,0,0.0400004,"Missing"
W14-0901,W12-3704,0,0.0262451,"Missing"
W14-0901,W10-0201,0,0.0226197,"Missing"
W14-0901,W10-0204,1,0.853491,"Missing"
W14-0901,W11-1709,1,0.881205,"Missing"
W14-0901,pak-paroubek-2010-twitter,0,0.030094,"Missing"
W14-0901,strapparava-valitutti-2004-wordnet,0,0.0481285,"Missing"
W14-0901,W10-0212,0,0.0700493,"Missing"
W14-0901,D11-1052,0,\N,Missing
W14-2607,P98-1013,0,0.207863,"compile a large dataset of tweets pertaining to the 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30. 1 Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco a"
W14-2607,J02-3001,0,0.0626632,"d ‘towards whom is the emotion directed (the stimulus)?’. We automatically compile a large dataset of tweets pertaining to the 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30. 1 Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda"
W14-2607,W10-0201,0,0.00979264,"sky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions su"
W14-2607,D11-1052,0,0.0233667,"results that are greater than random and major5.1 Detecting emotional state 5.1.1 Features We included the following features for detecting emotional state in tweets. Word n-grams: We included unigrams (single words) and bigrams (two-word sequences) into our feature set. All words were stemmed with Porter’s stemmer (Porter, 1980). Punctuations: number of contiguous sequences of exclamation marks, question marks, or a combination of them. Elongated words: the number of words with the final character repeated 3 or more times (soooo, mannnnnn, etc). (Elongated words have been used similarly in (Brody and Diakopoulos, 2011).) Emoticons: presence/absence of positive and negative emoticons. The emoticon and its polarity were determined through a regular expression adopted from Christopher Potts’ tokenizing script.4 Emotion Lexicons: We used the NRC word– emotion association lexicon (Mohammad and Turney, 2010) to check if a tweet contains emotional words. The lexicon contains human annotations of emotion associations for about 14,200 word types. The annotation includes whether a word is positive or negative (sentiments), and whether it is associated with the eight basic emotions (joy, sadness, anger, fear, surprise"
W14-2607,W06-0301,0,0.0126049,"s of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 3 Challenges of Semantic Role Labeling of Emotions in Tweets Semantic role labeling of emotions in tweets poses certain unique challenges. Firstly, there are many differences between tweets and linguistically wellformed texts, such as written news (Liu et al., 2012; Ritter et al., 2011). Tweets are often less well-formed—they tend to be colloquial, have misspellings, and have non-standard tokens. Thus, methods depending heavily on deep language understanding such as syntactic parsing (Kim and Hovy, 2006) are less reliable. 2 http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html 33 Secondly, in a traditional SRL system, an argument frame is a cohesive structure with strong dependencies between the arguments. Thus it is often beneficial to develop joint models to identify the various elements of a frame (Toutanova et al., 2005). However, these assumptions are less viable when dealing with emotions in tweets. For example, there is no reason to believe that people with a certain name will have the same emotions towards the same entities. On the other hand, if w"
W14-2607,H05-1043,0,0.0274172,"is not covered by more specific frame elements. The Parameter is a domain in which the Experiencer experiences the Stimulus. The Reason is the explanation for why the Stimulus evokes a certain emotional response. Related Work in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use many of the ideas developed in the sentiment analysis work and apply them to detect the stimulus of emotions in the electoral tweets data. Our work here is also related to template filling in information extraction (IE), for example as defined in MUC (Grishman, 1997), which extracts information (entities) from a"
W14-2607,E12-1049,0,0.0410302,"s politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, We automatically compile a large dataset of 2012 US presidential elections using a small number of hand-chosen hashtags. Next we annotate the tweets for Experiencer, State, and Stimulus by crowdsourcing to Amazon’s Mechanical Turk.1 We analyze the annotations to determine"
W14-2607,D11-1141,0,0.0843212,"Missing"
W14-2607,strapparava-valitutti-2004-wordnet,0,0.108711,"ion, target, and other information about an event. Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a"
W14-2607,C08-1111,0,0.0446536,"Missing"
W14-2607,P05-1073,0,0.0565337,"Missing"
W14-2607,C10-2167,0,0.0477533,"Missing"
W14-2607,W12-2104,0,0.0388205,"eason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, We automatically compile a large dataset of 2012 US presidential el"
W14-2607,J08-2001,0,0.0260615,"Missing"
W14-2607,W10-0204,1,0.807595,"re-defined template, such as the date, location, target, and other information about an event. Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for pol"
W14-2607,S13-2053,1,0.30783,"Missing"
W14-2607,N12-1071,1,0.948151,"roducts (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, We automatically compile a large dataset of 2012 US presidential elections using a small number of hand-chosen hashtags. Next we annotate the tweets for Experiencer, State, and Stimulus by crowdsourcing to Amazon’s Mechanical Turk.1 We analyze the annotations to determine the distribution"
W14-2607,S12-1033,1,0.757839,"Missing"
W14-2607,H05-2017,0,\N,Missing
W14-2607,C98-1013,0,\N,Missing
W15-2912,J91-1002,0,0.159463,"elated words and expressions. These categories can be thought of as coarse concepts. Widely used by writers, lexicographers, students, and the lay person, the thesaurus is most commonly accessed to identify a word or phrase that best captures what one wants to communicate. Researchers in many fields find use for the thesaurus, for example those exploring literary, social science, psychological, and cognitive theories involving word usage. Not surprisingly, there is a vast and growing body of work in Computational Linguistics that makes use of the Roget’s Thesaurus, including Masterman (1957), Morris and Hirst (1991), Yarowsky (1992), Mohammad and Hirst (2006), Mohammad (2008), and Grefenstette (2012). However, despite its substantial range and scope of use, manual access to information in the thesaurus is often restricted to looking up a word and finding its neighbors. Existing online portals for the Roget’s Thesaurus present a very traditional, non-interactive, text-only interface.1 We present an online interactive visualizer for the Roget’s Thesaurus, which we call Imagisaurus.2 Imagisaurus allows users to access information about words, classes, sections, and categories through four separate sub-visua"
W15-2912,strapparava-valitutti-2004-wordnet,0,0.0689379,"Tableau, help users better understand these lexical resources in terms of their make up as a whole.3 Both visualizers are made available online and are free to use.4 2 associations of thousands of words: The General Inquirer (GI) has sentiment labels for about 3,600 terms (Stone et al., 1966). Hu and Liu (2004) manually labeled about 6,800 words and used them for detecting sentiment of customer reviews. Affective Norms for English Words (ANEW) has pleasure (happy–unhappy), arousal (excited– calm), and dominance (controlled–in control) ratings for 1034 words.6 The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004, ) has a few hundred words annotated with associations to the six Ekman emotions. The NRC Emotion Lexicon has association labels for over 14,000 words with positive and negative sentiment, as well as the set of eight Plutchik emotions (Mohammad and Turney, 2010; Mohammad and Turney, 2013).7 These labels were compiled through crowdsourcing. Lexicons for word–affect associations are used in automatic classification systems as well as systems that track affectual words in text (for example in literary analysis and for assessing well-being in social media posts). We use the NRC Emotion Lexicon in"
W15-2912,C92-2070,0,0.0684281,"ons. These categories can be thought of as coarse concepts. Widely used by writers, lexicographers, students, and the lay person, the thesaurus is most commonly accessed to identify a word or phrase that best captures what one wants to communicate. Researchers in many fields find use for the thesaurus, for example those exploring literary, social science, psychological, and cognitive theories involving word usage. Not surprisingly, there is a vast and growing body of work in Computational Linguistics that makes use of the Roget’s Thesaurus, including Masterman (1957), Morris and Hirst (1991), Yarowsky (1992), Mohammad and Hirst (2006), Mohammad (2008), and Grefenstette (2012). However, despite its substantial range and scope of use, manual access to information in the thesaurus is often restricted to looking up a word and finding its neighbors. Existing online portals for the Roget’s Thesaurus present a very traditional, non-interactive, text-only interface.1 We present an online interactive visualizer for the Roget’s Thesaurus, which we call Imagisaurus.2 Imagisaurus allows users to access information about words, classes, sections, and categories through four separate sub-visualizations that ar"
W15-2912,W06-1605,1,0.762347,"ries can be thought of as coarse concepts. Widely used by writers, lexicographers, students, and the lay person, the thesaurus is most commonly accessed to identify a word or phrase that best captures what one wants to communicate. Researchers in many fields find use for the thesaurus, for example those exploring literary, social science, psychological, and cognitive theories involving word usage. Not surprisingly, there is a vast and growing body of work in Computational Linguistics that makes use of the Roget’s Thesaurus, including Masterman (1957), Morris and Hirst (1991), Yarowsky (1992), Mohammad and Hirst (2006), Mohammad (2008), and Grefenstette (2012). However, despite its substantial range and scope of use, manual access to information in the thesaurus is often restricted to looking up a word and finding its neighbors. Existing online portals for the Roget’s Thesaurus present a very traditional, non-interactive, text-only interface.1 We present an online interactive visualizer for the Roget’s Thesaurus, which we call Imagisaurus.2 Imagisaurus allows users to access information about words, classes, sections, and categories through four separate sub-visualizations that are linked to each other. Cli"
W15-2912,W10-0204,1,0.811454,"erms (Stone et al., 1966). Hu and Liu (2004) manually labeled about 6,800 words and used them for detecting sentiment of customer reviews. Affective Norms for English Words (ANEW) has pleasure (happy–unhappy), arousal (excited– calm), and dominance (controlled–in control) ratings for 1034 words.6 The WordNet Affect Lexicon (WAL) (Strapparava and Valitutti, 2004, ) has a few hundred words annotated with associations to the six Ekman emotions. The NRC Emotion Lexicon has association labels for over 14,000 words with positive and negative sentiment, as well as the set of eight Plutchik emotions (Mohammad and Turney, 2010; Mohammad and Turney, 2013).7 These labels were compiled through crowdsourcing. Lexicons for word–affect associations are used in automatic classification systems as well as systems that track affectual words in text (for example in literary analysis and for assessing well-being in social media posts). We use the NRC Emotion Lexicon in Imagisaurus because of its large coverage and associations with both sentiment and emotions. However, other affect lexicons can also be plugged into the same visualization design. Affect Associations Many words such as good and delighted express affectual state"
W15-2912,W11-1709,1,0.821161,"ore the NRC Emotion Lexicon. Both visualizers are made freely available online. This work explores the Roget’s Thesaurus and the NRC Emotion Lexicon, but the same framework can be used to explore other lexical resources too: for example, other thesauri in English and other languages; semantic networks such as WordNet and VerbNet; versions of the NRC Emotion Lexicon in other languages; and sentiment lexicons such as the NRC Hashtag Sentiment lexicon and Sentiment 140 Lexicon (Mohammad et al., 2013; Kiritchenko et al., 2014).8 Our future work will extend previous work on visualizing literature (Mohammad and Yang, 2011; Mohammad, 2012) by incorporating interactivity among subvisualizations and by capturing affectual information associated with characters and plot structure. We developed a second online interactive visualization to explore word–emotion and word– sentiment associations directly in the NRC Emotion Lexicon. Figure 8 shows a screenshot of this visualization. The treemap on the left shows the various affect categories. The sizes of the boxes in the treemap are proportional to the number of words associated with the corresponding affect. Observe that, word associations with negative sentiment are"
W15-2912,S13-2053,1,0.842653,"rus will benefit researchers, practitioners, and the lay persons alike. We also developed a second visualization to explore the NRC Emotion Lexicon. Both visualizers are made freely available online. This work explores the Roget’s Thesaurus and the NRC Emotion Lexicon, but the same framework can be used to explore other lexical resources too: for example, other thesauri in English and other languages; semantic networks such as WordNet and VerbNet; versions of the NRC Emotion Lexicon in other languages; and sentiment lexicons such as the NRC Hashtag Sentiment lexicon and Sentiment 140 Lexicon (Mohammad et al., 2013; Kiritchenko et al., 2014).8 Our future work will extend previous work on visualizing literature (Mohammad and Yang, 2011; Mohammad, 2012) by incorporating interactivity among subvisualizations and by capturing affectual information associated with characters and plot structure. We developed a second online interactive visualization to explore word–emotion and word– sentiment associations directly in the NRC Emotion Lexicon. Figure 8 shows a screenshot of this visualization. The treemap on the left shows the various affect categories. The sizes of the boxes in the treemap are proportional to"
W16-0410,D08-1083,0,0.197992,"p://www.saifmohammad.com/WebPages/SCL.html Contextual Valence Shifters: Negators, modals, and degree adverbs impact the sentiment of the word or phrase they modify and are commonly referred to as contextual valence shifters (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012). Conventionally, the impact of contextual valence shifters is captured by simple heuristics. For example, negation is often handled by reversing the polarities of the sentiment words in the scope of negation (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Choi and Cardie, 2008) or by shifting the sentiment score of a term in a negated context towards the opposite polarity by a fixed amount (Taboada et al., 2011). However, such heuristics do not adequately capture the true sentiment of multi-word expressions (Zhu et al., 2014). Liu and Seneff (2009) relax the assumption of a fixed shifting margin and estimate these margins for each modifier separately from data. Kiritchenko et al. (2014), on the other hand, estimate the impact of negation on each individual sentiment word through a corpus-based statistical method. Ruppenhofer et al. (2015) automatically rank English"
W16-0410,Q13-1023,0,0.028439,"Missing"
W16-0410,esuli-sebastiani-2006-sentiwordnet,0,0.235938,"s: There exist a number of manually created lexicons that provide lists of positive and negative words, for example, General Inquirer (Stone et al., 1966), Hu and Liu Lexicon (Hu and Liu, 2004), and NRC Emotion Lexicon (Mohammad and Turney, 2013). Only a few manually created lexicons provide real-valued scores of sentiment association (Bradley and Lang, 1999; Warriner et al., 2013; Dodds et al., 2011). None of these lexicons, however, contain multi-word phrases. Manually created sentiment lexicons can be used to automatically generate larger sentiment lexicons using semisupervised techniques (Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2013; De Melo and Bansal, 2013; Tang et al., 2014). (See Mohammad (2016) for a survey on manually created and automatically generated affect resources.) Automatically generated lexicons often have realvalued sentiment association scores, are larger in scale, and can easily be collected for a specific domain; therefore, they were found to be more beneficial in downstream applications, such as sentencelevel sentiment prediction (Kiritchenko et al., 2014). However, any analysis of the relationship between the sentiment of a phrase and its constituents"
W16-0410,N15-1124,0,0.0147506,"techniques: A widely used method of annotation for obtaining numerical scores is the rating scale method—where one is asked to rate an item on a five-, ten-, or hundred-point scale. While easy to understand, rating items on a scale is not natural for people. It is hard for annotators to remain consistent when annotating a large number of items. Also, respondents often use just a limited part of the scale reducing the discrimination among items (Cohen, 2003). To obtain reliable annotations, the rating scale methods require a high number of responses, typically 15 to 20 (Warriner et al., 2013; Graham et al., 2015). A more natural annotation task for humans is to compare items (e.g., whether one word is more positive than the other). Most commonly, the items are compared in pairs (Thurstone, 1927; David, 1963). In this work, we use Best–Worst Scaling—a technique that exploits the comparative approach to annotation while keeping the number of required annotations small (Section 3.2). It has been shown to produce reliable annotations of terms by sentiment (Kiritchenko and Mohammad, 2016a). 45 Term favor would be very easy did not harm increasingly difficult severe Sentiment score 0.653 0.431 0.194 -0.583"
W16-0410,N16-1095,1,0.903027,". Similar analysis for modals and degree adverbs shows that they impact sentiment less dramatically than negators. Furthermore, the impact of modifiers substantially varies even within a group, e.g., the average change in sentiment score brought by the negator ‘will not be’ is 0.41 larger than the change introduced by the negator ‘never’. Likewise, each individual modifier can affect sentiment words in different ways. As a result, in automatic sentiment prediction solutions based on statistical learning seem more promising than fixed hand-crafted rules. 1 This lexicon was first introduced in (Kiritchenko and Mohammad, 2016a) where we investigated the applicability and reliability of the Best–Worst Scaling annotation technique in capturing word–sentiment associations. In this paper, we provide further details on the creation of the lexicon and present analysis of how negators, modals, and degree adverbs impact the sentiment of the words they modify. 44 In related work (not described here), we also created a sentiment composition lexicon for another challenging category of phrases—phrases that include at least one positive word and at least one negative word (Kiritchenko and Mohammad, 2016b). We call such phrases"
W16-0410,N16-1128,1,0.928059,". Similar analysis for modals and degree adverbs shows that they impact sentiment less dramatically than negators. Furthermore, the impact of modifiers substantially varies even within a group, e.g., the average change in sentiment score brought by the negator ‘will not be’ is 0.41 larger than the change introduced by the negator ‘never’. Likewise, each individual modifier can affect sentiment words in different ways. As a result, in automatic sentiment prediction solutions based on statistical learning seem more promising than fixed hand-crafted rules. 1 This lexicon was first introduced in (Kiritchenko and Mohammad, 2016a) where we investigated the applicability and reliability of the Best–Worst Scaling annotation technique in capturing word–sentiment associations. In this paper, we provide further details on the creation of the lexicon and present analysis of how negators, modals, and degree adverbs impact the sentiment of the words they modify. 44 In related work (not described here), we also created a sentiment composition lexicon for another challenging category of phrases—phrases that include at least one positive word and at least one negative word (Kiritchenko and Mohammad, 2016b). We call such phrases"
W16-0410,S16-1004,1,0.767249,"rovide further details on the creation of the lexicon and present analysis of how negators, modals, and degree adverbs impact the sentiment of the words they modify. 44 In related work (not described here), we also created a sentiment composition lexicon for another challenging category of phrases—phrases that include at least one positive word and at least one negative word (Kiritchenko and Mohammad, 2016b). We call such phrases opposing polarity phrases. Both lexicons have been used as official test sets in SemEval-2016 Task 7 ‘Determining Sentiment Intensity of English and Arabic Phrases’ (Kiritchenko et al., 2016).2 The lexicons are made freely available to the research community.3 2 Related Work Sentiment Lexicons: There exist a number of manually created lexicons that provide lists of positive and negative words, for example, General Inquirer (Stone et al., 1966), Hu and Liu Lexicon (Hu and Liu, 2004), and NRC Emotion Lexicon (Mohammad and Turney, 2013). Only a few manually created lexicons provide real-valued scores of sentiment association (Bradley and Lang, 1999; Warriner et al., 2013; Dodds et al., 2011). None of these lexicons, however, contain multi-word phrases. Manually created sentiment lexi"
W16-0410,D09-1017,0,0.0139229,"Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012). Conventionally, the impact of contextual valence shifters is captured by simple heuristics. For example, negation is often handled by reversing the polarities of the sentiment words in the scope of negation (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Choi and Cardie, 2008) or by shifting the sentiment score of a term in a negated context towards the opposite polarity by a fixed amount (Taboada et al., 2011). However, such heuristics do not adequately capture the true sentiment of multi-word expressions (Zhu et al., 2014). Liu and Seneff (2009) relax the assumption of a fixed shifting margin and estimate these margins for each modifier separately from data. Kiritchenko et al. (2014), on the other hand, estimate the impact of negation on each individual sentiment word through a corpus-based statistical method. Ruppenhofer et al. (2015) automatically rank English adverbs by their intensifying or diminishing effect on adjectives using ratings metadata from product reviews. Annotation techniques: A widely used method of annotation for obtaining numerical scores is the rating scale method—where one is asked to rate an item on a five-, te"
W16-0410,S13-2053,1,0.797861,"hat provide lists of positive and negative words, for example, General Inquirer (Stone et al., 1966), Hu and Liu Lexicon (Hu and Liu, 2004), and NRC Emotion Lexicon (Mohammad and Turney, 2013). Only a few manually created lexicons provide real-valued scores of sentiment association (Bradley and Lang, 1999; Warriner et al., 2013; Dodds et al., 2011). None of these lexicons, however, contain multi-word phrases. Manually created sentiment lexicons can be used to automatically generate larger sentiment lexicons using semisupervised techniques (Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2013; De Melo and Bansal, 2013; Tang et al., 2014). (See Mohammad (2016) for a survey on manually created and automatically generated affect resources.) Automatically generated lexicons often have realvalued sentiment association scores, are larger in scale, and can easily be collected for a specific domain; therefore, they were found to be more beneficial in downstream applications, such as sentencelevel sentiment prediction (Kiritchenko et al., 2014). However, any analysis of the relationship between the sentiment of a phrase and its constituents is less reliable when made from an automatically"
W16-0410,L16-1623,1,0.261799,"tially among the members of the same group. Furthermore, each individual modifier can affect sentiment words in different ways. Therefore, solutions based on statistical learning seem more promising than fixed hand-crafted rules on the task of automatic sentiment prediction. 1 Introduction Sentiment associations are commonly captured in sentiment lexicons—lists of associated word– sentiment pairs (optionally with a score indicating the degree of association). They are mostly used in sentiment analysis (Pontiki et al., 2014; Rosenthal et al., 2015), but are also beneficial in stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons usually include only single words. Yet, the sentiment of a phrase can differ markedly from the sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, from its constituents. Lexicons that include sentiment associations for phrases as well as for their constituen"
W16-0410,S14-2004,0,0.0140332,"roups of modifiers on overall sentiment. We find that the effect of modifiers varies substantially among the members of the same group. Furthermore, each individual modifier can affect sentiment words in different ways. Therefore, solutions based on statistical learning seem more promising than fixed hand-crafted rules on the task of automatic sentiment prediction. 1 Introduction Sentiment associations are commonly captured in sentiment lexicons—lists of associated word– sentiment pairs (optionally with a score indicating the degree of association). They are mostly used in sentiment analysis (Pontiki et al., 2014; Rosenthal et al., 2015), but are also beneficial in stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons usually include only single words. Yet, the sentiment of a phrase can differ markedly from the sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, from its constitue"
W16-0410,S15-2078,1,0.742583,"overall sentiment. We find that the effect of modifiers varies substantially among the members of the same group. Furthermore, each individual modifier can affect sentiment words in different ways. Therefore, solutions based on statistical learning seem more promising than fixed hand-crafted rules on the task of automatic sentiment prediction. 1 Introduction Sentiment associations are commonly captured in sentiment lexicons—lists of associated word– sentiment pairs (optionally with a score indicating the degree of association). They are mostly used in sentiment analysis (Pontiki et al., 2014; Rosenthal et al., 2015), but are also beneficial in stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other applications. Manually created sentiment lexicons usually include only single words. Yet, the sentiment of a phrase can differ markedly from the sentiment of its constituent words. Sentiment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, from its constituents. Lexicons that includ"
W16-0410,R15-1071,0,0.0107517,"004; Kennedy and Inkpen, 2005; Choi and Cardie, 2008) or by shifting the sentiment score of a term in a negated context towards the opposite polarity by a fixed amount (Taboada et al., 2011). However, such heuristics do not adequately capture the true sentiment of multi-word expressions (Zhu et al., 2014). Liu and Seneff (2009) relax the assumption of a fixed shifting margin and estimate these margins for each modifier separately from data. Kiritchenko et al. (2014), on the other hand, estimate the impact of negation on each individual sentiment word through a corpus-based statistical method. Ruppenhofer et al. (2015) automatically rank English adverbs by their intensifying or diminishing effect on adjectives using ratings metadata from product reviews. Annotation techniques: A widely used method of annotation for obtaining numerical scores is the rating scale method—where one is asked to rate an item on a five-, ten-, or hundred-point scale. While easy to understand, rating items on a scale is not natural for people. It is hard for annotators to remain consistent when annotating a large number of items. Also, respondents often use just a limited part of the scale reducing the discrimination among items (C"
W16-0410,J11-2001,0,0.018521,"ord or phrase they modify and are commonly referred to as contextual valence shifters (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012). Conventionally, the impact of contextual valence shifters is captured by simple heuristics. For example, negation is often handled by reversing the polarities of the sentiment words in the scope of negation (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Choi and Cardie, 2008) or by shifting the sentiment score of a term in a negated context towards the opposite polarity by a fixed amount (Taboada et al., 2011). However, such heuristics do not adequately capture the true sentiment of multi-word expressions (Zhu et al., 2014). Liu and Seneff (2009) relax the assumption of a fixed shifting margin and estimate these margins for each modifier separately from data. Kiritchenko et al. (2014), on the other hand, estimate the impact of negation on each individual sentiment word through a corpus-based statistical method. Ruppenhofer et al. (2015) automatically rank English adverbs by their intensifying or diminishing effect on adjectives using ratings metadata from product reviews. Annotation techniques: A w"
W16-0410,C14-1018,0,0.0351406,"for example, General Inquirer (Stone et al., 1966), Hu and Liu Lexicon (Hu and Liu, 2004), and NRC Emotion Lexicon (Mohammad and Turney, 2013). Only a few manually created lexicons provide real-valued scores of sentiment association (Bradley and Lang, 1999; Warriner et al., 2013; Dodds et al., 2011). None of these lexicons, however, contain multi-word phrases. Manually created sentiment lexicons can be used to automatically generate larger sentiment lexicons using semisupervised techniques (Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2013; De Melo and Bansal, 2013; Tang et al., 2014). (See Mohammad (2016) for a survey on manually created and automatically generated affect resources.) Automatically generated lexicons often have realvalued sentiment association scores, are larger in scale, and can easily be collected for a specific domain; therefore, they were found to be more beneficial in downstream applications, such as sentencelevel sentiment prediction (Kiritchenko et al., 2014). However, any analysis of the relationship between the sentiment of a phrase and its constituents is less reliable when made from an automatically generated resource as opposed to when made fro"
W16-0410,W10-3111,0,0.311506,"opposed to when made from a manually created resource (as automatically generated resources are less accurate). In this work, we provide an extensive analysis of the impact of different modifiers on sentiment based on reliable finegrained manual annotations. 2 3 http://alt.qcri.org/semeval2016/task7/ http://www.saifmohammad.com/WebPages/SCL.html Contextual Valence Shifters: Negators, modals, and degree adverbs impact the sentiment of the word or phrase they modify and are commonly referred to as contextual valence shifters (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012). Conventionally, the impact of contextual valence shifters is captured by simple heuristics. For example, negation is often handled by reversing the polarities of the sentiment words in the scope of negation (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Choi and Cardie, 2008) or by shifting the sentiment score of a term in a negated context towards the opposite polarity by a fixed amount (Taboada et al., 2011). However, such heuristics do not adequately capture the true sentiment of multi-word expressions (Zhu et al., 2014). Liu and Seneff (2009) relax the assump"
W16-0410,H05-1044,0,0.33732,"e they tend to be more accurate than automatically generated ones; they can be used to automatically generate large high-coverage lexicons (Tang et al., 2014; Esuli and Sebastiani, 2006); they can be used to evaluate different methods of automatically creating sentiment lexicons; and they can be used for linguistic analysis such as examining how modifiers (negators, modals, degree adverbs, etc.) impact overall sentiment. However, most existing manually created sentiment lexicons tend to provide only lists of positive and negative words with very coarse levels of sentiment (Stone et al., 1966; Wilson et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less useful in downstream applications than having access to finegrained (real-valued) sentiment association scores. Negators, modals, and degree adverbs can significantly affect the sentiment of the words they modify. Often, their impact is modeled with simple heuristics; although, recent work has shown that such heuristics do not capture the true sentiment of multi-word phrases. We created a dataset of phrases that include various negators, modals, and degree adverbs, as well as their combinations. Both the phrases and thei"
W16-0410,P14-1029,1,0.40302,"and Inkpen, 2005; Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012). Conventionally, the impact of contextual valence shifters is captured by simple heuristics. For example, negation is often handled by reversing the polarities of the sentiment words in the scope of negation (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Choi and Cardie, 2008) or by shifting the sentiment score of a term in a negated context towards the opposite polarity by a fixed amount (Taboada et al., 2011). However, such heuristics do not adequately capture the true sentiment of multi-word expressions (Zhu et al., 2014). Liu and Seneff (2009) relax the assumption of a fixed shifting margin and estimate these margins for each modifier separately from data. Kiritchenko et al. (2014), on the other hand, estimate the impact of negation on each individual sentiment word through a corpus-based statistical method. Ruppenhofer et al. (2015) automatically rank English adverbs by their intensifying or diminishing effect on adjectives using ratings metadata from product reviews. Annotation techniques: A widely used method of annotation for obtaining numerical scores is the rating scale method—where one is asked to rate"
W16-0429,E14-1040,0,0.0295562,"terse, and reliant on the intuitions of native speakers of a language (rather than biasing the annotators by providing definitions of what it means to be positive, negative, and neutral). On the other hand, the lack of specification leaves the annotator in doubt over how to label certain kinds of instances— for example, sentences where one side wins against another, sarcastic sentences, or retweets. A different approach to sentiment annotation is to ask respondents to identify the target of opinion, and the sentiment towards this target of opinion (Pontiki et al., 2014; Mohammad et al., 2015; Deng and Wiebe, 2014). We will refer to such annotation schemes as the semantic-role based sentiment questionnaires. This approach of sentiment annotation is more specific, and more involved, than the simple sentiment questionnaire approach; however, it too is insufficient for handling several scenarios. Most notably, the emotional state of the speaker is not under the purview of this scheme. Many applications require that statements expressing positive or negative emotional state of the speaker should be marked as ‘positive’ or ‘negative’, respectively. Similarly, many applications require statements that describ"
W16-0429,W09-1904,0,0.0226253,"istics address these challenges: (1) a simple sentiment annotation questionnaire with more precise annotation directions and some additional label categories; and (2) a semantic-role based questionnaire with additional questions to account for the speaker’s emotional state and descriptions of valenced events. Aspects of annotation that are not specific to sentiment, such as good practices in crowdsourcing, how to aggregate information from multiple annotators, and how to automatically detect and discard poor annotations are beyond the scope of this paper; we refer the readers to Lease (2011), Hsueh et al. (2009), and Mohammad and Turney (2013) for that. Methods for obtaining real-valued sentiment scores are also not covered in this paper; we refer the reader to Kiritchenko and Mohammad (2016a) and Kiritchenko et al. (2014) for the use of best–worst scaling to obtain reliable real-valued sentiment associations. See Mohammad (2016) for a survey on sentiment and emotion datasets. 2 Types of Instances that are Difficult to Annotate for Sentiment There exist several types of sentences that are particularly challenging to annotate for sentiment. Some of the more notable ones are listed below: • Speaker’s e"
W16-0429,N16-1095,1,0.823646,"ear and simple instructions are crucial for obtaining high-quality annotations. This is true even for seemingly simple annotation tasks, such as sentiment annotation, where one is to label instances as positive, negative, or neutral. For word annotations, researchers have often framed the task as ‘is this word positive, negative, or neutral?’ (Hu and Liu, 2004), ‘does this word have associations with positive, negative, or neutral sentiment?’ (Mohammad and Turney, 2013), or ‘which word is more positive?’/‘which word has a greater association with positive sentiment’ (Kiritchenko et al., 2016; Kiritchenko and Mohammad, 2016b). Similar instructions are also widely used for sentence-level sentiment annotations—‘is this sentence positive, negative, or neutral?’ (Rosenthal et al., 2015; RosenIn this paper, we present a list of sentence types thal et al., 2014; Mohammad et al., 2016a; Mohammad et al., 2015). We will refer to such annota- that are especially challenging for sentiment annotation schemes as the simple sentiment questionnaires. tion. Next, we propose two annotation schemes that 174 Proceedings of NAACL-HLT 2016, pages 174–179, c San Diego, California, June 12-17, 2016. 2016 Association for Computational"
W16-0429,N16-1128,1,0.90338,"ear and simple instructions are crucial for obtaining high-quality annotations. This is true even for seemingly simple annotation tasks, such as sentiment annotation, where one is to label instances as positive, negative, or neutral. For word annotations, researchers have often framed the task as ‘is this word positive, negative, or neutral?’ (Hu and Liu, 2004), ‘does this word have associations with positive, negative, or neutral sentiment?’ (Mohammad and Turney, 2013), or ‘which word is more positive?’/‘which word has a greater association with positive sentiment’ (Kiritchenko et al., 2016; Kiritchenko and Mohammad, 2016b). Similar instructions are also widely used for sentence-level sentiment annotations—‘is this sentence positive, negative, or neutral?’ (Rosenthal et al., 2015; RosenIn this paper, we present a list of sentence types thal et al., 2014; Mohammad et al., 2016a; Mohammad et al., 2015). We will refer to such annota- that are especially challenging for sentiment annotation schemes as the simple sentiment questionnaires. tion. Next, we propose two annotation schemes that 174 Proceedings of NAACL-HLT 2016, pages 174–179, c San Diego, California, June 12-17, 2016. 2016 Association for Computational"
W16-0429,S16-1004,1,0.39207,"be labeled as neutral. Clear and simple instructions are crucial for obtaining high-quality annotations. This is true even for seemingly simple annotation tasks, such as sentiment annotation, where one is to label instances as positive, negative, or neutral. For word annotations, researchers have often framed the task as ‘is this word positive, negative, or neutral?’ (Hu and Liu, 2004), ‘does this word have associations with positive, negative, or neutral sentiment?’ (Mohammad and Turney, 2013), or ‘which word is more positive?’/‘which word has a greater association with positive sentiment’ (Kiritchenko et al., 2016; Kiritchenko and Mohammad, 2016b). Similar instructions are also widely used for sentence-level sentiment annotations—‘is this sentence positive, negative, or neutral?’ (Rosenthal et al., 2015; RosenIn this paper, we present a list of sentence types thal et al., 2014; Mohammad et al., 2016a; Mohammad et al., 2015). We will refer to such annota- that are especially challenging for sentiment annotation schemes as the simple sentiment questionnaires. tion. Next, we propose two annotation schemes that 174 Proceedings of NAACL-HLT 2016, pages 174–179, c San Diego, California, June 12-17, 2016. 201"
W16-0429,S16-1003,1,0.0695001,"Missing"
W16-0429,S14-2004,0,0.0108216,"this characterization of the task is simple, terse, and reliant on the intuitions of native speakers of a language (rather than biasing the annotators by providing definitions of what it means to be positive, negative, and neutral). On the other hand, the lack of specification leaves the annotator in doubt over how to label certain kinds of instances— for example, sentences where one side wins against another, sarcastic sentences, or retweets. A different approach to sentiment annotation is to ask respondents to identify the target of opinion, and the sentiment towards this target of opinion (Pontiki et al., 2014; Mohammad et al., 2015; Deng and Wiebe, 2014). We will refer to such annotation schemes as the semantic-role based sentiment questionnaires. This approach of sentiment annotation is more specific, and more involved, than the simple sentiment questionnaire approach; however, it too is insufficient for handling several scenarios. Most notably, the emotional state of the speaker is not under the purview of this scheme. Many applications require that statements expressing positive or negative emotional state of the speaker should be marked as ‘positive’ or ‘negative’, respectively. Similarly, man"
W16-0429,S14-2009,0,0.0481406,"Missing"
W16-0429,S15-2078,1,0.708758,"ere one is to label instances as positive, negative, or neutral. For word annotations, researchers have often framed the task as ‘is this word positive, negative, or neutral?’ (Hu and Liu, 2004), ‘does this word have associations with positive, negative, or neutral sentiment?’ (Mohammad and Turney, 2013), or ‘which word is more positive?’/‘which word has a greater association with positive sentiment’ (Kiritchenko et al., 2016; Kiritchenko and Mohammad, 2016b). Similar instructions are also widely used for sentence-level sentiment annotations—‘is this sentence positive, negative, or neutral?’ (Rosenthal et al., 2015; RosenIn this paper, we present a list of sentence types thal et al., 2014; Mohammad et al., 2016a; Mohammad et al., 2015). We will refer to such annota- that are especially challenging for sentiment annotation schemes as the simple sentiment questionnaires. tion. Next, we propose two annotation schemes that 174 Proceedings of NAACL-HLT 2016, pages 174–179, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics address these challenges: (1) a simple sentiment annotation questionnaire with more precise annotation directions and some additional label categorie"
W17-5205,esuli-sebastiani-2006-sentiwordnet,0,0.0354766,"Missing"
W17-5205,H05-1073,0,0.0898987,"ctiveTweets 35 Emotion anger fear joy sadness have also been proposed. The most popular among them, Russell’s circumplex model, asserts that all emotions are made up of two core dimensions: valence and arousal (Russell, 2003). We created datasets for four emotions that are the most common amongst the many proposals for basic emotions: anger, fear, joy, and sadness. However, we have also begun work on other affect categories, as well as on valence and arousal. The vast majority of emotion annotation work provides discrete binary labels to the text instances (joy–nojoy, fear–nofear, and so on) (Alm et al., 2005; Aman and Szpakowicz, 2007; Brooks et al., 2013; Neviarouskaya et al., 2009; Bollen et al., 2009). The only annotation effort that provided scores for degree of emotion is by Strapparava and Mihalcea (2007) as part of one of the SemEval2007 shared task. Annotators were given newspaper headlines and asked to provide scores between 0 and 100 via slide bars in a web interface. It is difficult for humans to provide direct scores at such fine granularity. A common problem is inconsistency in annotations. One annotator might assign a score of 79 to a piece of text, whereas another annotator may ass"
W17-5205,P11-2008,0,0.034036,"Missing"
W17-5205,W15-4321,0,0.0660369,"Missing"
W17-5205,N13-1062,0,0.0316205,"hose words were taken to be the query terms. been empirically shown that annotations for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016).6 Kiritchenko and Mohammad (2017) show through empirical experiments that BWS produces more reliable fine-grained scores than scores obtained using rating scales. Within the NLP community, Best–Worst Scaling (BWS) has thus far been used only to annotate words: for example, for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), and phrase sentiment composition (Kiritchenko and Mohammad, 2016). However, we use BWS to annotate whole tweets for intensity of emotion. 3 Data Mohammad and Bravo-Marquez (2017) describe how the Tweet Emotion Intensity Dataset was created. We summarize below the approach used and the key properties of the dataset. Not included in this summary are: (a) experiments showing marked similarities between emotion pairs in terms of how they manifest in language, (b) how training data for one emotion can be used to improve prediction performance f"
W17-5205,S12-1047,1,0.726108,"n Table 1: Categories from the Roget’s Thesaurus whose words were taken to be the query terms. been empirically shown that annotations for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016).6 Kiritchenko and Mohammad (2017) show through empirical experiments that BWS produces more reliable fine-grained scores than scores obtained using rating scales. Within the NLP community, Best–Worst Scaling (BWS) has thus far been used only to annotate words: for example, for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), and phrase sentiment composition (Kiritchenko and Mohammad, 2016). However, we use BWS to annotate whole tweets for intensity of emotion. 3 Data Mohammad and Bravo-Marquez (2017) describe how the Tweet Emotion Intensity Dataset was created. We summarize below the approach used and the key properties of the dataset. Not included in this summary are: (a) experiments showing marked similarities between emotion pairs in terms of how they manifest in language, (b) how training data for one emotion can"
W17-5205,N16-1095,1,0.583765,"nstances for degree of affect. This is a substantially more difficult undertaking than annotating only for the broad affect class: respondents are presented with greater cognitive load and it is particularly hard to ensure consistency (both across responses by different annotators and within the responses produced by an individual annotator). Thus, we used a technique called Best– Worst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff). It is an annotation scheme that addresses the limitations of traditional rating scales (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). We used BWS to create the Tweet Emotion Intensity Dataset, which currently includes four sets of tweets annotated for intensity of anger, fear, joy, and sadness, respectively (Mohammad and Bravo-Marquez, 2017). These are the first datasets of their kind. The competition is organized on a CodaLab website, where participants can upload their submissions, and the leaderboard reports the results.4 Twenty-two teams participated in the 2017 iteration of the competition. The best performing system, Prayas, obtained a Pearson correlation of 0.747 with the gold annotations. Seven teams obtaine"
W17-5205,P17-2074,1,0.565805,"e same text. It is also common that the same annotator assigns different scores to the same text instance at different points in time. Further, annotators often have a bias towards different parts of the scale, known as scale region bias. 2.2 Thes. Category 900 860 836 837 Head Word resentment fear cheerfulness dejection Table 1: Categories from the Roget’s Thesaurus whose words were taken to be the query terms. been empirically shown that annotations for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) (Louviere, 1991; Kiritchenko and Mohammad, 2016).6 Kiritchenko and Mohammad (2017) show through empirical experiments that BWS produces more reliable fine-grained scores than scores obtained using rating scales. Within the NLP community, Best–Worst Scaling (BWS) has thus far been used only to annotate words: for example, for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), and phrase sentiment composition (Kiritchenko and Mohammad, 2016). However, we use BWS to annotate whole tweets for intensity of emotion. 3 Data Mohammad and Bravo-Marquez (2017) describe how"
W17-5205,W10-0513,0,0.0434505,"Missing"
W17-5205,S12-1033,1,0.647476,"Canada system which ranked first in three sentiment shared tasks: SemEval-2013 Task 2, SemEval-2014 Task 9, and SemEval2014 Task 4. Mohammad et al. (2017) describes a stancedetection system that outperformed submissions from all 19 teams that participated in SemEval-2016 Task 6. 40 AFINN (Nielsen, 2011) BingLiu (Hu and Liu, 2004) MPQA (Wilson et al., 2005) NRC Affect Intensity Lexicon (NRC-Aff-Int) (Mohammad, 2017) NRC Word-Emotion Assn. Lexicon (NRC-EmoLex) (Mohammad and Turney, 2013) NRC10 Expanded (NRC10E) (Bravo-Marquez et al., 2016) NRC Hashtag Emotion Association Lexicon (NRC-Hash-Emo) (Mohammad, 2012a; Mohammad and Kiritchenko, 2015) NRC Hashtag Sentiment Lexicon (NRC-Hash-Sent) (Mohammad et al., 2013) Sentiment140 (Mohammad et al., 2013) SentiWordNet (Esuli and Sebastiani, 2006) SentiStrength (Thelwall et al., 2012) Twitter Yes No No Yes No Yes Yes Annotation Manual Manual Manual Manual Manual Automatic Automatic Scope Sentiment Sentiment Sentiment Emotions Emotions Emotions Emotions Label Numeric Nominal Nominal Numeric Nominal Numeric Numeric Yes Yes No Yes Automatic Automatic Automatic Manual Sentiment Sentiment Sentiment Sentiment Numeric Numeric Numeric Numeric Table 4: Affect lexic"
W17-5205,S17-1007,1,0.790134,"ensure consistency (both across responses by different annotators and within the responses produced by an individual annotator). Thus, we used a technique called Best– Worst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff). It is an annotation scheme that addresses the limitations of traditional rating scales (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). We used BWS to create the Tweet Emotion Intensity Dataset, which currently includes four sets of tweets annotated for intensity of anger, fear, joy, and sadness, respectively (Mohammad and Bravo-Marquez, 2017). These are the first datasets of their kind. The competition is organized on a CodaLab website, where participants can upload their submissions, and the leaderboard reports the results.4 Twenty-two teams participated in the 2017 iteration of the competition. The best performing system, Prayas, obtained a Pearson correlation of 0.747 with the gold annotations. Seven teams obtained scores higher than the score obtained by a competitive SVM-based benchmark system (0.66), which we had released at the start of the competition.5 Low-dimensional (dense) distributed representations of words (word emb"
W17-5205,S07-1013,0,0.382095,"lence and arousal (Russell, 2003). We created datasets for four emotions that are the most common amongst the many proposals for basic emotions: anger, fear, joy, and sadness. However, we have also begun work on other affect categories, as well as on valence and arousal. The vast majority of emotion annotation work provides discrete binary labels to the text instances (joy–nojoy, fear–nofear, and so on) (Alm et al., 2005; Aman and Szpakowicz, 2007; Brooks et al., 2013; Neviarouskaya et al., 2009; Bollen et al., 2009). The only annotation effort that provided scores for degree of emotion is by Strapparava and Mihalcea (2007) as part of one of the SemEval2007 shared task. Annotators were given newspaper headlines and asked to provide scores between 0 and 100 via slide bars in a web interface. It is difficult for humans to provide direct scores at such fine granularity. A common problem is inconsistency in annotations. One annotator might assign a score of 79 to a piece of text, whereas another annotator may assign a score of 62 to the same text. It is also common that the same annotator assigns different scores to the same text instance at different points in time. Further, annotators often have a bias towards dif"
W17-5205,S18-1001,1,0.615992,"Missing"
W17-5205,S13-2053,1,0.830489,"Missing"
W17-5205,H05-1044,0,0.161739,"Missing"
W17-5205,W16-6208,0,\N,Missing
W18-6206,D17-1169,0,0.0692008,"X X X X X X X X X X X X X X X X X X X X X X X X X X X X 23 20 12 9 7 5 5 3 2 1 1 1 1 1 Table 7: Overview of methods employed by different teams (sorted by popularity from left to right). 4.4 9 X SemEval 26 X Sentence/Document X Emotion Emb. X X X X X X X X X X X X X X X X X X X X X X X X X X Unlabeled Corpora 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X 8 X X 7 6 4 X 4 3 Table 8: Overview of information sources employed by different teams (sorted by popularity from left to right). Top 3 Submissions (Felbo et al., 2017) and “Universal Sentence Encoder” (Cer et al., 2018) as features. In the following, we briefly summarize the approaches used by the top three teams: Amobee, IIIDYT, and NTUA-SLP. For more information on these approaches and those of the other teams, we refer the reader to the individual system description papers. The three best performing systems are all ensemble approaches. However, they make use of different underlying machine learning architectures and rely on different kinds of information. 4.4.1 Lexicons LDA X Emoji X X X X Characters X X MLP Autoencoder Random Forrest k-Means Bagging Att"
W18-6206,W18-6208,0,0.0526748,"ree best performing systems are all ensemble approaches. However, they make use of different underlying machine learning architectures and rely on different kinds of information. 4.4.1 Lexicons LDA X Emoji X X X X Characters X X MLP Autoencoder Random Forrest k-Means Bagging Attention Linear Classifier Transfer Learning Language model Ensemble CNN/Capsules X X X X X X X X X X X X X X X X X Words X X X X X X X X X X X Rank X X X X X X X X X X X X X X LSTM/RNN/GRU Rank Embeddings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 4.4.2 IIIDYT The second-ranking system, IIIDYT (Balazs et al., 2018), preprocesses the dataset by tokenizing the sentences (including emojis), and normalizing the USERNAME, NEWLINE, URL and TRIGGERWORD indicators. Then, it feeds word-level representations returned by a pretrained ELMo layer into a Bi-LSTM with 1 layer of 2048 hidden units for each direction. The Bi-LSTM output word representations are max-pooled to generate sentencelevel representations, followed by a single hidden layer of 512 units and output size of 6. The team trains six models with different random initializations, obtains the probability distributions for each example, and then averages"
W18-6206,S07-1013,0,0.198525,"a.). This is motivated by the high availability of usergenerated text and by the challenge that manual annotation is typically tedious or expensive. This contrasts with the current data demand of machine learning, and especially, deep learning approaches. With our work in IEST, we combine the goal of the development of models which are able to recognize emotions from implicit descriptions without having access to explicit emotion words, with the paradigm of distant supervision. One of the first corpora annotated for emotions is that by Alm et al. (2005) who analyze sentences from fairy tales. Strapparava and Mihalcea (2007) annotate news headlines with emotions and valence, Mohammad et al. (2015) annotate tweets on elections, and Schuff et al. (2017) tweets of a stance dataset (Mohammad et al., 2017). The SemEval2018 Task 1: Affect in Tweets (Mohammad et al., 2018) includes several subtasks on inferring the affectual state of a person from their tweet: emotion intensity regression, emotion intensity ordinal classification, valence (sentiment) regression, valence ordinal classification, and multi-label emotion classification. In all of these prior shared tasks and datasets, no distinction is made between implicit"
W19-1308,S15-2078,1,0.81589,"icipants from seven different countries, including Germany, the Netherlands, and the USA. The questionnaire was divided into three subscales focusing on interaction, social influence, and emotion. In general, participants from the USA showed the most positive attitudes towards robots, particularly in their openness to interacting with robots, although they were more negative than the German or Dutch on the topic of robot emotion. Social media has proven to be a rich source of data for sentiment and emotion analysis on a variety of topics, using lexicon-based and machine learning methods (e.g. Rosenthal et al. (2015); Giachanou and Crestani (2016); Mohammad et al. (2018)). However, very little work has focused on the emotions expressed towards robots. Friedman et al. (2003) analyzed 3,119 forum posts relating to the AIBO robot dog. They developed (1) Were there differences in the type or scale of emotions expressed in each of the host countries? We compare the percentages of words associated with different emotions from the tweets produced during each trip, to examine any cultural factors in the public reaction to hitchBOT. (2) What emotions were triggered when hitchBOT was destroyed? We compare the perce"
W19-1308,P18-1017,1,0.901282,"Missing"
W19-1308,S14-2033,0,0.0681897,"Missing"
W19-1308,S18-1001,1,0.899238,"Missing"
