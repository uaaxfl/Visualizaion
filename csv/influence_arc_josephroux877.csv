2005.jeptalnrecital-long.2,C96-1034,0,0.04084,"Missing"
2005.jeptalnrecital-long.2,2003.jeptalnrecital-long.6,0,0.0992153,"Missing"
2005.jeptalnrecital-long.2,W02-2233,0,0.0595033,"Missing"
2005.jeptalnrecital-long.2,E03-1030,0,0.104968,"Missing"
2008.jeptalnrecital-court.2,W02-2220,0,0.128831,"Missing"
2008.jeptalnrecital-court.2,2006.jeptalnrecital-long.12,0,0.0718962,"Missing"
2008.jeptalnrecital-court.2,E03-1030,0,0.0666427,"Missing"
2008.jeptalnrecital-court.2,W04-3321,0,0.0468352,"Missing"
2008.jeptalnrecital-court.2,P07-1023,0,0.0431353,"Missing"
2008.jeptalnrecital-court.2,P07-1043,0,0.0427787,"Missing"
2008.jeptalnrecital-court.2,P02-1003,0,0.0718847,"Missing"
2008.jeptalnrecital-court.2,2004.jeptalnrecital-long.3,0,0.0790135,"Missing"
2008.jeptalnrecital-court.2,P94-1022,0,0.162311,"Missing"
2008.jeptalnrecital-court.2,P80-1024,0,0.81428,"Missing"
2008.jeptalnrecital-court.2,J95-4002,0,0.147794,"Missing"
2008.jeptalnrecital-court.2,J92-4004,0,0.274054,"Missing"
2009.jeptalnrecital-long.2,P81-1022,0,0.398857,"Missing"
2009.jeptalnrecital-long.2,2008.jeptalnrecital-long.14,0,0.0982848,"Missing"
2009.jeptalnrecital-long.2,W03-3020,0,0.0568363,"Missing"
2011.jeptalnrecital-long.26,W10-1408,1,0.877026,"Missing"
2011.jeptalnrecital-long.26,C10-1011,0,0.0269882,"Missing"
2011.jeptalnrecital-long.26,candito-etal-2010-statistical,0,0.0312059,"Missing"
2011.jeptalnrecital-long.26,W09-3821,0,0.0436454,"Missing"
2011.jeptalnrecital-long.26,2009.jeptalnrecital-long.4,0,0.0966482,"Missing"
2011.jeptalnrecital-long.26,C10-2013,0,0.0302856,"Missing"
2011.jeptalnrecital-long.26,P05-1022,0,0.145446,"Missing"
2011.jeptalnrecital-long.26,P97-1003,0,0.220523,"Missing"
2011.jeptalnrecital-long.26,2010.jeptalnrecital-long.3,0,0.0504872,"Missing"
2011.jeptalnrecital-long.26,N10-1095,0,0.0377103,"Missing"
2011.jeptalnrecital-long.26,P05-1010,0,0.0473543,"Missing"
2011.jeptalnrecital-long.26,C08-1071,0,0.0408272,"Missing"
2011.jeptalnrecital-long.26,P05-1012,0,0.152191,"Missing"
2011.jeptalnrecital-long.26,P06-1055,0,0.0763483,"Missing"
2011.jeptalnrecital-long.26,N07-1051,0,0.0986944,"Missing"
2011.jeptalnrecital-long.26,N10-1049,0,0.0288354,"Missing"
2020.coling-main.225,N16-1127,1,0.752843,"Missing"
2020.coling-main.225,C96-1058,0,0.0527063,"s in representation, we hypothesize that parsing the two treebank formalism along side each other will help improve both parsing outcomes. 3 Model We briefly describe the single task Easy-First (EF) parsing algorithm Kiperwasser and Goldberg (2016) and its components, then we discuss the multitask extension (MEF) inspired by (Constant et al., 2016) adapted to the tree-structured LSTM in terms of the updated parsing algorithm and its components. 3.1 Single-task Model The EF parsing model builds dependency trees bottom-up. Intuitively, it can be seen as a greedy version of the Eisner algorithm (Eisner, 1996) where each span contains at most one subtree and where each subtree, once created, is fixed and must be a part of the final structure. The algorithm maintains a sequence of pending subtrees. Two adjacent subtrees in the sequence may combine, by adding an arc from the root of one subtree to the other, and be replaced by a new (bigger) subtree. Since there are few subtrees to consider — at most n + 1 pending subtrees for a sentence of n words — the context of each decision, ie each arc creation between subtrees, can depend on the whole subtrees involved provided they can be encoded with a fixed"
2020.coling-main.225,Q13-1033,0,0.0220107,"main contributor is not the sharing of lexical information, as is commonly done in multitask systems, but the sharing of partial dependency trees given as input for arc weight prediction. Future work will explore further sharing between parsers. In particular, we expect tree encoders could benefit from the additional information (although redundancy with tree sharing in the score function 2505 could hurt the system). We also plan to work on model improvements to address the limitation arising from added dimensions. For instance, with the addition of the second dimension, the notion of oracle (Goldberg and Nivre, 2013) used for training becomes more fragile, since the number of correct actions grows, and the order in which to perform them is unknown and can have some long term consequences on the action in the other dimension. It would be interesting to explore how reinforcement learning, where the notion of planning ahead is crucial, could help. Acknowledgements This work is partially supported by a public grant overseen by the French National Research Agency (ANR) as part of the program Investissements d’Avenir (reference: ANR-10-LABX-0083). It contributes to the IdEx Université de Paris - ANR-18-IDEX-000"
2020.coling-main.225,P09-2056,1,0.867584,"ntroduction Dependency parsing is the task of assigning a syntactic structure to a sentence by linking its words with binary asymmetrical typed relations. In addition to syntactic information, dependency representations encode some semantic aspects of the sentence which make them important to downstream applications including sentiment analysis (Tai et al., 2015) and information extraction (Miwa and Bansal, 2016). In this paper we are interested in Arabic dependency parsing for which two formalisms have been developed (See §2). The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment. The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other languages (Nivre et al., 2017). While previous work on Arabic dependency parsing (Marton et al., 2013; Taji et al., 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993). MTL allows for m"
2020.coling-main.225,D17-1206,0,0.0202368,"sentences using parallel treebanks. Deep neural networks are particularly suited for multitask scenarios via straightforward parameter and representation sharing. Some hidden layers can be shared across all tasks while output layers are kept separate. In fact, most deep learning architectures for language processing start with sequential encoding components such as BiLSTMs or Transformer layers which can be readily shared across multiple tasks. This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017). This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019). In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial. Peng et al. (2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 24"
2020.coling-main.225,P18-1035,0,0.0195942,"s which makes it computationally expensive. On the same task, Kurita and Søgaard (2019) describe a model that shares a representation of the complete partial parse forest of one dimension while taking decisions on the other. This is similar in spirit to our sharing of parameters and representations. Furthermore, they show that their transition-based parser effectively learns easy-first strategies with policy gradient based reinforcement learning. This motivates further our choice of parsing framework. In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but none of them build on the easy-first framework nor target the Arabic language. Along the lines of multitask easy-first parsers, Constant et al. (2016) introduced a joint model for learning from multiple treebanks simultaneously. They show that syntactic dependency representations and tree-based representations of multiword expressions can help each other. However, they do not use a neural architecture and perform experiments only on English and French. When no parallel annotations on the same text"
2020.coling-main.225,Q16-1032,0,0.172354,"tive Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2497 Proceedings of the 28th International Conference on Computational Linguistics, pages 2497–2508 Barcelona, Spain (Online), December 8-13, 2020 factors that score combinations of substructures from each (Peng et al., 2017). Joint inference however comes with increased computational cost. To address this issue, we introduce a multitask learning model for dependency parsing (§3.2). This model is based on greedy arc selection similar to the neural easy-first approach proposed in (Kiperwasser and Goldberg, 2016) (§3). We use tree-structured LSTMs to encode substructures (partial trees) in each formalism which are then concatenated across tasks and scored jointly. Hence, we model interactions between substructures across tasks while keeping computational complexity low thanks to the easy-first framework. Furthermore, this approach enables the sharing of various components between tasks, a richer sharing than the mere sequential encoder sharing found in most multitask systems (Kurita and Søgaard, 2019). Our multitask architecture outperforms the single-task parser on both formalisms (§4.3). The parser"
2020.coling-main.225,P19-1232,0,0.201716,"me hidden layers can be shared across all tasks while output layers are kept separate. In fact, most deep learning architectures for language processing start with sequential encoding components such as BiLSTMs or Transformer layers which can be readily shared across multiple tasks. This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017). This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019). In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial. Peng et al. (2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2497 Proceedings of the 28th International Conference on Computational Linguistics, pages 2497–2508 Barcelona, Spain (Online), December 8-13, 2020 factors that score comb"
2020.coling-main.225,P19-1450,0,0.0851213,"ayers are kept separate. In fact, most deep learning architectures for language processing start with sequential encoding components such as BiLSTMs or Transformer layers which can be readily shared across multiple tasks. This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017). This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019). In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial. Peng et al. (2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2497 Proceedings of the 28th International Conference on Computational Linguistics, pages 2497–2508 Barcelona, Spain (Online), December 8-13, 2020 factors that score combinations of substructures from each (Peng et al., 2017). Joi"
2020.coling-main.225,J13-1008,1,0.803312,"ted in Arabic dependency parsing for which two formalisms have been developed (See §2). The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment. The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other languages (Nivre et al., 2017). While previous work on Arabic dependency parsing (Marton et al., 2013; Taji et al., 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993). MTL allows for more training data to be exploited while benefiting from the structural or statistical similarities between the tasks. We therefore propose to learn CATiB and UD dependency trees jointly on the same input sentences using parallel treebanks. Deep neural networks are particularly suited for multitask scenarios via straightforward parameter and representation sharing. Some hidden layers can be shared across all tasks while output layers are ke"
2020.coling-main.225,P16-1105,0,0.0280411,"tion tests show that the main contribution of this reduction is given by sharing tree representation between tasks, and not simply sharing BiLSTM layers as is usually performed in NLP multitask systems. 1 Introduction Dependency parsing is the task of assigning a syntactic structure to a sentence by linking its words with binary asymmetrical typed relations. In addition to syntactic information, dependency representations encode some semantic aspects of the sentence which make them important to downstream applications including sentiment analysis (Tai et al., 2015) and information extraction (Miwa and Bansal, 2016). In this paper we are interested in Arabic dependency parsing for which two formalisms have been developed (See §2). The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment. The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other languages (Nivre et al., 2017). While previous work"
2020.coling-main.225,nivre-etal-2006-maltparser,0,0.113115,"mponents. We found that for CATiB, the LAS score of the best system decreased on the 2504 DEV set from 86.66 to 86.41 in the pipeline setup CATiB → UD, and to 86.37 in UD → CATiB. For UD, the score decreased from 85.17 to 85.06 in pipeline CATiB → UD and to 85.00 in UD → CATiB. 5 Related Work Arabic Syntactic Dependency Parsing Earlier work on syntactic dependency parsing for Arabic had focused mainly on CATiB representation. Marton et al. (2013) explored the use several morpho-syntactic features in the easy-first framework, while Shahrour et al. (2015; Shahrour et al. (2016) used MaltParser (Nivre et al., 2006). Taji et al. (2017) presented the UD treebank more recently and conducted experiments on CATiB and UD separately in a single-task settings. Multitask systems that have been developed for Arabic were part of efforts to build one multilingual system for all UD dependencies. We present the first effort on multitask joint parsing for multiple Arabic formalisms. Multitask Dependency Parsing Research towards using multitask deep learning settings to resolve NLP tasks has been an active ongoing subject since the early work of Collobert and Weston (2008) where a single model is trained to perform mul"
2020.coling-main.225,S15-2153,0,0.0940977,"Missing"
2020.coling-main.225,P17-1186,0,0.29345,"as BiLSTMs or Transformer layers which can be readily shared across multiple tasks. This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017). This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019). In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial. Peng et al. (2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2497 Proceedings of the 28th International Conference on Computational Linguistics, pages 2497–2508 Barcelona, Spain (Online), December 8-13, 2020 factors that score combinations of substructures from each (Peng et al., 2017). Joint inference however comes with increased computational cost. To address this issue, we introduce a multitask learning model for dep"
2020.coling-main.225,N18-1135,0,0.0179694,"me task, Kurita and Søgaard (2019) describe a model that shares a representation of the complete partial parse forest of one dimension while taking decisions on the other. This is similar in spirit to our sharing of parameters and representations. Furthermore, they show that their transition-based parser effectively learns easy-first strategies with policy gradient based reinforcement learning. This motivates further our choice of parsing framework. In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but none of them build on the easy-first framework nor target the Arabic language. Along the lines of multitask easy-first parsers, Constant et al. (2016) introduced a joint model for learning from multiple treebanks simultaneously. They show that syntactic dependency representations and tree-based representations of multiword expressions can help each other. However, they do not use a neural architecture and perform experiments only on English and French. When no parallel annotations on the same text are available, it has been shown that a single"
2020.coling-main.225,K19-1017,0,0.0212216,"a model that shares a representation of the complete partial parse forest of one dimension while taking decisions on the other. This is similar in spirit to our sharing of parameters and representations. Furthermore, they show that their transition-based parser effectively learns easy-first strategies with policy gradient based reinforcement learning. This motivates further our choice of parsing framework. In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but none of them build on the easy-first framework nor target the Arabic language. Along the lines of multitask easy-first parsers, Constant et al. (2016) introduced a joint model for learning from multiple treebanks simultaneously. They show that syntactic dependency representations and tree-based representations of multiword expressions can help each other. However, they do not use a neural architecture and perform experiments only on English and French. When no parallel annotations on the same text are available, it has been shown that a single model can be trained to perform multiple task"
2020.coling-main.225,K17-3007,0,0.133901,"asks while output layers are kept separate. In fact, most deep learning architectures for language processing start with sequential encoding components such as BiLSTMs or Transformer layers which can be readily shared across multiple tasks. This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017). This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019). In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial. Peng et al. (2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2497 Proceedings of the 28th International Conference on Computational Linguistics, pages 2497–2508 Barcelona, Spain (Online), December 8-13, 2020 factors that score combinations of substructures from each"
2020.coling-main.225,D15-1152,1,0.899631,"Missing"
2020.coling-main.225,C16-2048,1,0.90366,"Missing"
2020.coling-main.225,P16-2038,0,0.0675848,"es jointly on the same input sentences using parallel treebanks. Deep neural networks are particularly suited for multitask scenarios via straightforward parameter and representation sharing. Some hidden layers can be shared across all tasks while output layers are kept separate. In fact, most deep learning architectures for language processing start with sequential encoding components such as BiLSTMs or Transformer layers which can be readily shared across multiple tasks. This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017). This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019). In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial. Peng et al. (2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. Li"
2020.coling-main.225,D18-1263,0,0.0169475,"onally expensive. On the same task, Kurita and Søgaard (2019) describe a model that shares a representation of the complete partial parse forest of one dimension while taking decisions on the other. This is similar in spirit to our sharing of parameters and representations. Furthermore, they show that their transition-based parser effectively learns easy-first strategies with policy gradient based reinforcement learning. This motivates further our choice of parsing framework. In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but none of them build on the easy-first framework nor target the Arabic language. Along the lines of multitask easy-first parsers, Constant et al. (2016) introduced a joint model for learning from multiple treebanks simultaneously. They show that syntactic dependency representations and tree-based representations of multiword expressions can help each other. However, they do not use a neural architecture and perform experiments only on English and French. When no parallel annotations on the same text are available, it has been"
2020.coling-main.225,P18-2098,0,0.241027,"Missing"
2020.coling-main.225,P15-1150,0,0.0534435,"D compared to a single-task baseline, and ablation tests show that the main contribution of this reduction is given by sharing tree representation between tasks, and not simply sharing BiLSTM layers as is usually performed in NLP multitask systems. 1 Introduction Dependency parsing is the task of assigning a syntactic structure to a sentence by linking its words with binary asymmetrical typed relations. In addition to syntactic information, dependency representations encode some semantic aspects of the sentence which make them important to downstream applications including sentiment analysis (Tai et al., 2015) and information extraction (Miwa and Bansal, 2016). In this paper we are interested in Arabic dependency parsing for which two formalisms have been developed (See §2). The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment. The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other l"
2020.coling-main.225,W17-1320,1,0.93229,"code some semantic aspects of the sentence which make them important to downstream applications including sentiment analysis (Tai et al., 2015) and information extraction (Miwa and Bansal, 2016). In this paper we are interested in Arabic dependency parsing for which two formalisms have been developed (See §2). The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment. The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other languages (Nivre et al., 2017). While previous work on Arabic dependency parsing (Marton et al., 2013; Taji et al., 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993). MTL allows for more training data to be exploited while benefiting from the structural or statistical similarities between the tasks. We therefore propose to learn CATiB and UD dependency trees jointly on the same input sentences usin"
2020.jeptalnrecital-deft.2,W16-3638,0,0.0519118,"Missing"
2020.jeptalnrecital-deft.2,E14-4001,0,0.0352491,"Missing"
2020.jeptalnrecital-deft.2,P19-1355,0,0.0243958,"Missing"
2021.iwpt-1.11,K18-2005,0,0.0654187,"Missing"
2021.iwpt-1.11,N19-1423,0,0.00524305,"nd Manning, 2017): lexical features are transformed for head or modifier roles by two feed-forward networks and combined to score arcs via a biaffine transformation. 4 https://github.com/kidlestar/MOE.git. For English we used Glove embeddings (Pennington et al., 2014), while for Chinese we extracted pretrained embeddings from the publicly available model of Zhang et al. (2020b). 5 On PTB, in order to compare with recent parsing results, we set up BFOP and BSOP (B for Bert), variants of the FOP and SOP settings: we follow Fonseca and Martins (2020) and concatenate an additional BERT embedding (Devlin et al., 2019) (the average of the 4 last layers of the bertbase-uncased model) to the embedding vector fed to the BiLSTM layers. Gating (mixture weights ω) is implemented by a K-class softmax over a feed-forward network whose input are the concatenation of initial and final contextual lexical feature vectors returned by the 3-layer BiLSTM. Hyper-parameters are set similarly to Zhang et al. (2020a), with the exception of the learning rate decreased to 10−4 and patience (that is the maximum number of epochs without LAS increase on the development set) set to 20. We train 12 independent models for each expert"
2021.iwpt-1.11,P18-1128,0,0.0132278,"CSOP with 6 experts for PTB and CoNLL09. Additionally for PTB, we show BFOP, CBFOP, BSOP and CBSOP with 6 experts to make comparison with recent parsers, often more sophisticated than our approach, with BERT. We give the results with the same typographical system as Zhang et al. (2020a) Please note that, while average results keep the same semantics, max and min give test results of the LAS highest- and lowest- (resp.) scoring systems on the development set. We note that results of Zhang et al. (2020a) would correspond our model with K = 1. For averaging models, we apply significance ttests (Dror et al., 2018) with level α = 0.05 to FOP, BFOP, SOP, BSOP with K = 6 against K = 1. For PTB and CoNLL09, p-value is always smaller than 0.005. We note that for parsers without BERT, averaging can achieve a considerable improvement with SOP and gives new SOTA. We also point out that, if FOP and SOP could find equivalently good models on dev, SOP models seem to better generalize. For parsers with BERT, with a simple averaging of BSOP, we achieve comparable performances (or even better in case of LAS) when comparing to more involved methods such as (Li et al., 2020; Mohammadshahi and Henderson, 2021). It rema"
2021.iwpt-1.11,P96-1011,0,0.47154,"te the energy of a parse tree, the global energy of a sentence (the sum of all parse tree energies, called the partition function) and the marginal probability of an arc in a sentence. In the remaining we focus on projective first- and second-order parsers, where these quantities are computed via tabular methods or backpropagation1 . Tree structure For a graph-based dependency parser, the tree probability is defined as: p(y|x) = exp(s(x, y)) P Z(x) ≡ y0 ∈Y(x) exp(s(x, y 0 )) with s(x, y) the tree energy giving the correctness of y for x, and Z(x) the partition function. In first-order models (Eisner, 1996), tree scores are sums of arc scores: X s(x, y) = s(h, d) (h,d)∈y Eisner (1997) generalizes scores to the secondorder by considering pairs of adjacent siblings: X X s(x, y) = s(h, d) + s(h, d1 , d2 ) (h,d)∈y (h,d1 ) (h,d2 )∈y with h &lt; d1 &lt; d2 or d2 &lt; d1 &lt; h. For projective first- or second-order models, Z(x) and p(y|x) are efficiently calculated (Zhang et al., 2020b).  Moreover marginal arc probability p (h, d)|x can be efficiently calculated from the partition function by applying backpropagation from log Z(x) to s(h, d), see (Eisner, 2016; Zmigrod et al., 2020; Zhang et al., 2020a): p((h, d"
2021.iwpt-1.11,1997.iwpt-1.10,0,0.4041,"a finite mixture model (MoE) is a weighted sum of the probabilities given by all experts: p(y|x) = K X ωk (x)pk (y|x) (3) y ∗ = argmax k=1  p (h, d)|x = MBR(x) y∈Y(x) (h,d)∈y where PK mixture weights verify ∀x, ωk (x) ≥ 0 and k=1 ωk (x) = 1 and can be adjusted by a gating network (Jacobs et al., 1991). We can interpret ω as a device whose role is to cluster input in K categories and assign each category to an expert. By forcing ωk (x) = K1 , ∀x, we have a simpler averaging model, sometimes called ensemble: p(y|x) = Y Once computed marginal log-probabilities, Eisner algorithm (Eisner, 1996), (Eisner, 1997) or Chu-Liu-Edmonds (McDonald et al., 2005) can be applied to solve MBR. 3.2 K 1 X pk (y|x) K k=1 Note that MoEs combine elementary probabilities, not tree scores: each expert energy is first normalized before the combination. A similar mixture is applied to labelling, i.e.: MBR Decoding with Labels In many dependency parsing models, decoding of arcs and labels is pipelined, see for instance (Dozat and Manning, 2017; Zhang et al., 2020a; Fossum and Knight, 2009): first arcs are decoded and then, with the decoded arcs, maximization is performed over labels: y ∗ = argmax p(y|x) then l∗ = argmax"
2021.iwpt-1.11,W16-5901,0,0.0228911,"Z(x) the partition function. In first-order models (Eisner, 1996), tree scores are sums of arc scores: X s(x, y) = s(h, d) (h,d)∈y Eisner (1997) generalizes scores to the secondorder by considering pairs of adjacent siblings: X X s(x, y) = s(h, d) + s(h, d1 , d2 ) (h,d)∈y (h,d1 ) (h,d2 )∈y with h &lt; d1 &lt; d2 or d2 &lt; d1 &lt; h. For projective first- or second-order models, Z(x) and p(y|x) are efficiently calculated (Zhang et al., 2020b).  Moreover marginal arc probability p (h, d)|x can be efficiently calculated from the partition function by applying backpropagation from log Z(x) to s(h, d), see (Eisner, 2016; Zmigrod et al., 2020; Zhang et al., 2020a): p((h, d)|x) = X p(y|x) = y∈Y(x) (h,d)∈y ∂ log Z(x) ∂s(h, d) Tree Labelling The labelling model is also a Boltzmann distribution: exp(s(l, h, d)) 0 l0 ∈L exp(s(l , h, d)) p(l|(h, d), x) = P where s(l, h, d) is the score for label l on (h, d). Following (Dozat and Manning, 2017; Zhang et al., 2020a), label predictions are independent: Y p(l(y)|y, x) = p(lhd |(h, d), x) (1) 1 Matrix-tree theorem could be used to adapt this work to non-projective first-order models (Smith and Smith, 2007) 107 (h,d)∈y Parse Probability Given the structure y and its labe"
2021.iwpt-1.11,P19-1012,0,0.0209921,"4.76 ±0.02 94.78 94.81 94.72 ±0.03 94.75 94.82 94.67 ±0.05 94.76 94.82 94.70 ±0.05 89.68 89.72 89.62 ±0.04 89.85 89.92 89.81 ±0.04 89.89 89.95 89.82 ±0.04 86.86 86.90 86.80 ±0.04 - - - - - - - - SOP CSOP BFOP CBFOP BSOP CBSOP 86.98 87.06 86.93 ±0.06 87.03 87.12 86.95 ±0.06 Table 4: Clustering Effect with K = 6 on dev, where CFOP, CSOP, CBSOP represent models after training We remark that on PTB similar performance on dev was achieved by FOP and SOP, with a slightly better UAS for SOP, which is expected by the capacity of the model to better represent structures. This corroborates findings of (Falenska and Kuhn, 2019). But this contradicts results for CoNLL09 where SOP always gives best results, in line with observations of Fonseca and Martins (2020). For BERT experiments on PTB, BSOP achieves better performance than BFOP with one or two experts. However, when the number of experts increases, BFOP outperforms BSOP. We complement our discussion with Figure 17 which depicts variance reduction by the number of 112 7 For CoNLL09, we found similar results. The figure is not shown for space limitation. experts in log-scale: almost linear of for all models, as predicted by our independence assumption. We note tha"
2021.iwpt-1.11,2020.acl-main.776,0,0.136194,"word embedding sequences. The scoring of arcs is then similar to (Dozat and Manning, 2017): lexical features are transformed for head or modifier roles by two feed-forward networks and combined to score arcs via a biaffine transformation. 4 https://github.com/kidlestar/MOE.git. For English we used Glove embeddings (Pennington et al., 2014), while for Chinese we extracted pretrained embeddings from the publicly available model of Zhang et al. (2020b). 5 On PTB, in order to compare with recent parsing results, we set up BFOP and BSOP (B for Bert), variants of the FOP and SOP settings: we follow Fonseca and Martins (2020) and concatenate an additional BERT embedding (Devlin et al., 2019) (the average of the 4 last layers of the bertbase-uncased model) to the embedding vector fed to the BiLSTM layers. Gating (mixture weights ω) is implemented by a K-class softmax over a feed-forward network whose input are the concatenation of initial and final contextual lexical feature vectors returned by the 3-layer BiLSTM. Hyper-parameters are set similarly to Zhang et al. (2020a), with the exception of the learning rate decreased to 10−4 and patience (that is the maximum number of epochs without LAS increase on the develop"
2021.iwpt-1.11,N09-2064,0,0.0535004,"ave a simpler averaging model, sometimes called ensemble: p(y|x) = Y Once computed marginal log-probabilities, Eisner algorithm (Eisner, 1996), (Eisner, 1997) or Chu-Liu-Edmonds (McDonald et al., 2005) can be applied to solve MBR. 3.2 K 1 X pk (y|x) K k=1 Note that MoEs combine elementary probabilities, not tree scores: each expert energy is first normalized before the combination. A similar mixture is applied to labelling, i.e.: MBR Decoding with Labels In many dependency parsing models, decoding of arcs and labels is pipelined, see for instance (Dozat and Manning, 2017; Zhang et al., 2020a; Fossum and Knight, 2009): first arcs are decoded and then, with the decoded arcs, maximization is performed over labels: y ∗ = argmax p(y|x) then l∗ = argmax p(l|y ∗ , x) l=l(y ∗ ) y∈Y(x) p(l(y)|y, x) = K X λk (x)pk (l(y)|y, x) However, solutions found this way are not the maximizers for p(l, y|x), as defined in Eq. 2. The problem is that the effect of labelling is not considered in arc decoding: a high probability arc can get picked up even with a low label score. First we remark that each label in l∗ is the most probable label l for a pair (h, d), denoted by Lhd = argmaxl∈L p(l|(h, d), x) . Decoding becomes: k=1 3"
2021.iwpt-1.11,W12-0503,0,0.0690324,"Missing"
2021.iwpt-1.11,P19-1237,0,0.0356402,"Missing"
2021.iwpt-1.11,P19-1228,0,0.0479332,"Missing"
2021.iwpt-1.11,N16-1030,0,0.0747882,"Missing"
2021.iwpt-1.11,W06-2903,0,0.0997915,"ments. 1 Introduction Combinations of elementary parsers are known to improve accuracy. Sometimes called joint systems, they often use different representations, i.e. lexicalized constituents and dependencies (Rush et al., ˇ 2010; Green and Zabokrtsk´ y, 2012; Le Roux et al., 2019; Zhou et al., 2020). These approaches have been devised to join the strengths and overcome the weaknesses of elementary systems. In this work, however, we follow another line of research consisting of mixtures and products of similar experts (Jacobs et al., 1991; Brown and Hinton, 2001), instantiated for parsing in (Petrov et al., 2006; Petrov, 2010) and especially appealing when individual experts have high variance, typically when training involves neural networks. Indeed Petrov (2010) used products of experts trained via Expectation-Maximization (a non-convex function minimization) converging to local minima. In this work we propose to study the combination of parsers, from a probabilistic point of view, as a mixture model, i.e. a learnable convex interpolation of probabilities. This has previously been studied in (Petrov et al., 2006) for PCFGs with the goal of overcoming the locality assumptions, and we want to see if"
2021.iwpt-1.11,K19-1023,1,0.890106,"Missing"
2021.iwpt-1.11,J93-2004,0,0.0814149,"t with (7) will In practice, this permitted reaching a lower loss generate increased gradient, and in turn increased value after training. probabilities, for experts with high value of ωk . 6 Experiments The two processes re-enforce each other and result quickly in an extreme partition between experts. Data We run experiments over two datasets for One may think that the degeneration problem can projective dependency parsing: The English Penn be alleviated with a smoothing prior or regulariza- Treebank (PTB) data with Stanford Dependencies tion. In practice, we tried entropy as regularization (Marcus et al., 1993) and CoNLL09 Chinese data to force towards a uniform distribution on ωk . We (Hajiˇc et al., 2009). We use standard train/dev/test found that a heavy entropy penalization is required splits and evaluate with UAS/LAS metrics. Custo avoid the degeneration problem, which makes tomarily, punctuation is ignored on PTB evaluation. ωk too uniform to be an accurate clustering device. Experts We run tests with first-order (FOP) and Avoid Extreme Partition Thus, to alleviate the second-order parsers (SOP) as mixture model exdegeneration problem without forcing a strong perts, with re-implemented version"
2021.iwpt-1.11,H05-1066,0,0.452916,"Missing"
2021.iwpt-1.11,2021.tacl-1.8,0,0.109099,"ply significance ttests (Dror et al., 2018) with level α = 0.05 to FOP, BFOP, SOP, BSOP with K = 6 against K = 1. For PTB and CoNLL09, p-value is always smaller than 0.005. We note that for parsers without BERT, averaging can achieve a considerable improvement with SOP and gives new SOTA. We also point out that, if FOP and SOP could find equivalently good models on dev, SOP models seem to better generalize. For parsers with BERT, with a simple averaging of BSOP, we achieve comparable performances (or even better in case of LAS) when comparing to more involved methods such as (Li et al., 2020; Mohammadshahi and Henderson, 2021). It remains to be seen whether they can also benefit from MoEs. Regarding clustering, even if we obtained an average improvement on dev, test data hardly benefits from it. Still, we note a small improvement of UAS on SOP CoNLL09. Finally we stress that best performing settings on PTB test, namely BSOP and CBSOP, were not better performing than BFOP and CBFOP on development data on average (although max systems were similar): second-order models seem to slightly better handle unseen data. 6.5 Parallel Training and Decoding Training averaging ensembles can be paralleled with sufficient GPUs, si"
2021.iwpt-1.11,2020.tacl-1.15,0,0.0264336,"Missing"
2021.iwpt-1.11,D14-1162,0,0.0850483,"nable and looked-up in a table, and the third is computed by a BiLSTM at the character level (CharLSTM). The first two embeddings are summed and concatenated with the char sequence embedding. For FOP and SOP, contextual lexical features are the results of 3-layer BiLSTMs applied to word embedding sequences. The scoring of arcs is then similar to (Dozat and Manning, 2017): lexical features are transformed for head or modifier roles by two feed-forward networks and combined to score arcs via a biaffine transformation. 4 https://github.com/kidlestar/MOE.git. For English we used Glove embeddings (Pennington et al., 2014), while for Chinese we extracted pretrained embeddings from the publicly available model of Zhang et al. (2020b). 5 On PTB, in order to compare with recent parsing results, we set up BFOP and BSOP (B for Bert), variants of the FOP and SOP settings: we follow Fonseca and Martins (2020) and concatenate an additional BERT embedding (Devlin et al., 2019) (the average of the 4 last layers of the bertbase-uncased model) to the embedding vector fed to the BiLSTM layers. Gating (mixture weights ω) is implemented by a K-class softmax over a feed-forward network whose input are the concatenation of init"
2021.iwpt-1.11,D10-1001,0,0.121496,"Missing"
2021.iwpt-1.11,D07-1014,0,0.338296,"tition function by applying backpropagation from log Z(x) to s(h, d), see (Eisner, 2016; Zmigrod et al., 2020; Zhang et al., 2020a): p((h, d)|x) = X p(y|x) = y∈Y(x) (h,d)∈y ∂ log Z(x) ∂s(h, d) Tree Labelling The labelling model is also a Boltzmann distribution: exp(s(l, h, d)) 0 l0 ∈L exp(s(l , h, d)) p(l|(h, d), x) = P where s(l, h, d) is the score for label l on (h, d). Following (Dozat and Manning, 2017; Zhang et al., 2020a), label predictions are independent: Y p(l(y)|y, x) = p(lhd |(h, d), x) (1) 1 Matrix-tree theorem could be used to adapt this work to non-projective first-order models (Smith and Smith, 2007) 107 (h,d)∈y Parse Probability Given the structure y and its labelling l(y), the parse probability is:   p l(y), y|x = p(y|x) × p l(y)|y, x (2) Learning Potential functions s can be implemented by feed-forward neural networks or biaffine functions (Dozat and Manning, 2017), and parameterized by maximizing a log-likelihood. 2.3 Mixture and Averaging The intuition behind MBR is that instead of maximizing the probability of the parse tree, we try to minimize the risk of choosing wrong arcs, i.e. to maximize the arc marginals in the parse tree: For arborescence probabilities a finite mixture mod"
2021.iwpt-1.11,2020.acl-main.302,0,0.102215,"-based dependency parser, the tree probability is defined as: p(y|x) = exp(s(x, y)) P Z(x) ≡ y0 ∈Y(x) exp(s(x, y 0 )) with s(x, y) the tree energy giving the correctness of y for x, and Z(x) the partition function. In first-order models (Eisner, 1996), tree scores are sums of arc scores: X s(x, y) = s(h, d) (h,d)∈y Eisner (1997) generalizes scores to the secondorder by considering pairs of adjacent siblings: X X s(x, y) = s(h, d) + s(h, d1 , d2 ) (h,d)∈y (h,d1 ) (h,d2 )∈y with h &lt; d1 &lt; d2 or d2 &lt; d1 &lt; h. For projective first- or second-order models, Z(x) and p(y|x) are efficiently calculated (Zhang et al., 2020b).  Moreover marginal arc probability p (h, d)|x can be efficiently calculated from the partition function by applying backpropagation from log Z(x) to s(h, d), see (Eisner, 2016; Zmigrod et al., 2020; Zhang et al., 2020a): p((h, d)|x) = X p(y|x) = y∈Y(x) (h,d)∈y ∂ log Z(x) ∂s(h, d) Tree Labelling The labelling model is also a Boltzmann distribution: exp(s(l, h, d)) 0 l0 ∈L exp(s(l , h, d)) p(l|(h, d), x) = P where s(l, h, d) is the score for label l on (h, d). Following (Dozat and Manning, 2017; Zhang et al., 2020a), label predictions are independent: Y p(l(y)|y, x) = p(lhd |(h, d), x) (1)"
2021.iwpt-1.11,2020.findings-emnlp.398,0,0.0125082,"ial, clustering requires good initialization and stabilization techniques, but its advantages over mere averaging seem to eventually vanish when enough experts are present. As a by product, we show how this leads to state-of-the-art results on the PTB and the CoNLL09 Chinese treebank, with low variance across experiments. 1 Introduction Combinations of elementary parsers are known to improve accuracy. Sometimes called joint systems, they often use different representations, i.e. lexicalized constituents and dependencies (Rush et al., ˇ 2010; Green and Zabokrtsk´ y, 2012; Le Roux et al., 2019; Zhou et al., 2020). These approaches have been devised to join the strengths and overcome the weaknesses of elementary systems. In this work, however, we follow another line of research consisting of mixtures and products of similar experts (Jacobs et al., 1991; Brown and Hinton, 2001), instantiated for parsing in (Petrov et al., 2006; Petrov, 2010) and especially appealing when individual experts have high variance, typically when training involves neural networks. Indeed Petrov (2010) used products of experts trained via Expectation-Maximization (a non-convex function minimization) converging to local minima."
C08-3003,C04-1044,1,0.86677,"rsing process as it is used to control syntactic composition. This principle can also be used to filter lexical selections. For a input sentence, a lexical selection is a choice of an elementary tree from the anchored grammar for each word of the sentence. Indeed, the number of possible lexical selections may present an exponential complexity in the length of the sentence. A way of filtering them consists in abstracting some information from the initial formalism F to a new formalism Fabs . Then, parsing in Fabs allows to eliminate wrong lexical selections at a minimal cost (Boullier, 2003). (Bonfante et al., 2004) shows that polarities allow original methods of abstraction. Following this idea, the lexical disambiguation module checks the global neutrality of every lexical selection for each polarized feature: a set of trees bearing negative and positive polarities can only be reduced to a neutral tree if the sum of the negative polarities for each feature equals the sum of its positive polarities. Counting the sum of positive and negative features can be done in a compact way by using an automaton. This automaton structure allows to share all paths that have the same global polarity balance (Bonfante"
C08-3003,W03-3006,0,0.0195939,"echanism in the parsing process as it is used to control syntactic composition. This principle can also be used to filter lexical selections. For a input sentence, a lexical selection is a choice of an elementary tree from the anchored grammar for each word of the sentence. Indeed, the number of possible lexical selections may present an exponential complexity in the length of the sentence. A way of filtering them consists in abstracting some information from the initial formalism F to a new formalism Fabs . Then, parsing in Fabs allows to eliminate wrong lexical selections at a minimal cost (Boullier, 2003). (Bonfante et al., 2004) shows that polarities allow original methods of abstraction. Following this idea, the lexical disambiguation module checks the global neutrality of every lexical selection for each polarized feature: a set of trees bearing negative and positive polarities can only be reduced to a neutral tree if the sum of the negative polarities for each feature equals the sum of its positive polarities. Counting the sum of positive and negative features can be done in a compact way by using an automaton. This automaton structure allows to share all paths that have the same global po"
C08-3003,P06-1018,0,0.489469,"Missing"
C08-3003,C96-2120,0,0.582231,"Missing"
C14-1177,P05-1038,0,0.156779,"a degree of relatedness, which can be learnt on a corpus. Some recent approaches use sequence labellers. The linear chain CRF model (Lafferty et al., 2001) is widely used, see for example (Vincze et al., 2011; Constant and Tellier, 2012). It has proven to be a very adequate model: it is flexible enough to incorporate information from labelled data and external resources (POS taggers, compound lexicons or named entity recognisers). The compound recognition may also be directly performed by syntactic parsers learnt from corpora where compounds are marked, such as the one we use in this paper1 (Arun and Keller, 2005; Green et al., 2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory. Green et al. (2011) experimentally show that a lexicalised model is better than an unlexicalised one. On the other hand, Constant et al. (2013b) show that, using products of PCFG-LA (Petrov, 2010), unlexicalised models can be more accurate. They obtain performance on a par with a linear chain CRF system without external information. But such information is difficult to incorporate directly in a PCFG-LA model. Constant et al. (2012) resort to a reranker to add arbitrary features in the parse"
C14-1177,W13-4916,0,0.013811,"E Recall 82.01 82.36 82.48 82.91 83.38 82.56 82.73 Precision 82.37 82.59 82.73 83.07 83.42 82.63 82.64 Fscore 82.19 82.47 82.61 82.99 83.40 82.59 82.69 EX 18.06 19.22 19.19 19.19 20.73 18.79 20.02 Tag 97.35 97.73 97.84 97.41 97.85 97.39 97.57 Table 3: Parse evaluation on dev set (recall, precision and F-score, exactness and POS tagging). Table 4 shows evaluation results of our best system and comparisons with baseline or alternative configurations on the SPMRL 2013 test set. Parsing The DD method performs better than our baseline, and better than the best system in the SPMRL 2013 shared task (Björkelund et al., 2013). This system is a pipeline consisting of a morpho-syntactic tagger with a very rich and informative tag set, a product of PCFG-LAs, and a parse reranker. Although this approach is quite different from ours, we believe our system is more accurate overall because our method is more resilient to an error from one of its components. Compound recognition and labelling For the task of recognition alone, where only the frontiers are evaluated, the DD combinations of CRFs performs better than the best single CRF which itself performs better than the parser alone, but the complete architecture is agai"
C14-1177,constant-tellier-2012-evaluating,1,0.855259,"onal to the SPMRL shared task data (Seddah et al., 2013), but were also used in (Constant et al., 2013a) for the shared task. 3 Compound Recognition 3.1 Related Work The compound recognition traditionally relies on 2 types of information: lexical and syntactic clues. A strong lexical association between the tokens of a compound can be detected using a compound dictionary or by measuring a degree of relatedness, which can be learnt on a corpus. Some recent approaches use sequence labellers. The linear chain CRF model (Lafferty et al., 2001) is widely used, see for example (Vincze et al., 2011; Constant and Tellier, 2012). It has proven to be a very adequate model: it is flexible enough to incorporate information from labelled data and external resources (POS taggers, compound lexicons or named entity recognisers). The compound recognition may also be directly performed by syntactic parsers learnt from corpora where compounds are marked, such as the one we use in this paper1 (Arun and Keller, 2005; Green et al., 2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory. Green et al. (2011) experimentally show that a lexicalised model is better than an unlexicalised one. On the othe"
C14-1177,P12-1022,1,0.924046,"compounds are marked, such as the one we use in this paper1 (Arun and Keller, 2005; Green et al., 2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory. Green et al. (2011) experimentally show that a lexicalised model is better than an unlexicalised one. On the other hand, Constant et al. (2013b) show that, using products of PCFG-LA (Petrov, 2010), unlexicalised models can be more accurate. They obtain performance on a par with a linear chain CRF system without external information. But such information is difficult to incorporate directly in a PCFG-LA model. Constant et al. (2012) resort to a reranker to add arbitrary features in the parse selection process, but their system showed inferior performance compared with a CRF model with access to the same external information. 1 Such an approach has been used already for joint named entity recognition and parsing based on CRF (Finkel and Manning, 2009). 1876 3.2 Annotation schemes Compound recognition can be seen as a segmentation task which consists in assigning to each token a label with segmentation information. We use label B if the token is the beginning of a word (single or compound), and label I if the token is insi"
C14-1177,W13-4905,1,0.862392,"sing from the training corpus. These resources are dictionaries, consisting of triplets (flexed form, lemma, POS tag), where form and lemma may be compound or simple. Several such dictionaries exist for French. We use: • DELA (Courtois et al., 1997) contains a million entries, among which 110,000 are compounds; • Lefff (Sagot, 2010) contains 500,000 entries, among which 25,000 are compounds; • Prolex (Piton et al., 1999) is a toponym dictionary with approximately 100,000 entries. The described resources are additional to the SPMRL shared task data (Seddah et al., 2013), but were also used in (Constant et al., 2013a) for the shared task. 3 Compound Recognition 3.1 Related Work The compound recognition traditionally relies on 2 types of information: lexical and syntactic clues. A strong lexical association between the tokens of a compound can be detected using a compound dictionary or by measuring a degree of relatedness, which can be learnt on a corpus. Some recent approaches use sequence labellers. The linear chain CRF model (Lafferty et al., 2001) is widely used, see for example (Vincze et al., 2011; Constant and Tellier, 2012). It has proven to be a very adequate model: it is flexible enough to incor"
C14-1177,N09-1037,0,0.194197,"l. (2013b) show that, using products of PCFG-LA (Petrov, 2010), unlexicalised models can be more accurate. They obtain performance on a par with a linear chain CRF system without external information. But such information is difficult to incorporate directly in a PCFG-LA model. Constant et al. (2012) resort to a reranker to add arbitrary features in the parse selection process, but their system showed inferior performance compared with a CRF model with access to the same external information. 1 Such an approach has been used already for joint named entity recognition and parsing based on CRF (Finkel and Manning, 2009). 1876 3.2 Annotation schemes Compound recognition can be seen as a segmentation task which consists in assigning to each token a label with segmentation information. We use label B if the token is the beginning of a word (single or compound), and label I if the token is inside a compound, but not in initial position. This lexical segmentation can be enriched with additional information, for example POS tags of compounds or tokens in compounds, and gives a variety of tag sets. This leads us to define 5 simple tag sets for our problem, each with very simple information, that will be combined in"
C14-1177,D11-1067,0,0.159424,"Missing"
C14-1177,J13-1009,0,0.27724,"Missing"
C14-1177,D10-1125,0,0.045533,"raints between phrase-structure parsers and CRF sequence labellers. In order to build a joint system we use dual decomposition, a way to combine several elementary systems which has proven successful in various NLP tasks. We evaluate this proposition on the French SPMRL corpus. This method compares favorably with pipeline architectures and improves state-of-the-art results. 1 Introduction Dual decomposition (DD), which can be used as a method to combine several elementary systems, has already been successfully applied to many NLP tasks, in particular syntactic parsing, see (Rush et al., 2010; Koo et al., 2010) inter alia. Intuitively, the principle can be described quite simply: at decoding time, the combined systems seek for a consensus on common subtasks, in general the prediction of some parts of the overall structure, via an iterative process imposing penalties where the systems disagree. If the systems converge to a solution, it is formally guaranteed to be optimal. Besides, this approach is quite flexible and easy to implement. One can add or remove elementary systems without rebuilding the architecture from the ground up. Moreover, the statistical models for the subsystems can often be estim"
C14-1177,P10-1052,0,0.0126854,"(t)  P1≤d≤n F yd(t)  (t)  P1≤d≤n D yd(t) c IB (t) c BI (t) − ; ∆c ← αt F y − 9: ∆c ← αt D y n n (t+1) (t) (t) 10: ΛBI ← ΛcBI + ∆BI ; ΛIB c c c 11: end for BI (t) IB (t) 12: if ∆c = 0 and ∆c = 0 for all c then 13: Exit loop 14: end if 15: end for (t) (t) 16: return (y 1 , · · · , y n ) (t+1) ← ΛIB c (t) + ∆IB c (t) We set the maximum number of iteration τ to 1000. For the step size, we use a common heuristic: 1 where k is the number of times that (Du) has increased between two successive iterations. αt = 1+k 4.3 Experimental results for CRF combinations We modified the wapiti software (Lavergne et al., 2010) with Algorithm 2. Table 1 reports segmentation results on the development set with the different tag sets, the best DD combination, and the best voting system.8 Tag Set CRF / combination partial-internal partial basic complete complete-internal MWE basic complete partial-internal vote (basic complete partial-internal) Recall 79.59 78.98 79.74 79.69 79.03 80.82 80.49 Precision 85.49 85.57 84.65 83.10 82.66 86.07 85.46 F-score 82.44 82.14 82.12 81.36 80.80 83.36 82.90 Table 1: Segmentation scores of CRF systems (dev) System complete MWE F-score (all) 94.29 94.59 F-score (compounds) 78.32 80.00"
C14-1177,D13-1116,1,0.899373,"Missing"
C14-1177,N10-1003,0,0.0269742,"incorporate information from labelled data and external resources (POS taggers, compound lexicons or named entity recognisers). The compound recognition may also be directly performed by syntactic parsers learnt from corpora where compounds are marked, such as the one we use in this paper1 (Arun and Keller, 2005; Green et al., 2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory. Green et al. (2011) experimentally show that a lexicalised model is better than an unlexicalised one. On the other hand, Constant et al. (2013b) show that, using products of PCFG-LA (Petrov, 2010), unlexicalised models can be more accurate. They obtain performance on a par with a linear chain CRF system without external information. But such information is difficult to incorporate directly in a PCFG-LA model. Constant et al. (2012) resort to a reranker to add arbitrary features in the parse selection process, but their system showed inferior performance compared with a CRF model with access to the same external information. 1 Such an approach has been used already for joint named entity recognition and parsing based on CRF (Finkel and Manning, 2009). 1876 3.2 Annotation schemes Compoun"
C14-1177,D10-1001,0,0.450467,"epresented as constraints between phrase-structure parsers and CRF sequence labellers. In order to build a joint system we use dual decomposition, a way to combine several elementary systems which has proven successful in various NLP tasks. We evaluate this proposition on the French SPMRL corpus. This method compares favorably with pipeline architectures and improves state-of-the-art results. 1 Introduction Dual decomposition (DD), which can be used as a method to combine several elementary systems, has already been successfully applied to many NLP tasks, in particular syntactic parsing, see (Rush et al., 2010; Koo et al., 2010) inter alia. Intuitively, the principle can be described quite simply: at decoding time, the combined systems seek for a consensus on common subtasks, in general the prediction of some parts of the overall structure, via an iterative process imposing penalties where the systems disagree. If the systems converge to a solution, it is formally guaranteed to be optimal. Besides, this approach is quite flexible and easy to implement. One can add or remove elementary systems without rebuilding the architecture from the ground up. Moreover, the statistical models for the subsystems"
C14-1177,sagot-2010-lefff,0,0.109492,"SUJ V ADV VP . situation est complètement bloquée Figure 1: Syntatic annotation in the SPRML FTB: So far, the situation has been completely blocked. The sequence labellers used in the experiments are able to exploit external lexical resources that will help coping with data missing from the training corpus. These resources are dictionaries, consisting of triplets (flexed form, lemma, POS tag), where form and lemma may be compound or simple. Several such dictionaries exist for French. We use: • DELA (Courtois et al., 1997) contains a million entries, among which 110,000 are compounds; • Lefff (Sagot, 2010) contains 500,000 entries, among which 25,000 are compounds; • Prolex (Piton et al., 1999) is a toponym dictionary with approximately 100,000 entries. The described resources are additional to the SPMRL shared task data (Seddah et al., 2013), but were also used in (Constant et al., 2013a) for the shared task. 3 Compound Recognition 3.1 Related Work The compound recognition traditionally relies on 2 types of information: lexical and syntactic clues. A strong lexical association between the tokens of a compound can be detected using a compound dictionary or by measuring a degree of relatedness,"
C14-1177,R11-1040,0,0.0869126,"resources are additional to the SPMRL shared task data (Seddah et al., 2013), but were also used in (Constant et al., 2013a) for the shared task. 3 Compound Recognition 3.1 Related Work The compound recognition traditionally relies on 2 types of information: lexical and syntactic clues. A strong lexical association between the tokens of a compound can be detected using a compound dictionary or by measuring a degree of relatedness, which can be learnt on a corpus. Some recent approaches use sequence labellers. The linear chain CRF model (Lafferty et al., 2001) is widely used, see for example (Vincze et al., 2011; Constant and Tellier, 2012). It has proven to be a very adequate model: it is flexible enough to incorporate information from labelled data and external resources (POS taggers, compound lexicons or named entity recognisers). The compound recognition may also be directly performed by syntactic parsers learnt from corpora where compounds are marked, such as the one we use in this paper1 (Arun and Keller, 2005; Green et al., 2011; Green et al., 2013; Constant et al., 2013b), but these results are contradictory. Green et al. (2011) experimentally show that a lexicalised model is better than an u"
C14-1177,W13-4917,0,\N,Missing
D13-1116,W10-1408,1,0.908254,"Missing"
D13-1116,A00-2031,0,0.071062,"t overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al., 2007). We recover function labels as part of the parsing process, and use dual decomposition to combine parsing models with and without function labels. We are not aware of any other work that leverages the benefits of both types of models. ber of latent annotations increases beyond two. Hall and Klein (2012) are forced to use head binarization when combining their lexicalized and unlexicalized parsers. Dual decomposition allows us to combine models with different binarization schemes. 3 Approximation of PCFG-LAs as Linear Models In this section, we explain how we can use PC"
D13-1116,D12-1133,0,0.0842281,"alse option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left DD Right Bin vs. Func Right DD Right Bin vs. No Func Right DD Left Bin vs."
D13-1116,cer-etal-2010-parsing,0,0.0718255,"Missing"
D13-1116,P12-1024,0,0.0205272,"= (N , T , H, RH , S, p) where: • N is a set of observed non-terminals, among which S is the distinguished initial symbol, PCFG-LA parsing amounts to, given a sequence of words, finding the most probable skeletal tree with this sequence as its yield according to a grammar G: T • T is a set of terminals (words), • H is a set of latent annotations or hidden states, • RH is a set of annotated rules, of the form a[h1 ] → b[h2 ] c[h3 ] for internal rules1 and a[h1 ] → w for lexical rules. Here a, b, c ∈ N are non-terminals, w ∈ T is a terminal and h1 , h2 , h3 ∈ H are latent annotations. Following Cohen et al. (2012) we also define the set of skeletal rules R, in other words, rules without hidden states, of the form a → b c or a → w. • p : RH → R≥0 defines the probabilities associated with rules conditioned on their left-hand side. Like Petrov and Klein (2007), we impose that the initial symbol S has only one latent annotation. In other words, among rules with S on the left-hand side, only those of the form S[0] → γ are in RH . With such a grammar G we can define probabilities over trees in the following way. We will consider two types of trees, annotated trees and skeletal trees. An annotated tree is a s"
D13-1116,W08-1301,0,0.136235,"Missing"
D13-1116,I11-1100,1,0.869851,"Missing"
D13-1116,N06-1024,0,0.193537,"accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al., 2007). We recover function labels as part of the parsing process, and use dual decomposition to combine parsing models with and without function labels. We are not aware of any other work that leverages the benefits of both types of models. ber of latent annotations inc"
D13-1116,D12-1105,0,0.0643145,", 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. 2 Grammar Binarization Matsuzaki et al. (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achie"
D13-1116,W99-0623,0,0.0791198,"grammars which encode different function label/binarization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must u"
D13-1116,P08-1067,0,0.212547,"ghtBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left DD Right Bin vs. Func Right DD Right Bin vs. No Fu"
D13-1116,W07-2416,0,0.0754764,"Missing"
D13-1116,D10-1125,0,0.656702,"of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept – or discarded – in order to guide the learning algorithm. These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture. Recently Rush et al. (2010), Martins et al. (2011) and Koo et al. (2010) have shown that Dual Decomposition or Lagrangian Relaxation is an elegant framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA) in order to compare ou"
D13-1116,H94-1020,0,0.170918,"In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. 2 Grammar Binarization Matsuzaki et al. (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover t"
D13-1116,D11-1022,0,0.343438,"cause of the large amount of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept – or discarded – in order to guide the learning algorithm. These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture. Recently Rush et al. (2010), Martins et al. (2011) and Koo et al. (2010) have shown that Dual Decomposition or Lagrangian Relaxation is an elegant framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA)"
D13-1116,P05-1010,0,0.288366,"odels (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. 2 Grammar Binarization Matsuzaki et al. (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trai"
D13-1116,H05-1078,0,0.030401,"ave a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al., 2007). We recover function labels as part of the parsing process, and use dual decomposition to combine parsing models with and without function labels. We are not aware of any other work that leverages the benefits of both types of models. ber"
D13-1116,P08-1108,0,0.0283388,"reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we employ PCFG-LA product models with dual decomposition, and"
D13-1116,C10-1094,0,0.0453386,"Missing"
D13-1116,N07-1051,0,0.676186,"yield according to a grammar G: T • T is a set of terminals (words), • H is a set of latent annotations or hidden states, • RH is a set of annotated rules, of the form a[h1 ] → b[h2 ] c[h3 ] for internal rules1 and a[h1 ] → w for lexical rules. Here a, b, c ∈ N are non-terminals, w ∈ T is a terminal and h1 , h2 , h3 ∈ H are latent annotations. Following Cohen et al. (2012) we also define the set of skeletal rules R, in other words, rules without hidden states, of the form a → b c or a → w. • p : RH → R≥0 defines the probabilities associated with rules conditioned on their left-hand side. Like Petrov and Klein (2007), we impose that the initial symbol S has only one latent annotation. In other words, among rules with S on the left-hand side, only those of the form S[0] → γ are in RH . With such a grammar G we can define probabilities over trees in the following way. We will consider two types of trees, annotated trees and skeletal trees. An annotated tree is a sequence of rules from RH , while a skeletal tree is a sequence of skeletal rules from R. An annotated tree TH is obtained by left-most derivation from S[0]. Its probability is: 1 For brevity and without loss of generality, we omit unary and n-ary r"
D13-1116,D10-1069,0,0.0319238,"No Func DD3 DD4 Stanford LAS UAS 92.18 94.32 92.03 94.47 91.86 94.06 91.83 94.29 92.56 94.60 92.01 94.38 92.19 94.36 92.19 94.57 92.77 94.79 92.59 94.62 LAS 89.51 65.31 89.28 65.33 89.81 89.62 89.67 65.44 90.04 89.95 LTH UAS 93.92 92.22 93.75 92.18 94.17 94.05 94.06 92.37 94.33 94.24 p2m UAS 94.2 94.2 93.9 94.1 94.5 94.2 94.2 94.3 94.5 94.4 Table 4: Dependency accuracies on the dev set Dependency-based evaluation of phrase structure parser output has been used in recent years to provide a more rounded view on parser performance and to compare with direct dependency parsers (Cer et al., 2010; Petrov et al., 2010; Nivre et al., 2010; Foster et al., 2011; Petrov and McDonald, 2012). We evaluate our various parsing models on their ability to recover three types of dependencies: basic Stanford dependencies (de Marneffe and Manning, 2008)5 , LTH dependencies (Johansson and Nugues, 5 We used the latest version at the time of writing, i.e. 3.20. 1166 2007)6 and penn2malt dependencies.7 The latter are a simpler version of the LTH dependencies but are still used when reporting unlabeled attachment scores for dependency parsing. The results, shown in Table 4, mirror the constituency evaluation results in that"
D13-1116,N10-1003,0,0.231563,"well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA) in order to compare our approach with a strong baseline of high-quality parses. Dual Decomposition is used to mix several systems (between two and four) that may in turn be combinations of grammars, here products of PCFG-LAs (Petrov, 2010). The systems being combined make different choices with regard to i) function labels and ii) grammar binarization. Common sense would suggest that information in the form of function labels – syntactic labels such as SBJ and PRD and semantic labels such as TMP and LOC – might help in obtaining a fine-grained analysis. On the other hand, the independence hypothe1158 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1158–1169, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics sis on which CFGs rely and on w"
D13-1116,D10-1001,0,0.659077,"tion Introduction Because of the large amount of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept – or discarded – in order to guide the learning algorithm. These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture. Recently Rush et al. (2010), Martins et al. (2011) and Koo et al. (2010) have shown that Dual Decomposition or Lagrangian Relaxation is an elegant framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent"
D13-1116,D07-1111,0,0.0241372,"UAS) of 94.3 on Section 23 of the Wall Street Journal) are obtained by combining three grammars which encode different function label/binarization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this"
D13-1116,P12-1046,0,0.0554704,"se it in our experiments. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left"
D13-1116,E03-1008,0,0.0183593,"eled attachment score (UAS) of 94.3 on Section 23 of the Wall Street Journal) are obtained by combining three grammars which encode different function label/binarization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall a"
D13-1116,N10-1091,0,0.0184497,"rization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we emp"
D13-1116,P07-1031,0,0.0314891,"results with state-of-the-art systems (the second half of Table 5). We present parser accuracy results, measured using Parseval F-score and penn2malt UAS, and, for our systems, function label accuracy for labels produced during parsing and after parsing using Funtag. We also carried out statistical significance testing8 on the F-score differences between our various systems on the development and test sets. The results 6 nlp.cs.lth.se/software/treebank_converter. It is recommended that LTH is used with the version of the Penn Treebank which contains the more detailed NP bracketing provided by Vadas and Curran (2007). However, to facilitate comparison with other parsers and dependency schemes, we did not use it in our experiments. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right"
D13-1116,W03-3023,0,0.028621,"r various systems on the development and test sets. The results 6 nlp.cs.lth.se/software/treebank_converter. It is recommended that LTH is used with the version of the Penn Treebank which contains the more detailed NP bracketing provided by Vadas and Curran (2007). However, to facilitate comparison with other parsers and dependency schemes, we did not use it in our experiments. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 –"
D13-1116,D09-1161,0,0.113831,"ts. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left DD Right Bin vs. Func"
D15-1157,W09-3821,0,0.114961,"Missing"
D15-1157,W13-1703,0,0.0201367,"asure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared task"
D15-1157,P03-1054,0,0.0826799,"Missing"
D15-1157,W11-2838,0,0.0263959,"of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae"
D15-1157,W12-2006,0,0.0143893,"er, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) develo"
D15-1157,D14-1108,0,0.231583,"English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French technical support forums.1 The phrase"
D15-1157,H94-1020,0,0.246303,"of a PCFG-LA parser on Foreebank, examining the effect of grammatical errors on parsing and experimenting with different training sets. 2 Related Work Other treebanks of English web text include the English Web Treebank (aka Google Web Treebank) (Mott et al., 2012), the small treebank of tweets and football discussion forum posts described in Foster et al. (2011) and the tweet dependency bank described in Kong et al. (2014). The English Web Treebank is a corpus of over 250K words, selected from blogs, newsgroups, emails, local business reviews and Yahoo! answers. It adapts the Penn Treebank (Marcus et al., 1994) and Switchboard (Taylor, 1996) annotation guidelines to address the phenomena specific 2 www.computing.dcu.ie/mt/confidentmt. html 1341 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. to this type of text. The annotation of the 1000sentence treebank described in Foster et al. (2011) is based on the Penn Treebank, whereas the annotation of the treebank described in Kong et al. (2014) is dependency-based. The French Social Media Bank developed by Sed"
D15-1157,P11-1121,0,0.180933,"(2010), and annotate it as is. They use two POS tags and two dependency labels for error cases: one for the surface form and one for the intended form. Ros´en and De Smedt (2010) criticise the approach of Dickinson and Ragheb (2009) involving “annotating language text as is” arguing that interpretation of the language is required at all annotation levels. They use NorGram, a Norwegian LexicalFunctional Grammar, to annotate a learner corpus with constituency structure, functional structure and semantic structure, in order to provide a means to search for contexts in which learner errors occur. Nagata et al. (2011) describe an English learner corpus which has been manually annotated with shallow syntax, introducing two new POS tags and two new chunk labels for errors. 3 Building the Foreebank The Foreebank treebank contains 1000 English sentences and 1000 French sentences. The English sentences come from the Symantec Norton technical support user forum. Half of the French sentences come from the French Norton forum and the other half are human translations of sentences from the English forum. Four annotators were involved in the annotation process. Their main task was to correct automatically parsed phr"
D15-1157,N10-1060,1,0.910493,"Missing"
D15-1157,N07-1051,0,0.0161872,"n of the tokens in both data sets. We also calculate the edit distance between each Foreebank sentence and its correction by summing the number of error suffixes and dividing by the maximum of the original and corrected sentence lengths. The average edit distance for the English section of Foreebank is 0.04 and for the French section is 0.03. Despite the existence of some near-to-incomprehensible sentences, the overall error level is very low. 5 Parsing the Foreebank We first evaluate newswire-trained parsers on Foreebank, using our in-house PCFG-LA parser with the max-rule parsing algorithm (Petrov and Klein, 2007) and 6 split-merge cycles. The English model is trained on the entire WSJ and the French model on the entire FTB. For comparison, we parse the WSJ/FTB and so we additionally use models trained only on the training sections. We remove the error suffixes and any Dsuffixed nodes (representing deleted words) from the gold Foreebank trees before evaluation. The results are shown in Table 3. As expected, we see a significant drop for both languages when we move from in-domain data to Foreebank. Compared to parsing the English side of Foreebank, the performance drop for French is relatively smaller:"
D15-1157,P06-1055,0,0.16059,"Missing"
D15-1157,W07-0604,0,0.0355908,", 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) developed for first language learners. They treat the developing language of learners as an interlanguage, as suggested by D´ıaz-Negrillo et al. (2010), and annotate it as is. They use two POS tags and two dependency labels for error cases: one for the surface form and one for the intended form. Ros´en and De Smedt (2010) criticise the approach of Dickinson and Ragheb (2009) involving “annotating language text as is” arguing that interpretation of the language is required at all annotation levels. They use NorGram, a Norwegian LexicalFunctional Grammar, to annotate a learner corpus with const"
D15-1157,C12-1149,0,0.520114,"om Abstract We present a new treebank of English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French"
D15-1157,P11-1019,0,0.0216086,"ght into this type of text but it also enables us to directly measure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et"
D15-1157,W14-1701,0,\N,Missing
D15-1157,N13-1037,0,\N,Missing
D15-1157,P08-2056,1,\N,Missing
D17-1172,P13-1020,0,0.0200053,"{V0 , . . . , Vn } and V = ∪nk=0 Vk . For every couple u, v of vertices such that u ∈ Vh and v ∈ Vm , h 6= m and m 6= 0, we associate an arc uv corresponding to the best adjunction of the root of spine sm associated with v of Vm to spine sh associated with vertex u of Vh . The weight of this arc is given by φuv = α(h, m; w) + ν(sm ; h, m, w) + max γ(lhm ; h, m, sh , w) the arc from V3 to V2 represents the adjunction of spine NP-PRP to spine S-VP-VB at index 0. 4 Efficient Decoding Lagrangian relaxation has been successfully applied to various NLP tasks (Koo et al., 2010; Le Roux et al., 2013; Almeida and Martins, 2013; Das et al., 2012; Corro et al., 2016). Intuitively, given an integer linear program, it consists in relaxing some linear constraints which make the program difficult to solve and penalizing their violation in the objective function. We propose a new decoding method for GMSA based on dual decomposition, a special flavor of Lagrangian relaxation where the problem is decomposed in several independent subproblems. 4.1 lhm which is the score of the best adjunction of sm to sh . This ends the construction of (D, π, φ). There is a 1-to-1 correspondence between the solutions to GMSA and those to the"
D17-1172,D09-1021,0,0.0633449,"Missing"
D17-1172,W08-2102,0,0.0556909,"Missing"
D17-1172,E17-1118,0,0.263046,"Missing"
D17-1172,P97-1003,0,0.685688,"t0 (γ 0 ) exp ont,h,m (γ 00 ) Pγ 00 (a|h, m, t) = P exp ot,h,m a0 (γ 00 ) exp oa0 ,h,m Experiments We ran a series of experiments on two corpora annotated with discontinuous constituents. English We used an updated version of the Wall Street Journal part of the Penn Treebank corpus (Marcus et al., 1994) which introduces discontinuity (Evang and Kallmeyer, 2011). Sections 2-21 are used for training, 22 for developpement and 23 for testing. We used gold and predicted POS tags by the Stanford tagger,10 trained with 10jackknifing. Dependencies are extracted following the head-percolation table of Collins (1997). German We used the Tiger corpus (Brants et al., 2004) with the split defined for the SPMRL 2014 shared task (Maier, 2015; Seddah et al., 2013). Following Maier (2015) and Coavoux and Crabb´e (2017), we removed sentences number 46234 and 50224 as they contain annotation errors. We only used the given gold POS tags. Dependencies are extracted following the head-percolation table distributed with Tulipa (Kallmeyer et al., 2008). We emphasize that long sentences are not filtered out. Our derivation extraction algorithm is similar to the one proposed in Carreras et al. (2008). Regarding decoding,"
D17-1172,P16-1034,1,0.911144,"Missing"
D17-1172,S12-1029,0,0.0631704,"Missing"
D17-1172,P81-1022,0,0.761475,"Missing"
D17-1172,W11-2913,0,0.452059,"Le Roux Mathieu Lacroix Laboratoire d’Informatique de Paris Nord, Universit´e Paris 13 – SPC, CNRS UMR 7030, F-93430, Villetaneuse, France {corro,leroux,lacroix}@lipn.fr Abstract cident to exactly one vertex per cluster. This problem is NP-complete even for arc-factored models. In order to bypass complexity, we resort to Lagrangian relaxation and propose an efficient resolution based on dual decomposition which combines a simple non-projective dependency parser on a contracted graph and a local search on each cluster to find a global consensus. We evaluated our model on the discontinuous PTB (Evang and Kallmeyer, 2011) and the Tiger (Brants et al., 2004) corpora. Moreover, we show that our algorithm is able to quickly parse the whole test sets. Section 2 presents the parsing problem. Section 3 introduces GMSA from which we derive an effective resolution method in Section 4. In Section 5 we define a parameterization of the parser which uses neural networks to model local probabilities and present experimental results in Section 6. We discuss related work in Section 7. We present a new method for the joint task of tagging and non-projective dependency parsing. We demonstrate its usefulness with an application"
D17-1172,P15-1147,0,0.206035,"Missing"
D17-1172,W08-1701,0,0.0204873,"t and 23 for testing. We used gold and predicted POS tags by the Stanford tagger,10 trained with 10jackknifing. Dependencies are extracted following the head-percolation table of Collins (1997). German We used the Tiger corpus (Brants et al., 2004) with the split defined for the SPMRL 2014 shared task (Maier, 2015; Seddah et al., 2013). Following Maier (2015) and Coavoux and Crabb´e (2017), we removed sentences number 46234 and 50224 as they contain annotation errors. We only used the given gold POS tags. Dependencies are extracted following the head-percolation table distributed with Tulipa (Kallmeyer et al., 2008). We emphasize that long sentences are not filtered out. Our derivation extraction algorithm is similar to the one proposed in Carreras et al. (2008). Regarding decoding, we use a beam of P size 10 for spines w.r.t. Pν (sm |m, w) = h Pν (sm |h, m, w) × Pα (h|m, w) but allow every possible adjunction. The maximum number of iterations of the subgradient descent is set to 500 and the stepsize η t is fixed following the rule of Polyak (1987). Parsing results and timing on short sentences only (≤ 40 words) and full test set using the de9 If a spine contains repeated non-terminal sequences, we selec"
D17-1172,Q16-1023,0,0.0405419,"t pipeline solution found so far, we can guarantee that xv = 0 in any optimal solution. We can check the whole graph in linear time if we keep local weights c in memory. 5 Neural Parameterization We present a probabilistic model for our framework. We implement our probability distributions with neural networks, more specifically we build a neural architecture on top of bidirectional recurrent networks that compute context sensitive representations of words. At each step, the recurrent architecture is given as input a concatenation of word and part-of-speech embeddings. We refer the reader to (Kiperwasser and Goldberg, 2016; Dozat and Manning) for further explanations about bidirectional LSTMs (Hochreiter and Schmidhuber, 1997). In the rest of this section, bm denotes the context sensitive representation of word wm . We now describe the neural network models used to learn and assign weight functions α, ν and γ under a probabilistic model. Given a sentence w of length n, we assume a derivation (d, s, l) is generated by three distinct tasks. By chain rule, P (d, s, l|w) = Pα (d|w) × Pν (s|d, w) × Pγ (l|d, s, w). We follow a common approach in dependency parsing and assign labels l in a postprocessing step, althoug"
D17-1172,D10-1125,0,0.200205,"tains depending on the context. Let π = {V0 , . . . , Vn } and V = ∪nk=0 Vk . For every couple u, v of vertices such that u ∈ Vh and v ∈ Vm , h 6= m and m 6= 0, we associate an arc uv corresponding to the best adjunction of the root of spine sm associated with v of Vm to spine sh associated with vertex u of Vh . The weight of this arc is given by φuv = α(h, m; w) + ν(sm ; h, m, w) + max γ(lhm ; h, m, sh , w) the arc from V3 to V2 represents the adjunction of spine NP-PRP to spine S-VP-VB at index 0. 4 Efficient Decoding Lagrangian relaxation has been successfully applied to various NLP tasks (Koo et al., 2010; Le Roux et al., 2013; Almeida and Martins, 2013; Das et al., 2012; Corro et al., 2016). Intuitively, given an integer linear program, it consists in relaxing some linear constraints which make the program difficult to solve and penalizing their violation in the objective function. We propose a new decoding method for GMSA based on dual decomposition, a special flavor of Lagrangian relaxation where the problem is decomposed in several independent subproblems. 4.1 lhm which is the score of the best adjunction of sm to sh . This ends the construction of (D, π, φ). There is a 1-to-1 corresponden"
D17-1172,D13-1116,1,0.904427,"Missing"
D17-1172,D11-1109,0,0.018861,", we look for the leftmost sl and rightmost sr dependents attached with regular adjunction. If any, we insert a new node between sh [i] and sh [i + 1] with the same grammatical category as the first one. This new node fills the role of the foot node in TAGs. Every dependent of sh [i] with anchor in interval [l + 1, r − 1] is moved to the newly created node. Remaining sister and regular adjunctions are simply attached to sh [i]. The complexity of the parsing problem depends on the type of dependency trees. In the case of projective trees, it has been shown (Eisner, 2000; Carreras et al., 2008; Li et al., 2011) that this could be performed in cubic worst-case time complexity with dynamic programming, whether supertags are fixed beforehand or not. However, the modification of the original Eisner algorithm requires that chart cells must be indexed not only by spans, or pairs of positions, but also by pairs of supertags. In practice the problem is intractable unless heavy 1645 pruning is performed first in order to select a subset of spines at each position. In the case of non-projective dependency trees, the problem has quadratic worst-case time complexity when supertags are fixed, since the problem t"
D17-1172,P15-1116,0,0.155243,"ents on two corpora annotated with discontinuous constituents. English We used an updated version of the Wall Street Journal part of the Penn Treebank corpus (Marcus et al., 1994) which introduces discontinuity (Evang and Kallmeyer, 2011). Sections 2-21 are used for training, 22 for developpement and 23 for testing. We used gold and predicted POS tags by the Stanford tagger,10 trained with 10jackknifing. Dependencies are extracted following the head-percolation table of Collins (1997). German We used the Tiger corpus (Brants et al., 2004) with the split defined for the SPMRL 2014 shared task (Maier, 2015; Seddah et al., 2013). Following Maier (2015) and Coavoux and Crabb´e (2017), we removed sentences number 46234 and 50224 as they contain annotation errors. We only used the given gold POS tags. Dependencies are extracted following the head-percolation table distributed with Tulipa (Kallmeyer et al., 2008). We emphasize that long sentences are not filtered out. Our derivation extraction algorithm is similar to the one proposed in Carreras et al. (2008). Regarding decoding, we use a beam of P size 10 for spines w.r.t. Pν (sm |m, w) = h Pν (sm |h, m, w) × Pα (h|m, w) but allow every possible ad"
D17-1172,H94-1020,0,0.781631,"ne s at position m to a site labeled with nt on spine t at position h with type a ∈ {regular, sister} is: Pγ (nt, a|h, m) = Pγ 0 (nt|h, m, w) × Pγ 00 (a|h, mw) Pγ and Pγ 00 are again defined as distributions from the exponential family using biaffine attention classifiers: (γ 0 ) Pγ 0 (nt|h, m, t) = P exp ont,h,m nt0 (γ 0 ) exp ont,h,m (γ 00 ) Pγ 00 (a|h, m, t) = P exp ot,h,m a0 (γ 00 ) exp oa0 ,h,m Experiments We ran a series of experiments on two corpora annotated with discontinuous constituents. English We used an updated version of the Wall Street Journal part of the Penn Treebank corpus (Marcus et al., 1994) which introduces discontinuity (Evang and Kallmeyer, 2011). Sections 2-21 are used for training, 22 for developpement and 23 for testing. We used gold and predicted POS tags by the Stanford tagger,10 trained with 10jackknifing. Dependencies are extracted following the head-percolation table of Collins (1997). German We used the Tiger corpus (Brants et al., 2004) with the split defined for the SPMRL 2014 shared task (Maier, 2015; Seddah et al., 2013). Following Maier (2015) and Coavoux and Crabb´e (2017), we removed sentences number 46234 and 50224 as they contain annotation errors. We only us"
D17-1172,H05-1066,0,0.192552,"s are fixed beforehand or not. However, the modification of the original Eisner algorithm requires that chart cells must be indexed not only by spans, or pairs of positions, but also by pairs of supertags. In practice the problem is intractable unless heavy 1645 pruning is performed first in order to select a subset of spines at each position. In the case of non-projective dependency trees, the problem has quadratic worst-case time complexity when supertags are fixed, since the problem then amounts to non-projective parsing and reduces to the Maximum Spanning Arborescence problem (MSA) as in (McDonald et al., 2005). Unfortunately, the efficient algorithm for MSA is greedy and does not store potential substructure candidates. Hence, when supertags are not fixed beforehand, a new arborescence must be recomputed for each choice of supertags. This problem can be seen as instance of the Generalized Maximum Spanning Arborescence problem, an NPcomplete problem, which we review in the next section. Note that arc labels do not impact the asymptotic complexity of an arc-factored model. Indeed, only the labeled arc with maximum weight between two vertices is considered when parsing. 3 The Generalized Maximum Spann"
D17-1172,D12-1067,0,0.0187257,"m is the first time that GMSA is used to solve a NLP problem. Dual decomposition has been used to derive efficient practical resolution methods in NLP, mostly for machine translation and parsing, see (Rush et al., 2010) for an overview and (Koo et al., 2010) for an application to dependency parsing. To accelerate the resolution, our method relies heavily on problem reduction (Beasley, 1993), which uses the primal/dual bounds to filter out suboptimal assignments. Exact pruning based on duality has already been studied in parsing, with branch and bound (Corro et al., 2016) or column generation (Riedel et al., 2012) and in machine translation with beam search (Rush et al., 2013). 8 Conclusion We presented a novel framework for the joint task of supertagging and parsing by a reduction to GMSA. Within this framework we developed a model able to produce discontinuous constituents. The scoring model can be decomposed into tagging and dependency parsing and thus may rely on advances in those active fields. This work could benefit from several extensions. Bigram scores on spines could be added at the expense of a third subproblem in the dual objective. High-order scores on arcs like grandparent or siblings can"
D17-1172,D13-1022,0,0.0209947,"decomposition has been used to derive efficient practical resolution methods in NLP, mostly for machine translation and parsing, see (Rush et al., 2010) for an overview and (Koo et al., 2010) for an application to dependency parsing. To accelerate the resolution, our method relies heavily on problem reduction (Beasley, 1993), which uses the primal/dual bounds to filter out suboptimal assignments. Exact pruning based on duality has already been studied in parsing, with branch and bound (Corro et al., 2016) or column generation (Riedel et al., 2012) and in machine translation with beam search (Rush et al., 2013). 8 Conclusion We presented a novel framework for the joint task of supertagging and parsing by a reduction to GMSA. Within this framework we developed a model able to produce discontinuous constituents. The scoring model can be decomposed into tagging and dependency parsing and thus may rely on advances in those active fields. This work could benefit from several extensions. Bigram scores on spines could be added at the expense of a third subproblem in the dual objective. High-order scores on arcs like grandparent or siblings can be handled in subproblem P2 with the algorithms described in (K"
D17-1172,D10-1001,0,0.0241247,"le with the TAG formalism (or more generally LCFRS) could be recovered by the use of a constrained version of MSA (Corro et al., 2016). Table 3: Dependency parsing and tagging results. Results marked with † use predicted partof-speech tags. it is the optimal solution (Beasley, 1993), and thus the stopping criterion for the subgradient descent is usually slow to obtain. To our knowledge, our system is the first time that GMSA is used to solve a NLP problem. Dual decomposition has been used to derive efficient practical resolution methods in NLP, mostly for machine translation and parsing, see (Rush et al., 2010) for an overview and (Koo et al., 2010) for an application to dependency parsing. To accelerate the resolution, our method relies heavily on problem reduction (Beasley, 1993), which uses the primal/dual bounds to filter out suboptimal assignments. Exact pruning based on duality has already been studied in parsing, with branch and bound (Corro et al., 2016) or column generation (Riedel et al., 2012) and in machine translation with beam search (Rush et al., 2013). 8 Conclusion We presented a novel framework for the joint task of supertagging and parsing by a reduction to GMSA. Within this framew"
D17-1172,H05-1102,0,0.12252,"Missing"
D17-1172,D08-1052,0,0.208375,"MD PRP VB should I do ? Figure 1: A derivation with spines and adjunctions (dashed arrows). The induced dependency tree is non-projective. Each color corresponds to a spine. We omit punctuation to simplify figures. a lexical projection of the anchor. Carreras et al. (2008) showed how spine-based parsing could be reduced to dependency parsing: since spines are attached to words, equivalent derivations can be represented as a dependency tree where arcs are labeled by spine operations, an adjunction together with information about the adjunction site. However, we depart from previous approaches (Shen and Joshi, 2008; Carreras et al., 2008) by relaxing the projectivity constraint to represent all discontinuous phrase-structure trees (see Figure 1). We assume a finite set of spines S. A spine s can be defined as a sequence of grammatical categories, beginning at root. For a sentence w = (w0 , w1 , . . . , wn ) where wk is the word at position k and w0 is a dummy root symbol, a derivation is a triplet (d, s, l) defined as follows. Adjunctions are described by a dependency tree rooted at 0 written as a sequence of arcs d. If (h, m) ∈ d with h ∈ {0, . . . , n} and m ∈ {1, . . . , n}, then the derivation conta"
D17-1172,W14-6104,0,0.485641,"rangian relaxation. We evaluate this model and obtain state-of-the-art results despite strong independence assumptions. 1 Introduction Discontinuous phrase-structure parsing relies either on formal grammars such as LCFRS, which suffer from a high complexity, or on reductions to non-projective dependency parsing with complex labels to encode phrase combinations. We propose an alternative approach based on a variant of spinal TAGs, which allows parses with discontinuity while grounding this work on a lexicalized phrase-structure grammar. Contrarily to previous approaches, (Hall and Nivre, 2008; Versley, 2014; Fern´andez-Gonz´alez and Martins, 2015), we do not model supertagging nor spine interactions with a complex label scheme. We follow Carreras et al. (2008) but drop projectivity. We first show that our discontinuous variant of spinal TAG reduces to the Generalized Maximum Spanning Arborescence (GMSA) problem (Myung et al., 1995). In a graph where vertices are partitioned into clusters, GMSA consists in finding the arborescence of maximum weight in2 Joint Supertagging and Spine Parsing In this section we introduce our problem and set notation. The goal of phrase-structure parsing is to produce"
E06-2005,W02-2233,1,0.821185,"Missing"
E06-2005,E03-1030,0,0.0627882,"Missing"
I11-1100,J04-4004,0,0.077163,"Missing"
I11-1100,cer-etal-2010-parsing,0,0.032005,"Missing"
I11-1100,C08-1071,0,0.0212923,"Missing"
I11-1100,N10-1004,0,0.0312252,"iques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a particular document. We intend to experiment with this idea in the context of Web 2.0 parsing. More Parser Evaluation The cross-parser evaluation we have presented in the first half of the paper is by no means exhaustive. For example, to measure the positive effect of discriminative reranking, the first-stage Brown parser should also be included in the evaluation. Other statistical parsers could be evaluated, and it would be interesting to examine the performance of systems which employ hand-crafted grammars and tre"
I11-1100,de-marneffe-etal-2006-generating,0,0.00984564,"Missing"
I11-1100,P05-1012,0,0.0540053,"ers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser p"
I11-1100,N10-1060,1,0.883765,", Joachim Wagner1 , Joseph Le Roux2 Joakim Nivre3 , Deirdre Hogan1 and Josef van Genabith1 1,3 NCLT/CNGL, Dublin City University, Ireland 2 LIF - CNRS UMR 6166, Universit´e Aix-Marseille, France 3 Department of Linguistics and Philology, Uppsala University, Sweden 1 {jfoster,ocetinoglu,jwagner,dhogan,josef}@computing.dcu.ie 2 joseph.le-roux@lif.univ-mrs.fr, 3 joakim.nivre@lingfil.uu.se Abstract social media is particularly challenging since Web 2.0 is not really a domain, consisting, as it does, of utterances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parser"
I11-1100,gimenez-marquez-2004-svmtool,0,0.0101406,"Missing"
I11-1100,P11-2008,0,0.0596175,"Missing"
I11-1100,D09-1087,0,0.0469586,"Missing"
I11-1100,D10-1002,0,0.00568548,"d user-generated content can be used to improve parser accuracy. The reasons for the improvements yielded by the three types of retraining need to be determined.6 The underperformance of the TwitterTrain material in comparison to the FootballTrain material suggests that sample selection involving language and topic identification needs to be applied before parser retraining. We also intend to test the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example,"
I11-1100,P06-1063,1,0.829525,"Missing"
I11-1100,H94-1020,0,0.0475196,"Missing"
I11-1100,P06-1043,0,0.281537,"he discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser performance by modifying the Penn Treebank trees to reflect observed differences between Wall Street Journal (WSJ) sentences and discussion forum sentences (subject ellipsis, non-standard capitalisation, etc.). We approach the problem from a different perspective, by seeing how far we can get by exploiting unlabelled target domain data. We employ three types of parser retraining, namely, 1) the McClosky et al. (2006) self-training protocol, 2) uptraining of Malt using dependency trees produced by a slightly more accurate phrase structure parser (Petrov et al., 2010), and 3) PCFG-LA self-training (Huang We investigate the problem of parsing the noisy language of social media. We evaluate four Wall-Street-Journal-trained statistical parsers (Berkeley, Brown, Malt and MST) on a new dataset containing 1,000 phrase structure trees for sentences from microblogs (tweets) and discussion forum posts. We compare the four parsers on their ability to produce Stanford dependencies for these Web 2.0 sentences. We find"
I11-1100,P08-1108,1,0.789006,"ng. We also intend to test the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a p"
I11-1100,nivre-etal-2006-maltparser,1,0.187118,"rances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a t"
I11-1100,W09-3811,1,0.21318,"ntaining the supplied POS tag for a given word may be removed from the chart during coarse-to-fine pruning.2 3 Baseline Evaluation We first evaluate four widely used WSJ-trained statistical parsers on our new Web 2.0 datasets: Berkeley (Petrov et al., 2006) We train a PCFG-LA using 6 iterations and we run the parser in accurate mode. 3.1 Results Brown (Charniak and Johnson, 2005) We employ this parser in its out-of-the-box settings. Table 2 shows the Parseval f-score and part-ofspeech (POS) tagging accuracy for the Berkeley Malt (Nivre et al., 2006) We use the stacklazy algorithm described in Nivre et al. (2009). We train a linear classifier where the feature interactions are modelled explicitly. 2 In the interest of replicability, detailed information on experimental settings is available at http: //nclt.computing.dcu.ie/publications/ foster_ijcnlp11.html. 895 Parser Berk O Berk P Brown Malt P MST P Berk G Malt G MST G LAS UAS WSJ22 90.5 93.2 89.9 92.5 91.5 94.2 88.0 90.6 88.8 91.3 91.6 93.4 90.0 91.6 90.7 92.3 LAS UAS FootballDev 79.8 84.8 80.1 84.9 82.0 86.3 76.1 81.5 76.4 81.1 83.1 86.4 80.4 83.7 80.8 83.4 LAS UAS TwitterDev 68.9 75.1 68.2 74.2 71.4 77.3 67.3 73.6 68.1 73.8 76.8 80.8 78.3 81.6 78"
I11-1100,C10-1094,1,0.884961,"Missing"
I11-1100,P06-1055,0,0.0152372,"in City University, Ireland 2 LIF - CNRS UMR 6166, Universit´e Aix-Marseille, France 3 Department of Linguistics and Philology, Uppsala University, Sweden 1 {jfoster,ocetinoglu,jwagner,dhogan,josef}@computing.dcu.ie 2 joseph.le-roux@lif.univ-mrs.fr, 3 joakim.nivre@lingfil.uu.se Abstract social media is particularly challenging since Web 2.0 is not really a domain, consisting, as it does, of utterances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms"
I11-1100,D10-1069,0,0.0996262,"t of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser performance by modifying the Penn Treebank trees to reflect observed differences between Wall Street Journal (WSJ) sentences and discussion forum sentences (subject ellipsis, non-standard capitalisation, etc.). We approach the problem from a different perspective, by seeing how far we can get"
I11-1100,N10-1003,0,0.00470176,"the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a particular docum"
I11-1100,W10-2105,0,0.0674734,"Missing"
I11-1100,W09-2205,0,0.0221942,"Missing"
I11-1100,P07-1078,0,0.0355222,"Missing"
I11-1100,W10-2606,0,0.0124848,"Missing"
I11-1100,E03-1008,0,0.0496129,"Missing"
I11-1100,W99-0623,0,\N,Missing
J13-3005,C96-1034,0,0.376661,"nsider a particular type of computational grammar, namely, tree-based grammars—that is, grammars where the basic units are trees (or tree descriptions) of arbitrary depth, such as Tree-Adjoining Grammar (TAG; Joshi, Levy, and Takahashi 1975), D-Tree Grammar (DTG; Rambow, Vijay-Shanker, and Weir 1995), Tree Description Grammars (TDG; Kallmeyer 1999) or Interaction Grammars (IG; Perrier 2000)— environments sharing all of the listed features are lacking. As we shall see in Section 7 of this article, there have been some proposals for grammar engineering environments for tree-based grammar (e.g., Candito 1996; Xia, Palmer, and Vijay-Shanker 1999, but these lack notational expressivity. This is partly due to the fact that tree-based formalisms offer an extended domain of locality where one can encode constraints between remote syntactic constituents. If one wants to define such constraints while giving a modular and incremental specification of the grammar, one needs a high level of notational expressivity, as we shall see throughout the article (and especially in Section 4). In this article, we present XMG (eXtensible MetaGrammar), a framework for specifying tree-based grammars. Focusing mostly on"
J13-3005,P03-1024,0,0.507858,"Missing"
J13-3005,C69-0101,0,0.536249,"Missing"
J13-3005,J11-1003,0,0.0149523,"s the criticisms leveled by Cohen-Sygal and Wintner (2007, 2009) against non-associative constraints on node unification do not apply. Briefly, in their work, Cohen-Sygal and Wintner (2007, 2009) showed that any polarity-based tree description formalism is not associative. In other words, when describing trees in terms of combinations of polarized structures, the order in which the structures are combined matters (i.e., the output structures depend on the combination order). This feature makes such formalisms not appropriate for a modular and collaborative grammar engineering, such as that of Cohen-Sygal and Wintner (2011) for Unification Grammar. In the XMG case, when using node colors, the tree description solver does not rely on any specific fragment combination order. It computes all possible combination orders. In this context, the grammar designer cannot think in terms of sequences of node identifications. This would lead to tree overgeneration. Again, it is important to remember that tree solving computes any valid tree model, independently of any specific sequence of node identifications (all valid node identifications are computed). In this context, non-associativity of color-based node identification"
J13-3005,copestake-flickinger-2000-open,0,0.0911674,"Missing"
J13-3005,P01-1019,0,0.109092,"Missing"
J13-3005,P95-1011,0,0.232963,"Missing"
J13-3005,W02-2233,1,0.737385,"Missing"
J13-3005,C08-1032,1,0.9499,"a modular account of the syntax/semantics interface in which linking constraints can be stipulated separately and reused to specify the various diatheses. In other words, the DYN feature structure allows us to extend the scope of some specific variables so that they can be unified with variables (or values) introduced in some other classes of the metagrammar. This concept of scope extension can be compared with that of hook in Copestake, Lascarides, and Flickinger (2001). 9 For more details on the interpretation of flat semantics and on its association with a grammar of natural language, see Gardent (2008). 601 Computational Linguistics Volume 39, Number 3 Control language. The linguistic units (named Content here) defined by the linguist can be abstracted and combined as follows: C ,...,C → Class ::= Namex11,...,xnk Content ::= SYN, SEM, DYN |Name |Content ∧ Content |Content ∨ Content Content The first clause states that the linguistic information encoded in Content is abstracted in a class named Name and that this class inherits classes C1 ,..., Ck and exports variables x1 ,..., xn . That is, XMG allows for abstraction, inheritance, and variable exports. By default, variables (referring to no"
J13-3005,P07-1042,1,0.873061,"Missing"
J13-3005,P07-2004,1,0.894592,"Missing"
J13-3005,J05-2003,0,0.0673111,"Missing"
J13-3005,kallmeyer-etal-2008-developing,1,0.809222,"roduce a TAG covering the following frames: intransitive (tree family N0V), transitive with a nominal complement (N0VN1), transitive with a clausal complement (N0VS1), transitive with modal complement (N0V0V1), ditransitive (N0VN1N2), ditransitive with a preposition (N0VN1ON2), ditransitive with a verbal complement (N0V0N1V1), ditransitive with an adjectival complement (N0VN1A), movement verbs with a nominal complement (N0V0V1N1), movement verbs with an adjectival complement (N0V0AV1), and movement ditransitive (N0V0N1V1N2). GerTT. Another XMG-based grammar corresponds to the German MC-TAG of Kallmeyer et al. (2008). This grammar, called GerTT, is in fact an MC-TAG with Tree Tuples (Lichte 2007). This variant of MCTAG has been designed to model free word order phenomena. This is done by imposing node sharing constraints on MCTAG derivations (Kallmeyer 2005). GerTT covers phenomena such as scrambling, coherent constructions, relative clauses, embedded questions, copula verbs, complementized sentences, verbs with various sub-categorization frames, nouns, prepositions, determiners, adjectives, and partly includes semantics. It is made of 103 tree tuples, compiled from 109 classes. 7. Related Work We now com"
J13-3005,W04-3325,0,0.0745752,"Missing"
J13-3005,W04-3321,0,0.0685973,"Missing"
J13-3005,W97-1508,0,0.235579,"he modeling of the tree fragment hierarchies required to specify tree-based grammars and of a syntax/semantics interface between semantic representations and syntactic trees. Finally, we briefly report on several grammars for French, English, and German that were implemented using XMG and compare XMG with other existing grammar specification frameworks for tree-based grammars. 1. Introduction In the late 1980s and early 1990s, many grammar engineering environments were developed to support the specification of large computational grammars for natural language. One may, for instance, cite XLE (Kaplan and Newman 1997) for specifying Lexical-Functional Grammars (LFG), LKB (Copestake and Flickinger 2000) for specifying Head-driven Phrase Structure Grammars (HPSG), and D OT CCG (Baldridge et al. 2007) for specifying Combinatory Categorial Grammars (CCG). Concretely, such environments usually rely on (i) a formal language used to describe a target computational grammar, and (ii) a processor for this language, which aims at generating the actual described grammar (and potentially at checking it, e.g., by feeding it to a parser). Although these environments were tailored for specific grammar formalisms, they sha"
J13-3005,C00-1065,0,0.042807,"c formula can be used to underspecify the meaning of the sentence “Every dog chases a cat”: l0 : ∀(x, h1 , h2 ) ∧ l1 ≤ h1 ∧ l3 : ∃(y, h3 , h4 ) ∧ ∧ l1 : Dog(x) l4 ≤ h3 ∧ ∧ l2 ≤ h2 l4 : Cat(y) ∧ ∧ l2 : Chase(x, y) l2 ≤ h4 (14) This formula denotes the following two first-order logic formulae, thereby describing the two possibles readings of this sentence.9 l0 : ∀(x, l1 , l3 ) ∧ l1 : Dog(x) ∧ l2 : Chase(x, y) ∧ l3 : ∃(y, l4 , l2 ) ∧ l4 : Cat(y) (15) l0 : ∀(x, l1 , l2 ) ∧ l1 : Dog(x) ∧ l2 : Chase(x, y) ∧ l3 : ∃(y, l4 , l0 ) ∧ l4 : Cat(y) (16) DYN. The DYN dimension generalizes Kinyon’s hypertag (Kinyon 2000) which is unified whenever two tree fragments are combined. Similarly, in XMG the DYN dimension is a feature structure that is unified whenever two XMG classes are combined through inheritance or through conjunction (see the discussion on XMG control language, subsequently). For instance, the following constraints ensure a coreference between the index I occurring in the syntactic dimension and the argument X occurring in the semantic dimension (indexsubject and arg1 are feature names, and E, I, X, and V local unification variables). C1 → Node [idx : I] ∧ indexsubject : I (17) C2 → L : P(E) ∧"
J13-3005,W06-1503,0,0.051829,"Missing"
J13-3005,C96-2120,0,0.331007,"Missing"
J13-3005,W98-0129,0,0.099141,"ations are usually called reified constraints. 610 Crabb´e et al. XMG: eXtensible MetaGrammar and if this is the case, we add to the input description a strict precedence constraint on these nodes according to their respective values of the property p:19 bn,m ∧ (pn < pm ) ⇒ n ≺+ m (26) bn,m ∧ (pm < pn ) ⇒ m ≺ n (27) + 5.3.3 Adding Color Constraints to Facilitate Grammar Writing. To further ease grammar development, XMG supports a node coloring mechanism that permits nameless node identification (Crabb´e and Duchier 2004), reminiscent of the polarity-based node identification first proposed by Muskens and Krahmer (1998) and later used by Duchier and Thater (1999) and Perrier (2000). Such a mechanism offers an alternative to explicit node identification using equations between node variables. The idea is to label node variables with a color property, whose value (either red, black, or white) can trigger node identifications. This mechanism is another parameter of the tree solver. When in use, the valid tree models must satisfy some color constraints, namely, they must only have red or black nodes (no remaining white nodes; these have to be identified with some black nodes). As shown in the following table, no"
J13-3005,C02-1153,0,0.164995,"le and second, the Passive meta-rule to the base tree. 596 Crabb´e et al. XMG: eXtensible MetaGrammar Passive meta-rule ⇒ Sr ?1 NP? Sr VP V []  mode Wh-Subject meta-rule 3  ?2 NP? VP ⎡ ⎣ ⎣ 3 ⎤ mode 2 passive 1 ⎡ V ?2NP? ↓  mode ?2 NP? ⇒ Sr  ?1 Sq  ?2NP? ↓  wh + NP↓NA ⎦ 1 + mode 2 ppart ?1  ⎤ passive Sr [] ⎦ PP [] P ?1 NP? by Figure 4 Simplified meta-rules for passive and wh-subject extraction. More generally a meta-rule is a procedural device that, given a tree instance, generates a new tree instance by adding, suppressing (hence possibly substituting) information in grammatical units. Prolo (2002) defines a set of meta-rules that can be used to specify a large FB-LTAG for English. Given an ordered set of meta-rules, however, there is no guarantee that the trees they derive are linguistically appropriate and that the derivation process terminates. Thus, to ensure termination and consistency, Prolo needs to additionally provide rule ordering schemes (expressed as automata). 3.2 XMG: Capturing Diathesis Using Disjunction XMG provides an alternative account for describing tree sets such as that of Figure 2 without lexical rules and without the related ordering constraints. In essence, the"
J13-3005,P95-1021,0,0.701406,"Missing"
J13-3005,P84-1075,0,0.45061,"rase Structure Grammars (HPSG), and D OT CCG (Baldridge et al. 2007) for specifying Combinatory Categorial Grammars (CCG). Concretely, such environments usually rely on (i) a formal language used to describe a target computational grammar, and (ii) a processor for this language, which aims at generating the actual described grammar (and potentially at checking it, e.g., by feeding it to a parser). Although these environments were tailored for specific grammar formalisms, they share a number of features. Firstly, they are expressive enough to characterize subsets of natural language. Following Shieber (1984), we call this feature weak completeness. Secondly, they are notationally expressive enough to relatively easily formalize important theoretical notions. Thirdly, they are rigorous, that is, the semantics of their underlying language is well defined and understood. Additionally, for an environment to be useful in practice, it should be simple to use (by a linguist), and make it possible to detect errors in the described target grammar. If we consider a particular type of computational grammar, namely, tree-based grammars—that is, grammars where the basic units are trees (or tree descriptions)"
J13-3005,C88-2147,0,0.90341,"be the same as that of the root node. Substitution inserts a tree onto a substitution node of some other tree and adjunction inserts an auxiliary tree 593 Computational Linguistics Volume 39, Number 3 S N↓ N Marie Mary vu seen V V V V S N↓ N Jean John ∗ ⇒ N Marie Mary V V N V a vu has seen Jean John a has Figure 1 Sample derivation of Marie a vu Jean ‘Mary has seen John’ in a TAG. into a tree. Figure 1 shows a toy TAG generating the sentence Marie a vu Jean ‘Mary has seen John’ and sketches its derivation.1 Among existing variants of TAG, one commonly used in practice is Lexicalized FB-LTAG (Vijay-Shanker and Joshi 1988). A lexicalized TAG is such that each elementary tree has at least one leaf labeled with a lexical item (word), whereas in an FB-LTAG, tree nodes are additionally decorated with two feature structures (called top and bottom). These feature structures are unified during derivation as follows. On substitution, the top features of the substitution node are unified with the top features of the root node of the tree being substituted in. On adjunction, the top features of the root of the auxiliary tree are unified with the top features of the node where adjunction takes place; and the bottom featur"
J13-3005,C92-1034,0,0.421336,"ng Lexical Rules Following Flickinger (1987), redundancy among grammatical descriptions is often handled using two devices: an inheritance hierarchy and a set of lexical rules. Whereas the inheritance hierarchy permits us to encode the sharing of common substructures, lexical rules (sometimes called meta-rules) permit us to capture relationships between trees by deriving new trees from already specified ones. For instance, passive trees will be derived from active ones. Although Flickinger’s (1987) approach was developed for HPSGs, several similar approaches have been put forward for FB-LTAG (Vijay-Shanker and Schabes 1992; Becker 1993; Evans, Gazdar, and Weir 1995; XTAG Research Group 2001). One important drawback of these approaches, however, is that they are procedural in that the order in which lexical rules apply matters. For instance, consider again the set of trees given in Figure 2. In the meta-rule representation scheme adopted by Becker (1993), the base tree (a) would be specified in the inheritance hierarchy grouping all base trees, and the derived trees (b, c, d) would be generated by applying one or more meta-rules on this base tree. Figure 4 sketches these meta-rules. The left-hand side of the met"
J13-3005,W10-4414,0,0.427943,"Missing"
J13-3005,P06-2032,1,\N,Missing
J13-3005,J15-1003,1,\N,Missing
J13-3005,C00-2087,0,\N,Missing
K19-1023,P89-1018,0,0.753454,"Missing"
K19-1023,D17-1003,0,0.0375191,"Missing"
K19-1023,2020.inlg-1.14,0,0.0205527,"Missing"
K19-1023,D17-1172,1,0.895181,"Missing"
K19-1023,Q15-1040,0,0.0631837,"Missing"
K19-1023,D15-1166,0,0.0117158,"d part of the subderivation. A reduction builds a new item by extending the well-nested part of the left antecedent with a new matching arc obtained from the right antecedent and the new reduction arc. 4.1 Notation A feed-forward layer is a sequence of an affine transformation, a ReLU filter and a linear transformation, i.e. FF(x) = V (max(0, (W x + b))), with V, W, b trainable parameters. 242 where softmax is applied row-wise and µ is a smoothing factor between 0 and 1, which is set to d−0.5 , where d is the size of a query vector, following previous implementations of dot-product attention (Luong et al., 2015). The second sublayer Bi applies the same feedforward transformation to each element of the sequence of vectors returned by Ai . We call interpolation layers functions like I(x, y) = C(x)·x+(1−C(x))·y where C is a linear transformation followed by a sigmoid squashing, and · denotes the component-wise product. Combining the previous two, we define highway layers (Greff et al., 2017) as functions H(x) = I(x, F F (x)), i.e. an interpolation of input x and a feed-forward transformation of x. We also make use of biaffine functions following (Dozat and Manning, 2017) that we define as functions of t"
K19-1023,P15-1033,0,0.0276887,"over transition scores s` (c` , t` ), we retrieve the well-studied cumulative sum of its transition scores. s(γ) = X s` (c` , t` ) (1) 1≤`≤2n 2.2 General Formulation for Dynamic Programming We present a dynamic programming (DP) algorithm for arc-hybrid parsing with cumulative transition scores as in Eq. 1. This algorithm cannot be used as such since states references complete stack contents, of which there is an exponential number, leading to an intractable complexity. To use this algorithm in practice we would need to resort to beam search in order to approximate solutions, see for instance (Dyer et al., 2015). However, this constitutes a general framework from which efficient algorithms can be derived by considering various equivalence classes over states and independence assumptions in the scoring function. For instance we can retrieve the Minimal Feature Set algorithm of Shi et al. (2017), or the new algorithm presented in Section 3. In our algorithm, an item σ,A hi, jiB represents the following set of subsequences of a derivation: shift [σ, b|β, A] →S [σ|b, β, A] left [σ|d, h|β, A] →L [σ, h|β, A ∪ {(h, d)}] right [σ|h|d, β, A] →R [σ|h, β, A ∪ {(h, d)}] A derivation for w is a sequence γ = c1 ,"
K19-1023,H94-1020,0,0.175814,"Missing"
K19-1023,C96-1058,0,0.337691,"tree allowing the scoring function to be expressed naturally as a sum over these 2 structures. While we restrict our presentation to archybrid systems, our method can be applied quite directly to other transition rule systems. Secondly we show that this representation leads to an exact O(n4 ) parsing algorithm using dynamic programming. This algorithm can be seen as an extension of the minimal feature set archybrid parsing algorithm presented in (Shi et al., 2017) where the contribution of the dependency arcs can be explicitly added to the scoring function as in the Eisner parsing algorithm (Eisner, 1996). 2 2.1 Arc-Hybrid Dependency Parsing Arc-Hybrid Derivations Intuitively, the arc-hybrid parsing strategy (G´omez-Rodr´ıguez et al., 2008; Kuhlmann et al., 2011) builds dependency parses incrementally by reading the sentence from left to right. The pending words are words which have been given all their left modifiers but may have not been given all their right dependents yet. The pending words are stored in the stack which 238 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 238–248 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational"
K19-1023,H05-1066,0,0.690077,"Missing"
K19-1023,P08-1110,0,0.0821801,"Missing"
K19-1023,D14-1162,0,0.0837797,"at the major part of scoring comes from dependencies. We may also conclude that (i) derivation information is useful per se but most importantly as an auxiliary task to improve dependencies and (ii) the stack size is paramount in this model since otherwise the network has no indication of the stack content. If not considered, accuracy drop considerably: step indexes alone are too vague as they can correspond to many different stack and buffer contents. Implementation Hyperparameters are given in Table 1. In addition to word embeddings parameterized on the PTB, we use pretrained Glove vectors (Pennington et al., 2014). Our prototype is written in c++ with DYNET9 for neural computations (Adam optimizer and default values) and U D P IPE10 for reading and writing data files. Minibatches contain around 1,000 tokens. We train each model for 100 epochs and select our best model according to its development UAS. We follow previous works with transformers to set the learning rate (Vaswani et al., 2017). For the first 8, 000 updates the learning increases linearly with the number of steps, then it decreases proportionally to the squared root of the number of steps. We use the formula of Strubell et al. (2018). 6 De"
K19-1023,Q16-1023,0,0.399426,"esent a new method for transition-based parsing where a solution is a pair made of a dependency tree and a derivation graph describing the construction of the former. From this representation we are able to derive an efficient parsing algorithm and design a neural network that learns vertex representations and arc scores. Experimentally, although we only train via local classifiers, our approach improves over previous arc-hybrid systems and reach state-of-the-art parsing accuracy. 1 Introduction While transition-based dependency parsing is usually implemented as a beam-search procedure, e.g. (Kiperwasser and Goldberg, 2016), some recent work such as (Shi et al., 2017) showed that global inference can be performed efficiently with dynamic programming. To this end, the stack representing pending subparses and the buffer representing the unconsumed input must both be abstracted into equivalence classes, while remaining rich enough to help with accurate predictions. In this paper we first explicitly consider that a solution in transition-based parsing is represented as a pair made of a derivation graph and a derived dependency tree allowing the scoring function to be expressed naturally as a sum over these 2 structu"
K19-1023,D17-1002,0,0.0433835,"Missing"
K19-1023,P18-1249,0,0.0227684,"articular, there are two promising avenues for future improvements. First, the learning framework could be enriched in a setting where the derivation graph is modeled as a latent variable and marginalized over. It remains to be seen if this can be done exactly or if sampling is required for efficiency. Second, since the score of a solution is the sum of the scores of elements in a pair, it should be possible to design an approximate solver based on lagrangian decomposition more efficient in practice. Machine Learning Aspects Self-attention networks have been used in parsing, see for instance (Kitaev and Klein, 2018), whether based on dependencies or syntagms. Curiously we found few models of transition-based parsing based on these networks, and bidirectional recurrent network are still preferred in most architectures, where they are believed to capture some information about the sequential nature of transition-based algorithms. Instead we present a non-sequential model of transition-based parsing where representation vectors are obtained via unrolled iterative estimation (Greff et al., 2017). Our encoder-decoder architecture together with independence assumptions made in the probabilistic model which dec"
K19-1023,D18-1548,0,0.0268044,"Missing"
N16-1127,P14-1070,1,0.892717,"Missing"
N16-1127,F12-2024,0,0.0696544,"Missing"
N16-1127,P12-1022,1,0.860987,"ng the parser of Y. Goldberg7 also used as a baseline. We trained all models for 20 iterations with dynamic oracle (Goldberg and Nivre, 2013) using the following exploration policy: always choose an oracle transition in the first 2 iterations (k = 2), then choose model prediction with probability p = 0.9. Features. One-dimensional features were taken directly from the code supporting Goldberg and Nivre (2013). We added information on typographical cues (hyphenation, digits, capitalization, . . . ) and the existence of substrings in MWE dictionaries in order to help lexical analysis. Following Constant et al. (2012) and Schneider et al. (2014a), we used dictionary lookups to build a first naive segmentation and incorporate it as a set of features. Twodimensional features were used in both pipeline and joint strategies. We first added syntactic path features to the lexical dimension, so syntax can guide segmentation. Conversely, we also added lexical path features to the syntactic dimension to provide information about lexical connectivity. For instance, two nodes being checked for attachment in the syntactic dimension can be associated with information describing whether one of the corresponding node is"
N16-1127,W11-3806,0,0.334094,"Missing"
N16-1127,N10-1115,0,0.232467,"to create arcs to connect them and form a complete tree : that we call ex2 The second argument l corresponds to the embedding level. 1096 ternal dependencies. LUs are sequentially linked together: each pair of consecutive LUs with roots lex (wi ,wj ), i < j, gives an arc wi −−→ wj . Figure 1 and Figure 2 respectively display the deep and shallow lexical segmentations of the sentence The Los Angeles Lakers made a big deal out of it. For readibility, we note mwe for sub0 mwe and submwe for sub1 mwe. 3 Multidimensional Easy-first Parsing 3.1 Easy-first parsing Informally, easy-first proposed in Goldberg and Elhadad (2010) predicts easier dependencies before risky ones. It decides for each token whether it must be attached to the root of an adjacent subtree and how this attachment should be labeled3 . The order in which these decisions are made is not decided in advance: highest-scoring decisions are made first and constrain the following decisions. This framework looks appealing in order to test our assumption that segmentation and parsing are mutually informative, while leaving the exact flow of information to be learned by the system itself: we do not postulate any priority between the tasks nor that all att"
N16-1127,Q13-1033,0,0.12149,"Missing"
N16-1127,C14-1177,1,0.698074,"Missing"
N16-1127,Q14-1016,0,0.221554,"g7 also used as a baseline. We trained all models for 20 iterations with dynamic oracle (Goldberg and Nivre, 2013) using the following exploration policy: always choose an oracle transition in the first 2 iterations (k = 2), then choose model prediction with probability p = 0.9. Features. One-dimensional features were taken directly from the code supporting Goldberg and Nivre (2013). We added information on typographical cues (hyphenation, digits, capitalization, . . . ) and the existence of substrings in MWE dictionaries in order to help lexical analysis. Following Constant et al. (2012) and Schneider et al. (2014a), we used dictionary lookups to build a first naive segmentation and incorporate it as a set of features. Twodimensional features were used in both pipeline and joint strategies. We first added syntactic path features to the lexical dimension, so syntax can guide segmentation. Conversely, we also added lexical path features to the syntactic dimension to provide information about lexical connectivity. For instance, two nodes being checked for attachment in the syntactic dimension can be associated with information describing whether one of the corresponding node is an ancestor of the other on"
N16-1127,schneider-etal-2014-comprehensive,0,0.296368,"g7 also used as a baseline. We trained all models for 20 iterations with dynamic oracle (Goldberg and Nivre, 2013) using the following exploration policy: always choose an oracle transition in the first 2 iterations (k = 2), then choose model prediction with probability p = 0.9. Features. One-dimensional features were taken directly from the code supporting Goldberg and Nivre (2013). We added information on typographical cues (hyphenation, digits, capitalization, . . . ) and the existence of substrings in MWE dictionaries in order to help lexical analysis. Following Constant et al. (2012) and Schneider et al. (2014a), we used dictionary lookups to build a first naive segmentation and incorporate it as a set of features. Twodimensional features were used in both pipeline and joint strategies. We first added syntactic path features to the lexical dimension, so syntax can guide segmentation. Conversely, we also added lexical path features to the syntactic dimension to provide information about lexical connectivity. For instance, two nodes being checked for attachment in the syntactic dimension can be associated with information describing whether one of the corresponding node is an ancestor of the other on"
N16-1127,I13-1024,0,0.217934,"Missing"
N16-1127,W14-0804,0,0.159722,"ficantly helps syntactic parsing. 1 2 Introduction Lexical segmentation is a crucial task for natural language understanding as it detects semantic units of texts. One of the main difficulties comes from the identification of multiword expressions [MWE] (Sag et al., 2002), which are sequences made of multiple words displaying multidimensional idiomaticity (Nunberg et al., 1994). Such expressions may exhibit syntactic freedom and varying degree of compositionality, and many studies show the advantages of combining MWE identification with syntactic parsing (Savary et al., 2015), for both tasks (Wehrli, 2014). Indeed, MWE detection may help parsing, as it reduces the number of lexical units, and in turn parsing may help detect MWEs with syntactic freedom (syntactic variations, discontinuity, etc.). In the dependency parsing framework, some previous work incorporated MWE annotations within Deep Segmentation and Dependencies This section describes a lexical representation able to handle nested MWEs, extended from Constant and Le Roux (2015) which was limited to shallow MWEs. Such a lexical analysis is particularly relevant to perform deep semantic analysis. A lexical unit [LU] is a subtree of the le"
P11-4015,W10-1408,1,0.44138,"Missing"
P11-4015,2006.jeptalnrecital-long.5,0,0.0461776,"rk has been funded by the French Agence Nationale pour la Recherche, through the projects SEQUOIA (ANR-08EMER-013) and DECODA (2009-CORD-005-01) 1 Annotation must be taken here in a general sense which includes tagging, segmentation or the construction of more complex objets as syntagmatic or dependencies trees. 86 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 86–91, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics of processing. 2 Several processing tools suites alread exist for French among which SXPIPE (Sagot and Boullier, 2008), OUTILEX (Blanc et al., 2006), NOOJ2 or UNI TEX 3 . A general comparison of MACAON with these tools is beyond the scope of this paper. Let us just mention that MACAON shares with most of them the use of finite state machines as core data representation. Some modules are implemented as standard operations on finite state machines. The MACAON exchange format is based on four concepts: segment, attribute, annotation level and segmentation. A segment refers to a segment of the text or speech signal that is to be processed, as a sentence, a clause, a syntactic constituent, a lexical unit, a named entity . . . A segment can be"
P11-4015,P06-1055,0,0.0181302,"ace that allows to inspect MACAON XML files and run the components. 3.3 sentation of many NLP tools input and output in the MACAON format. MACAON has been interfaced with the SPEERAL Automatic Speech Recognition System (Nocera et al., 2006). The word lattices produced by SPEERAL can be converted to pre-lexical MACAON automata. MACAON does not provide any native module for parsing yet but it can be interfaced with any already existing parser. For the purpose of this demonstration we have chosen the LORG parser developed at NCLT, Dublin14 . This parser is based on PCFGs with latent annotations (Petrov et al., 2006), a formalism that showed state-of-the-art parsing accuracy for a wide range of languages. In addition it offers a sophisticated handling of unknown words relying on automatically learned morphological clues, especially for French (Attia et al., 2010). Moreover, this parser accepts input that can be tokenized, postagged or pre-bracketed. This possibility allows for different settings when interfacing it with MACAON. 4 Applications MACAON has been used in several projects, two of which are briefly described here, the D EFINIENS project and the L UNA project. D EFINIENS (Barque et al., 2010) is"
P11-4015,W05-0618,0,\N,Missing
P11-4015,J08-2002,0,\N,Missing
P11-4015,sagot-etal-2006-lefff,0,\N,Missing
P12-1082,D11-1113,0,0.0592684,"e initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected in the parses and introduced as new features in the parser. The way we introduce lexical affinity measures in the parser, in 5.1, shares some ideas with (Anguiano and Candito, 2011), who modify some attachments in the parser output, based on lexical information. The main difference is that we only take attachments that appear in an n-best parse list into account, while 778 they consider the first best parse and compute all potential alternative attachments, that may not actually occur in the n-best forests. 3 The Parser The parser used in this work is the second order graph based parser (McDonald et al., 2005; K¨ubler et al., 2009) implementation of (Bohnet, 2010). The parser was trained on the French Treebank (Abeill´e et al., 2003) which was transformed into dependency"
P12-1082,P11-1070,0,0.0752681,"et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in a window of a few words. (Bansal and Klein, 2011; Nakov and Hearst, 2005) rely on the same sort of techniques but use more sophisticated patterns, based on simple paraphrase rules, for identifying co-occurrences. Our work departs from those approaches by the fact that we do not extract the lexical information directly on a raw corpus, but we first parse it and then extract the co-occurrences on the parse trees, based on some predetermined lexico-syntactic patterns. The first reason for this choice is that the linguistic phenomena that we are interested in, such as as PP attachment, coordination, verb subject and object can range over long d"
P12-1082,W09-3821,0,0.0626406,"Missing"
P12-1082,W10-1409,0,0.0210375,"ted from this procedure, the first Previous Work Coping with lexical sparsity of treebanks using raw corpora has been an active direction of research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of word"
P12-1082,2009.jeptalnrecital-long.4,0,0.35731,"Missing"
P12-1082,D09-1060,0,0.020148,"ated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected in the parses and introduced as new features in the parser. The way we introduce lexical affinity measures in the parser, in 5.1, shares some ideas with (Anguiano and Candito, 2011), who modify some attachments in the parser output, based on lexical information. The main difference is that we only take attachments that appear in an n-best parse list into account, while 778 they consider the first best parse and compute all potential alternative attachments, that"
P12-1082,J90-1003,0,0.123387,"flect the tendency of lg and ld to appear together in configuration C. It should be maximal if whenever lg occurs as the governor of configuration C, the dependent position is occupied by ld and, symmetrically, if whenever ld occurs as the dependent of configuration C, the governor position is occupied by lg . A function that conforms such a behavior is the following: s(C, lg , ld ) = 1 2  C(C, lg , ld ) C(C, lg , ld ) + C(C, lg , ∗) C(C, ∗, ld )  it takes its values between 0 (lg and ld never co-occur) and 1 (g and d always co-occur). This function is close to pointwise mutual information (Church and Hanks, 1990) but takes its values between 0 and 1. Description (V, g) CORPUS AFP EST REP WIKI TOTAL (N, d) (N, d) ADJ (N, d) (N, d) (N, d) (N, d) (N, d) (V, d) 4.2 Evaluation Table 4: List of the 9 configurations. The computation of the number of occurrences of an instantiated configuration in the corpus is quite straightforward, it consists in traversing the dependency trees produced by the parser and detect the occurrences of this configuration. At the end of the counts collection, we have gath780 Lexical affinities were computed on three corpora of slightly different genres. The first one, is a collect"
P12-1082,2010.jeptalnrecital-long.3,0,0.0258924,"d in this work is the second order graph based parser (McDonald et al., 2005; K¨ubler et al., 2009) implementation of (Bohnet, 2010). The parser was trained on the French Treebank (Abeill´e et al., 2003) which was transformed into dependency trees by (Candito et al., 2009). The size of the treebank and its decomposition into train, development and test sets is represented in table 1. FTB TRAIN FTB DEV FTB TEST nb of sentences 9 881 1 239 1 235 nb of words 278 083 36 508 36 340 Table 1: Size and decomposition of the French Treebank The part of speech tagging was performed with the MELT tagger (Denis and Sagot, 2010) and lemmatized with the MACAON tool suite (Nasr et al., 2011). The parser gave state of the art results for parsing of French, reported in table 2. LAS UAS pred. POS tags punct no punct 88.02 90.24 90.02 92.50 gold POS tags punct no punct 88.88 91.12 90.71 93.20 Table 2: Labeled and unlabeled accuracy score for automatically predicted and gold POS tags with and without taking into account punctuation on FTB TEST. Figure 1 shows the distribution of the 100 most common error types made by the parser. In this figure, x axis shows the error types and y axis shows the error ratio of the related er"
P12-1082,J04-3001,0,0.172645,"Missing"
P12-1082,P08-1068,0,0.0501283,"in specific lexico-syntactic configurations, are then injected back in the parser. Two outcomes are expected from this procedure, the first Previous Work Coping with lexical sparsity of treebanks using raw corpora has been an active direction of research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics raw corpus or inte"
P12-1082,J93-2004,0,0.0408799,"e parser. Two outcomes are expected from this procedure, the first Previous Work Coping with lexical sparsity of treebanks using raw corpora has been an active direction of research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are comp"
P12-1082,N06-1020,0,0.0601272,"is choice is that the linguistic phenomena that we are interested in, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence"
P12-1082,H05-1066,0,0.218273,"Missing"
P12-1082,W11-2917,1,0.81381,"ve strategy that was used in 4.2.2. But it has an important drawback: it creates inconsistent parses. Recall that the parser we are using is based on a second order model, which means that the score of a dependency depends on some neighboring ones. Since with the post processing method only a subset of the dependencies are modified, the resulting parse is inconsistent: the score of some dependencies is computed on the basis of other dependencies that have been modified. In order to compute a new optimal parse tree that preserves the modified dependencies, we have used a technique proposed in (Mirroshandel and Nasr, 2011) that modifies the scoring function of the parser in such a way that the dependencies that we want to keep in the parser output get better scores than all competing dependencies. The double parsing method is therefore a three stage method. First, sentence S is parsed, producing the n-best parses. Then, the post processing method is used, modifying the first best parse. Let’s note D the set of dependencies that were changed in this process. In the last stage, a new parse is produced, that preserves D. 5.3 Feature Based Method In the feature based method, new features are added to the parser tha"
P12-1082,H05-1105,0,0.090773,"f research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in"
P12-1082,P11-4015,1,0.877849,"Missing"
P12-1082,C10-1100,0,0.0472971,"s. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in a window of a few wo"
P12-1082,D07-1111,0,0.0991287,"n, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected in the parses and introd"
P12-1082,R09-1070,0,0.252787,"Missing"
P12-1082,E03-1008,0,0.0230249,"hat we are interested in, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected"
P12-1082,D09-1058,0,0.0352758,"inguistic phenomena that we are interested in, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interestin"
P12-1082,P11-1156,0,0.0326054,"ective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in a window of a few words. (Bansal and Kle"
P12-1082,J04-4004,0,\N,Missing
P16-1034,W06-2922,0,0.246632,"Missing"
P16-1034,K15-1029,0,0.0385342,"Missing"
P16-1034,W08-2102,0,0.376229,"Missing"
P16-1034,W02-1001,0,0.457232,"Missing"
P16-1034,S12-1029,0,0.144004,"Missing"
P16-1034,P15-1149,0,0.0359796,"Missing"
P16-1034,W00-2011,0,0.801994,"Missing"
P16-1034,P15-1147,0,0.213931,"Missing"
P16-1034,E09-1034,0,0.342612,"Missing"
P16-1034,J11-3004,0,0.183276,"Missing"
P16-1034,N07-2016,0,0.0678523,"Missing"
P16-1034,W12-4613,0,0.289313,"Missing"
P16-1034,P10-1001,0,0.0584847,"Missing"
P16-1034,D10-1125,0,0.121728,"Missing"
P16-1034,W03-3017,0,0.112351,"Missing"
P16-1034,N15-1068,0,0.116598,"Missing"
P16-1034,D12-1044,0,0.481957,"Missing"
P16-1034,Q13-1004,0,0.0404383,"Missing"
P16-1034,P09-1039,0,0.0768998,"Missing"
P16-1034,D12-1067,0,0.242891,"Missing"
P16-1034,D10-1004,0,0.0519394,"Missing"
P16-1034,E06-1011,0,0.168345,"Missing"
P16-1034,H05-1066,0,0.531832,"Missing"
P16-1034,P13-2017,0,0.0684282,"Missing"
P16-1034,D13-1022,0,0.316479,"Missing"
P16-1034,W14-6111,0,0.0957503,"Missing"
P16-1034,P07-1031,0,0.0942568,"Missing"
P16-1034,afonso-etal-2002-floresta,0,\N,Missing
S13-1023,S12-1059,0,0.0484014,"Missing"
S13-1023,P09-1053,0,0.0237935,"Missing"
S13-1023,P05-1045,0,0.00413754,"Missing"
S13-1023,O97-1002,0,0.838454,"Missing"
S13-1023,N04-3012,0,0.558882,"Missing"
S13-1023,W09-3204,0,0.0430859,"Missing"
S13-1023,W06-3104,0,0.0096483,"Missing"
S13-1023,P94-1019,0,\N,Missing
S13-1023,S12-1080,1,\N,Missing
S14-2069,N04-3012,0,0.191944,"measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus3 . 2.3 4 √ P simIR (p, q) = 1 − d∈Lp ∩Lq (sp (d)−sq (d))2 max(sp (d),sq (d)) |Lp ∩ Lq | if |Lp ∩ Lq |= 6 ∅, 0 otherwise. For the participation in the English sub-task we indexed a collection composed by the AQUAINT26 and the English NTCIR-87 document collections, using the Lucene8 4.2 search engine with BM25 similarity. The Spanish index was created using the Spanish QA@CLEF 2005 (agencia EFE1994-95, El Mundo 1994-95) and multiUN Syntactic Dependencies This measure tries to capture the syntactic similarity between two sentences using dependencies. Previous exper"
S14-2069,S13-1023,1,0.839911,"Missing"
S14-2069,eisele-chen-2010-multiun,0,0.0801157,"data/data desc.html#AQUAINT2 7 http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8databases.php 8 http://lucene.apache.org/core http://www.d.umn.edu/ tpederse/similarity.html https://github.com/CNGLdlab/LORG-Release 401 used as clues: we use Yago9 to check whether the NE corresponds to a famous leader or not, and in the affirmative case we include the related nation to the geographical context of the sentence. For instance, “Merkel” is mapped to “Germany”. Given Gp and Gq the sets of places found in sentences p and q, respectively, the geographical context similarity is calculated as follows: (Eisele and Chen, 2010) collections. The K value was set to 70 after a study detailed in (Buscaldi, 2013). 2.5 N-gram Based Similarity This measure tries to capture the fact that similar sentences have similar n-grams, even if they are not placed in the same positions. The measure is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009) for the passage retrieval task. The similarity between a text fragment p and another text fragment q is calculated as: simngrams (p, q) = X ∀x∈Q   simgeo (p, q) = 1−logK 1 + h(x, P ) i=1 wi d(x, xmax ) Pn min d(x, y) x∈Gp y∈Gq max(|Gp"
S14-2069,O97-1002,0,0.465701,"that p and q are similar if the documents retrieved by S for the two texts, used as input queries, are ranked similarly. Let be Lp = {dp1 , . . . , dpK } and Lq = {dq1 , . . . , dqK }, dxi ∈ D the sets of the top K documents retrieved by S for texts p and q, respectively. Let us define sp (d) and sq (d) the scores assigned by S to a document d for the query p and q, respectively. Then, the similarity score is calculated as: IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus3 . 2.3 4 √ P simIR (p, q) = 1 − d∈Lp ∩Lq (sp ("
S14-2069,buscaldi-rosso-2008-geo,1,\N,Missing
S14-2069,P94-1019,0,\N,Missing
W06-1502,W02-2233,1,0.887812,"Missing"
W06-1502,2006.jeptalnrecital-long.12,0,0.101658,"Missing"
W08-2319,W98-0106,0,0.0368273,"on. NP VP D NP V NP P D NP N of the N one has Introduction Each sentence derivation in a tree adjoining grammar (Joshi and Schabes, 1997, TAG) results in two parse trees: a derived tree (Figure 1a), that represents the phrase structure of the sentence, and a derivation tree (Figure 1b), that records how the elementary trees of the grammar were combined. Each type of parse tree is better suited for a different set of language processing tasks: the derived tree is closely related to the lexical elements of the sentence, and the derivation tree offers a first insight into the sentence semantics (Candito and Kahane, 1998). Furthermore, the derivation tree language of a TAG, being a regular tree language, is much simpler to manipulate than the corresponding derived tree language. Derivation trees are thus the cornerstone of several approaches to sentence generation (Koller and Striegnitz, 2002; Koller and Stone, 2007), that rely crucially on the ease of encoding regular tree grammars, as dependency grammars and planning problems respectively. Derivation trees also serve as intermediate representations from which both derived trees (and thus the linear order information) and semantics can be computed, e.g. with"
W08-2319,W02-2220,0,0.753145,"a TAG, being a regular tree language, is much simpler to manipulate than the corresponding derived tree language. Derivation trees are thus the cornerstone of several approaches to sentence generation (Koller and Striegnitz, 2002; Koller and Stone, 2007), that rely crucially on the ease of encoding regular tree grammars, as dependency grammars and planning problems respectively. Derivation trees also serve as intermediate representations from which both derived trees (and thus the linear order information) and semantics can be computed, e.g. with the abstract categorial grammars of de Groote (2002), NP V caught cats D NP a N fish (a) Derived tree. caught NP0 1 VP NP1 VP cats has fish NP NP the a NPr one of (b) Derivation tree. Figure 1: Parse trees for “One of the cats has caught a fish.” using the grammar of Figure 2. Pogodalla (2004), and Kanazawa (2007), or similarly with the bimorphisms of Shieber (2006). Nevertheless, these results do not directly apply to many real-world grammars, which are expressed in a feature-based variant of TAGs (VijayShanker, 1992). Each elementary tree node of these grammars carries two feature structures that constrain the allowed substitution or adjuncti"
W08-2319,E03-1030,0,0.0683394,": mode : ppart  VPr V Vbot:[ mode : ppart ]VP∗f caught agr : 3sg mode : ind has ]VP N NP1 ↓ h bot: agr : 3sg const : + def : − D i NPr  bot: agr : 3sg const : − fish  NP∗ f a Figure 2: A feature-based tree adjoining grammar. For the sake of clarity, we identify elementary trees with their anchors in our examples. plementation: for instance, the 50 features used in the XTAG English grammar (XTAG Research Group, 2001) together define a domain containing more than 1019 different structures. Furthermore, finiteness does not hold for some grammars, for instance with the semantic features of Gardent and Kallmeyer (2003). Ignoring feature structures typically results in massive over-generation in derivation-centric systems. We define a formalism, feature-based regular tree grammars, that produces derivation trees that account for the feature structures found in a tree adjoining grammar. In more details, • we recall how to generate the derivation trees of a tree adjoining grammar through a regular tree grammar (Section 2), then • we define feature-based regular tree grammars and present the translation from featurebased TAG (Section 3); finally, • we provide an improved translation inspired by left corner tran"
W08-2319,2006.jeptalnrecital-long.12,0,0.0827051,"Missing"
W08-2319,P80-1024,0,0.378337,"Missing"
W08-2319,J95-4002,0,0.0726734,"Missing"
W08-2319,E06-1048,0,0.0954952,"tree grammars, as dependency grammars and planning problems respectively. Derivation trees also serve as intermediate representations from which both derived trees (and thus the linear order information) and semantics can be computed, e.g. with the abstract categorial grammars of de Groote (2002), NP V caught cats D NP a N fish (a) Derived tree. caught NP0 1 VP NP1 VP cats has fish NP NP the a NPr one of (b) Derivation tree. Figure 1: Parse trees for “One of the cats has caught a fish.” using the grammar of Figure 2. Pogodalla (2004), and Kanazawa (2007), or similarly with the bimorphisms of Shieber (2006). Nevertheless, these results do not directly apply to many real-world grammars, which are expressed in a feature-based variant of TAGs (VijayShanker, 1992). Each elementary tree node of these grammars carries two feature structures that constrain the allowed substitution or adjunction operations at this node (see for instance Figure 2). In theory, such structures are unproblematic, because the possible feature values are drawn from finite domains, and thus the number of grammar categories could be increased in order to account for all the possible structures. In practice, the sheer number of"
W08-2319,J92-4004,0,0.266181,"Missing"
W08-2319,W04-3321,0,0.0509006,", trlc (β2 ), . . . , trlc (βn )) |β ∈ A, n = rk(β), X = lab(βr )} → ∪{(XA , in(β)) − β(tr(β1 ), trlc (β2 ), . . . , trlc (βn )) |β ∈ A, n = rk(β), X = lab(βr )}  top : t  ∪{XA bot : t − → εA |X A ∈ N A } Again, the translation can be computed in linear time, and results in a grammar with at worst twice the size of the original TAG. 5 Conclusion We have introduced in this paper feature-based regular tree grammars as an adequate representation for the derivation language of large coverage TAG grammars. Unlike the restricted unification computations on the derivation tree considered before by Kallmeyer and Romero (2004), feature-based RTGs accurately translate the full range of unification mechanisms employed in TAGs. Moreover, left-corner transformed grammars make derivations more predictable, thus avoiding some backtracking in top-down generation. Among the potential applications of our results, let us further mention more accurate reachability computations between elementary trees, needed for instance in order to check whether a TAG complies with the tree insertion grammar (Schabes and Waters, 1995, TIG) or regular form (Rogers, 1994, RFTAG) conditions. In fact, among the formal checks one might wish to p"
W08-2319,P07-1023,0,0.0526189,"), that rely crucially on the ease of encoding regular tree grammars, as dependency grammars and planning problems respectively. Derivation trees also serve as intermediate representations from which both derived trees (and thus the linear order information) and semantics can be computed, e.g. with the abstract categorial grammars of de Groote (2002), NP V caught cats D NP a N fish (a) Derived tree. caught NP0 1 VP NP1 VP cats has fish NP NP the a NPr one of (b) Derivation tree. Figure 1: Parse trees for “One of the cats has caught a fish.” using the grammar of Figure 2. Pogodalla (2004), and Kanazawa (2007), or similarly with the bimorphisms of Shieber (2006). Nevertheless, these results do not directly apply to many real-world grammars, which are expressed in a feature-based variant of TAGs (VijayShanker, 1992). Each elementary tree node of these grammars carries two feature structures that constrain the allowed substitution or adjunction operations at this node (see for instance Figure 2). In theory, such structures are unproblematic, because the possible feature values are drawn from finite domains, and thus the number of grammar categories could be increased in order to account for all the p"
W08-2319,P07-1043,0,0.0211254,"w the elementary trees of the grammar were combined. Each type of parse tree is better suited for a different set of language processing tasks: the derived tree is closely related to the lexical elements of the sentence, and the derivation tree offers a first insight into the sentence semantics (Candito and Kahane, 1998). Furthermore, the derivation tree language of a TAG, being a regular tree language, is much simpler to manipulate than the corresponding derived tree language. Derivation trees are thus the cornerstone of several approaches to sentence generation (Koller and Striegnitz, 2002; Koller and Stone, 2007), that rely crucially on the ease of encoding regular tree grammars, as dependency grammars and planning problems respectively. Derivation trees also serve as intermediate representations from which both derived trees (and thus the linear order information) and semantics can be computed, e.g. with the abstract categorial grammars of de Groote (2002), NP V caught cats D NP a N fish (a) Derived tree. caught NP0 1 VP NP1 VP cats has fish NP NP the a NPr one of (b) Derivation tree. Figure 1: Parse trees for “One of the cats has caught a fish.” using the grammar of Figure 2. Pogodalla (2004), and K"
W08-2319,P02-1003,0,0.326401,"(Figure 1b), that records how the elementary trees of the grammar were combined. Each type of parse tree is better suited for a different set of language processing tasks: the derived tree is closely related to the lexical elements of the sentence, and the derivation tree offers a first insight into the sentence semantics (Candito and Kahane, 1998). Furthermore, the derivation tree language of a TAG, being a regular tree language, is much simpler to manipulate than the corresponding derived tree language. Derivation trees are thus the cornerstone of several approaches to sentence generation (Koller and Striegnitz, 2002; Koller and Stone, 2007), that rely crucially on the ease of encoding regular tree grammars, as dependency grammars and planning problems respectively. Derivation trees also serve as intermediate representations from which both derived trees (and thus the linear order information) and semantics can be computed, e.g. with the abstract categorial grammars of de Groote (2002), NP V caught cats D NP a N fish (a) Derived tree. caught NP0 1 VP NP1 VP cats has fish NP NP the a NPr one of (b) Derivation tree. Figure 1: Parse trees for “One of the cats has caught a fish.” using the grammar of Figure 2"
W08-2319,W04-3309,0,0.0908495,"Koller and Stone, 2007), that rely crucially on the ease of encoding regular tree grammars, as dependency grammars and planning problems respectively. Derivation trees also serve as intermediate representations from which both derived trees (and thus the linear order information) and semantics can be computed, e.g. with the abstract categorial grammars of de Groote (2002), NP V caught cats D NP a N fish (a) Derived tree. caught NP0 1 VP NP1 VP cats has fish NP NP the a NPr one of (b) Derivation tree. Figure 1: Parse trees for “One of the cats has caught a fish.” using the grammar of Figure 2. Pogodalla (2004), and Kanazawa (2007), or similarly with the bimorphisms of Shieber (2006). Nevertheless, these results do not directly apply to many real-world grammars, which are expressed in a feature-based variant of TAGs (VijayShanker, 1992). Each elementary tree node of these grammars carries two feature structures that constrain the allowed substitution or adjunction operations at this node (see for instance Figure 2). In theory, such structures are unproblematic, because the possible feature values are drawn from finite domains, and thus the number of grammar categories could be increased in order to"
W08-2319,P94-1022,0,0.155998,"Missing"
W09-3809,P81-1022,0,0.618228,"m, departing from previous methods which rely on constraint-solving techniques. 1 Introduction An Interaction Grammar (IG) (Guillaume and Perrier, 2008) is a lexicalized grammatical formalism that primarily focuses on valency, explicitly expressed using polarities decorating syntagms. These polarities and the use of underspecified structures naturally lead parsing to be viewed as a constraint-solving problem – for example (Bonfante et al.) reduce the parsing problem to a graphrewriting problem in (2003) . However, in this article we depart from this approach and present an algorithm close to (Earley, 1970) for context-free grammars. We introduce this algorithm using the standard framework of deductive parsing (Shieber et al., 1995). This article is organised as follows: we first present IGs (section 2), then we describe the algorithm (section 3). Finally we discuss some technical points and conclude (sections 4 and 5). 2 2.2 An IG is a tuple G = {Σ, C, S, P, phon}, where Σ is the terminal alphabet, C the non-terminal alphabet, S ∈ C the initial symbol, P is a set of PTDs with node labels in C × P, and phon is a function from anchors in P to Σ. The structure obtained from parsing is a syntactic"
W09-3809,W03-3020,0,0.0681587,"Missing"
W09-3809,P95-1021,0,0.087558,"leaf L is labelled with a terminal, this terminal is denoted word(L). Interaction Grammars We briefly introduce IGs as in (Guillaume and Perrier, 2008)1 . However, we omit polarized feature structures, for the sake of exposition. 2.1 Grammars Polarized Tree Descriptions The structures associated with words by the lexicon are Polarized Tree Descriptions (PTDs). They represent fragments of parse trees. The nodes of these structures are labelled with a category and a 2 This name comes from the superposition introduced in previous presentations of IGs. 3 For readers familiar with D-Tree Grammars (Rambow et al., 1995), &gt; adds an i-edge while &gt;∗ adds a d-edge. 1 This paper also discusses the linguistic motivations behind IGs. 65 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 65–68, c Paris, October 2009. 2009 Association for Computational Linguistics 3.1 We will write M  N if the node M is the mother of N and N  [N1 , . . . , Nk ] if the N is the mother of the ordered list of nodes [N1 , . . . , Nk ]. The order between siblings can also be expressed using the relation ≺≺: M ≺≺ N means that N is the immediate successor of M . ≺≺+ is the transitive closure of ≺≺ and"
W10-1408,P05-1038,0,0.0366439,"the only edge which is added to the chart at this position is the one corresponding to the rule V BD → UNK-ed. For our English experiments we use the unknown word classes (or signatures) which are used in the Berkeley parser. A signature indicates whether a words contains a digit or a hyphen, if a word starts with a capital letter or ends with one of the following English suffixes (both derivational and inflectional): -s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al. For our French experiments we employ the same signature list as Crabb´e and Candito (2008), which itself was adapted from Arun and Keller (2005). This list consists of (a) conjugation suffixes of reguIn order to use morphological clues for Arabic we go further than just looking at suffixes. We exploit all the richness of the morphology of this language which can be expressed through morphotactics. these indicators are prefixes, suffixes and word templates. A template (Beesley and Karttunen, 2003) is a kind of vocalization mould in which a word fits. In derivational morphology Arabic words are formed through the amalgamation of two tiers, namely, root and template. A root is a sequence of three (rarely two or four) consonants which are"
W10-1408,W98-1007,0,0.0593928,"signature information for French and English is shown in Table 3. Beside each f-score the absolute improvement over the UNKNOWN baseline (Table 2) is given. For both languages there is an improvement at all unknown thresholds. The improvement for English is statistically significant at unknown thresholds 1 and 10.4 The improvement is more marked for French and is statistically significant at all levels. In the next section, we experiment with signature lists for Arabic.5 6 Arabic Signatures Handling Arabic Morphotactics Morphotactics refers to the way morphemes combine together to form words (Beesley, 1998; Beesley and Karttunen, 2003). Generally speaking, morphotactics can be concatenative, with morphemes either prefixed or suffixed to stems, or non-concatenative, with stems undergoing internal alternations to convey morphosyntactic information. Arabic is considered a typical example of a language that employs non-concatenative morphotactics. Arabic words are traditionally classified into three types: verbs, nouns and particles. Adjectives take almost all the morphological forms of, and share the same templatic structures with, nouns. Adjectives, for example, can be definite, and are inflected"
W10-1408,W09-3821,0,0.223368,"Missing"
W10-1408,W09-1008,0,0.0505203,"Missing"
W10-1408,A00-2018,0,0.244617,"61 94.90 92.99 91.56 Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model of the words in the Arabic and French development sets are unknown, and this is reflected in the drop in parsing performance at these thresholds. 5 Making use of Morphology Unknown words are not all the same. We exploit this fact by examining the effect on parsing accuracy of clustering rare training set words using cues from the word’s morphological structure. Affixes have been shown to be useful in part-of-speech tagging (Schmid, 1994; Tseng et al., 2005) and have been used in the Charniak (Charniak, 2000), Stanford (Klein and Manning, 2003) and Berkeley (Petrov et al., 2006) parsers. In this section, we contrast the effect on parsing accuracy of making use of such information for our three languages of interest. Returning to our toy English example in Figures 1 and 2, and given the input sentence The shares recovered, we would like to use the fact that the un70 known word recovered ends with the past tense suffix -ed to boost the probability of the lexical rule V BD → UNKNOWN. If we specialise the UNKNOWN terminal using information from English morphology, we can do just that, resulting in the"
W10-1408,2008.jeptalnrecital-long.17,0,0.145906,"Missing"
W10-1408,E09-1038,0,0.160992,"Missing"
W10-1408,P08-2015,0,0.0114925,"Missing"
W10-1408,D09-1087,0,0.13706,"Missing"
W10-1408,J98-4004,0,0.381711,"hnique is extended to include morphological information and present parsing results for English and French. In Section 6, we describe the Arabic morphological system and explain how we used heuristic rules to cluster words into word-classes or signatures. We present parsing results for the version of the parser which uses this information. In Section 7, we describe our attempts to automatically determine the signatures for a language and present parsing results for the three languages. Finally, in Section 8, we discuss how this work might be fruitfully extended. 2 Latent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations a"
W10-1408,W04-1602,0,0.0128084,"Missing"
W10-1408,H94-1020,0,0.166217,"Missing"
W10-1408,P05-1010,0,0.1217,"uses this information. In Section 7, we describe our attempts to automatically determine the signatures for a language and present parsing results for the three languages. Finally, in Section 8, we discuss how this work might be fruitfully extended. 2 Latent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. 1. Split each annotation of each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. E"
W10-1408,W05-0711,0,0.0499235,"Missing"
W10-1408,N07-1051,0,0.0697355,"each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. Evaluate the impact of the newly created annotations and discard the least useful ones. Reestimate probabilities with the new set of annotations. 3. Smooth the probabilities to prevent overfitting. We use our own parser which trains a PCFG-LA using the above procedure and parses using the max1 Estimation of the parameters is performed by running Expectation/Maximisation on the training corpus. 68 rule parsing algorithm (Petrov et al., 2006; Petrov and Klein, 2007). PCFG-LA parsing is relatively language-independent but has been shown to be very effective on several languages (Petrov, 2009). For our experiments, we set the number of iterations to be 5 and we test on sentences less than or equal to 40 words in length. All our experiments, apart from the final one, are carried out on the development sets of our three languages. 3 The Datasets Arabic We use the the Penn Arabic Treebank (ATB) (Bies and Maamouri, 2003; Maamouri and Bies., 2004). The ATB describes written Modern Standard Arabic newswire and follows the style and guidelines of the English Penn"
W10-1408,P06-1055,0,0.487124,"tent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. 1. Split each annotation of each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. Evaluate the impact of the newly created annotations and discard the least useful ones. Reestimate probabilities with the new set of annotations. 3. Smooth the probabilities to prevent overfitting. We use our own parser which trains a PCFG-LA using the"
W10-1408,I05-3005,0,0.0234663,"URE UNTagging Accuracy 94.03 91.16 89.06 95.60 94.66 93.61 94.90 92.99 91.56 Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model of the words in the Arabic and French development sets are unknown, and this is reflected in the drop in parsing performance at these thresholds. 5 Making use of Morphology Unknown words are not all the same. We exploit this fact by examining the effect on parsing accuracy of clustering rare training set words using cues from the word’s morphological structure. Affixes have been shown to be useful in part-of-speech tagging (Schmid, 1994; Tseng et al., 2005) and have been used in the Charniak (Charniak, 2000), Stanford (Klein and Manning, 2003) and Berkeley (Petrov et al., 2006) parsers. In this section, we contrast the effect on parsing accuracy of making use of such information for our three languages of interest. Returning to our toy English example in Figures 1 and 2, and given the input sentence The shares recovered, we would like to use the fact that the un70 known word recovered ends with the past tense suffix -ed to boost the probability of the lexical rule V BD → UNKNOWN. If we specialise the UNKNOWN terminal using information from Engli"
W10-1408,P03-1054,0,\N,Missing
W12-3408,W10-1408,1,0.916496,"Missing"
W12-3408,W10-1409,1,0.884663,"ols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are collected on the training set, ranked according to their information gain with regards to the partof-speech tagging task. Hence, all the experiments are presented in two settings. In the first one, called generic, unknown words are replaced with a dummy token UNK"
W12-3408,chrupala-etal-2008-learning,0,0.220092,"Missing"
W12-3408,J05-1003,0,0.0105145,"nce (around 2.3 points better, see Table 8). Table 7: Lemmmatization Experiments In all cases reduced2 is below the other tagsets wrt. to Parseval F1 although tagging accuracy is better. We can conclude that it is too poor from an informational point of view. 4 Discussion There is relatively few works actively pursued on statistical constituency parsing for Spanish. The initial work of Cowan and Collins (2005) consisted in a thorough study of the impact of various morphological features on a lexicalized parsing model (the Collins Model 1) and on the performance gain brought by the reranker of Collins and Koo (2005) used in conjunction with the feature set developed for English. Direct comparison is difficult as they used a different test set (approximately, the concatenation of our development and test sets). They report an F-score of 85.1 on sentences of length less than 40.5 However, we are directly comparable with Chrupała (2008)6 who adapted the Collins Model 2 to Spanish. As he was focusing on wide coverage LFG grammar induction, he enriched the non terminal annotation scheme with functional paths rather than trying to obtain the optimal tagset with respect to pure parsing performance. Nevertheless"
W12-3408,H05-1100,0,0.200659,"ce techniques. We rely on accurate data-driven lemmatization and partof-speech tagging to reduce data sparseness and ease the burden on the parser. We try to see how we can improve parsing structure predictions solely by modifying the terminals and/or the preterminals of the trees. We keep the rest of the tagset as is. In order to validate our method, we perform experiments on the Cast3LB constituent treebank for Spanish (Castillan). This corpus is quite small, around 3,500 trees, and Spanish is known to have a rich verbal morphology, making the tag set quite complex and difficult to predict. Cowan and Collins (2005) and Chrupała (2008) already showed interesting results on this corpus that will provide us with a comparison for this work, especially on the lexical aspects as they used lexicalized frameworks while we choose PCFG-LAs. This paper is structured as follows. In Section 2 we describe the Cast3LB corpus in details. In Section 3 we present our experimental setup and results which we discuss and compare in Section 4. Finally, Section 5 concludes the presentation. 2 Data Set The Castillan 3LB treebank (Civit and Martì, 2004) contains 3,509 constituent trees with functional annotations. It is divided"
W12-3408,Y09-1013,1,0.835017,"strategy dubbed simple lexicon in the Berkeley parser. Rare words – words occurring less than 3 times in the training set – are replaced by a special token, which depends on the OOV handling method (generic or IG), before collecting counts. POS tagging We performed parsing experiments with three different settings regarding POS information provided as an input to the parser: (i) with no POS information, which constitutes our baseline; (ii) with gold POS information, which can be considered as a topline for a given parser setting; (iii) with POS information predicted using the MElt POS-tagger (Denis and Sagot, 2009), using three different tagsets that we describe below. MElt is a state-of-the-art sequence labeller that is trained on both an annotated corpus and an external lexicon. The standard version of MElt relies on Maximum-Entropy Markov models (MEMMs). However, in this work, we have used a multiclass perceptron instead, as it allows for much faster training with very small performance drops (see Table 2). For training purposes, we used the training section of the Cast3LB (76,931 tokens) and the Leffe lexicon (Molinero et al., 2009), which contains almost 800,000 distinct (form, category) pairs.3 We"
W12-3408,P05-1010,0,0.0162841,"iments We conducted experiments on the Cast3LB development set in order to test various treebank modifications, that can be divided in two categories: (i) modification of the preterminal symbols of the treebank by using simplified POS tagsets; (ii) modification of the terminal symbols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-"
W12-3408,R09-1049,1,0.830632,"ng; (iii) with POS information predicted using the MElt POS-tagger (Denis and Sagot, 2009), using three different tagsets that we describe below. MElt is a state-of-the-art sequence labeller that is trained on both an annotated corpus and an external lexicon. The standard version of MElt relies on Maximum-Entropy Markov models (MEMMs). However, in this work, we have used a multiclass perceptron instead, as it allows for much faster training with very small performance drops (see Table 2). For training purposes, we used the training section of the Cast3LB (76,931 tokens) and the Leffe lexicon (Molinero et al., 2009), which contains almost 800,000 distinct (form, category) pairs.3 We performed experiments using three different 1 Names generic and IG originally come from Attia et al. (2010). 2 We tried to perform 4 and 5 rounds but 3 rounds proved to be optimal on this corpus. 3 Note that MElt does not use information from the exterTAGSET baseline reduced2 Nb. of tags 106 42 Multiclass Perceptron Overall Acc. 96.34 97.42 Unk. words Acc. 91.17 93.35 Maximum-Entropy Markov model (MEMM) Overall Acc. 96.46 97.42 Unk. words Acc. 91.57 93.76 reduced3 57 97.25 92.30 97.25 92.87 Table 2: MElt POS tagging accuracy"
W12-3408,W08-1005,0,0.0710606,"rd tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are collected on the training set, ranked according to their information gain with regards to the partof-speech tagging task. Hence, all the experiments are presented in two settings. In the first one, called generic, unknown words are replaced with a dummy token UNK, while in the second one, dubbed"
W12-3408,P06-1055,0,0.111335,"riments on the Cast3LB development set in order to test various treebank modifications, that can be divided in two categories: (i) modification of the preterminal symbols of the treebank by using simplified POS tagsets; (ii) modification of the terminal symbols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are"
W12-3408,P81-1022,0,0.762867,"Missing"
W12-3408,N07-1051,0,\N,Missing
W12-3412,W09-3821,0,0.0545879,"Missing"
W12-3412,C10-2013,0,0.219842,"ith a linear model (5). The parse with the best score is considered as final. The structure of the paper is the following: in Section 2 we describe the details of our generative parser and in Section 3 our reranking model together with the features templates. Section 4 reports the results of the experiments conducted on the Penn Treebank (Marcus et al., 1994) as well as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introduced in (Matsuzaki et al., 2005) can be seen as automatically specialised PCFGs learnt from treebanks. Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol. More formally, the probability of an unannotated tree i"
W12-3412,candito-etal-2010-statistical,0,0.196867,"ith a linear model (5). The parse with the best score is considered as final. The structure of the paper is the following: in Section 2 we describe the details of our generative parser and in Section 3 our reranking model together with the features templates. Section 4 reports the results of the experiments conducted on the Penn Treebank (Marcus et al., 1994) as well as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introduced in (Matsuzaki et al., 2005) can be seen as automatically specialised PCFGs learnt from treebanks. Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol. More formally, the probability of an unannotated tree i"
W12-3412,W10-1408,1,0.890534,"Missing"
W12-3412,W08-2102,0,0.107256,"for Computational Linguistics, pages 89–99, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics pendents of a predicate. On the other hand, dependencies extracted from constituent parses are known to be more accurate than dependencies obtained from dependency parsers. Therefore the solution we choose is an indirect one: we use a phrase-based parser to generate n-best lists and convert them to lists of dependency structures that are reranked. This approach can be seen as trade-off between phrasebased reranking experiments (Collins, 2000) and the approach of Carreras et al. (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism. Our architecture, illustrated in Figure 1, is based on two steps. During the first step, a syntagmatic parser processes the input sentence and produces nbest parses as well as their probabilities. They are annotated with a functional tagger which tags syntagms with standard syntactic functions subject, object, indirect object . . . and converted to dependency structures by application of percolation rules. In the second step, we extract a set of features"
W12-3412,P05-1022,0,0.610797,"ect of some recent work thanks to progresses achieved in the field of Machine Learning. A parse tree is represented as a vector of features and its accuracy is measured as the distance between this vector and the reference. One way to take advantage of both approaches is to combine them sequentially, as initially proposed by Collins (2000). A generative parser produces a set of candidates structures that constitute the input of a second, discriminative module, whose search space is limited to this set of candidates. Such an approach, parsing followed by reranking, is used in the Brown parser (Charniak and Johnson, 2005). The approach can be extended in order to feed the reranker with the output of different parsers, as shown by (Johnson and Ural, 2010; Zhang et al., 2009). In this paper we are interested in applying reranking to dependency structures. The main reason is that many linguistic constraints are straightforward to implement on dependency structures, as, for example, subcategorization frames or selectional constraints that are closely linked to the notion of deProceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89–99, c Jeju, Republic of Korea, 12 July 201"
W12-3412,P97-1003,0,0.249871,"his can be implemented efficiently using two weight vectors as for the averaged perceptron. parses as possible, but specific enough to characterize good and bad configurations in the parse tree. We extended the feature set from (McDonald, 2006) which showed to be effective for a range of languages. Our feature templates can be categorized in 5 classes according to their domain of locality. In the following, we describe and exemplify these templates on the following sentence from the Penn treebank, in which we target the PMOD dependency between “at” and “watch.” like bilexical dependencies in (Collins, 1997). l Given dependency xi → xj , the feature created is (word xi , lemma xi , pos-tag xi , word xj , lemma xj , pos-tag xj , distance3 from i to j, direction, type). The previous example generates the following feature: [at, at, IN, watch, watch, NN, 2, R, PMOD] Where 2 is the distance between “at” and “watch”. Probability Three features are derived from the P CFG -L A parser, namely the posterior probability of the parse (eq. 1), its normalized probability relative to the 1-best, and its rank in the n-best list. Unigram Unigram features are the most simple as they only involve one word. Given a"
W12-3412,Y09-1013,0,0.0423233,"odel, namely Tree Adjoining Grammars, and the system of Suzuki et al. (2009) that makes use of external data (unannotated text). Huang, 2008 Bohnet, 2010 Zhang et al, 2008 Huang and Sagae, 2010 Charniak et al, 2005 Carreras et al. 2008 Suzuki et al. 2009 This work F 91.7 – 91.4 – 91.5 – – 91.1 LAS – 90.3 – – 90.0 – – 89.8 UAS – – 93.2 92.1 94.0 93.5 93.8 93.9 Table 3: Comparison on PTB Test set For French, see Table 4, we compare our system with the M ATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al., 2005) with hash kernels, using the ME LT part-of-speech tagger (Denis and Sagot, 2009) and our own lemmatiser. We also compare the French system with results drawn from the benchmark performed by Candito et al. (2010a). The first system (BKY-FR) is close to ours without the reranking module, using the Berkeley parser adapted to French. The second (MSTFR) is based on MSTParser (McDonald et al., 2005). These two system use word clusters as well. The next section takes a close look at the models 96 This work M ATE + ME LT B KY-F R M ST-F R F < 40 89.2 – 88.2 – LAS 89.2 89.2 86.8 88.2 UAS 92.1 91.8 91.0 90.9 Table 4: Comparison on FTB Test set 4.3.4 Model Analysis It is interesting"
W12-3412,P08-1067,0,0.164776,"k + MElt 87.4 89.2 89.2 92.1 7 The model is always trained with 100 candidates. F < 40 is the parseval F-score for sentences with less than 40 words. 8 95 Table 2: System results on FTB Test set 4.3.3 Comparison with Related Work of the reranker and its impact on performance. We compare our results with related parsing results on English and French. For English, the main results are shown in Table 3. From the presented data, we can see that indirect reranking on LAS may not seem as good as direct reranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. However, our system does not rely on any language specific feature and can be applied to other languages/treebanks. It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). Our system also compares favourably with the system of Carreras et al. (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al. (2009) that makes use of external data (unannotated text). Huang, 2"
W12-3412,W07-2416,0,0.0538469,"Missing"
W12-3412,H94-1020,0,0.383534,"tion w (5) MIRA reranking Final constituency & dependency parse Figure 1: The parsing architecture: production of the nbest syntagmatic trees (1) tagged with functional labels (2), conversion to a dependency structure (3) and feature extraction (4), scoring with a linear model (5). The parse with the best score is considered as final. The structure of the paper is the following: in Section 2 we describe the details of our generative parser and in Section 3 our reranking model together with the features templates. Section 4 reports the results of the experiments conducted on the Penn Treebank (Marcus et al., 1994) as well as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introdu"
W12-3412,P05-1010,0,0.0421253,"as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introduced in (Matsuzaki et al., 2005) can be seen as automatically specialised PCFGs learnt from treebanks. Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol. More formally, the probability of an unannotated tree is the sum of the probabilities of its annotated counterparts. For a P CFG -L A G, R is the set of annotated rules, D(t) is the set of (annotated) derivations of an unannotated tree t, and R(d) is the set of rules used in a derivation d. Then the probability assigned by G to t is: PG (t) = X d∈D(t) PG (d) = X Y PG (r) (1) 3 Discriminative model d∈D(t) r∈R(d) Because of t"
W12-3412,P05-1012,0,0.0505025,"h the system of Carreras et al. (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al. (2009) that makes use of external data (unannotated text). Huang, 2008 Bohnet, 2010 Zhang et al, 2008 Huang and Sagae, 2010 Charniak et al, 2005 Carreras et al. 2008 Suzuki et al. 2009 This work F 91.7 – 91.4 – 91.5 – – 91.1 LAS – 90.3 – – 90.0 – – 89.8 UAS – – 93.2 92.1 94.0 93.5 93.8 93.9 Table 3: Comparison on PTB Test set For French, see Table 4, we compare our system with the M ATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al., 2005) with hash kernels, using the ME LT part-of-speech tagger (Denis and Sagot, 2009) and our own lemmatiser. We also compare the French system with results drawn from the benchmark performed by Candito et al. (2010a). The first system (BKY-FR) is close to ours without the reranking module, using the Berkeley parser adapted to French. The second (MSTFR) is based on MSTParser (McDonald et al., 2005). These two system use word clusters as well. The next section takes a close look at the models 96 This work M ATE + ME LT B KY-F R M ST-F R F < 40 89.2 – 88.2 – LAS 89.2 89.2 86.8 88.2 UAS 92.1 91.8 91."
W12-3412,P08-1108,0,0.0159804,"al linguistic knowledge (lexical preferences, subcategorisation frames, copula verbs, coordination symmetry . . . ). Integrating features from the phrase-structure trees is also an option that needs to be explored. Third this architecture enables the integration of several systems. We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnson and Ural, 2010) or (Zhang et al., 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). Finally it should be noted that this system does not rely on any language specific feature, and thus can be applied to languages other that French or English S NP VP NP syntagmatic parses S NP PP QP VP NP PP NP PP QP NP NNP May PP NP NP CD 31 NNS VBD CD CD IN NNS Stocks were 698 million bushels on IN DT NN . of this year . NNP May CD 31 IN DT NN . of this year . dependency parses IN NNS VBD CD CD NNS Stocks were 698 million bushels on NP Before reranking After reranking Figure 3: English sentence from the WSJ test set for which the reranker selected the correct tree while the first candidate"
W12-3412,N07-1051,0,0.0129372,"ore formally, the probability of an unannotated tree is the sum of the probabilities of its annotated counterparts. For a P CFG -L A G, R is the set of annotated rules, D(t) is the set of (annotated) derivations of an unannotated tree t, and R(d) is the set of rules used in a derivation d. Then the probability assigned by G to t is: PG (t) = X d∈D(t) PG (d) = X Y PG (r) (1) 3 Discriminative model d∈D(t) r∈R(d) Because of this alternation of sums and products that cannot be optimally factorised, there is no exact polynomial dynamic programming algorithm for parsing. Matsuzaki et al. (2005) and Petrov and Klein (2007) discuss approximations of the decoding step based on a Bayesian variational approach. This enables cubic time decoding that can be further enhanced with coarse-to-fine methods (Charniak and Johnson, 2005). This type of grammars has already been tested on a variety of languages, in particular English and French, giving state-of-the-art results. Let us stress that this phrase-structure formalism is not lexicalised as opposed to grammars previously used in reranking experiments (Collins, 2000; Charniak and Johnson, 2005). The notion of lexical head is therefore absent at parsing time and will be"
W12-3412,P06-1055,0,0.0101291,"entence and produces nbest parses as well as their probabilities. They are annotated with a functional tagger which tags syntagms with standard syntactic functions subject, object, indirect object . . . and converted to dependency structures by application of percolation rules. In the second step, we extract a set of features from the dependency parses and the associated probabilities. These features are used to reorder the n-best list and select a potentially more accurate parse. Syntagmatic parses are produced by the implementation of a P CFG - LA parser of (Attia et al., 2010), similar to (Petrov et al., 2006), a functional tagger and dependency converter for the target language. The reranking model is a linear model trained with an implementation of the M IRA algorithm (Crammer et al., 2006)1 . Charniak and Johnson (2005) and Collins (2000) rerank phrase-structure parses and they also include head-dependent information, in other words unlabelled dependencies. In our approach we take into account grammatical functions or labelled dependencies. It should be noted that the features we use are very generic and do not depend on the linguistic knowledge of the authors. We applied our method to English,"
W12-3412,N10-1049,0,0.020729,"(Candito et al., 2010b) showed that a sequential approach is better than an integrated one for contextfree grammars, because the strong independence hypothesis of this formalism implies a restricted domain of locality which cannot express the context needed to properly assign functions. Most functional taggers, such as the ones used in the following experiments, rely on classifiers whose feature sets can describe the whole context of a node in order to make a decision. Dependency Structures A syntactic theory can either be expressed with phrase structures or dependencies, as advocated for in (Rambow, 2010). However, some information may be simpler to describe in one of the representations. This equivalence between the modes of representations only stands if the informational contents are the same. Unfortunately, this is not the case here because the phrase structures that we use do not contain functional annotations and lexical heads, whereas labelled dependencies do. 91 Our discriminative model is a linear model trained with the Margin-Infused Relaxed Algorithm (M IRA) (Crammer et al., 2006). This model computes the score of a parse tree as the inner product of a feature vector and a weight ve"
W12-3412,sagot-2010-lefff,0,0.0216271,"r as in (Candito and Crabbé, 2009). For both languages we constructed 10-fold training data from train sets in order to avoid overfitting the training data. The trees from training sets were divided into 10 subsets and the parses for each subset were generated by a parser trained on the other 4 Functions are omitted. 94 9 subsets. Development and test parses are given by a parser using the whole training set. Development sets were used to choose the best reranking model. For lemmatisation, we use the MATE lemmatiser for English and a home-made lemmatiser for French based on the lefff lexicon (Sagot, 2010). 4.2 Generative Model The performances of our parser are summarised in Figure 2, (a) and (b), where F-score denotes the Parseval F-score5 , and LAS and UAS are respectively the Labelled and Unlabelled Attachment Score of the converted dependency structures6 . We give oracle scores (the score that our system would get if it selected the best parse from the n-best lists) when the parser generates n-best lists of depth 10, 20, 50 and 100 in order to get an idea of the effectiveness of the reranking process. One of the issues we face with this approach is the use of an imperfect functional annota"
W12-3412,D09-1058,0,0.0661606,"Missing"
W12-3412,D09-1161,0,0.19433,"sured as the distance between this vector and the reference. One way to take advantage of both approaches is to combine them sequentially, as initially proposed by Collins (2000). A generative parser produces a set of candidates structures that constitute the input of a second, discriminative module, whose search space is limited to this set of candidates. Such an approach, parsing followed by reranking, is used in the Brown parser (Charniak and Johnson, 2005). The approach can be extended in order to feed the reranker with the output of different parsers, as shown by (Johnson and Ural, 2010; Zhang et al., 2009). In this paper we are interested in applying reranking to dependency structures. The main reason is that many linguistic constraints are straightforward to implement on dependency structures, as, for example, subcategorization frames or selectional constraints that are closely linked to the notion of deProceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89–99, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics pendents of a predicate. On the other hand, dependencies extracted from constituent parses are known to b"
W12-3412,C10-1011,0,\N,Missing
W12-3412,N10-1095,0,\N,Missing
W14-3614,I11-1017,0,0.0193196,"weighted finite-state transducer library. In CIAA, pages 11–23. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Foundation). The statements made"
W14-3614,W14-3605,1,0.853538,"First, typical NLP tools lack in robustness against errors in their input. This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web. Second, automated spell and grammar checkers facilitate text editing and can be of great help to nonnative speakers of a language. Several resources and shared tasks appeared recently, including the HOO task (Dale and Kilgarriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b). In this paper we describe our participation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014). While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors. Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels. All the more so for Arabic which 2 Pipeline Approach to Error Correction The PBSMT system accounts for context by learning, from a parallel corpus of annotated errors, mappings from erroneous multi-word segments of text to their corrections, and using a language model to help select the suitable cor"
W14-3614,P08-2015,1,0.810233,"ues (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Gabor Berend, Veronika Vincze, Sina Zarrieß, and Rich´ard Farkas. 2013. Lfg-based features for noun number and article grammatical errors. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62–67, Sofia, Bulgaria, August. Association for Computational Linguistics. Chris Brockett, William B. Dolan, and Mi"
W14-3614,P12-2059,0,0.0210059,"s that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Gabor Berend, Veronika Vincze, Sina Zarrieß, and Rich´ard Farkas. 2013. Lfg-based features for noun number and article grammatical errors. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning"
W14-3614,W13-3610,0,0.0214864,"ojciech Skut, and Mehryar Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In CIAA, pages 11–23. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rat"
W14-3614,W13-3601,0,0.301464,"error correction generated considerable interest in the community since the early 1960s (Kukich, 1992) for at least two reasons. First, typical NLP tools lack in robustness against errors in their input. This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web. Second, automated spell and grammar checkers facilitate text editing and can be of great help to nonnative speakers of a language. Several resources and shared tasks appeared recently, including the HOO task (Dale and Kilgarriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b). In this paper we describe our participation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014). While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors. Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels. All the more so for Arabic which 2 Pipeline Approach to Error Correction The PBSMT system accounts for context by learning, from a parallel corpus of annotated errors"
W14-3614,N03-1017,0,0.00389689,"e tagged themselves, cause their adjacent words to be marked as PROB instead. In this way, the subsequent components in the pipeline can be alerted to the possibility of a missing word via its surroundings. Any words not marked as PROB are given an “OK” tag. Gold tags, necessary for training, can be generated by comparing the text to its correction using some sequence alignment technique, for which we use SCLITE (Fiscus, 1998). For this task, we use Yamcha (Kudo and Mat2.3 Word-level PBSMT Correction We formalize the correction process as a phrasebased statistical machine translation problem (Koehn et al., 2003), at the word-level, and solve 1 We did not use MADAMIRA (the newest version of MADA) since it was not available when this component was built. 115 Back-off Primary Error Detection Character-level Correction Phrase tables Word-level PBSMT Correction N-best Reranking Punctuation Insertion , Input Reranked best hypothesis Error-tagged text . Output N-best hypotheses Figure 1: Input text is run through the error detection component which labels the problematic words. The labeled text is then fed to the character-level correction components which constructs a back-off phrase table. The PBSMT compo"
W14-3614,P07-2045,0,0.00354637,"eranking Punctuation Insertion , Input Reranked best hypothesis Error-tagged text . Output N-best hypotheses Figure 1: Input text is run through the error detection component which labels the problematic words. The labeled text is then fed to the character-level correction components which constructs a back-off phrase table. The PBSMT component then uses two phrase tables to generate n-best correction hypotheses. The reranking component selects the best hypothesis, and pass it to the punctuation insertion component in order to produce the final output. it using Moses, a well-known PBSMT tool (Koehn et al., 2007). The decoder constructs a correction hypothesis by first segmenting the input text into phrases, and mapping each phrase into its best correction using a combination of scores including a context-sensitive LM score. Unlike translation, error correction is mainly monotonic, therefore we set disallow reordering by setting the distortion limit in Moses to 0.2 When no mapping can be found for a given phrase in the primary phrase table, the decoder looks it up in the back-off model. The decoder searches the space of all possible correction hypotheses, resulting from alternative segmentations and m"
W14-3614,pasha-etal-2014-madamira,1,0.869619,"Missing"
W14-3614,W13-3612,0,0.0274093,"nder, number, person, aspect, voice, case, mood, state, proclitics and enclitics. This was done for two preceding words and two following words. However, the results were significantly outperformed by our final set-up. −1.7 0.8 Precision 0.6 AUC= 0.715 PRBE= 0.483, Cutoff= −0.349 Prec@rec(0.800)= 0.345, Cutoff= −1.045 0.2 0.4 3.3 1.61 1.0 4.93 Precision−Recall Curve 4 Character-level correction Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013). Unlike our approach, most of We evaluate the character-level correction model by measuring the percentage of erroneous phrases that have been mapped to their in-context reference corrections. We found this percentage to be 117 System PBSMT backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI PR 75.5 74.1 61.3 75.7 74.9 RC 49.5 51.8 45.4 52.1 54.2 F1 59.8 60.9 52.2 61.7 62.8 Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. References Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. Open"
W14-3614,W13-3602,0,0.0694383,"4.1 61.3 75.7 74.9 RC 49.5 51.8 45.4 52.1 54.2 F1 59.8 60.9 52.2 61.7 62.8 Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. References Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In CIAA, pages 11–23. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character"
W14-3614,W13-3613,0,0.0184864,"s where we train the complete list of features produced by MADAMIRA; that is part-ofspeech, gender, number, person, aspect, voice, case, mood, state, proclitics and enclitics. This was done for two preceding words and two following words. However, the results were significantly outperformed by our final set-up. −1.7 0.8 Precision 0.6 AUC= 0.715 PRBE= 0.483, Cutoff= −0.349 Prec@rec(0.800)= 0.345, Cutoff= −1.045 0.2 0.4 3.3 1.61 1.0 4.93 Precision−Recall Curve 4 Character-level correction Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013). Unlike our approach, most of We evaluate the character-level correction model by measuring the percentage of erroneous phrases that have been mapped to their in-context reference corrections. We found this percentage to be 117 System PBSMT backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI PR 75.5 74.1 61.3 75.7 74.9 RC 49.5 51.8 45.4 52.1 54.2 F1 59.8 60.9 52.2 61.7 62.8 Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. Referen"
W14-3614,P13-2098,1,0.846224,"f n-best scoring hypotheses. scored n-best list is used for supervised training of a reranking model. We employ a pairwise approach to ranking which takes pairs of hypotheses as instances in learning, and formalizes the ranking problem as pairwise classification. For this task we use RankSVM (Joachims, 2002) which is a method based on Support Vector Machines (SVMs). We use only linear kernels to keep complexity low. We use a rich set of features including LM scores on surface forms, POS tags and lemmas. We also use a feature based on a global model of the semantic coherence of the hypotheses (Tomeh et al., 2013). The new top ranked hypothesis is the output of this step which is then fed to the next component. 2.4 We developed a model that predicts the occurrence of periods and commas in a given Arabic text. The core model is a decision tree classifier trained on the QALB parallel training data using WEKA (Hall et al., 2009). For each space between two words, the classifier decides whether or not to insert a punctuation mark, using a window size of three words surrounding the underlying space. The model uses the following features: 2.5 N-best List Reranking In this step, we combine LM information with"
W14-3614,W13-3617,0,0.0307163,"res produced by MADAMIRA; that is part-ofspeech, gender, number, person, aspect, voice, case, mood, state, proclitics and enclitics. This was done for two preceding words and two following words. However, the results were significantly outperformed by our final set-up. −1.7 0.8 Precision 0.6 AUC= 0.715 PRBE= 0.483, Cutoff= −0.349 Prec@rec(0.800)= 0.345, Cutoff= −1.045 0.2 0.4 3.3 1.61 1.0 4.93 Precision−Recall Curve 4 Character-level correction Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013). Unlike our approach, most of We evaluate the character-level correction model by measuring the percentage of erroneous phrases that have been mapped to their in-context reference corrections. We found this percentage to be 117 System PBSMT backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI PR 75.5 74.1 61.3 75.7 74.9 RC 49.5 51.8 45.4 52.1 54.2 F1 59.8 60.9 52.2 61.7 62.8 Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. References Cyril Allauzen, Michael Riley, Joha"
W14-3614,W13-3607,0,0.0138704,"transducer library. In CIAA, pages 11–23. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Foundation). The statements made herein are solely the"
W14-3614,zaghouani-etal-2014-large,1,0.830378,"Missing"
W14-3614,P11-1088,1,\N,Missing
W14-3614,W13-3606,0,\N,Missing
W14-3614,P06-1032,0,\N,Missing
W14-3614,N12-1067,0,\N,Missing
W14-3614,W13-3608,0,\N,Missing
W14-3614,P14-2027,1,\N,Missing
W14-3614,W10-4236,0,\N,Missing
W17-6212,W08-2102,0,0.193453,"ck for downstream applications where speed and low memory use is important. Moreover, by reducing boolean matrix multiplication to TAG parsing, Satta (1994) argued that obtaining a lower complexity bound for the latter problem is unlikely to be straightforward. Hence, parsing with weighted TAGs (Resnik, 1992) has received too little attention even though Chiang (2000) experimentally demonstrated their usefulness in the Penn Treebank parsing task. In order to bypass this major bottleneck, two main strategies have been explored. On the one hand, splittable grammars (Schabes and C. Waters, 1995; Carreras et al., 2008) are interesting because they have a lower asymptotic complexity than LTAGs. However, they cannot directly encode several properties that make TAGs linguistically plausible, such as cross-serial dependencies. In fact, they are restricted to context-free languages. On the other hand, a popular approach to speed up LTAG parsing is to include a preliminary step called supertagging: only a subset of tree fragments per word are retained as candidates, or exactly one in the most aggressive form (Chen and Bangalore, 1999). However, this pruning does not improve the asymptotic complexity of the parser"
W17-6212,P16-1034,1,0.894953,"Missing"
W17-6212,W04-3320,0,0.116602,"rest of the paper, we use tree to denote the linguistic object and arborescence to denote the graph-theoretic object. The term tree may be confusing because in graph theory it refers to an undirected type of graph. 113 pute the intersection of the tree language defined by an LTAG and the tree language consisting of a single dependency tree. In this way our method can be seen as an instance of the framework defined in (Nederhof, 2009). Finally, we follow the common idea that derivation trees should be built directly and that a derived tree is a by-product of a derivation tree. See for instance Debusmann et al. (2004) which introduced LTAG parsing as a constraint satisfaction problem. phrase-structure treebank. More Recently, following this observation, Fern´andez-Gonz´alez and Martins (2015) proposed to encode constituents as bilexical dependency labels without relying on a grammar, solving the two problems jointly. In this setting, constituent parse tree is recovered from a labeled dependency parse, which makes it possible to use tools developed for dependency parsing. This is appealing because dependency parsing has received a considerable amount of attention recently and is now well understood. Moreove"
W17-6212,W00-2011,0,0.131553,"a packed forest. This result is of practical interest to the development of efficient weighted LTAG parsers based on derivation tree decoding. 1 Introduction Lexicalized Tree Adjoining Grammars (LTAGs), that is TAGs where each elementary tree contains exactly one lexical anchor, have been proposed as an attractive formalism to model the phrasestructure construction in natural languages (Schabes et al., 1988; Abeille et al., 1990). An important property of lexicalized grammars is their ability to directly encode semantic information in combination operations. Borrowing the example provided by Eisner and Satta (2000), in the sentence “She deliberately walks the dog”, the tree anchored with “dog” is combined to the tree anchored with “walks”, see Figure 1a.1 Thus, the object associated with a transitive realization “walks” can be restricted to a subset of allowed words, including “dog” but not “river”. Unfortunately, parsing with a LTAG is hardly tractable. Eisner and Satta (2000) proposed the 1 2 Intuitively, even when lexicon assigns only one elementary tree per word, a sentence can have several derivations which exhibit different tree structures. We use a simplified grammar to ease the presentation. 112"
W17-6212,P15-1147,0,0.0234919,"Missing"
W17-6212,E09-1034,0,0.0644223,"Missing"
W17-6212,J99-4004,0,0.229797,"t to A, including itself. We note (vk )⇐ and (vk )⇒ the leftmost and rightmost vertices in the yield of 3.3 Parsing So far we defined LTAGs and discussed the structure of derivation trees. We will now briefly focus 7 6 More precisely, the algorithm proposed in Section 4 cannot parse ill-nested arborescences. Well-nestedness is a required property of the input. These should not be confused with wrapping, left and right auxiliary trees which only describe the structure of auxiliary trees. 115 every allowed derivation tree. Moreover, rules can be augmented with scores to build a weighted parser (Goodman, 1999). Intuitively, our algorithm is an adaptation of the standard CYK variant for TAG parsing (VijayShankar and K. Joshi, 1986; Vijay-Shanker and J. Weir, 1993), extended to handle constraints of lexicalized grammars. It is bottom-up in two ways. First, vertices of the dependency structure are traversed from leaves to root: a vertex is considered only after all its children. Second, given a vertex of the dependency structure, its corresponding elementary tree is visited in a similar fashion as in the CYK-like algorithm. The resulting algorithm has a linear time complexity. This is due to the fact"
W17-6212,C92-2065,0,0.510478,"ire d’Informatique de Paris Nord, Universit´e Paris 13 – SPC, CNRS UMR 7030, F-93430, Villetaneuse, France {corro,leroux}@lipn.fr Abstract best-known parsing strategy with a O(n7 ) worst case time complexity and O(n5 ) space complexity where n is the length of the input sentence. This is a major drawback for downstream applications where speed and low memory use is important. Moreover, by reducing boolean matrix multiplication to TAG parsing, Satta (1994) argued that obtaining a lower complexity bound for the latter problem is unlikely to be straightforward. Hence, parsing with weighted TAGs (Resnik, 1992) has received too little attention even though Chiang (2000) experimentally demonstrated their usefulness in the Penn Treebank parsing task. In order to bypass this major bottleneck, two main strategies have been explored. On the one hand, splittable grammars (Schabes and C. Waters, 1995; Carreras et al., 2008) are interesting because they have a lower asymptotic complexity than LTAGs. However, they cannot directly encode several properties that make TAGs linguistically plausible, such as cross-serial dependencies. In fact, they are restricted to context-free languages. On the other hand, a po"
W17-6212,J94-2002,0,0.498004,"Missing"
W17-6212,W12-4613,0,0.0278279,"is linear (Section 6). 2 Related work Syntactic content must not be confused with the type of representation, as clearly argued by Rambow (2010). As such, phrase structures can be encoded into dependency-like structures as long as the transformation is correctly formalized. One example of this fact is the correspondence between derivation trees constructed from lexicalized grammar and bilexical dependency relations. However, the equivalence with dependency-based linguistic theories may not be as straightforward as the similarity in the type of representation suggests (Rambow and Joshi, 1997; Kallmeyer and Kuhlmann, 2012). Following this line of thought, we reduce LTAG parsing to dependency parsing where unannotated bilexical dependencies represents abstract attachment operations and dependency labels specify the type of attachment and the site of the operation on the head elementary tree. Historically, many syntactic dependency treebanks have been built by transforming phrasestructures using head-percolation tables (Collins, 2003; Yamada and Matsumoto, 2003), and thus one could learn a dependency parser from a 1. a parser starts by assigning a single head to each lexical item, without taking into account the"
W17-6212,C88-2121,0,0.789326,"atible with these elementary trees and predefined dependencies. Moreover, we prove that this algorithm has a lineartime complexity in the input length. This algorithm returns all compatible derivation trees as a packed forest. This result is of practical interest to the development of efficient weighted LTAG parsers based on derivation tree decoding. 1 Introduction Lexicalized Tree Adjoining Grammars (LTAGs), that is TAGs where each elementary tree contains exactly one lexical anchor, have been proposed as an attractive formalism to model the phrasestructure construction in natural languages (Schabes et al., 1988; Abeille et al., 1990). An important property of lexicalized grammars is their ability to directly encode semantic information in combination operations. Borrowing the example provided by Eisner and Satta (2000), in the sentence “She deliberately walks the dog”, the tree anchored with “dog” is combined to the tree anchored with “walks”, see Figure 1a.1 Thus, the object associated with a transitive realization “walks” can be restricted to a subset of allowed words, including “dog” but not “river”. Unfortunately, parsing with a LTAG is hardly tractable. Eisner and Satta (2000) proposed the 1 2"
W17-6212,J95-4002,0,0.53862,"Missing"
W17-6212,N15-1080,0,0.0408429,"Missing"
W17-6212,H86-1020,0,0.564336,"Missing"
W17-6212,P06-2066,0,0.0510032,"2, its gap is the vertex set of block degree 1 including a vertex with its predecessor and one with its successor in the yield of vk . We note (vk )← and (vk )→ the leftmost and rightmost nodes in the gap of vk , respectively, (vk )← = (vk )→ = − if there is none (ie. vk as a block degree of 1, thus its yield is equal to its span and contains no gap). An example is illustrated in Figure 2. Second, G is well-nested, that is two distinct sub-trees may not interleave. We will not explicitly use this property in the following and refer readers interested dependency structures characterization to Kuhlmann and Nivre (2006). However, we assume dependency structures to be wellnested.7 Derivation tree Given a sentence s = s1 . . . sn and a LTAG, a derivation tree is a dependency structure representing a valid parse of the sentence by means of elementary trees and combination operations. It is a directed graph G = (V, A) with V = {v1 . . . vn } the set of vertices, vi corresponding to word si . The set of arcs A ⊂ V × V describes a spanning arborescence: A contains n−1 arcs with no circuit and each vertex has at most one incoming arc. The unique vertex vr without incoming arc is called the root vertex. The set of c"
W17-6212,J93-4002,0,0.553466,"Missing"
W17-6212,W09-3802,0,0.0241611,"problem hardly tractable for sentences longer than 15 ∼ 20 words. More recently, Corro et al. (2016) proposed an experimentally fast alternative based on combinatorial 3 In the rest of the paper, we use tree to denote the linguistic object and arborescence to denote the graph-theoretic object. The term tree may be confusing because in graph theory it refers to an undirected type of graph. 113 pute the intersection of the tree language defined by an LTAG and the tree language consisting of a single dependency tree. In this way our method can be seen as an instance of the framework defined in (Nederhof, 2009). Finally, we follow the common idea that derivation trees should be built directly and that a derived tree is a by-product of a derivation tree. See for instance Debusmann et al. (2004) which introduced LTAG parsing as a constraint satisfaction problem. phrase-structure treebank. More Recently, following this observation, Fern´andez-Gonz´alez and Martins (2015) proposed to encode constituents as bilexical dependency labels without relying on a grammar, solving the two problems jointly. In this setting, constituent parse tree is recovered from a labeled dependency parse, which makes it possibl"
W17-6212,P83-1021,0,0.723719,"ree; Not every labelled graph G describing an arborescence is a valid dependency tree. Indeed, many constraints must be satisfied in order to transform the derivation tree to into a derived tree. Since an arc vh → vm represents a combination operation of the tree of vm into the tree of vh , non-terminals at attachment sites must be equal and if the destination site is a leaf (resp. internal) node, the tree of vm must be an initial (resp. auxiliary) tree, among others. In this section, we propose a new algorithm for LTAG parsing with given bilexical relations in the form of a deduction system (Pereira and Warren, 1983). We formalize the algorithm as a recognizer: the Goal item can only be obtained if the input can be generated by the grammar. As usual, it can be implemented as a chart-based dynamic program with back-pointers in order to retrieve 4. combination flag c ∈ {⊥, &gt;} specifying if a combination operation has already been investigated &gt; or not ⊥ at node p; ← → 5. left boundary bl ∈ V ∪{lh , gh , gh , gh } defines the left boundary of the yield of the item, which is discussed in more details below; ← → 6. right boundary br ∈ V ∪ {lh , gh , gh , gh } defines its right boundary. In most approaches, bou"
W17-6212,N10-1049,0,0.0217253,"ther way, this means that it is now possible to obtain dependency trees that are compatible with LTAG parsing. Still, we are left with the second step: the parse labeler. Our contribution is a novel algorithm for this second step that can infer elementary trees and operation sites (Sections 4 and 5) as a postprocessing step to build all compatible derivation trees, from which derived trees are completely specified. The time complexity in the length of the sentence is linear (Section 6). 2 Related work Syntactic content must not be confused with the type of representation, as clearly argued by Rambow (2010). As such, phrase structures can be encoded into dependency-like structures as long as the transformation is correctly formalized. One example of this fact is the correspondence between derivation trees constructed from lexicalized grammar and bilexical dependency relations. However, the equivalence with dependency-based linguistic theories may not be as straightforward as the similarity in the type of representation suggests (Rambow and Joshi, 1997; Kallmeyer and Kuhlmann, 2012). Following this line of thought, we reduce LTAG parsing to dependency parsing where unannotated bilexical dependenc"
W17-6212,W03-3023,0,\N,Missing
W17-6212,E99-1025,0,\N,Missing
W17-6212,J03-4003,0,\N,Missing
W17-6212,P00-1058,0,\N,Missing
W17-6212,P85-1011,0,\N,Missing
