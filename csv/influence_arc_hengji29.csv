2020.acl-demos.11,W18-2501,0,0.0205612,"Missing"
2020.acl-demos.11,S10-1059,0,0.0339721,"2016). For types without coarse-grained type training data in ACE/ERE, we design dependency pathbased patterns instead and implement a rule-based system to detect their fine-grained relations directly from the text (Li et al., 2019b). 3.3 Text Event Extraction and Coreference We start by extracting coarse-grained events and arguments using a Bi-LSTM CRF model and a CNNbased model (Zhang et al., 2018b) for three languages, and then detect the fine-grained event types by applying verb-based rules, context-based rules, and argument-based rules (Li et al., 2019b). We also extract FrameNet frames (Chen et al., 2010) in English corpora to enrich the fine-grained events. We apply a graph-based algorithm (AlBadrashiny et al., 2017) for our languageindependent event coreference resolution. For each event type, we cast the event mentions as nodes in a graph, so that the undirected, weighted edges between these nodes represent coreference confidence scores between their corresponding events. We then apply hierarchical clustering to obtain event clusters and train a Maximum Entropy binary classifier on the cluster features (Li et al., 2019b). 4 Visual Knowledge Extraction The Visual Knowledge Extraction (VKE) b"
2020.acl-demos.11,D11-1142,0,0.490499,"at GitHub3 and DockerHub4 , with complete documentation5 . 1 Introduction Knowledge Extraction (KE) aims to find entities, relations and events involving those entities from unstructured data, and link them to existing knowledge bases. Open source KE tools are useful for many real-world applications including disaster monitoring (Zhang et al., 2018a), intelligence analysis (Li et al., 2019a) and scientific knowledge mining (Luan et al., 2017; Wang et al., 2019). Recent years have witnessed the great success and wide usage of open source Natural Language Processing tools (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017), but there is no comprehensive open source system for KE. We release ∗ These authors contributed equally to this work. System page: http://blender.cs.illinois.edu/ software/gaia-ie 2 http://tac.nist.gov/2019/SM-KBP/index.html 3 GitHub: https://github.com/GAIA-AIDA 4 DockerHub: text knoweldge extraction components are in https://hub.docker.com/orgs/blendernlp/ repositories, visual knowledge extraction components are in https://hub.docker.com/u/dannapierskitoptal 5 Video: http://blender.cs.illinois.edu/aida/ gaia.mp4 1 Fi"
2020.acl-demos.11,P16-2096,0,0.0126616,"l entities. 7 Related Work Existing knowledge extraction systems mainly focus on text (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017; Pan et al., 2017; Li et al., 2019a), and do not readily support fine-grained 13 https://trec.nist.gov/trec_eval/ LDC2018E01, LDC2018E52, LDC2018E63, LDC2018E76, LDC2019E77 15 http://blender.cs.illinois.edu/demo/video_ recommendation/index_attack_dark.html 14 Innovations in technology often face the ethical dilemma of dual use: the same advance may offer potential benefits and harms (Ehni, 2008; Hovy and Spruit, 2016; Brundage et al., 2018). We first discuss dual use,17 as it relates to this demo in particular and then discuss two other considerations for applying this technology, data bias and privacy. For our demo, the distinction between beneficial use and harmful use depends, in part, on the data. Proper use of the technology requires that input documents/images are legally and ethically obtained. Regulation and standards (e.g. GDPR18 ) provide a legal framework for ensuring that such data is properly used and that any individual whose data is used has the right to request its removal. In the absence"
2020.acl-demos.11,N19-4019,1,0.475259,"seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation2 . The system is publicly available at GitHub3 and DockerHub4 , with complete documentation5 . 1 Introduction Knowledge Extraction (KE) aims to find entities, relations and events involving those entities from unstructured data, and link them to existing knowledge bases. Open source KE tools are useful for many real-world applications including disaster monitoring (Zhang et al., 2018a), intelligence analysis (Li et al., 2019a) and scientific knowledge mining (Luan et al., 2017; Wang et al., 2019). Recent years have witnessed the great success and wide usage of open source Natural Language Processing tools (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017), but there is no comprehensive open source system for KE. We release ∗ These authors contributed equally to this work. System page: http://blender.cs.illinois.edu/ software/gaia-ie 2 http://tac.nist.gov/2019/SM-KBP/index.html 3 GitHub: https://github.com/GAIA-AIDA 4 DockerHub: text knoweldge extrac"
2020.acl-demos.11,D19-1641,1,0.720878,"und KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019b) to same-named mentions within each document to form NIL clusters. A NIL cluster is a cluster of entity mentions referring to the same entity but do not have corresponding KB entries (Ji et al., 2014). Fine-grained Entity Typing We develop an attentive fine-grained type classification model with latent type representation (Lin and Ji, 2019). It takes as input a mention with its context sentence and predicts the most likely fine-grained type. We obtain the YAGO (Suchanek et al., 2008) fine-grained types from the results of Freebase entity linking, and map these types to the DARPA AIDA ontology. For mentions with identified, coarse-grained GPE and LOC types, we further determine their fine-grained types using GeoNames attributes feature class and feature code from the GeoNames entity linking result. Given that most nominal mentions are descriptions and thus do not link to entries in Freebase or GeoNames, we develop a nominal keywo"
2020.acl-demos.11,P19-1016,1,0.755197,"a primary event and its connected events from the knowledge graph (screenshot in Figure 2). 3 Text Knowledge Extraction As shown in Figure 3, the Text Knowledge Extraction (TKE) system extracts entities, relations, and events from input documents. Then it clusters identical entities through entity linking and coreference, and clusters identical events using event coreference. 8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology 3.1 Text Entity Extraction and Coreference Coarse-grained Mention Extraction We extract coarse-grained named and nominal entity mentions using a LSTM-CRF (Lin et al., 2019) model. We use pretrained ELMo (Peters et al., 2018) word embeddings as input features for English, and pretrain Word2Vec (Le and Mikolov, 2014) models on Wikipedia data to generate Russian and Ukrainian word embeddings. Entity Linking and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019"
2020.acl-demos.11,D17-1279,0,0.0504435,"Missing"
2020.acl-demos.11,N15-1119,1,0.814395,"using event coreference. 8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology 3.1 Text Entity Extraction and Coreference Coarse-grained Mention Extraction We extract coarse-grained named and nominal entity mentions using a LSTM-CRF (Lin et al., 2019) model. We use pretrained ELMo (Peters et al., 2018) word embeddings as input features for English, and pretrain Word2Vec (Le and Mikolov, 2014) models on Wikipedia data to generate Russian and Ukrainian word embeddings. Entity Linking and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019b) to same-named mentions within each document to form NIL clusters. A NIL cluster is a cluster of entity mentions referring to the same entity but do not have corresponding KB entries (Ji et al., 2014). Fine-grained Entity Typing We develop an attentive fine-grained type classification model with latent type representation (Lin and Ji, 2019). It take"
2020.acl-demos.11,P17-1178,1,0.874756,"f events being viewed, such as the fine-grained type, place, time, attacker, target, and instrument. The demo is publicly available15 with a user interface as shown in Figure 2, displaying extracted text entities and events across languages, visual entities, visual entity linking and coreference results from face, landmark and flag recognition, and the results of grounding text entities to visual entities. 7 Related Work Existing knowledge extraction systems mainly focus on text (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017; Pan et al., 2017; Li et al., 2019a), and do not readily support fine-grained 13 https://trec.nist.gov/trec_eval/ LDC2018E01, LDC2018E52, LDC2018E63, LDC2018E76, LDC2019E77 15 http://blender.cs.illinois.edu/demo/video_ recommendation/index_attack_dark.html 14 Innovations in technology often face the ethical dilemma of dual use: the same advance may offer potential benefits and harms (Ehni, 2008; Hovy and Spruit, 2016; Brundage et al., 2018). We first discuss dual use,17 as it relates to this demo in particular and then discuss two other considerations for applying this technology, data bias and privacy. For ou"
2020.acl-demos.11,N18-1202,0,0.0435292,"knowledge graph (screenshot in Figure 2). 3 Text Knowledge Extraction As shown in Figure 3, the Text Knowledge Extraction (TKE) system extracts entities, relations, and events from input documents. Then it clusters identical entities through entity linking and coreference, and clusters identical events using event coreference. 8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology 3.1 Text Entity Extraction and Coreference Coarse-grained Mention Extraction We extract coarse-grained named and nominal entity mentions using a LSTM-CRF (Lin et al., 2019) model. We use pretrained ELMo (Peters et al., 2018) word embeddings as input features for English, and pretrain Word2Vec (Le and Mikolov, 2014) models on Wikipedia data to generate Russian and Ukrainian word embeddings. Entity Linking and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019b) to same-named mentions within each document to fo"
2020.acl-demos.11,P14-5010,0,0.0168957,"is publicly available at GitHub3 and DockerHub4 , with complete documentation5 . 1 Introduction Knowledge Extraction (KE) aims to find entities, relations and events involving those entities from unstructured data, and link them to existing knowledge bases. Open source KE tools are useful for many real-world applications including disaster monitoring (Zhang et al., 2018a), intelligence analysis (Li et al., 2019a) and scientific knowledge mining (Luan et al., 2017; Wang et al., 2019). Recent years have witnessed the great success and wide usage of open source Natural Language Processing tools (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017), but there is no comprehensive open source system for KE. We release ∗ These authors contributed equally to this work. System page: http://blender.cs.illinois.edu/ software/gaia-ie 2 http://tac.nist.gov/2019/SM-KBP/index.html 3 GitHub: https://github.com/GAIA-AIDA 4 DockerHub: text knoweldge extraction components are in https://hub.docker.com/orgs/blendernlp/ repositories, visual knowledge extraction components are in https://hub.docker.com/u/dannapierskitoptal 5 Video: http://blender.cs.illinois.edu"
2020.acl-demos.11,N18-2002,0,0.0505647,"Missing"
2020.acl-demos.11,W03-0419,0,0.625358,"Missing"
2020.acl-demos.11,D18-1125,1,0.868466,"Missing"
2020.acl-demos.11,W15-0812,0,0.139288,"Missing"
2020.acl-demos.11,P19-1191,1,0.841816,"Missing"
2020.acl-demos.11,N18-5009,1,0.887178,"Missing"
2020.acl-main.230,W13-2322,0,0.0370833,"Missing"
2020.acl-main.230,P18-1239,0,0.029152,"al branch, but exploit object detection and attention to enable localization of arguments. Silberer and Pinkal redefine the problem of visual argument role labeling with event types and bounding boxes as input. Different from their work, we extend the problem scope to including event identification and coreference, and further advance argument localization by proposing an attention framework which does not require bounding boxes for training nor testing. Multimedia Representation Multimedia common representation has attracted much attention recently (Toselli et al., 2007; Weegar et al., 2015; Hewitt et al., 2018; Chen et al., 2019; Liu et al., 2019; Su et al., 2019a; Sarafianos et al., 2019; Sun et al., 2019b; Tan and Bansal, 2019; Li et al., 2019a,b; Lu et al., 2019; Sun et al., 2019a; Rahman et al., 2019; Su et al., 2019b). However, previous methods focus on aligning images with their captions, or regions with words and entities, but ignore structure and semantic roles. UniVSE (Wu et al., 2019b) incorporates entity attributes and relations into cross-media alignment, but does not capture graph-level structures of images or text. 6 Conclusions and Future Work In this paper we propose a new task of m"
2020.acl-main.230,E12-1029,0,0.0592514,"Missing"
2020.acl-main.230,P08-1030,1,0.89237,"tion embedding. We then input the word sequence to a bi-directional long short term memory (Bi-LSTM) (Graves et al., 2013) network to encode the word order and get the representation of each word w. Given the AMR graph, we apply a Graph Convolutional Network (GCN) (Kipf and Welling, 2016) to encode the graph contextual information following (Liu et al., 2018a): (k+1) wi = f( X (k) (k) gij (WE(i,j) wj  exp We wC + be , P (ye |w) = P C e′ exp (We′ w + be′ ) P (ya |t) = P (2) exp(Wa [tC ; wC ] + ba ) . C C a′ exp(Wa′ [t ; w ] + ba′ ) We take ground truth text entity mentions as input following (Ji and Grishman, 2008) during training, and obtain testing entity mentions using a named entity extractor (Lin et al., 2019). (k) + bE(i,j) )), j∈N (i) (1) where N (i) is the neighbour nodes of wi in the AMR graph, E(i, j) is the edge type between wi and wj , gij is the gate following (Liu et al., 2018a), k represents GCN layer number, and f is the Sigmoid function. W and b denote parameters of neural layers in this paper. We take the hidden states of the last GCN layer for each word as the common-space representation wC , where C stands for the common (multimedia) embedding space. For each entity t, we obtain its"
2020.acl-main.230,P15-1017,0,0.0586298,"coreference clustering metrics because we only focus on mention-level cross-media event coreference instead of the full coreference in all documents. Text Event Extraction Text event extraction has been extensively studied for general news do2564 Entity: people Place: street Entity: troops Figure 7: Argument labeling error examples: correct entity name but wrong localization. Entity: people Place: street Entity: dissent Figure 8: Attention heatmaps lose focus due to large instance candidate number. main (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Hong et al., 2018; Liu et al., 2018b; Chen et al., 2018; Zhang et al., 2019; Liu et al., 2018a; Wang et al., 2019; Yang et al., 2019; Wadden et al., 2019). Multimedia features has been proven to effectively improve text event extraction (Zhang et al., 2017). Visual Event Extraction “Events” in NLP usually refer to complex events that involve multiple entities in a large span of time (e.g. protest), while in CV (Chang et al., 2016; Zhang et al., 2007; Ma et al., 2017) events are less complex singleentity activities (e.g. washing dishes) or actions (e.g. jumping). Visual e"
2020.acl-main.230,D18-1158,0,0.0128608,"edia event coreference instead of the full coreference in all documents. Text Event Extraction Text event extraction has been extensively studied for general news do2564 Entity: people Place: street Entity: troops Figure 7: Argument labeling error examples: correct entity name but wrong localization. Entity: people Place: street Entity: dissent Figure 8: Attention heatmaps lose focus due to large instance candidate number. main (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Hong et al., 2018; Liu et al., 2018b; Chen et al., 2018; Zhang et al., 2019; Liu et al., 2018a; Wang et al., 2019; Yang et al., 2019; Wadden et al., 2019). Multimedia features has been proven to effectively improve text event extraction (Zhang et al., 2017). Visual Event Extraction “Events” in NLP usually refer to complex events that involve multiple entities in a large span of time (e.g. protest), while in CV (Chang et al., 2016; Zhang et al., 2007; Ma et al., 2017) events are less complex singleentity activities (e.g. washing dishes) or actions (e.g. jumping). Visual event ontologies focus on daily life domains, such as “dogshow” and “wedding ce"
2020.acl-main.230,P13-1008,1,0.930574,"sentence s, compute the aggregated multimedia features m′′ and o′′i , and feed into the shared classifiers (Equation 3) to predict visual event and argument roles. Finally, we corefer the cross-media events of the same event type if the similarity hs, mi is higher than a threshold. 4 Experiments 4.1 Evaluation Setting Evaluation Metrics We conduct evaluation on text-only, image-only, and multimedia event mentions in M2 E2 dataset in Section 2.2. We adopt the traditional event extraction measures, i.e., Precision, Recall and F 1 . For text-only event mentions, we follow (Ji and Grishman, 2008; Li et al., 2013): a textual event mention is correct if its event type and trigger offsets match a reference trigger; and a textual event argument is correct if its event type, offsets, and role label match a reference argument. We make a similar definition for image-only event mentions: a visual event mention is correct if its event type and image match a reference visual event mention; and a visual event argument is correct if its event type, localization, and role label match a reference argument. A visual argument is correctly localized if the Intersection over Union (IoU) of the predicted bounding box wi"
2020.acl-main.230,R11-1002,0,0.0346327,"Missing"
2020.acl-main.230,P19-1016,1,0.783035,"s et al., 2013) network to encode the word order and get the representation of each word w. Given the AMR graph, we apply a Graph Convolutional Network (GCN) (Kipf and Welling, 2016) to encode the graph contextual information following (Liu et al., 2018a): (k+1) wi = f( X (k) (k) gij (WE(i,j) wj  exp We wC + be , P (ye |w) = P C e′ exp (We′ w + be′ ) P (ya |t) = P (2) exp(Wa [tC ; wC ] + ba ) . C C a′ exp(Wa′ [t ; w ] + ba′ ) We take ground truth text entity mentions as input following (Ji and Grishman, 2008) during training, and obtain testing entity mentions using a named entity extractor (Lin et al., 2019). (k) + bE(i,j) )), j∈N (i) (1) where N (i) is the neighbour nodes of wi in the AMR graph, E(i, j) is the edge type between wi and wj , gij is the gate following (Liu et al., 2018a), k represents GCN layer number, and f is the Sigmoid function. W and b denote parameters of neural layers in this paper. We take the hidden states of the last GCN layer for each word as the common-space representation wC , where C stands for the common (multimedia) embedding space. For each entity t, we obtain its representation tC by averaging the embeddings of its tokens. Event and Argument Classifier: We classif"
2020.acl-main.230,D18-1156,0,0.415361,"Missing"
2020.acl-main.230,P14-5010,0,0.00325241,"and visual events and arguments, followed by cross-modal coreference resolution. 3.2 Text Event Extraction Table 2: M2 E2 data statistics. 3 Figure 2: Example of bounding boxes. Text Structured Representation: As shown in Figure 4, we choose Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to represent text because it includes a rich set of 150 fine-grained semantic roles. To encode each text sentence, we run the CAMR parser (Wang et al., 2015b,a, 2016) to generate an AMR graph, based on the named entity recognition and partof-speech (POS) tagging results from Stanford CoreNLP (Manning et al., 2014). To represent each word w in a sentence s, we concatenate its 5 http://host.robots.ox.ac.uk/pascal/VOC/ voc2011/guidelines.html 2559 Testing Phase Training Phase Alignment ACE Text Event Liana Owen [Participant] drove from Pennsylvania to attend [Contact.Meet] the rally in Manhattan with her parents [Participant]. imSitu Image Event Multimedia News destroying [Conﬂict.Attack] VOA Image-Caption Pairs Item [Target]: ship Tool [Instrument]: bomb For the rebels, bravado goes hand-inhand with the desperate resistance the insurgents have mounted..... Cross-media Structured Common Representation Enc"
2020.acl-main.230,N16-1034,0,0.045845,"ring metrics because we only focus on mention-level cross-media event coreference instead of the full coreference in all documents. Text Event Extraction Text event extraction has been extensively studied for general news do2564 Entity: people Place: street Entity: troops Figure 7: Argument labeling error examples: correct entity name but wrong localization. Entity: people Place: street Entity: dissent Figure 8: Attention heatmaps lose focus due to large instance candidate number. main (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Hong et al., 2018; Liu et al., 2018b; Chen et al., 2018; Zhang et al., 2019; Liu et al., 2018a; Wang et al., 2019; Yang et al., 2019; Wadden et al., 2019). Multimedia features has been proven to effectively improve text event extraction (Zhang et al., 2017). Visual Event Extraction “Events” in NLP usually refer to complex events that involve multiple entities in a large span of time (e.g. protest), while in CV (Chang et al., 2016; Zhang et al., 2007; Ma et al., 2017) events are less complex singleentity activities (e.g. washing dishes) or actions (e.g. jumping). Visual event ontologies focus"
2020.acl-main.230,D14-1162,0,0.0831296,"tion Encoder entity region ... trigger image trigger image entity ... region ... attend Liana Owen resistance ... insurgents Cross-media Shared Event Classiﬁer Contact.Meet Conﬂict.Attack Conﬂict.Attack Cross-media Shared Argument Classiﬁer Contact.Meet Participant Conﬂict.Attack Instrument Conﬂict.Attack Conﬂict.Attack Attacker Instrument Figure 3: Approach overview. During training (left), we jointly train three tasks to establish a cross-media structured embedding space. During test (right), we jointly extract events and arguments from multimedia articles. pre-trained GloVe word embedding (Pennington et al., 2014), POS embedding, entity type embedding and position embedding. We then input the word sequence to a bi-directional long short term memory (Bi-LSTM) (Graves et al., 2013) network to encode the word order and get the representation of each word w. Given the AMR graph, we apply a Graph Convolutional Network (GCN) (Kipf and Welling, 2016) to encode the graph contextual information following (Liu et al., 2018a): (k+1) wi = f( X (k) (k) gij (WE(i,j) wj  exp We wC + be , P (ye |w) = P C e′ exp (We′ w + be′ ) P (ya |t) = P (2) exp(Wa [tC ; wC ] + ba ) . C C a′ exp(Wa′ [t ; w ] + ba′ ) We take ground"
2020.acl-main.230,C18-1315,0,0.0542881,"Missing"
2020.acl-main.230,D18-1282,0,0.0717591,"Missing"
2020.acl-main.230,D19-1514,0,0.0354715,"the problem of visual argument role labeling with event types and bounding boxes as input. Different from their work, we extend the problem scope to including event identification and coreference, and further advance argument localization by proposing an attention framework which does not require bounding boxes for training nor testing. Multimedia Representation Multimedia common representation has attracted much attention recently (Toselli et al., 2007; Weegar et al., 2015; Hewitt et al., 2018; Chen et al., 2019; Liu et al., 2019; Su et al., 2019a; Sarafianos et al., 2019; Sun et al., 2019b; Tan and Bansal, 2019; Li et al., 2019a,b; Lu et al., 2019; Sun et al., 2019a; Rahman et al., 2019; Su et al., 2019b). However, previous methods focus on aligning images with their captions, or regions with words and entities, but ignore structure and semantic roles. UniVSE (Wu et al., 2019b) incorporates entity attributes and relations into cross-media alignment, but does not capture graph-level structures of images or text. 6 Conclusions and Future Work In this paper we propose a new task of multimedia event extraction and setup a new benchmark. We also develop a novel multimedia structured common space construc"
2020.acl-main.230,W07-0902,0,0.0591461,"as an auxiliary task for training our visual branch, but exploit object detection and attention to enable localization of arguments. Silberer and Pinkal redefine the problem of visual argument role labeling with event types and bounding boxes as input. Different from their work, we extend the problem scope to including event identification and coreference, and further advance argument localization by proposing an attention framework which does not require bounding boxes for training nor testing. Multimedia Representation Multimedia common representation has attracted much attention recently (Toselli et al., 2007; Weegar et al., 2015; Hewitt et al., 2018; Chen et al., 2019; Liu et al., 2019; Su et al., 2019a; Sarafianos et al., 2019; Sun et al., 2019b; Tan and Bansal, 2019; Li et al., 2019a,b; Lu et al., 2019; Sun et al., 2019a; Rahman et al., 2019; Su et al., 2019b). However, previous methods focus on aligning images with their captions, or regions with words and entities, but ignore structure and semantic roles. UniVSE (Wu et al., 2019b) incorporates entity attributes and relations into cross-media alignment, but does not capture graph-level structures of images or text. 6 Conclusions and Future Wor"
2020.acl-main.230,D19-1585,0,0.0460407,"ext event extraction has been extensively studied for general news do2564 Entity: people Place: street Entity: troops Figure 7: Argument labeling error examples: correct entity name but wrong localization. Entity: people Place: street Entity: dissent Figure 8: Attention heatmaps lose focus due to large instance candidate number. main (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Hong et al., 2018; Liu et al., 2018b; Chen et al., 2018; Zhang et al., 2019; Liu et al., 2018a; Wang et al., 2019; Yang et al., 2019; Wadden et al., 2019). Multimedia features has been proven to effectively improve text event extraction (Zhang et al., 2017). Visual Event Extraction “Events” in NLP usually refer to complex events that involve multiple entities in a large span of time (e.g. protest), while in CV (Chang et al., 2016; Zhang et al., 2007; Ma et al., 2017) events are less complex singleentity activities (e.g. washing dishes) or actions (e.g. jumping). Visual event ontologies focus on daily life domains, such as “dogshow” and “wedding ceremony” (Perera et al., 2012). Moreover, most efforts ignore the structure of events including argu"
2020.acl-main.230,S16-1181,1,0.903707,"Missing"
2020.acl-main.230,P15-2141,0,0.0178882,"ting phase (Section 3.5), given a multimedia news article, we encode the sentences and images into the structured common space, and jointly extract textual and visual events and arguments, followed by cross-modal coreference resolution. 3.2 Text Event Extraction Table 2: M2 E2 data statistics. 3 Figure 2: Example of bounding boxes. Text Structured Representation: As shown in Figure 4, we choose Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to represent text because it includes a rich set of 150 fine-grained semantic roles. To encode each text sentence, we run the CAMR parser (Wang et al., 2015b,a, 2016) to generate an AMR graph, based on the named entity recognition and partof-speech (POS) tagging results from Stanford CoreNLP (Manning et al., 2014). To represent each word w in a sentence s, we concatenate its 5 http://host.robots.ox.ac.uk/pascal/VOC/ voc2011/guidelines.html 2559 Testing Phase Training Phase Alignment ACE Text Event Liana Owen [Participant] drove from Pennsylvania to attend [Contact.Meet] the rally in Manhattan with her parents [Participant]. imSitu Image Event Multimedia News destroying [Conﬂict.Attack] VOA Image-Caption Pairs Item [Target]: ship Tool [Instrument]"
2020.acl-main.230,N15-1040,0,0.0191767,"ting phase (Section 3.5), given a multimedia news article, we encode the sentences and images into the structured common space, and jointly extract textual and visual events and arguments, followed by cross-modal coreference resolution. 3.2 Text Event Extraction Table 2: M2 E2 data statistics. 3 Figure 2: Example of bounding boxes. Text Structured Representation: As shown in Figure 4, we choose Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to represent text because it includes a rich set of 150 fine-grained semantic roles. To encode each text sentence, we run the CAMR parser (Wang et al., 2015b,a, 2016) to generate an AMR graph, based on the named entity recognition and partof-speech (POS) tagging results from Stanford CoreNLP (Manning et al., 2014). To represent each word w in a sentence s, we concatenate its 5 http://host.robots.ox.ac.uk/pascal/VOC/ voc2011/guidelines.html 2559 Testing Phase Training Phase Alignment ACE Text Event Liana Owen [Participant] drove from Pennsylvania to attend [Contact.Meet] the rally in Manhattan with her parents [Participant]. imSitu Image Event Multimedia News destroying [Conﬂict.Attack] VOA Image-Caption Pairs Item [Target]: ship Tool [Instrument]"
2020.acl-main.230,K15-1019,0,0.0601312,"Missing"
2020.acl-main.230,P19-1522,0,0.0163405,"Event Extraction Text event extraction has been extensively studied for general news do2564 Entity: people Place: street Entity: troops Figure 7: Argument labeling error examples: correct entity name but wrong localization. Entity: people Place: street Entity: dissent Figure 8: Attention heatmaps lose focus due to large instance candidate number. main (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Hong et al., 2018; Liu et al., 2018b; Chen et al., 2018; Zhang et al., 2019; Liu et al., 2018a; Wang et al., 2019; Yang et al., 2019; Wadden et al., 2019). Multimedia features has been proven to effectively improve text event extraction (Zhang et al., 2017). Visual Event Extraction “Events” in NLP usually refer to complex events that involve multiple entities in a large span of time (e.g. protest), while in CV (Chang et al., 2016; Zhang et al., 2007; Ma et al., 2017) events are less complex singleentity activities (e.g. washing dishes) or actions (e.g. jumping). Visual event ontologies focus on daily life domains, such as “dogshow” and “wedding ceremony” (Perera et al., 2012). Moreover, most efforts ignore the structure of"
2020.acl-main.230,D19-1027,0,0.028534,"all documents. Text Event Extraction Text event extraction has been extensively studied for general news do2564 Entity: people Place: street Entity: troops Figure 7: Argument labeling error examples: correct entity name but wrong localization. Entity: people Place: street Entity: dissent Figure 8: Attention heatmaps lose focus due to large instance candidate number. main (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Hong et al., 2018; Liu et al., 2018b; Chen et al., 2018; Zhang et al., 2019; Liu et al., 2018a; Wang et al., 2019; Yang et al., 2019; Wadden et al., 2019). Multimedia features has been proven to effectively improve text event extraction (Zhang et al., 2017). Visual Event Extraction “Events” in NLP usually refer to complex events that involve multiple entities in a large span of time (e.g. protest), while in CV (Chang et al., 2016; Zhang et al., 2007; Ma et al., 2017) events are less complex singleentity activities (e.g. washing dishes) or actions (e.g. jumping). Visual event ontologies focus on daily life domains, such as “dogshow” and “wedding ceremony” (Perera et al., 2012). Moreover, most efforts igno"
2020.acl-main.230,P18-1048,0,\N,Missing
2020.acl-main.713,P15-1017,0,0.0709233,".ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual f"
2020.acl-main.713,Q16-1026,0,0.188115,"alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact t"
2020.acl-main.713,N19-1423,0,0.0415352,"is the node type label. Each edge eij = hi, j, lij i ∈ E is represented similarly, whereas i and j denote the indices of involved nodes. For example, in Figure 2, the trigger “injured” is represented as h7, 7, INJUREi, the entity mention “Kashmir region” is represented as h10, 8000 11, LOCi, and the event-argument edge between them is h2, 3, PLACEi. 3 Approach As Figure 2 illustrates, our O NE IE framework extracts the information network from a given sentence in four steps: encoding, identification, classification, and decoding. We encode the input sentence using a pre-trained BERT encoder (Devlin et al., 2019) and identify entity mentions and event triggers in the sentence. After that, we compute the type label scores for all nodes and pairwise edges among them. During decoding, we explore possible information networks for the input sentence using beam search and return the one with the highest global score. 3.1 Encoding Given an input sentence of L words, we obtain the contextualized representation xi for each word using a pre-trained BERT encoder. If a word is split into multiple word pieces (e.g., Mondrian → Mon, ##dr, ##ian), we use the average of all piece vectors as its word representation. W"
2020.acl-main.713,Q14-1037,0,0.0657049,"Missing"
2020.acl-main.713,P19-1136,0,0.0490037,"th global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum span width. 6 Conclusions and Future Work"
2020.acl-main.713,D19-1041,0,0.0484095,"s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum span width. 6 Conclusions and Future Work We propose a joint end-to-end IE framework that incorporates global features to capture the interdep"
2020.acl-main.713,P05-1051,1,0.749578,"Missing"
2020.acl-main.713,H05-1003,1,0.706275,"Missing"
2020.acl-main.713,P17-1085,0,0.0964285,"s efforts, we propose a joint neural framework with global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum sp"
2020.acl-main.713,P16-4011,0,0.0585262,", Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role"
2020.acl-main.713,D14-1198,1,0.882531,"Missing"
2020.acl-main.713,P16-1200,0,0.0255855,"hich leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual for an ELECT event t"
2020.acl-main.713,D15-1102,0,0.0400079,"n with Global Features Ying Lin1 , Heng Ji1 , Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1"
2020.acl-main.713,N19-1308,0,0.2067,"ctions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual for an ELECT event to have two PERSON arguments. Most existing joint neural models fo"
2020.acl-main.713,D14-1200,0,0.0778197,"r Information Extraction with Global Features Ying Lin1 , Heng Ji1 , Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and"
2020.acl-main.713,W04-2401,0,0.391549,"fies “camp” as a facility, and a 8006 DIE event triggered by “dying” in the following sentence “Russia hints ‘peace camp’ alliance with Germany and France is dying by Dmitry Zaks.”. The IE community is lacking of newer data sets with end-to-end annotations. Unfortunately, the annotation quality of the ACE data set is not perfect due to some long-term debates on the annotation guideline; e.g., Should “government” be tagged as a GPE or an ORG? Should “dead” be both an entity and event trigger? Should we consider designator word as part of the entity mention or not? 5 Related Work Previous work (Roth and Yih, 2004; Li et al., 2011) encodes inter-dependency among knowledge elements as global constraints in an integer linear programming framework to effectively remove extraction errors. Such integrity verification results can be used to find knowledge elements that violate the constraints and identify possible instances of detector errors or failures. Inspired by these previous efforts, we propose a joint neural framework with global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instan"
2020.acl-main.713,P19-1131,0,0.0515604,"hts are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum span width. 6 Conclusions and Future Work We propose a joint end-to-end IE frame"
2020.acl-main.713,D19-1585,0,0.387379,"and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual for an ELECT event to have two PERSON arguments. Most existing jo"
2020.acl-main.713,N16-1033,0,0.106581,"res Ying Lin1 , Heng Ji1 , Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example whe"
2020.acl-main.713,P17-1113,0,0.0759927,"red by these previous efforts, we propose a joint neural framework with global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not re"
2020.emnlp-main.22,N10-1084,0,0.161071,"estfeld and Pfitzmann, 1999; bin Mohamed Amin et al., 2003; Chang and Clark, 2014). One useful cover signal for steganography is natural language text because of its prevalence and innocuity in daily life. Traditional linguistic steganography methods are mostly edit-based, i.e., they try to directly edit the secret message and transform it into an innocent text that will not raise the eavesdropper’s suspicious eyes. Typical strategies include synonym 1 Code and datasets are available at https://github. com/mickeystroller/StegaText. substitution (Topkara et al., 2006), paraphrase substitution (Chang and Clark, 2010), and syntactic transformation (Safaka et al., 2016), applied to various text media such as Email (Tutuncu and Hassan, 2015) and Twitter (Wilson et al., 2014). Although being able to maintain the grammatical correctness of output text, those edit-based methods cannot encode information efficiently. For example, the popular CoverTweet system (Wilson and Ker, 2016) can only encode two bits of information in each tweet on average. Recent advances in neural language models (LMs) (Józefowicz et al., 2016; Radford et al., 2019; Yang et al., 2019a) have enabled a diagram shift from edit-based methods"
2020.emnlp-main.22,J14-2006,0,0.126979,"COVID-19 lockdown rules Bob Let’s meet in (receiver) Room 9112A at decrypt decode Figure 1: Linguistic steganography pipeline. Introduction Privacy is central to modern communication systems such as email services and online social networks. To protect privacy, two research fields are established: (1) cryptography which encrypts secret messages into codes such that an eavesdropper is unable to decrypt, and (2) steganography which encodes messages into cover signals such that an eavesdropper is not even aware a secret message exists (Westfeld and Pfitzmann, 1999; bin Mohamed Amin et al., 2003; Chang and Clark, 2014). One useful cover signal for steganography is natural language text because of its prevalence and innocuity in daily life. Traditional linguistic steganography methods are mostly edit-based, i.e., they try to directly edit the secret message and transform it into an innocent text that will not raise the eavesdropper’s suspicious eyes. Typical strategies include synonym 1 Code and datasets are available at https://github. com/mickeystroller/StegaText. substitution (Topkara et al., 2006), paraphrase substitution (Chang and Clark, 2010), and syntactic transformation (Safaka et al., 2016), applie"
2020.emnlp-main.22,P19-1422,0,0.464294,"cover text, this encoder function f must be both deterministic and invertible. Moreover, this encoder f , together with the ciphertext distribution and the input LM, implicitly define a distribution of cover text y which we denote as Q(y). When cover texts are transmitted in the public channel, this distribution Q(y) is what an eavesdropper would observe. Imperceptibility. To avoid raising eavesdropper’s suspicion, we want the cover text distribution Q to be similar to the true natural language distribution (i.e., what this eavesdropper would expect to see in this public channel). Following (Dai and Cai, 2019), we formulate “imperceptibility” using the total variation distance (TVD) as follows: TVD(P∗LM , Q) = 1 kQ − P∗LM k1 , 2 (1) where P∗LM denotes the true language distribution. As we approximate P∗LM using a LM PLM (e.g., OpenAI GPT-2 (Radford et al., 2019)), we further decompose TVD(P∗LM , Q) as follows: TVD(P∗LM , Q) ≤ 1 ∗ 1 kPLM − PLM k1 + kPLM − Qk1 , 2 2 (2) where the first term measures how good this LM is and the second term, that is the main focus of this study, indicates the gap induced by the steganography encoder. Even without knowing the first term, we can still obtain a relative i"
2020.emnlp-main.22,P17-3017,0,0.599167,"014). Although being able to maintain the grammatical correctness of output text, those edit-based methods cannot encode information efficiently. For example, the popular CoverTweet system (Wilson and Ker, 2016) can only encode two bits of information in each tweet on average. Recent advances in neural language models (LMs) (Józefowicz et al., 2016; Radford et al., 2019; Yang et al., 2019a) have enabled a diagram shift from edit-based methods to generation-based methods which directly output a cover text by encoding the message reversibly in the choices of tokens. Various encoding algorithms (Fang et al., 2017; Yang et al., 2019b; Ziegler et al., 2019) have been proposed to leverage neural LMs to generate high-quality cover texts in terms of both fluency and information hiding capacity. However, most of the existing methods do not provide explicit guarantees on the imperceptibility of generated cover text (i.e., to what extent the cover text is indistinguishable from natural texts without hidden messages). One recent exception is the work (Dai 303 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 303–313, c November 16–20, 2020. 2020 Association for Compu"
2020.emnlp-main.22,W18-4203,1,0.926519,"es a pre-defined per-step imperceptibility guarantee. Specifically, the sender can set a small per-step imperceptibility gap δ  1 and at time step t, we set the Kt as: (8) This result shows our proposed SAAC algorithm is near-imperceptible for linguistic steganography. DKL (Q(yt |y<t )kPLM (yt |y<t )) = − log ZK , X ZK = PLM (y 0 |y<t ), (6) X Drug (5) where TK (y<t ) = argtopKy0 PLM (y 0 |y<t ). Accordingly, we have the imperceptibility of one generation step to be: Kt = min({K| Dataset Experiment Setups Datasets. We conduct our experiments on four datasets from different domains: (1) Drug (Ji and Knight, 2018), which contains a set of Reddit comments related to drugs, (2) News, which includes a subset of news articles in the CNN/DailyMail dataset (Hermann et al., 2015), (3) COVID-19, which is a subset of research papers related to COVID-19 in the CORD-19 dataset (Wang et al., 2020), and (4) Random, which is a collection of uniformly sampled bit sequences. The first three datasets contain natural language texts and we convert them into bit sequences5 following the same process in Ziegler et al. (2019). Table 1 summarizes the dataset statistics. PLM (y 0 |y<t ) ≥ 2−δ }). (7) y 0 ∈TK (y<t ) This selec"
2020.emnlp-main.22,2020.nlpcovid19-acl.1,0,0.0220763,"Missing"
2020.emnlp-main.22,P14-2115,1,0.817354,"Missing"
2020.emnlp-main.22,P15-1057,1,0.863635,"Missing"
2020.emnlp-main.22,D19-1115,0,0.207619,"Missing"
2020.emnlp-main.50,D13-1185,1,0.75481,"Missing"
2020.emnlp-main.50,P08-1090,1,0.872582,"Missing"
2020.emnlp-main.50,P09-1068,1,0.7437,"tory provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among event"
2020.emnlp-main.50,chambers-jurafsky-2010-database,1,0.801355,"Missing"
2020.emnlp-main.50,N13-1104,0,0.133398,"Missing"
2020.emnlp-main.50,N19-1423,0,0.0311453,"Missing"
2020.emnlp-main.50,N15-1165,0,0.026451,"Missing"
2020.emnlp-main.50,W16-1701,1,0.907272,"Missing"
2020.emnlp-main.50,P16-1025,1,0.902933,"Missing"
2020.emnlp-main.50,E12-1034,0,0.126036,"Missing"
2020.emnlp-main.50,W19-3311,0,0.0837118,"Missing"
2020.emnlp-main.50,Q18-1023,0,0.0569661,"Missing"
2020.emnlp-main.50,2020.acl-main.713,1,0.645499,"Missing"
2020.emnlp-main.50,N16-1098,1,0.812451,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,W16-1007,1,0.880849,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,P15-1019,0,0.145444,"Missing"
2020.emnlp-main.50,S19-1012,0,0.0429448,"ent schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations. Temporal relations exist between almost all events, even those that are not semantically related; while research in identifying causal relations has been hobbled by low inter-annotator agreement (Hong et al., 2016). In this paper, we hypothesize that two events are connected when their entity arguments are coreferential or semantically related. For example, in Figure 1, (a) and (b)"
2020.emnlp-main.50,P17-1178,1,0.846469,"ument roles. We follow our recent work on ACE IE (Lin et al., 2020) to split the data. We consider the training set as historical data to train the LM, and the test set as our target data to induce schema for target scenarios. The instance graphs of the target data set are constructed from manual annotations. For historical data, we construct event instance graphs from both manual annotations (Historicalann ) and system extraction results (Historicalsys ) from the state-ofthe-art IE model (Lin et al., 2020). We perform cross-document entity coreference resolution by applying an entity linker (Pan et al., 2017) for both annotated and system generated instance graphs. Table 2 shows the data statistics. Split Historicalann Historicalsys Validation Target The cardinality for an instance graph and a schema will be the number of substructures in each, i.e., X |g|I = count(hvm , emn , vn i), hvm ,emn ,vn i∈g |s|S = 47,525 48,664 3,422 3,673 7,152 7,018 728 802 4,419 4,426 468 424 By extension, each path of length l=5 in a graph schema [φi , ψij , φj , ψjk , φk ] contains two consecutive triples hφi , ψij ,φj i, hφj , ψjk , φk i∈s, and a matched instance path contains two consecutive instance triples hvm ,"
2020.emnlp-main.50,K19-1051,0,0.111184,"Missing"
2020.emnlp-main.50,N18-1202,0,0.129853,"event but fails to extract the I NVESTIGATE C RIME triggered by “discovered” and its D EFENDANT argument “Mohammed A. Salameh”. Event graph scehmas can inform the model that a person who is arrested was usually investigated, our IE system can fix this missing error. Therefore we also conduct extrinsic evaluations and show the effectiveness of the induced schema repository in enhancing downstream end-to-end IE tasks. PART- WHOLE PLACE−1 −−−−−−−→ GPE −−−−−→ ATTACK. We train the path language model on two tasks: learning an auto-regressive language model (Ponte and Croft, 1998; Dai and Le, 2015; Peters et al., 2018; Radford et al.; Yang et al., 2019) to predict an edge or a node, given previous edges and nodes in a path, and a neighboring path classification task to predict how likely two paths co-occur. The path language model is trained from all the paths between two event instances from the same document, based on the assumption that events from the same document (especially news document) tell a coherent story. We propose two intrinsic evaluation metrics, instance coverage and instance coherence, to assess when event instance graphs are covered by each 685 In summary, we make the following novel con"
2020.emnlp-main.50,E14-1024,0,0.100189,"Missing"
2020.emnlp-main.50,D15-1195,0,0.203725,"Missing"
2020.emnlp-main.50,N16-1049,0,0.151836,"Missing"
2020.emnlp-main.50,W17-0901,0,0.0336757,"Missing"
2020.emnlp-main.50,W19-3404,0,0.0428587,"Missing"
2020.emnlp-main.53,P98-1013,0,0.593461,"ing, pages 718–724, c November 16–20, 2020. 2020 Association for Computational Linguistics representation for each seen and unseen type, and optimizes them during the process of projecting each candidate trigger into a particular seen or unseen type. The candidate triggers are discovered with a heuristic approach. Experiments under the setting of both supervised event detection and new event type induction demonstrate that our approach can not only detect event mentions for seen types with high precision, but also discover high-quality new unseen types. 2 or nominal lexical units in FrameNet (Baker et al., 1998) are also considered as candidate triggers. 2.2 Given a sentence s = [w1 , ..., wn ], where we assume wi is identified as a candidate trigger, we use a pre-trained BERT encoder to encode the whole sentence and get a contextual representation for wi . If wi can be split into multiple subwords or words, we use the average of all subword vectors as the final trigger representation. Approach 2.3 Seen Types ... ... Seen Type Annotations Unseen Types Encoder fc(vt) Trigger Representation yt=Efc(vt) Encoder fe(vt) Linear Decoder fr(zt, yt) Linear BERT Encoding EAyman Ewas Earrested Eand Ewas Esentenc"
2020.emnlp-main.53,D13-1185,0,0.330497,"ance on supervised event detection but also discover high-quality new event types. 1 1 Annotated Mentions Seen Types New Mentions Unseen Types Figure 1: Semi-supervised new event type induction: discovering a set of new event types and their event mentions given the annotations for a few seen types. the expected events. Moreover, the coverage of manually crafted schemas is often very low, making them fail to generalize to new scenarios. Recent studies have shown that it’s possible to automatically induce an event schema from raw text. Some researchers explore probabilistic generative methods (Chambers, 2013; Nguyen et al., 2015; Yuan et al., 2018; Liu et al., 2019) or ad-hoc clustering-based algorithms (Huang et al., 2016) to discover a set of event types and argument roles. Several studies (Huang et al., 2018; Lai and Nguyen, 2019) also explore zero-shot and few-shot learning approaches to leverage available resources and extend event extraction to new types. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. I"
2020.emnlp-main.53,P19-3006,0,0.0204552,"alysis, we further pick 6 unseen ACE types and randomly select at most 100 event mentions for each type. We visualize their type distribution y using TSNE 5 . As Figure 3 shows, most of the event mentions that are annotated with the same ACE type tends to be predicted to the same new unseen type. et al., 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) assume all the target event types and annotations are given. They can extract high-quality event mentions for the given types, but cannot extract mentions for any new types. Recent studies (Huang et al., 2018; Chan et al., 2019; Ferguson et al., 2018) leverage annotations for a few seen event types or several keywords provided for the new types to extract mentions for new types. However, all these studies assume all the target types are given, which is very costly when moving to a new scenario. Recent studies have explored probabilistic generative methods (Chambers, 2013; Nguyen et al., 2015; Yuan et al., 2018; Liu et al., 2019) or ad-hoc clustering based algorithms (Huang et al., 2016) to automatically discover a set of event types as well as argument roles. Most of these studies are completely unsupervised and mai"
2020.emnlp-main.53,P15-1017,0,0.596725,"vent schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants for each trigger (Argument Extraction) from natural language text. Traditional event extraction studies (Ji and Grishman, 2008; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Yang and Mitchell, 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) usually assume there exists a set of predefined event types and argument roles, so that supervised machine learning models, e.g., deep neural networks, can be employed to extract events for each type based on human annotations. However, in practice, it is usually very expensive and time-consuming to manually craft an event schema, which defines the types and complex templates of 1 The programs are publicly available for research purpose at https://github.com/wilburOne/SSVQVAE Event Types We"
2020.emnlp-main.53,W10-2301,1,0.717117,"In particular, the recall of our approach is much higher than other methods, which demonstrate the effectiveness of the trigger identification step. It can narrow the learning space of the model. The ablation studies also prove the effectiveness of the VQ and VAE components. 3.3 representations and group all candidate triggers into clusters with a Constrained K-means (Wagstaff et al., 2001), a semi-supervised clustering algorithm which enforces all trigger candidates annotated with the same seen type to belong to the same cluster. Table 2 shows the performance with several clustering metrics (Chen and Ji, 2010), which measure the agreement between the ground truth class assignment and system based unseen type prediction. New Event Type Induction For new event type induction, we compare our approach with another intuitive baseline, BERT-CKmeans, which takes in the BERT based trigger N M I(Y, C) = 2 × I(Y ; C) [H(Y ) + H(C)] where Y denotes the ground truth class labels, C denotes the cluster labels, H(.) denotes the entropy function and I(Y ; C) is the mutual information between Y and C. Fowlkes Mallows (Fowlkes and Mallows, 1983) is to evaluate the similarity between the clusters obtained from our a"
2020.emnlp-main.53,N19-1423,0,0.00867267,"... ... Seen Type Annotations Unseen Types Encoder fc(vt) Trigger Representation yt=Efc(vt) Encoder fe(vt) Linear Decoder fr(zt, yt) Linear BERT Encoding EAyman Ewas Earrested Eand Ewas Esentenced Eto Elife Ein Eprison Ayman was arrested and was sentenced to life in prison. Figure 2: Architecture Overview. Ayman EAyman was Ewas arrested and was sentenced to life in prison . Trigger Representation Learning As Figure 2 shows, given an input sentence, we first automatically discover all candidate triggers and encode each trigger with a contextual vector Linear Classiﬁer using a pre-trained BERT (Devlin et al., 2019) encoder. Then, we predict the type of each candidate trigger by looking up a dictionary of discrete latent representations of all seen and unseen types. Meanwhile, to avoid the type prediction to be over-fitted to seen types, we apply a variational autoencoder (VAE) as a regularizer to first project each trigger into a latent variational embedding and then reconstruct the trigger conditioned on its type distribution. 2.1 Event Trigger Identification Event Type Prediction with Vector Quantization To predict a type for a candidate trigger, an intuitive approach is to learn a classifier using th"
2020.emnlp-main.53,P16-2011,1,0.905484,"Missing"
2020.emnlp-main.53,N18-2058,0,0.178078,"pick 6 unseen ACE types and randomly select at most 100 event mentions for each type. We visualize their type distribution y using TSNE 5 . As Figure 3 shows, most of the event mentions that are annotated with the same ACE type tends to be predicted to the same new unseen type. et al., 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) assume all the target event types and annotations are given. They can extract high-quality event mentions for the given types, but cannot extract mentions for any new types. Recent studies (Huang et al., 2018; Chan et al., 2019; Ferguson et al., 2018) leverage annotations for a few seen event types or several keywords provided for the new types to extract mentions for new types. However, all these studies assume all the target types are given, which is very costly when moving to a new scenario. Recent studies have explored probabilistic generative methods (Chambers, 2013; Nguyen et al., 2015; Yuan et al., 2018; Liu et al., 2019) or ad-hoc clustering based algorithms (Huang et al., 2016) to automatically discover a set of event types as well as argument roles. Most of these studies are completely unsupervised and mainly rely on statistical"
2020.emnlp-main.53,N06-2015,0,0.0629373,"unannotated event mentions. Assuming there are m seen types, we arbitrarily assign E [1:m] as their type representations. Given a candidate trigger t and its contextual vector v t , we first apply a linear encoder fc (v t ) ∈ Rd to extract type-specific features. Then, we compute a type distribution y based on fc (v t ) by looking up all the discrete latent event type embeddings with inner-product operation Similar to (Huang et al., 2016), we identify all candidate triggers based on word sense induction. Specifically, for each word, we disambiguate its senses and link each sense to OntoNotes (Hovy et al., 2006) using a word sense disambiguation system — IMS (Zhong and Ng, 2010) 2 . We consider all noun and verb concepts that can be mapped to OntoNotes senses as candidate triggers. In addition, the concepts that can be matched with verbs The feature encoder fc (.) is optimized using all event annotations for seen types (the cross-entropy term in Equation 2) and event mentions for unseen types (the second term in Equation 2 3 ). The intuition of the second term in Equation 2 is that, for each new event mention, we don’t know the correct type but we know that the type must be from 2 We use the OntoNote"
2020.emnlp-main.53,P16-1025,1,0.961878,"pes New Mentions Unseen Types Figure 1: Semi-supervised new event type induction: discovering a set of new event types and their event mentions given the annotations for a few seen types. the expected events. Moreover, the coverage of manually crafted schemas is often very low, making them fail to generalize to new scenarios. Recent studies have shown that it’s possible to automatically induce an event schema from raw text. Some researchers explore probabilistic generative methods (Chambers, 2013; Nguyen et al., 2015; Yuan et al., 2018; Liu et al., 2019) or ad-hoc clustering-based algorithms (Huang et al., 2016) to discover a set of event types and argument roles. Several studies (Huang et al., 2018; Lai and Nguyen, 2019) also explore zero-shot and few-shot learning approaches to leverage available resources and extend event extraction to new types. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically ide"
2020.emnlp-main.53,P18-1201,1,0.947216,"ing a set of new event types and their event mentions given the annotations for a few seen types. the expected events. Moreover, the coverage of manually crafted schemas is often very low, making them fail to generalize to new scenarios. Recent studies have shown that it’s possible to automatically induce an event schema from raw text. Some researchers explore probabilistic generative methods (Chambers, 2013; Nguyen et al., 2015; Yuan et al., 2018; Liu et al., 2019) or ad-hoc clustering-based algorithms (Huang et al., 2016) to discover a set of event types and argument roles. Several studies (Huang et al., 2018; Lai and Nguyen, 2019) also explore zero-shot and few-shot learning approaches to leverage available resources and extend event extraction to new types. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants fo"
2020.emnlp-main.53,P08-1030,1,0.892135,"esources and extend event extraction to new types. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants for each trigger (Argument Extraction) from natural language text. Traditional event extraction studies (Ji and Grishman, 2008; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Yang and Mitchell, 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) usually assume there exists a set of predefined event types and argument roles, so that supervised machine learning models, e.g., deep neural networks, can be employed to extract events for each type based on human annotations. However, in practice, it is usually very expensive and time-consuming to manually craft an event schema, which defines the types and complex templates of 1 The programs are publicly available for research pur"
2020.emnlp-main.53,D19-5532,0,0.0160371,"nt types and their event mentions given the annotations for a few seen types. the expected events. Moreover, the coverage of manually crafted schemas is often very low, making them fail to generalize to new scenarios. Recent studies have shown that it’s possible to automatically induce an event schema from raw text. Some researchers explore probabilistic generative methods (Chambers, 2013; Nguyen et al., 2015; Yuan et al., 2018; Liu et al., 2019) or ad-hoc clustering-based algorithms (Huang et al., 2016) to discover a set of event types and argument roles. Several studies (Huang et al., 2018; Lai and Nguyen, 2019) also explore zero-shot and few-shot learning approaches to leverage available resources and extend event extraction to new types. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants for each trigger (Argumen"
2020.emnlp-main.53,2020.acl-main.230,1,0.876559,"Missing"
2020.emnlp-main.53,P13-1008,1,0.947061,"pes. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants for each trigger (Argument Extraction) from natural language text. Traditional event extraction studies (Ji and Grishman, 2008; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Yang and Mitchell, 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) usually assume there exists a set of predefined event types and argument roles, so that supervised machine learning models, e.g., deep neural networks, can be employed to extract events for each type based on human annotations. However, in practice, it is usually very expensive and time-consuming to manually craft an event schema, which defines the types and complex templates of 1 The programs are publicly available for research purpose at https://github.com/wilburOne/SSV"
2020.emnlp-main.53,2020.acl-main.713,1,0.894187,"scover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants for each trigger (Argument Extraction) from natural language text. Traditional event extraction studies (Ji and Grishman, 2008; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Yang and Mitchell, 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) usually assume there exists a set of predefined event types and argument roles, so that supervised machine learning models, e.g., deep neural networks, can be employed to extract events for each type based on human annotations. However, in practice, it is usually very expensive and time-consuming to manually craft an event schema, which defines the types and complex templates of 1 The programs are publicly available for research purpose at https://github.com/wilburOne/SSVQVAE Event Types We propose a task of semi-supervised event type induction, which is shown in Figure 1 an"
2020.emnlp-main.53,P19-1276,0,0.0483446,"h-quality new event types. 1 1 Annotated Mentions Seen Types New Mentions Unseen Types Figure 1: Semi-supervised new event type induction: discovering a set of new event types and their event mentions given the annotations for a few seen types. the expected events. Moreover, the coverage of manually crafted schemas is often very low, making them fail to generalize to new scenarios. Recent studies have shown that it’s possible to automatically induce an event schema from raw text. Some researchers explore probabilistic generative methods (Chambers, 2013; Nguyen et al., 2015; Yuan et al., 2018; Liu et al., 2019) or ad-hoc clustering-based algorithms (Huang et al., 2016) to discover a set of event types and argument roles. Several studies (Huang et al., 2018; Lai and Nguyen, 2019) also explore zero-shot and few-shot learning approaches to leverage available resources and extend event extraction to new types. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. I"
2020.emnlp-main.53,D18-1156,0,0.0943857,"Missing"
2020.emnlp-main.53,P11-1163,0,0.232017,"nt extraction to new types. Generally, event schema induction can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants for each trigger (Argument Extraction) from natural language text. Traditional event extraction studies (Ji and Grishman, 2008; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Yang and Mitchell, 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) usually assume there exists a set of predefined event types and argument roles, so that supervised machine learning models, e.g., deep neural networks, can be employed to extract events for each type based on human annotations. However, in practice, it is usually very expensive and time-consuming to manually craft an event schema, which defines the types and complex templates of 1 The programs are publicly available for research purpose at https://github."
2020.emnlp-main.53,P15-1019,0,0.198397,"Missing"
2020.emnlp-main.53,N16-1034,0,0.040838,"struction of t conditioned on z and y, p(z) is the prior Gaussian distribution. For each unlabeled candidate trigger t, the likelihood p(t) approximates to Dataset We perform experiments on Automatic Content Extraction (ACE) 2005 dataset and evaluate our approach under two settings: (1) supervised event extraction, where the target types include 33 ACE predefined types and other, thus k is set as 34. Giving all candidate triggers, the goal is to correctly identify all ACE event mentions and classify them into corresponding types. We follow the same data split with prior work (Li et al., 2013; Nguyen et al., 2016) in which 529/30/40 newswire documents are used for training/dev/test set. (2) new event type induction, where we follow a previous study (Huang et al., 2018) and use top-10 most popular event types from ACE05 data as seen and the remaining 23 types as unseen. Given all ACE annotated event mentions, the goal of this task is to test whether the approach can automatically discover the remaining 23 unseen ACE types and categorize each candidate trigger into a particular seen or unseen type. In this experiment, k is set as 500. In terms of implementation details, we use the pre-trained bert-large-"
2020.emnlp-main.53,D07-1043,0,0.27739,"Missing"
2020.emnlp-main.53,N19-1105,0,0.29211,"Missing"
2020.emnlp-main.53,D19-1582,0,0.370626,"Missing"
2020.emnlp-main.53,N16-1033,0,0.0839218,"on can be divided into two steps: event type induction, aiming to discover a set of new event types for the given scenario, and argument role induction which discovers a set of argument roles for each type. In this work, we focus on tackling the first problem only. Introduction Event extraction is a task of automatically identifying and typing event trigger words (Event Detection), and extracting participants for each trigger (Argument Extraction) from natural language text. Traditional event extraction studies (Ji and Grishman, 2008; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Yang and Mitchell, 2016; Liu et al., 2018; Nguyen and Nguyen, 2019; Lin et al., 2020; Li et al., 2020) usually assume there exists a set of predefined event types and argument roles, so that supervised machine learning models, e.g., deep neural networks, can be employed to extract events for each type based on human annotations. However, in practice, it is usually very expensive and time-consuming to manually craft an event schema, which defines the types and complex templates of 1 The programs are publicly available for research purpose at https://github.com/wilburOne/SSVQVAE Event Types We propose a task of semi-s"
2020.emnlp-main.53,P10-4014,0,0.0121988,"itrarily assign E [1:m] as their type representations. Given a candidate trigger t and its contextual vector v t , we first apply a linear encoder fc (v t ) ∈ Rd to extract type-specific features. Then, we compute a type distribution y based on fc (v t ) by looking up all the discrete latent event type embeddings with inner-product operation Similar to (Huang et al., 2016), we identify all candidate triggers based on word sense induction. Specifically, for each word, we disambiguate its senses and link each sense to OntoNotes (Hovy et al., 2006) using a word sense disambiguation system — IMS (Zhong and Ng, 2010) 2 . We consider all noun and verb concepts that can be mapped to OntoNotes senses as candidate triggers. In addition, the concepts that can be matched with verbs The feature encoder fc (.) is optimized using all event annotations for seen types (the cross-entropy term in Equation 2) and event mentions for unseen types (the second term in Equation 2 3 ). The intuition of the second term in Equation 2 is that, for each new event mention, we don’t know the correct type but we know that the type must be from 2 We use the OntoNotes based IMS word sense disambiguator (https://github.com/c-amr/camr)"
2020.emnlp-main.568,D19-1468,0,0.234736,"ilable at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant reviews in Fig. 1 together with their expected output—aspect and sentiment labels. With a closer look at these two example reviews, we observe that S2 includes a general opinion word “good” and a pure aspect word “seafood”, which are separate hints for sentiment and aspect classification respectively. S1, on the other hand, does not address the target with plain and general words, but instead use more"
2020.emnlp-main.568,D19-1465,0,0.052273,"classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant reviews in Fig. 1 together with their expected output—aspect and sentiment labels. With a closer look at these two example reviews, we observe that S2 includes a general opinion word “good” and a pure aspect word “seafood”, which are separate hints for sentiment and aspect classification respectively. S1, on the other"
2020.emnlp-main.568,N10-1122,0,0.0605301,"2.2). 2.1 Aspect Extraction Early studies towards aspect extraction are mainly based on manually defined rules (Hu and Liu, 2004; Zhuang et al., 2006), which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineering. While CNN (Xu et al., 2018) and RNN (Liu et al., 2015) based models have shown the powerful expressiveness of neural models, they can easily consume thousands of labeled documents thus suffer from the label scarcity bottleneck. Various unsupervised approaches are proposed to model different aspects automatically. LDAbased methods (Brody and Elhadad, 2010; Chen et al., 2014) model each document as a mixture of aspects (topics) and output a word distribution for each aspect. Recently, neural models have shown to extract more coherent topics. ABAE (He 6990 et al., 2017) uses an autoencoder to reconstruct sentences through aspect embedding and removes irrelevant words through attention mechanisms. CAt (Tulkens and van Cranenburgh, 2020) introduces a single head attention calculated by a Radial Basis Function (RBF) kernel to be the sentence summary. The unsupervised nature of these algorithms is hindered by the fact that the learned aspects often"
2020.emnlp-main.568,D15-1168,0,0.0673054,"elp, aspect-based sentiment analysis, which extracts opinions about certain facets of entities from text, becomes increasingly essential and benefits a wide range of downstream applications (Bauman et al., 2017; Nguyen et al., 2015). Aspect-based sentiment analysis contains two sub-tasks: Aspect extraction and sentiment polarity classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sampl"
2020.emnlp-main.568,J92-4003,0,0.432377,"Missing"
2020.emnlp-main.568,P14-1033,0,0.138704,"on Early studies towards aspect extraction are mainly based on manually defined rules (Hu and Liu, 2004; Zhuang et al., 2006), which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineering. While CNN (Xu et al., 2018) and RNN (Liu et al., 2015) based models have shown the powerful expressiveness of neural models, they can easily consume thousands of labeled documents thus suffer from the label scarcity bottleneck. Various unsupervised approaches are proposed to model different aspects automatically. LDAbased methods (Brody and Elhadad, 2010; Chen et al., 2014) model each document as a mixture of aspects (topics) and output a word distribution for each aspect. Recently, neural models have shown to extract more coherent topics. ABAE (He 6990 et al., 2017) uses an autoencoder to reconstruct sentences through aspect embedding and removes irrelevant words through attention mechanisms. CAt (Tulkens and van Cranenburgh, 2020) introduces a single head attention calculated by a Radial Basis Function (RBF) kernel to be the sentence summary. The unsupervised nature of these algorithms is hindered by the fact that the learned aspects often do not well align wi"
2020.emnlp-main.568,N19-1423,0,0.0298348,"(He et al., 2017): An attention-based model to unsupervisedly extract aspects. An autoencoder is trained to reconstruct sentences through aspect embeddings. The learned topics need to be manually mapped to aspects. • CAt (Tulkens and van Cranenburgh, 2020): A recent method for unsupervised aspect extraction. A single head attention is calculated by a Radio Basis Function kernel to be the sentence summary. • W2VLDA (Garc´ıa-Pablos et al., 2018): A stateof-the-art topic modeling based method that leverages keywords for each aspect/sentiment to jointly do aspect/sentiment classification. • BERT (Devlin et al., 2019): A recent proposed deep language model. We utilize the pre-trained BERT (12-layer, 768 dimension, uncased) and implement a simple weakly-supervised method that fine-tunes the model by providing pseudo 6994 Methods Accuracy CosSim ABAE(He et al., 2017) CAt(Tulkens and van Cranenburgh, 2020) W2VLDA(Garc´ıa-Pablos et al., 2018) BERT(Devlin et al., 2019) JASen w/o joint JASen w/o self train JASen 61.43 67.34 66.30 70.75 72.98 81.03 82.90 83.83 Restaurant Precision Recall 50.12 46.63 49.20 58.82 58.20 61.66 63.15 64.73 50.26 50.79 50.61 57.44 74.63 65.91 72.51 72.95 macro-F1 Accuracy 42.31 45.31 4"
2020.emnlp-main.568,2020.emnlp-main.724,1,0.889829,"ed to map topics to certain aspects, not to mention some topics are irrelevant of interested aspects. Several weakly-supervised methods address this problem by using a few keywords per aspect as supervision to guide the learning process. MATE (Angelidis and Lapata, 2018) extends ABAE by initializing aspect embedding using weighted average of keyword embeddings from each aspect. ISWD (Karamanolakis et al., 2019) co-trains a bagof-word classifier and an embedding-based neural classifier to generalize the keyword supervision. Other text classification methods leverage pre-trained language model (Meng et al., 2020b) to learn the semantics of label names or metadata (Zhang et al., 2020) to propagate document labels. The above methods do not take aspect-specific opinion words into consideration. The semantic meaning captured by a hsentiment, aspecti joint topic preserves more fine-grained information to imply the aspect of a sentence and thus can be used to improve the performance of aspect extraction. 2.2 Joint Extraction of Aspect and Sentiment Most previous studies that jointly perform aspect and sentiment extraction are LDA-based methods. Zhao et al. (2010) include aspect-specific opinion models alon"
2020.emnlp-main.568,P17-1036,0,0.603521,"ntiment polarity classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant reviews in Fig. 1 together with their expected output—aspect and sentiment labels. With a closer look at these two example reviews, we observe that S2 includes a general opinion word “good” and a pure aspect word “seafood”, which are separate hints for sentiment and aspect classification respectivel"
2020.emnlp-main.568,P18-2092,0,0.0190604,"e. The embedding-based prediction is effectively leveraged by neural models to generalize on unlabeled data via self-training. (3) We demonstrate that JASen generates high-quality joint topics and outperforms baselines significantly on two benchmark datasets. 2 Related Work The problem of aspect-based sentiment analysis can be decomposed into two sub-tasks: aspect extraction and sentiment polarity classification. Most previous studies deal with them individually. There are various related efforts on aspect extraction (He et al., 2017), which can be followed by sentiment classification models (He et al., 2018). Other methods (Garc´ıa-Pablos et al., 2018) jointly solve these two sub-tasks by first separating target words from opinion words and then learning joint topic distributions over words. Below we first review relevant work on aspect extraction (Sec 2.1) and then turn to studies that jointly extract aspects and sentiment polarity (Sec 2.2). 2.1 Aspect Extraction Early studies towards aspect extraction are mainly based on manually defined rules (Hu and Liu, 2004; Zhuang et al., 2006), which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineeri"
2020.emnlp-main.568,S15-2082,0,0.0874436,"ment tips manager waitress servers service warranty coverage replace windows ios mac system screen led monitor resolution life charge last power hp toshiba dell lenovo touch track button pad programs apps itunes photoshop key space type keys Table 2: Keywords of each aspect. 5.1 Experimental Setup Datasets: The following two datasets are used for evaluation: • Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge2 . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment polarity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it). • Laptop: We leverage 14,683 unlabeled Amazon reviews under the laptop category collected by (He and McAuley, 2016) as in-domain training corpus. We also use the benchmark dataset in the laptop domain in SemEval-2016 and SemEval-2015 for evaluation. Detailed statistics of both datasets are listed in Table 1, and the aspects along with their keywords are in Table 2. Pr"
2020.emnlp-main.568,2020.acl-main.290,0,0.288473,"Missing"
2020.emnlp-main.568,P15-1060,0,0.333942,"Missing"
2020.emnlp-main.568,P18-2094,0,0.120755,"sentiment analysis, which extracts opinions about certain facets of entities from text, becomes increasingly essential and benefits a wide range of downstream applications (Bauman et al., 2017; Nguyen et al., 2015). Aspect-based sentiment analysis contains two sub-tasks: Aspect extraction and sentiment polarity classification. The former identifies the aspect 1 Our code and data are available at https://github. com/teapot123/JASen. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models (Liu et al., 2015; Xu et al., 2018) have outperformed rule-based models (Hu and Liu, 2004; Zhuang et al., 2006), but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised (He et al., 2017; Liao et al., 2019) or weaklysupervised setting (Angelidis and Lapata, 2018; Karamanolakis et al., 2019), without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant revie"
2020.emnlp-main.568,D10-1006,0,0.270234,"020 Association for Computational Linguistics is hard for models that are solely trained for one sub-task. If a model can automatically learn the semantics of each joint topic of hsentiment, aspecti, it will be able to identify representative terms of the joint topics such as “semi-private” which provide information for aspect and sentiment simultaneously, and will consequently benefit both aspect extraction and sentiment classification. Therefore, leveraging more fine-grained information by coupling the two subtasks will enhance both. Several LDA-based methods consider learning joint topics (Zhao et al., 2010; Wang et al., 2015; Xu et al., 2012), but they rely on external resources such as part-of-speech (POS) tagging or opinion word lexicons. A recent LDA-based model (Garc´ıaPablos et al., 2018) uses pre-trained word embedding to bias the prior in topic models to jointly model aspect words and opinion words. Though working fairly well, topic models are generative models and do not enforce topic distinctiveness— topic-word distribution can largely overlap among different topics, allowing topics to resemble each other. Besides, topic models yield unstable results, causing large variance in classifi"
2020.emnlp-main.724,2020.acl-main.194,0,0.0998556,"on large-scale labeled documents (usually over tens of thousands), thanks to their strong representation learning power that effectively captures the high-order, long-range semantic dependency in text sequences for accurate classification. Recently, increasing attention has been paid to semi-supervised text classification which requires a much smaller amount of labeled data. The success of semi-supervised methods stems from the usage of abundant unlabeled data: Unlabeled documents provide natural regularization for constraining the model predictions to be invariant to small changes in input (Chen et al., 2020; Miyato et al., 2017; Xie et al., 2019), thus improving the generalization ability of the model. Despite mitigating the annotation burden, semi-supervised methods still require manual efforts from domain experts, which might be difficult or expensive to obtain especially when the number of classes is large. Contrary to existing supervised and semisupervised models which learn from labeled documents, a human expert will just need to understand the label name (i.e., a single or a few representative words) of each class to classify documents. For example, we can easily classify news articles whe"
2020.emnlp-main.724,N19-1423,0,0.532004,"will just need to understand the label name (i.e., a single or a few representative words) of each class to classify documents. For example, we can easily classify news articles when given the label names such as “sports”, “business”, and “politics” because we are able to understand these topics based on prior knowledge. In this paper, we study the problem of weaklysupervised text classification where only the label name of each class is provided to train a classifier on purely unlabeled data. We propose a language model self-training approach wherein a pre-trained neural language model (LM) (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2018; Yang et al., 2019) is used as both the general knowledge source for category understanding and feature representation learning model for classification. The LM 9006 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9006–9017, c November 16–20, 2020. 2020 Association for Computational Linguistics creates contextualized word-level category supervision from unlabeled data to train itself, and then generalizes to document-level classification via a self-training objective. Specifically, we propose the LOTClass"
2020.emnlp-main.724,2020.emnlp-main.568,1,0.741729,"loss. Test Acc. (20, 0.822) 0.15 Loss Test Acc. Test Acc. 0.84 LOTClass BERT w. simple match 0.90 Acc. Loss 0.86 (50, 0.867) 0.85 0.80 0.75 0 200 400 600 Steps 800 (c) LOTClass vs. BERT w. simple match during self-training. Figure 2: (On AG News dataset.) (a) The performance of LOTClass is close to that of Supervised BERT with 48 labeled documents per class. (b) The self-training loss of LOTClass decreases in a period of 50 steps; the performance of LOTClass gradually improves. (c) BERT w. simple match does not benefit from self-training. tity recognition and aspect-based sentiment analysis (Huang et al., 2020). Sometimes a label name could be too generic to interpret (e.g., “person”, “time”, etc). To apply similar methods as introduced in this paper to these scenarios, one may consider instantiating the label names with more concrete example terms like specific person names. Limitation of weakly-supervised classification. There are difficult cases where label names are not sufficient to teach the model for correct classification. For example, some review texts implicitly express sentiment polarity that goes beyond wordlevel understanding: “I find it sad that just because Edward Norton did not want"
2020.emnlp-main.724,D14-1181,0,0.00335127,"ocuments but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name1 . 1 Introduction Text classification is a classic and fundamental task in Natural Language Processing (NLP) with a wide spectrum of applications such as question answering (Rajpurkar et al., 2016), spam detection (Jindal and Liu, 2007) and sentiment analysis (Pang et al., 2002). Building an automatic text classification model has been viewed as a task of training machine learning models from human-labeled documents. Indeed, many deep learning-based classifiers including CNNs (Kim, 2014; Zhang et al., 2015) and RNNs (Tang et al., 2015a; Yang et al., 1 Source code can be found at https://github.com/ yumeng5/LOTClass. 2016) have been developed and achieved great success when trained on large-scale labeled documents (usually over tens of thousands), thanks to their strong representation learning power that effectively captures the high-order, long-range semantic dependency in text sequences for accurate classification. Recently, increasing attention has been paid to semi-supervised text classification which requires a much smaller amount of labeled data. The success of semi-sup"
2020.emnlp-main.724,2020.acl-main.703,0,0.0161316,"l category prediction task that trains LM to predict the implied category of a word using its contexts. The LM so trained generalizes well to document-level classification upon self-training on unlabeled corpus. • On four benchmark datasets, LOTClass outperforms significantly weakly-supervised models and has comparable performance to strong semisupervised and supervised models. 2 2.1 Related Work Neural Language Models ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019) and autoencoding LMs such as BERT (Devlin et al., 2019) and its variants (Lan et al., 2020; Lewis et al., 2020; Liu et al., 2019b), has brought astonishing performance improvement to a wide range of NLP tasks, mainly for two reasons: (1) LMs are pre-trained on largescale text corpora, which allow the models to learn generic linguistic features (Tenney et al., 2019) and serve as knowledge bases (Petroni et al., 2019); and (2) LMs enjoy strong feature representation learning power of capturing high-order, long-range dependency in texts thanks to the Transformer architecture (Vaswani et al., 2017). 2.2 For semi-supervised text classification, two lines of framework are developed to leverage unlabeled dat"
2020.emnlp-main.724,D19-1486,0,0.0800171,"n task that trains LM to predict the implied category of a word using its contexts. The LM so trained generalizes well to document-level classification upon self-training on unlabeled corpus. • On four benchmark datasets, LOTClass outperforms significantly weakly-supervised models and has comparable performance to strong semisupervised and supervised models. 2 2.1 Related Work Neural Language Models ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019) and autoencoding LMs such as BERT (Devlin et al., 2019) and its variants (Lan et al., 2020; Lewis et al., 2020; Liu et al., 2019b), has brought astonishing performance improvement to a wide range of NLP tasks, mainly for two reasons: (1) LMs are pre-trained on largescale text corpora, which allow the models to learn generic linguistic features (Tenney et al., 2019) and serve as knowledge bases (Petroni et al., 2019); and (2) LMs enjoy strong feature representation learning power of capturing high-order, long-range dependency in texts thanks to the Transformer architecture (Vaswani et al., 2017). 2.2 For semi-supervised text classification, two lines of framework are developed to leverage unlabeled data. Augmentation-ba"
2020.emnlp-main.724,2021.ccl-1.108,0,0.107411,"Missing"
2020.emnlp-main.724,P11-1015,0,0.192289,",000 7,600 70,000 25,000 400,000 Table 5: Dataset statistics. Supervised models are trained on the entire training set. Semi-supervised models use 10 labeled documents per class from the training set and the rest as unlabeled data. Weakly-supervised models are trained by using the entire training set as unlabeled data. All models are evaluated on the test set. 4 4.1 Experiments unlabeled data. All methods are evaluated on the test set. Datasets Weakly-supervised methods: We use four benchmark datasets for text classification: AG News (Zhang et al., 2015), DBPedia (Lehmann et al., 2015), IMDB (Maas et al., 2011) and Amazon (McAuley and Leskovec, 2013). The dataset statistics are shown in Table 5. All datasets are in English language. 4.2 Compared Methods We compare LOTClass with a wide range of weakly-supervised methods and also state-of-theart semi-supervised and supervised methods. The label names used as supervision on each dataset for the weakly-supervised methods are shown in Tables 2, 3, 4 and 9. (Table 9 can be found in Appendix A.) Fully supervised methods use the entire training set for model training. Semi-supervised method UDA uses 10 labeled documents per class from the training set and t"
2020.emnlp-main.724,2020.acl-main.30,0,0.360439,"label name semantics and derive documentconcept relevance via explicit semantic analysis (Gabrilovich and Markovitch, 2007). Since the classifier is learned purely from general knowledge without even requiring any unlabeled domainspecific data, these methods are called dataless classification (Chang et al., 2008; Song and Roth, 2014; Yin et al., 2019). Later, topic models (Chen et al., 2015; Li et al., 2016) are exploited for seedguided classification to learn seed word-aware topics by biasing the Dirichlet priors and to infer posterior document-topic assignment. Recently, neural approaches (Mekala and Shang, 2020; Meng et al., 2018, 2019) have been developed for weaklysupervised text classification. They assign documents pseudo labels to train a neural classifier by either generating pseudo documents or using LMs to detect category-indicative words. While achieving inspiring performance, these neural approaches train classifiers from scratch on the local corpus and fail to take advantage of the general knowledge source used by dataless classification. In this paper, we build our method upon pre-trained LMs, which are used both as general linguistic knowledge sources for understanding the semantics of"
2020.emnlp-main.724,W02-1011,0,0.0394697,"Missing"
2020.emnlp-main.724,D14-1162,0,0.0820586,"Missing"
2020.emnlp-main.724,N18-1202,0,0.335119,"erstand the label name (i.e., a single or a few representative words) of each class to classify documents. For example, we can easily classify news articles when given the label names such as “sports”, “business”, and “politics” because we are able to understand these topics based on prior knowledge. In this paper, we study the problem of weaklysupervised text classification where only the label name of each class is provided to train a classifier on purely unlabeled data. We propose a language model self-training approach wherein a pre-trained neural language model (LM) (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2018; Yang et al., 2019) is used as both the general knowledge source for category understanding and feature representation learning model for classification. The LM 9006 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9006–9017, c November 16–20, 2020. 2020 Association for Computational Linguistics creates contextualized word-level category supervision from unlabeled data to train itself, and then generalizes to document-level classification via a self-training objective. Specifically, we propose the LOTClass model for Label-Name"
2020.emnlp-main.724,D19-1250,0,0.0584868,"Missing"
2020.emnlp-main.724,N16-1174,0,0.188184,"Missing"
2020.emnlp-main.724,D19-1404,0,0.155237,"ication Weakly-supervised text classification aims to categorize text documents based only on word-level descriptions of each category, eschewing the need of any labeled documents. Early attempts rely on distant supervision such as Wikipedia to interpret 9007 the label name semantics and derive documentconcept relevance via explicit semantic analysis (Gabrilovich and Markovitch, 2007). Since the classifier is learned purely from general knowledge without even requiring any unlabeled domainspecific data, these methods are called dataless classification (Chang et al., 2008; Song and Roth, 2014; Yin et al., 2019). Later, topic models (Chen et al., 2015; Li et al., 2016) are exploited for seedguided classification to learn seed word-aware topics by biasing the Dirichlet priors and to infer posterior document-topic assignment. Recently, neural approaches (Mekala and Shang, 2020; Meng et al., 2018, 2019) have been developed for weaklysupervised text classification. They assign documents pseudo labels to train a neural classifier by either generating pseudo documents or using LMs to detect category-indicative words. While achieving inspiring performance, these neural approaches train classifiers from scra"
2020.emnlp-main.724,D18-1352,0,0.0367154,"al., 2015b; Zhang et al., 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on semantic attributes and descriptions of all classes (Liu et al., 2019a; Pushp and Srivastava, 2017; Xia et al., 2018), correlations among classes (Rios and Kavuluru, 2018; Zhang et al., 2019) or joint embeddings of classes and documents (Nam et al., 2016). However, zero-shot learning still requires labeled data for the seen label set and cannot be applied to cases where no labeled documents for any class is available. 2.3 Pre-training deep neural models for language modeling, including autoregressive LMs such as 2 Other semi-supervised/weakly-supervised methods usually take advantage of distant supervision like Wikipedia dump (Chang et al., 2008), or augmentation systems like trained back translation models (Xie et al., 2019). Semi-Supervised and Zero-Shot Tex"
2020.emnlp-main.724,P16-1009,0,0.318549,"t al., 2019) and serve as knowledge bases (Petroni et al., 2019); and (2) LMs enjoy strong feature representation learning power of capturing high-order, long-range dependency in texts thanks to the Transformer architecture (Vaswani et al., 2017). 2.2 For semi-supervised text classification, two lines of framework are developed to leverage unlabeled data. Augmentation-based methods generate new instances and regularize the model’s predictions to be invariant to small changes in input. The augmented instances can be either created as real text sequences (Xie et al., 2019) via back translation (Sennrich et al., 2016) or in the hidden states of the model via perturbations (Miyato et al., 2017) or interpolations (Chen et al., 2020). Graph-based methods (Tang et al., 2015b; Zhang et al., 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on se"
2020.emnlp-main.724,D15-1167,0,0.257446,"supervised by at most 3 words (1 in most cases) per class as the label name1 . 1 Introduction Text classification is a classic and fundamental task in Natural Language Processing (NLP) with a wide spectrum of applications such as question answering (Rajpurkar et al., 2016), spam detection (Jindal and Liu, 2007) and sentiment analysis (Pang et al., 2002). Building an automatic text classification model has been viewed as a task of training machine learning models from human-labeled documents. Indeed, many deep learning-based classifiers including CNNs (Kim, 2014; Zhang et al., 2015) and RNNs (Tang et al., 2015a; Yang et al., 1 Source code can be found at https://github.com/ yumeng5/LOTClass. 2016) have been developed and achieved great success when trained on large-scale labeled documents (usually over tens of thousands), thanks to their strong representation learning power that effectively captures the high-order, long-range semantic dependency in text sequences for accurate classification. Recently, increasing attention has been paid to semi-supervised text classification which requires a much smaller amount of labeled data. The success of semi-supervised methods stems from the usage of abundant"
2020.emnlp-main.724,N19-1108,0,0.0189471,", 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on semantic attributes and descriptions of all classes (Liu et al., 2019a; Pushp and Srivastava, 2017; Xia et al., 2018), correlations among classes (Rios and Kavuluru, 2018; Zhang et al., 2019) or joint embeddings of classes and documents (Nam et al., 2016). However, zero-shot learning still requires labeled data for the seen label set and cannot be applied to cases where no labeled documents for any class is available. 2.3 Pre-training deep neural models for language modeling, including autoregressive LMs such as 2 Other semi-supervised/weakly-supervised methods usually take advantage of distant supervision like Wikipedia dump (Chang et al., 2008), or augmentation systems like trained back translation models (Xie et al., 2019). Semi-Supervised and Zero-Shot Text Classification Weak"
2020.emnlp-main.724,D18-1348,0,0.017091,"hen et al., 2020). Graph-based methods (Tang et al., 2015b; Zhang et al., 2020) build text networks with words, documents and labels and propagate labeling information along the graph via embedding learning (Tang et al., 2015c) or graph neural networks (Kipf and Welling, 2017). Zero-shot text classification generalizes the classifier trained on a known label set to an unknown one without using any new labeled documents. Transferring knowledge from seen classes to unseen ones typically relies on semantic attributes and descriptions of all classes (Liu et al., 2019a; Pushp and Srivastava, 2017; Xia et al., 2018), correlations among classes (Rios and Kavuluru, 2018; Zhang et al., 2019) or joint embeddings of classes and documents (Nam et al., 2016). However, zero-shot learning still requires labeled data for the seen label set and cannot be applied to cases where no labeled documents for any class is available. 2.3 Pre-training deep neural models for language modeling, including autoregressive LMs such as 2 Other semi-supervised/weakly-supervised methods usually take advantage of distant supervision like Wikipedia dump (Chang et al., 2008), or augmentation systems like trained back translation models"
2020.findings-emnlp.23,Q14-1037,0,0.0296602,"en these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models ("
2020.findings-emnlp.23,D17-1181,0,0.0426636,"Missing"
2020.findings-emnlp.23,P19-1136,0,0.0256531,"Missing"
2020.findings-emnlp.23,C16-1239,0,0.0720788,"Missing"
2020.findings-emnlp.23,P19-1407,0,0.0251793,"th Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) E E [sE 0 , s1 , . . . , sn ] = Encoder([x0 , x1 , . . . , xn ]) (1) Then we pass the output s sequence to Conven : E E o0 = Conven ([sE 0 , s1 , . . . , sn ]) (2) where Conven is the encoder convolutional layer. Conven maps sE to o0 , which is also a sequence and has the identical dimension as the s sequence. The output is denoted as o0 ∈ Rn×h , where h is the hidden size, n is the length of the input sentence. o0 is the auxiliary representation of the sentence, which is used for decoding with scratchpad attention mechanism (Benmalek et al., 2019): on−1 is used to calculate attention score, and on−1 will be updated to on at every decoding step. During decoding, we use different input embeddings and output layers for relation and entity ex238 SOS sentence Encoder Obama Decoder Obama HLR Decoder Encoder Decoder Obama HLR sentence Encoder Obama Decoder graduate Decoder graduate president CU SOS sentence graduate SOS HLS sentence president HLR Decoder Decoder Obama HLR afﬁliation Encoder Decoder Decoder graduate president Obama HLR Decoder graduate president SOS Obama HLR Decoder HLR graduate afﬁliation Decoder CU HLS president Decoder HLR"
2020.findings-emnlp.23,P11-1056,0,0.0387397,"d time step, thus the model is prone to feed entity pairs to the classification layer with an low odds (low recall) but high confidence (high precision). In contrast, for the order h-r-t, given the predicted h, the corresponding r can be easily identified according to the context. Subsequently, the predicted h-r pair gives strong hint to the last time step prediction, hence the model will not collapse from the no-relation. This also applies to any other order with r in the first two time steps. 5 Related Work Previous work uses P IPELINE to extract triplets from text (Nadeau and Sekine, 2007; Chan and Roth, 2011). They first recognize all entities in the input sentence then classify relations for each entity pair exhaustively. Li and Ji (2014) point out that the classification errors may propagate across subtasks. Instead of treating these two subtasks separately, for joint entities and relations extraction (JERE), TABLE methods calculate the similarity score of all token pairs and relations by exhaustive enumeration and the extracted triplets are found by the position of the output in the table (Miwa and Bansal, 2016; Gupta et al., 2016). However, as a triplet may contain entities with different leng"
2020.findings-emnlp.23,K19-1055,0,0.0136371,"es auto-regressive decoding strategy. The decoder predicts the nodes layer by layer, where the prediction results of the previous layer are used as the input of the next time step separately, as shown in Fig. 3b. 3 3.1 Experiments Settings Dataset We evaluate our model on two datasets, NYT and DuIE1 . NYT (Riedel et al., 2010) is a English news dataset that is generated by distant supervision without manual annotation, which is widely used in JERE studies (Zheng et al., 2017; Zeng et al., 2018; Takanobu et al., 2018; Dai et al., 2019; Fu et al., 2019; Nayak and Ng, 2019; Zeng et al., 2019a,b; Chen et al., 2019; Wei et al., 2019). We use the same data split as CopyRE (Zeng et al., 2018). DuIE (Li et al., 2019) is a large-scale Chinese JERE dataset where sentences are from Baidu 1 https://ai.baidu.com/broad/introduction? dataset=dureader Baselines We compare the proposed model, Seq2UMTree, with strong baselines under the same hyperparameters, as follows: 1) CopyMTL (Zeng et al., 2019a) is a Seq2Seq model with copy mechanism, and the entities are found by multi-task learning. 2) WDec (Nayak and Ng, 2019) is a standard Seq2Seq model with dynamic masking, and decode the entity token by token. 3) MHS (Be"
2020.findings-emnlp.23,D14-1179,0,0.0369231,"Missing"
2020.findings-emnlp.23,P82-1020,0,0.803956,"Missing"
2020.findings-emnlp.23,P05-1051,1,0.612724,"iversity), Obama and Columbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belong"
2020.findings-emnlp.23,H05-1003,1,0.628363,"lumbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple rel"
2020.findings-emnlp.23,P17-1085,0,0.0853559,"classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et"
2020.findings-emnlp.23,P16-4011,0,0.0283357,"ethods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can"
2020.findings-emnlp.23,P14-1038,1,0.924787,"he relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSe"
2020.findings-emnlp.23,D14-1198,1,0.921782,"Missing"
2020.findings-emnlp.23,2020.acl-main.713,1,0.797784,"the effects of exposure bias. Our method differs from previous solution on exposure bias that we remove the order by structure decoding rather than random sampling (Tsai and Lee, 2019). CASREL (Wei et al., 2020) is a recently proposed two-step tagging method, which first finds all the head entities in the sentence then labels a relation-tail table for each head entity, which can also be seen as a UMTree decoder with a decoding length two. However, they overlook the data bias problem in NYT, which causing model unreliability and possible model bias. Note that our task is different from ONEIE (Lin et al., 2020), which models event extraction, entity span detection, entity type recognition and relation extraction in a Seq2Graph way. In contrast to ONEIE, JERE aims to extract only relation-entity triplets, which can be modeled by our UMTree structure naturally. The simplicity of the tree enables the model to conduct global extraction. 243 6 Conclusions In this paper, we thoroughly analyze the effects of exposure bias of Seq2Seq models on joint entity and relation extraction. Exposure bias causes overfitting that hurts the reliability of the performance scores. To solve the problem of exposure bias, we"
2020.findings-emnlp.23,D19-1241,1,0.820988,") and propose a novel model Seq2UMTree. The Seq2UMTree model is based on an Encoder-Decoder framework, which is composed of a conventional encoder and a UMTree decoder. The UMTree decoder models entities and relations jointly and structurally, using a copy mechanism with unordered multi-label classification as the output layer. This multi-label classification model ensures the nodes in the same layer are unordered and discards the predefined triplet order so that the prediction deviation will not aggregate and affect other triplets. Different from the standard Seq2Tree (Dong and Lapata, 2016; Liu et al., 2019), the decoding length is limited to three (one triplet), which is the shortest feasible length for JERE task. In this way, the exposure bias is minimized under the triplet-level F1 metrics. In conclusion, our contributions are listed as follows: To mitigate the exposure bias problem while keeping the simplicity of Seq2Seq, we recast the one-dimension triplet sequence to two-dimension 237 • We point out the redundancy of the predefined triplet order of the Seq2Seq model, and propose a novel Seq2UMTree model to minimize exposure bias by recasting the ordered triplet sequence to an Unordered-Mult"
2020.findings-emnlp.23,D15-1102,0,0.027607,"xtraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an e"
2020.findings-emnlp.23,D15-1166,0,0.0236156,"sidered as depth 0, (b) relation embedding: wtr ∈ Rh , e2 h (c) entity embedding: wte = oe1 t−1 + ot−1 ∈ R , where e1 and e2 are the beginning position and the end position of the predicted entity respectively. t ∈ {1, 2, 3}, which is the decoding time step. The decoding order can be predefined arbitrarily, such as h-r-t or t-r-h. Given the input embedding wt and the output of the previous time step sD t−1 , a unary LSTM decoder is used to generate decoder hidden state: D sD t = Decoder(wt , st−1 ) (3) D where sD t is the decoder hidden states; s0 is iniE tialized by sn . Attention mechanism (Luong et al., 2015) is used to generate context-aware embedding: at = Attention(ot−1 , sD t ) (5) where Convde maps dimension 2h to h and at is replicated n times before concatenation. The output layer of the relation prediction is a linear transformation followed by a max-pooling over sequence: probr = σ(Max(ot Wr + br )) The output layers of the entity prediction are two binary classification layers over the whole sequence, predicting the positions of the beginning and the end of the entities respectively: probeb = σ(WeTb ot + beb ) probee = σ(WeTe ot + bee ) (7) where We ∈ Rh×1 , be is a scalar and probe ∈ Rn"
2020.findings-emnlp.23,P16-1105,0,0.174523,"ty extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one enti"
2020.findings-emnlp.23,D14-1200,0,0.0636813,"r supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are a"
2020.findings-emnlp.23,W04-2401,0,0.330979,"mple, in the triplet (Obama, graduate from, Columbia University), Obama and Columbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a sin"
2020.findings-emnlp.23,D18-1249,0,0.0156993,"oint out that the classification errors may propagate across subtasks. Instead of treating these two subtasks separately, for joint entities and relations extraction (JERE), TABLE methods calculate the similarity score of all token pairs and relations by exhaustive enumeration and the extracted triplets are found by the position of the output in the table (Miwa and Bansal, 2016; Gupta et al., 2016). However, as a triplet may contain entities with different lengths, the table methods either suffer from exponential computational burden (Adel and Sch¨utze, 2017) or roll back to pipeline methods (Sun et al., 2018; Bekoulis et al., 2018; Fu et al., 2019). Furthermore, such table enumeration dilutes the positive labels quadratically, thus aggravating the classimbalanced problem. To model the task in a more concise way, Zheng et al. (2017) propose a N OVELTAGGING scheme, which represents relation and entity in one tag, so that the joint extraction can be solved by the well-studied sequence labeling approach. However, this tagging scheme cannot assign multiple tags to one token thus fail on overlapping triplets. The follow-on methods revise the tagging scheme to enable multi-pass sequence labeling (Takano"
2020.findings-emnlp.23,2020.acl-main.136,0,0.0997078,". As the exposure bias problem stems from the ordered left-to-right triplet decoding, we block the decoding of them from each other by removing the order of the triplet generation, thus the possible prediction error cannot propagate from triplet to triplet. Furthermore, because each triplet is generated by an independent decoding process, the decoding length has been extremely shortened, thus minimizes the effects of exposure bias. Our method differs from previous solution on exposure bias that we remove the order by structure decoding rather than random sampling (Tsai and Lee, 2019). CASREL (Wei et al., 2020) is a recently proposed two-step tagging method, which first finds all the head entities in the sentence then labels a relation-tail table for each head entity, which can also be seen as a UMTree decoder with a decoding length two. However, they overlook the data bias problem in NYT, which causing model unreliability and possible model bias. Note that our task is different from ONEIE (Lin et al., 2020), which models event extraction, entity span detection, entity type recognition and relation extraction in a Seq2Graph way. In contrast to ONEIE, JERE aims to extract only relation-entity triplet"
2020.findings-emnlp.23,P17-1113,0,0.504717,"asks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et al., 2018, 2019a,b; Nayak and Ng, 2019). Specifically, all existing Seq2Seq models pre-define a sequential order for the target triplets, e.g. triplet alphab"
2020.findings-emnlp.23,N16-1033,0,0.0501813,"udies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thu"
2020.findings-emnlp.23,P19-1518,0,0.0496315,"Missing"
2020.findings-emnlp.23,C18-1330,0,0.0233643,"lapping triplets problem. Although this paper introduces a problem that multi-token entities cannot be predicted, this problem has been solved by multiple follow-up papers (Zeng et al., 2019a; Nayak and Ng, 2019). However, there still remains a weakness in Seq2Seq models, i.e., the exposure bias, which has been overlooked. Exposure bias originates from the discrepancy between training and testing: Seq2Seq models use data distribution for training and model distribution for testing (Ranzato et al., 2015). Existing work mainly focuses on how to mitigate the information loss of arg max sampling (Yang et al., 2018, 2019; Zhang et al., 2019). Nam et al. (2017) notice that different orders affect the performance of the Seq2Seq models in Multi-Class Classification (MCC), and conduct thoroughly experiments on frequency order and topology order. In JERE, Zeng et al. (2019b) study additional rule-based triplet prediction orders, including alphabetical, shuffle and fix-unsort, and then propose a reinforcement learning framework to generate triplets in adaptive orders dynamically. Tsai and Lee (2019) first point out the unnecessary order causes exposure bias altering the performance in MCC, and they find that"
2020.findings-emnlp.23,C10-2160,0,0.0361426,"are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the predic"
2020.findings-emnlp.23,D19-1035,1,0.90365,"g phase, the UMTree uses auto-regressive decoding strategy. The decoder predicts the nodes layer by layer, where the prediction results of the previous layer are used as the input of the next time step separately, as shown in Fig. 3b. 3 3.1 Experiments Settings Dataset We evaluate our model on two datasets, NYT and DuIE1 . NYT (Riedel et al., 2010) is a English news dataset that is generated by distant supervision without manual annotation, which is widely used in JERE studies (Zheng et al., 2017; Zeng et al., 2018; Takanobu et al., 2018; Dai et al., 2019; Fu et al., 2019; Nayak and Ng, 2019; Zeng et al., 2019a,b; Chen et al., 2019; Wei et al., 2019). We use the same data split as CopyRE (Zeng et al., 2018). DuIE (Li et al., 2019) is a large-scale Chinese JERE dataset where sentences are from Baidu 1 https://ai.baidu.com/broad/introduction? dataset=dureader Baselines We compare the proposed model, Seq2UMTree, with strong baselines under the same hyperparameters, as follows: 1) CopyMTL (Zeng et al., 2019a) is a Seq2Seq model with copy mechanism, and the entities are found by multi-task learning. 2) WDec (Nayak and Ng, 2019) is a standard Seq2Seq model with dynamic masking, and decode the entity toke"
2020.findings-emnlp.23,P18-1047,1,0.937366,", 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et al., 2018, 2019a,b; Nayak and Ng, 2019). Specifically, all existing Seq2Seq models pre-define a sequential order for the target triplets, e.g. triplet alphabetical order, and then decode the triplet sequence according to the order autoregressively, which means the current triplet prediction relies on the previous output. For exmaple, in Figure 1, the triplet list is flattened to [Obama]-[graduate from][Columbia University]-[Obama]-[graduate from][Harvard Law School]... However, the autoregressive decoding of the Seq2Seq models introduces exposure bias problem which may severely reduce the performance."
2020.inlg-1.44,D19-1299,0,0.0133532,"019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are som"
2020.inlg-1.44,E17-1059,0,0.0266285,"to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu e"
2020.inlg-1.44,N16-1087,0,0.0241933,"ee levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation"
2020.inlg-1.44,P17-1017,0,0.0118964,"2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 App"
2020.inlg-1.44,P19-1106,0,0.0248858,"w sentences that are also supported by corresponding sentences in the target papers. This process is very time-consuming and expensive. We need to build a better review infrastructure in our community, e.g., asking authors to provide feedback and rating to select constructive reviews as in NAACL20188 . 4 Related Work Paper Acceptance Prediction. Kang et al. (2018) has constructed a paper review corpus, PeerRead, and trained paper acceptance classifiers. Huang (2018) applies an interesting visual feature to compare the pdf layouts and proves its effectiveness to make paper acceptance decision. Ghosal et al. (2019) applies sentiment analysis features to improve acceptance prediction. The KDD2014 PC chairs exploit author status and review comments for predicting paper acceptance (Leskovec and Wang, 2014). We extend these methods to score prediction and comment generation with detailed knowledge element level evidence for each specific review category. Paper Review Generation. Bartoli et al. (2016) proposes the first deep neural network framework to generate paper review comments. The generator is trained with 48 papers from their own lab. In comparison, we perform more concrete and explainable review gen"
2020.inlg-1.44,D19-5615,0,0.0120979,"ration can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu"
2020.inlg-1.44,E17-1060,0,0.0190896,"on as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gard"
2020.inlg-1.44,P16-1154,0,0.0256106,"edge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are some example scenarios where ReviewRobot should and should not be used: • Should-Do: Reviewers use ReviewRobot merely as an assistant to write more constructive comments and compare notes. background"
2020.inlg-1.44,D14-1179,0,0.00835998,"Missing"
2020.inlg-1.44,P17-1055,0,0.091861,"ed for State-of-theart systems Cloze-style reading comprehension problem • attention-over-attention reader, n-best re-ranking strategy is verified in the related work section Neural architecture Used for Cloze-style reading comprehension problem Used for Large-scale training data Attention mechanism (Bahdanau et al., 2015; Hermann et al., 2015) • 5 new knowledge elements • 1 new architecture Overall Recom- • All features mentioned in the above categories mendations • Abstract Table 1: Evidence Extraction for the example paper Attention-over-Attention Neural Networks for Reading Comprehension (Cui et al., 2017) 2 2.1 Approach 2018): Overview Figure 1 illustrates the overall architecture of ReviewRobot. ReviewRobot first constructs knowledge graphs (KGs) for each target paper and a large collection of background papers, then it extracts evidence by comparing knowledge elements across multiple sections and papers, and uses the evidence to predict scores and generate comments for each review category. We adopt the following most common categories from NeurIPS20193 and PeerRead (Kang et al., 3 https://nips.cc/Conferences/2019/ PaperInformation/ReviewerGuidelines 386 • Summary: What is this paper about?"
2020.inlg-1.44,2020.acl-main.457,0,0.0164717,"2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are some example scenarios where ReviewRobot should and should not be used: • Should-Do: Reviewers use ReviewRobot merely as an assistant to write more constructive comments and compare notes. background papers, and summarize the pros and cons o"
2020.inlg-1.44,N18-2101,0,0.0182384,"9), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pinea"
2020.inlg-1.44,N18-1149,0,0.170369,"Missing"
2020.inlg-1.44,D16-1032,0,0.0181868,"16; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huan"
2020.inlg-1.44,N19-1238,0,0.0216935,"Missing"
2020.inlg-1.44,N18-1153,0,0.045738,"Missing"
2020.inlg-1.44,D16-1128,0,0.0361734,"Missing"
2020.inlg-1.44,P19-1190,0,0.111919,"ch review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment genera"
2020.inlg-1.44,D19-1319,0,0.0223965,"ates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled"
2020.inlg-1.44,P19-1479,0,0.115089,"ch review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment genera"
2020.inlg-1.44,P18-1138,0,0.024255,"on. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski"
2020.inlg-1.44,P19-1600,0,0.090912,"sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and"
2020.inlg-1.44,D19-1187,0,0.134256,"sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and"
2020.inlg-1.44,D18-1360,0,0.0127694,"stics and Examples for Template Generalization sit with respect to existing literature? Are the references adequate? • Potential Impact: How significant is the work described? If the ideas are novel, will they also be useful or inspirational? Does the paper bring any new insights into the nature of the problem? 2.2 Knowledge Graph Construction Generating meaningful and explainable reviews requires ReviewRobot to understand the knowledge elements of each paper. We apply a state-of-theart Information Extraction (IE) system for Natural Language Processing (NLP) and Machine Learning (ML) domains (Luan et al., 2018) to construct the following knowledge graphs (KGs): • GPτ : A KG constructed from the abstract and conclusion sections of a target paper under review Pτ , which describes the main techniques. ¯ Pτ : A KG constructed from the related work • G section of Pτ , which describes related techniques. • GB : A background KG constructed from all of the old NLP/ML papers published before the publication year of Pτ , in order to teach ReviewRobot what’s happening in the field. assigned one of six types: Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic Terms. Following the pre"
2020.inlg-1.44,P19-1197,0,0.0163511,"017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2"
2020.inlg-1.44,N16-1086,0,0.0158925,"ns and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge"
2020.inlg-1.44,N19-1236,0,0.0258224,"Missing"
2020.inlg-1.44,D19-1316,0,0.0171174,"cting paper acceptance (Leskovec and Wang, 2014). We extend these methods to score prediction and comment generation with detailed knowledge element level evidence for each specific review category. Paper Review Generation. Bartoli et al. (2016) proposes the first deep neural network framework to generate paper review comments. The generator is trained with 48 papers from their own lab. In comparison, we perform more concrete and explainable review generation by predicting scores and generating comments for each review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2"
2020.inlg-1.44,N18-1139,0,0.0480425,"Missing"
2020.inlg-1.44,P18-2112,0,0.021782,"cal errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (K"
2020.inlg-1.44,W19-8619,0,0.0116313,"text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organiz"
2020.inlg-1.44,P17-1187,0,0.0220323,"Missing"
2020.inlg-1.44,W17-4904,0,0.0278238,"ng scores and generating comments for each review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods generally apply a sequence-to-sequence model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information"
2020.inlg-1.44,D18-1073,0,0.0149775,"ng (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially bene"
2020.inlg-1.44,P19-1195,0,0.0199132,"Missing"
2020.inlg-1.44,2020.acl-main.355,0,0.0208021,"rishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 201"
2020.inlg-1.44,D19-1323,0,0.0195583,"tion (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who uses it. Here are some example scenarios where ReviewRobot should and should not be used: • Should-Do: Reviewers use ReviewRobot merely as an assistant to write more constructive comments and compare notes. background papers, and summariz"
2020.inlg-1.44,P18-1150,0,0.0141311,"e Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b)"
2020.inlg-1.44,P15-2050,0,0.0193771,"Missing"
2020.inlg-1.44,D19-1513,0,0.0232792,"Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summ"
2020.inlg-1.44,P10-1140,0,0.0868757,"Missing"
2020.inlg-1.44,D19-1406,0,0.0194965,"e model with attention to aspects and attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Ka"
2020.inlg-1.44,I08-4006,0,0.0247663,"rd representations (unsure about wsi) by modeling sememe information than other competitive baselines. Meaningful Comparison * [SYSTEM] The following related papers are missing: 1. About low-dimensional semantic space: (a) Unsupervised approximate-semantic vocabulary learning for human action and video classification (Zhao and Ip, 2013) Qiong Zhao and Horace HS Ip. 2013. Unsupervised Approximate-semantic Vocabulary Learning for Human Action and Video Classification. Pattern Recognition Letters, 34(15):1870–1878. 2. About sememes: (a) Chinese Word Sense Disambiguation with PageRank and HowNet (Wang et al., 2008): Jinghua Wang, Jianyi Liu, and Ping Zhang. 2008. Chinese Word Sense Disambiguation with PageRank and HowNet. In Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing. (b) A maximum entropy approach to HowNet-based Chinese word sense disambiguation (Wong and Yang, 2002): Ping Wai Wong and Yongsheng Yang. 2002. A Maximum Entropy Approach to HowNet-based Chinese Word Sense Disambiguation. In COLING-02: SEMANET: Building and Using Semantic Networks. 3. About word similarity and word analogy: (a) Open IE as an Intermediate Structure for Semantic Tasks (Stanovsky et al., 2015): Ga"
2020.inlg-1.44,P19-1191,1,0.875487,"Missing"
2020.inlg-1.44,P18-2042,1,0.848758,"tion. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis,"
2020.inlg-1.44,2020.acl-main.101,0,0.0148584,"nd attributes (e.g. food type). Compared to these domains, paper review generation is much more challenging because it requires the model to perform deep understanding on paper content, construct knowledge graphs to compare knowledge elements across sections and papers, and synthesize information as input evidence for comment generation. Controlled Knowledge-Driven Generation. There have been some other studies on text generation controlled by sentiment (Hu et al., 2017), topic (Krishna and Srinivasan, 2018), text style (Shen et al., 2017; Liu et al., 2019a; Tikhonov et al., 2019), and facts (Wang et al., 2020). The usage of external supportive knowledge in text generation can be roughly divided into the following three levels: (1) Knowledge Description, which transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-"
2020.inlg-1.44,C18-1320,0,0.0127207,"duppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to other scientific domains such as biomedical science and chemistry. Whether ReviewRobot is essentially beneficial to the scientific community also depends on who us"
2020.inlg-1.44,D18-1433,1,0.840706,"Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation (Wang et al., 2018b, 2019; Cachola et al., 2020) , and abstractive summarization (Gu et al., 2016; Sharma et al., 2019; Huang et al., 2020). 5 Application Limitations and Ethical Statement The types of evidence we have designed in this paper are limited to NLP, ML or related areas, and thus they are not applicable to oth"
2020.inlg-1.44,2020.acl-main.550,0,0.0276593,"ation with detailed knowledge element level evidence for each specific review category. Paper Review Generation. Bartoli et al. (2016) proposes the first deep neural network framework to generate paper review comments. The generator is trained with 48 papers from their own lab. In comparison, we perform more concrete and explainable review generation by predicting scores and generating comments for each review category following a rich set of evidence, and use a much larger data set. Nagata (2019) generates comment sentences to explain grammatical errors as feedback to improve paper writing. (Xing et al., 2020; Luu et al., 2020) extract paper-paper relations and 8 https://naacl2018.wordpress.com/2018/02/26/acceptanceand-author-feedback/ use them to guide citation text generation. Review Generation in other Domains. Automatic review generation techniques have been applied to many other domains including music (Tata and Di Eugenio, 2010), restaurants (Oraby et al., 2017; Juuti et al., 2018; Li et al., 2019a; Braˇzinskas et al., 2020), and products (Catherine and Cohen, 2018; Li et al., 2019a; Li and Tuzhilin, 2019; Dong et al., 2017; Ni and McAuley, 2018; Braˇzinskas et al., 2020). These methods gene"
2020.inlg-1.44,D19-1548,0,0.0133477,"h transforms structured data into unstructured text, such as Table-to-Text Generation (Mei et al., 2016; Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2018; Liu et al., 2018b; Nema et al., 2018; Wang et al., 2018a; Moryossef et al., 2019; Nie et al., 2019; Castro Ferreira et al., 2019; Wang et al., 2020; Shahidi et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018a,b), Data-to-Document (Wiseman et al., 2017; Puduppully et al., 2019; Gong et al., 2019; Iso et al., 2019), Graph-toText (Flanigan et al., 2016; Song et al., 2018; Zhu et al., 2019; Koncel-Kedziorski et al., 2019), and Topic-to-text (Tang et al., 2019), and Knowledge Base Description (Kiddon et al., 2016; Gardent et al., 2017; Koncel-Kedziorski et al., 2019); (2) Knowledge Synthesis, which retrieves knowledge base and organizes text answers, such as Video Caption Generation (Whitehead et al., 2018), KB-supported Dialogue Generation (Han et al., 2015; Zhou et al., 2018; Parthasarathi and Pineau, 2018; Liu et al., 2018a; Young et al., 2018; Wen et al., 2018; Chen et al., 2019; Liu et al., 2019b), 391 Knowledge-guided comment Generation (Li et al., 2019b), paper generation"
2020.lrec-1.243,P15-1017,0,0.0888802,"for the nodes (words). We train a BiAffine Dependency Parser (Dozat and Manning, 2016) for a particular language using the Universal Dependency treebanks (Nivre et al., 2016), and then apply the dependency parser to sentences to obtain universal dependency trees. For fully connected graphs, we regard each token in a sentence as a node in the graph and there’s an edge between each pair of nodes. Then we apply Tree-LSTM encoder and Transformer encoder to generate word representations in the latent space, respectively. Tree-LSTM Encoder. We exploit the Child-Sum TreeLSTMs proposed by Tai et al. (2015). In contrast to the standard LSTM, here the memory cell updates of the TreeLSTM unit are dependent on the states of all children units. The Tree-LSTM unit selectively incorporates information from each child. Transformer Encoder. Our multi-layer bidirectional Transformer encoder is based on the architecture proposed in Vaswani et al. (2017), composed of a stack of N identical layers, where each layer has a multi-head self-attention sub-layer and a position-wise feed-forward sub-layer. Our implementation is identical to the original, except that here, crucially, we do not include positional en"
2020.lrec-1.243,doddington-etal-2004-automatic,0,0.27252,"Missing"
2020.lrec-1.243,L18-1245,0,0.141208,"n Extraction (IE) that aims to identify event triggers and arguments from unstructured texts and classify them into predefined categories. Compared to other IE tasks such as name tagging, the annotations for Event Extraction are more costly because they are structured and require a rich label space; full event structure annotation consists of its trigger span and type label as well as each of its one or more argument spans and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fir"
2020.lrec-1.243,C16-1114,0,0.538714,"l., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose event extraction by attention mechanism, e.g. the former use a gated multi-lingual attention technique. To the best of our knowledge, this is the first work to design a cross-lingual structure transfer framework to enable event extraction for a language without any event training data. 5. Conclusions and Future Work In this paper, we propose a"
2020.lrec-1.243,P08-1030,1,0.911889,"sonnel Business Manufacture 2,613 346 539 453 2,411 301 53 52 1,956 207 116 86 1,688 275 651 225 1,340 146 58 43 279 0 14 8 158 58 9 5 Table 2: Distribution of event types in various datasets (Number of event mentions). The statistics for English are from the training split, and the statistics for Spanish, Russian and Ukrainian are from testing splits. Hyperparameter Value word embedding size 300 hidden dimension size 768 filter size 768 number of head 12 number of layer 12 dropout 0.2 learning rate 0.003 batch size 16 of event types for each language. We follow the criteria in previous work (Ji and Grishman, 2008; Li et al., 2013) for evaluation. 3.2. Training Details Treebanks. We use the Version 2.3 treebanks released by Universal Dependencies 3 to train the dependency parsers. Tokenization. We use Spacy tokenization (Honnibal and Montani, 2017) for English and Spanish and the NLTK toktok tokenizer (Dehdari, 2014) for Russian and Ukrainian. Word Embedding. We use multilingual word embeddings released by Facebook Research (Lample et al., 2017) 4 . The algorithm aligns word embeddings of various languages, which are pre-trained from Wikipedia articles (Joulin et al., 2016) 5 , in a single vector space"
2020.lrec-1.243,D12-1092,0,0.0290845,"large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose"
2020.lrec-1.243,P13-1008,1,0.885221,"ture 2,613 346 539 453 2,411 301 53 52 1,956 207 116 86 1,688 275 651 225 1,340 146 58 43 279 0 14 8 158 58 9 5 Table 2: Distribution of event types in various datasets (Number of event mentions). The statistics for English are from the training split, and the statistics for Spanish, Russian and Ukrainian are from testing splits. Hyperparameter Value word embedding size 300 hidden dimension size 768 filter size 768 number of head 12 number of layer 12 dropout 0.2 learning rate 0.003 batch size 16 of event types for each language. We follow the criteria in previous work (Ji and Grishman, 2008; Li et al., 2013) for evaluation. 3.2. Training Details Treebanks. We use the Version 2.3 treebanks released by Universal Dependencies 3 to train the dependency parsers. Tokenization. We use Spacy tokenization (Honnibal and Montani, 2017) for English and Spanish and the NLTK toktok tokenizer (Dehdari, 2014) for Russian and Ukrainian. Word Embedding. We use multilingual word embeddings released by Facebook Research (Lample et al., 2017) 4 . The algorithm aligns word embeddings of various languages, which are pre-trained from Wikipedia articles (Joulin et al., 2016) 5 , in a single vector space. It learns a mapp"
2020.lrec-1.243,D14-1198,1,0.831152,"when the model generates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al"
2020.lrec-1.243,R11-1002,0,0.0219009,"visualized attention weights, we can clearly see when the model generates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recen"
2020.lrec-1.243,P18-1074,1,0.785986,"icly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual event transfer learning in a zero-resource"
2020.lrec-1.243,D18-1156,0,0.0860644,"nt of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extr"
2020.lrec-1.243,D18-1517,0,0.0123448,"guagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose event extraction by attention mechanism, e.g. the former use a gated multi-lingual attention technique. To the best of our knowledge, this is the first work to design a cross-lingual structure transfer framework to enable event extraction for a language without any event training data. 5. Conclusions and Future Work In this paper, we propose a novel cross-lingual structure transfer framework for zero-resource event extraction. Experiments on three lang"
2020.lrec-1.243,P15-2060,0,0.107531,"пину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extract"
2020.lrec-1.243,N16-1034,0,0.0723032,"es besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et a"
2020.lrec-1.243,L16-1262,0,0.0919272,"Missing"
2020.lrec-1.243,P15-1150,0,0.122428,"Missing"
2020.lrec-1.243,P17-2046,0,0.0487613,"Missing"
2020.lrec-1.243,D18-1034,0,0.0245405,"re argument spans and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual eve"
2020.lrec-1.243,N16-1033,0,0.0142582,"nerates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018"
2020.lrec-1.243,N18-5009,1,0.761275,"and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual event transfer learning"
2021.acl-long.133,P15-1034,0,0.0223896,"ccuracy on fake documents. A third of the fake news documents were predicted incorrectly by over half of the human subjects. This indicates that our automatically generated fake documents are also very hard for humans to detect. The most common clues humans used to detect fake news include linguistic style, topic coherence, specific event details and novel entities. 6 Related Work Fake News Detection. Traditional approaches to fake news detection are largely based on factchecking, text-style, or context from a single modality (Ciampaglia et al., 2015; Shi and Weninger, 2016; Pan et al., 2018; Angeli et al., 2015). Other approaches include detecting previously factchecked claims (Shaar et al., 2020), retrieving sentences that explain fact-checking (Nadeem et al., 2019; Atanasova et al., 2020), and leveraging context and discourse information (Nakov et al., 2019). 1689 Image Caption Body Text Misinformative KEs Aerial view of Fort McHenry. The battle of Fort McHenry, which took place in September of 1814, was a pivotal moment in the U.S. War of Independence...When the British finally left, they left behind a trail of destruction, including the destruction of the twin towers of the World Trade Center ..."
2021.acl-long.133,2020.findings-emnlp.89,0,0.011227,"stan, the Taliban released to the media this picture, which it said shows the suicide bombers who attacked the army base in Mazar-i-Sharif, April 21, 2017 Fake caption: On 21 April 2017 the Taliban released this picture to the army in Afghanistan which they said was a suicide bomber hiding at a media base in the city of Mazar-i-Sharif Figure 4: Example of AMR-to-text fake caption generation. The roles of army and media (in blue) are switched and the node corresponding to the event trigger (in red) attacked is negated. To obtain the AMR graphs, we use the stacktransformer based AMR parser from Astudillo et al. (2020) and train it on AMR 3.03 . Given the AMR graph, we vary the manipulation as follows: (1) Role switching - we randomly select two entity mentions that are present in different argument subgraphs of the AMR root node and interchange their positions in the AMR graph. (2) Predicate negation - we randomly pick predicates in the AMR graph corresponding to event triggers and other verbs, and replace them with their antonyms, which we obtain from WordNet (Fellbaum, 1998). This manipulation also includes reverting nodes with negative polarity, thereby negating the sentence. 1687 3 https://catalog.ldc."
2021.acl-long.133,2020.acl-main.656,0,0.0166883,"ocuments are also very hard for humans to detect. The most common clues humans used to detect fake news include linguistic style, topic coherence, specific event details and novel entities. 6 Related Work Fake News Detection. Traditional approaches to fake news detection are largely based on factchecking, text-style, or context from a single modality (Ciampaglia et al., 2015; Shi and Weninger, 2016; Pan et al., 2018; Angeli et al., 2015). Other approaches include detecting previously factchecked claims (Shaar et al., 2020), retrieving sentences that explain fact-checking (Nadeem et al., 2019; Atanasova et al., 2020), and leveraging context and discourse information (Nakov et al., 2019). 1689 Image Caption Body Text Misinformative KEs Aerial view of Fort McHenry. The battle of Fort McHenry, which took place in September of 1814, was a pivotal moment in the U.S. War of Independence...When the British finally left, they left behind a trail of destruction, including the destruction of the twin towers of the World Trade Center ... <British, Conflict.Attack, twin towers> Table 4: An example fake document which Tan et al. (2020) misses, but InfoSurgeon successfully detects. Text Features P´erez-Rosas et al. (20"
2021.acl-long.133,D18-1389,0,0.0330051,"Missing"
2021.acl-long.133,W13-2322,0,0.0137395,"Missing"
2021.acl-long.133,D14-1162,0,0.0850423,"mbeddings: We define an attribute function, A : Nt , Er|a ) F , that transforms each of the nodes and edges to its initial representation by concatenating the following features: • Background Embeddings - For the entity nodes Nt that can be linked to Freebase, we use data dump from Google Developers resources2 to map them to their respective Wikipedia pages, which serve as a rich source of established background knowledge. Background node embedding features are initialized from passing a Long Short Term Memory networks (LSTM) based architecture (Gers et al., 2000) through the word embeddings (Pennington et al., 2014) of the first paragraph in the Wikipedia page, which usually starts with a mention of the Wiki page’s title. Background edge embedding features are initialized from passing the LSTM through the paragraphs that contain the mentions of both the head and tail nodes. These embeddings are set to a default zero vector for unlinkable nodes. 1685 2 https://developers.google.com/freebase/ • News Embeddings - These are the surface-level features circumstantial to the entities, relations, and events extracted. News-based node features are initialized from passing an LSTM through the word embeddings of th"
2021.acl-long.133,C18-1287,0,0.040838,"Missing"
2021.acl-long.133,N18-1119,0,0.0250419,"Missing"
2021.acl-long.133,D19-1216,0,0.0412103,"Missing"
2021.acl-long.133,2020.emnlp-main.163,0,0.565405,"the node features. We feed the body text and each caption through the summarization-based BERT encoder from Liu and Lapata (2019), which averages the encoded token embeddings across sentences through a weighted mechanism. For metadata, we run the text encoder on a string containing the article domain, publication date, author, and headline. For images, we concatenate object-based (Anderson et al., 2018) and event-based (Pratt et al., 2020) visual features. Features for the edges between global context nodes are initialized by the attention-based semantic similarity between the node features (Tan et al., 2020). 3.3 Local KG Representation Constructing a KG from each Multimedia News Article: We leverage a publicly available multimedia Information Extraction (IE) system (Li et al., 2020; Lin et al., 2020) to construct a withindocument knowledge graph KG = (Nt , Er|a ) for each multimedia article. The IE system can extract 197 types of entities, 61 types of relations, and 144 types of events from text and images. Then, it performs entity linking (Pan et al., 2017) to map entities extracted from both text and images to a background knowledge base e.g. Freebase (Bollacker et al., 2008) and NIL (unlinkab"
2021.acl-long.133,N18-1074,0,0.0540313,"Missing"
2021.acl-long.133,P17-2067,0,0.0247961,"guage processing (Zellers et al., 2019) and computer vision (Choi et al., 2018) have become the frontier for malicious actors to controllably generate misinformation at scale. These realistic-looking AI-generated “fake news” have been shown to easily deceive humans, and it is, thus, critical for us to develop robust verification techniques against machine-generated fake news (Tan et al., 2020; Zellers et al., 2019; Kaliyar et al., 2020). Current misinformation detection approaches mainly focus on document-level fake news detection using lexical features and semantic embedding representations (Wang, 2017; Karimi et al., 2018; Tan et al., 2020). However, fake news is often generated based on manipulating (misusing, exaggerating, or falsifying) only a small part of the true information, namely the knowledge 1 The code, data and resources related to the misinformation detector are made publicly available at https://github. com/yrf1/InfoSurgeon for research purposes. elements (KEs, including entities, relations and events). Moreover, recent news oftentimes makes claims that do not have verified evidence yet, and evaluating the truthfulness of these real-time claims depends more on their consisten"
2021.acl-long.260,P17-1147,0,0.0212785,"tions. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outper"
2021.acl-long.260,Q19-1026,0,0.0120968,"able 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outperforms all baselines, indicating that thro"
2021.acl-long.260,2021.ccl-1.108,0,0.049509,"Missing"
2021.acl-long.260,2020.emnlp-main.298,1,0.863483,"Missing"
2021.acl-long.260,D19-1005,0,0.0318608,"heir relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representation"
2021.acl-long.260,D16-1264,0,0.0361457,"hich are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table"
2021.acl-long.260,W04-2401,0,0.417814,"Missing"
2021.acl-long.260,W03-0419,0,0.619684,"Missing"
2021.acl-long.260,P19-1279,0,0.23953,"s of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity representations. Note eij may appear multiple times in di , the k-th occurrence of eij , which contains the tokens from index nkstart to nkend , is represented as: mkeij = MeanPoo"
2021.acl-long.260,2020.coling-main.327,0,0.0338092,"y. Although achieving great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or"
2021.acl-long.260,P19-1485,0,0.020828,"elf is considerably smaller, measuring only. [6] Culiacán is a rail junction and is located on the Panamerican Highway that runs south to Guadalajara and Mexico City. [7] Culiacán is connected to the north with Los Mochis, and to the south with Mazatlán, Tepic. 1 Q: where is Guadalajara? Mexico Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019). Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks. Corresponding author. Our code and data are publicly available at https:// github.com/thunlp/ERICA. 1 A: Mexico. o Panamerican Highway Los Mochis Sinaloa Mexico City Guadalajara Figure 1: An example for a document “Culiacán”, in which all entities are underlined. We show entities and their relations as a relational graph,"
2021.acl-long.260,W18-5446,0,0.0555444,"Missing"
2021.acl-long.260,K17-1028,0,0.0151597,"reading multiple documents and conducting multi-hop reasoning. It has both standard and masked settings, where the latter setting masks all entities with random IDs to avoid information leakage. We first concatenate the question and documents into a long sequence, then we find all the occurrences of an entity in the documents, encode them into hidden representations and obtain the global entity representation by applying mean pooling on these hidden representations. Finally, we use a classifier on top of the entity representation for prediction. We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying"
2021.acl-long.260,Q18-1021,0,0.0536187,"Missing"
2021.acl-long.260,C14-1220,0,0.139289,"Missing"
2021.acl-long.260,D17-1004,0,0.026425,"easoning patterns in the pre-training; (2) both MTB and CP achieve worse results than BERT, which means sentence-level pre-training, lacking consideration for complex reasoning patterns, hurts PLM’s performance on document-level RE tasks to some extent; (3) ERICA outperforms baselines by a larger margin on smaller training sets, which means ERICA has gained pretty good document-level relation reasoning ability in contrastive learning, and thus obtains improvements more extensively under low-resource settings. Sentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019). We insert extra marker tokens to indicate the head and tail entities in each sentence. For baselines, we compare ERICA with BERT, RoBERTa, MTB and CP. From the results shown in Table 2, we observe that ERICA achieves almost comparable results on sentence-level RE tasks with CP, which means document-level pre-training in 10 In practice, documents are split into sentences and we only keep within-sentence entity pairs. 11 https://github.com/thunlp/ RE-Context-or-Names - 27.2 49.7 53.7 54.4 56.4 51.7 50.4 57.8 69.5 68.8 70.7 68.4 67.4 69.7 37.9 39"
2021.acl-long.260,P19-1139,1,0.80299,"ative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining bette"
2021.acl-long.260,2020.emnlp-main.523,0,0.162157,"ing great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in p"
2021.acl-long.260,P19-1074,1,0.884737,"Missing"
2021.acl-long.260,2020.emnlp-main.582,1,0.844179,"e methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity"
2021.acl-long.488,2021.naacl-main.274,1,0.765017,"action from biomedical text. Experimental results show that KECI is highly effective, achieving new stateof-the-art results on two datasets: BioRelEx and ADE. Theoretically, KECI can take an entire document as input; however, the tested datasets are only sentence-level datasets. In the future, we plan to evaluate our framework on more document-level datasets. We also plan to explore a broader range of properties and information that can be extracted from external KBs to facilitate biomedical IE tasks. Finally, we also plan to apply KECI to other information extraction tasks (Li et al., 2020a; Lai et al., 2021; Wen et al., 2021). Acknowledgement We thank the three reviewers and the Area Chair for their insightful comments and suggestions. This research is based upon work supported by the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, NSF No. 2034562, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract No. FA8650-17-C9116. Any opinions, findings and conclusions or recommendations expressed in this document are those"
2021.acl-long.488,D17-1018,0,0.0295093,"ur model first constructs a contextualized representation for each input token using SciBERT (Beltagy et al., 2019). Let X = (x1 , ..., xn ) be the output of the token-level encoder, where n denotes the number of tokens in D. Then, for each span si whose length is not more than L, we compute its span representation si ∈ Rd as:   si = FFNNg xSTART(i) , xEND(i) , ˆxi , φ(si ) (1) where START(i) and END(i) denote the start and end indices of si respectively. xSTART(i) and xEND(i) are the boundary token representations. ˆxi is an attention-weighted sum of the token representations in the span (Lee et al., 2017). φ(si ) is a feature vector denoting the span length. FFNNg is a feedforward network with ReLU activations. Overview 2.3 KECI considers text spans as the basic units for feature extraction and prediction. This design choice allows us to handle nested entity mentions (Sohrab and Miwa, 2018). Also, joint entity and relation extraction can be naturally formulated as the task of extracting a span graph from an input document (Luan et al., 2019). In a span graph, each node represents a (predicted) entity mention, and each edge represents a (predicted) relation between two entity mentions. Given an"
2021.acl-long.488,N19-1145,1,0.830084,"Missing"
2021.acl-long.488,D17-1159,0,0.0273102,"rst prune out spans of text that are unlikely to be entity mentions. We only keep up to λn spans with the lowest probability scores of being a non-entity. The value of λ is selected empirically and set to be 6250 0.5. Spans that pass the filter are represented as nodes in the initial span graph. For every span pair hsi , sj i, we create |R |directed edges from the node representing si to the node representing sj . Each edge represents one relation type and is weighted by the corresponding probability score in rij . Let Gs = {Vs , Es } denote the initial span graph. We use a bidirectional GCN (Marcheggiani and Titov, 2017; Fu et al., 2019) to recursively update each span representation:   X X ~(l) l ~(l) ~ l hi = rij [k] Wk hj + bk sj ∈Vs {si } k∈R h~li = X X   ~(l) l ~(l) rji [k] Wk hj + bk sj ∈Vs {si } k∈R hl+1 = hli + FFNN(l) a i !   ~l ~l  ReLU hi , hi (4) where hli is the hidden feature vector of span si at (l) layer l. We initialize h0i to be si (Eq. 1). FFNNa is a feedforward network whose output dimension is the same as the dimension of hli . After multiple iterations of message passing, each span representation will contain the global relational information of Gs . Let hi denote the feature"
2021.acl-long.488,P14-1038,1,0.763339,"l attention because of iliocaval manifestations of retroperitoneal fibrosis while she was taking methysergide. #3: TITLE: Acute abdomen due to endometriosis in a premenopausal woman taking tamoxifen. Table 6: Examples showing how external knowledge improves the quality of extracted span graphs. Edges represent relations of type Adverse-Effect. Only relations with predicted probabilities of at least 0.5 are shown. propagation problem. To overcome these limitations, many studies have proposed joint models that perform entity extraction and relation extraction simultaneously (Roth and Yih, 2007; Li and Ji, 2014; Li et al., 2017; Zheng et al., 2017; Bekoulis et al., 2018a,b; Wadden et al., 2019; Fu et al., 2019; Luan et al., 2019; Zhao et al., 2020; Wang and Lu, 2020; Li et al., 2020b; Lin et al., 2020). Particularly, span-based joint extraction methods have gained much popularity lately because of their ability to detect overlapping entities. For example, Eberts and Ulges (2020) propose SpERT, a simple but effective span-based model that utilizes BERT as its core. The recent work of Ji et al. (2020) also closely follows the overall architecture of SpERT but differs in span-specific and contextual se"
2021.acl-long.488,P18-1076,0,0.0243675,"EANet) to utilize domain knowledge from UMLS. In the framework, a global KG for the entire corpus is first constructed, and then a 6255 sentence-level KG is created for each individual sentence in the corpus. Our method of KG construction is more flexible as we directly create a KG for each input text. Furthermore, the work of Huang et al. (2020) only deals with event extraction and assumes that gold-standard entity mentions are provided at inference time. Some previous work has focused on integrating external knowledge into neural architectures for other tasks, such as reading comprehension (Mihaylov and Frank, 2018), question answering (Pan et al., 2019), natural language inference (Sharma et al., 2019), and conversational modeling (Parthasarathi and Pineau, 2018). Different from these studies, our work explicitly emphasizes the benefit of collective inference using global relational information. Many previous studies have also used GNNs for various IE tasks (Nguyen and Grishman, 2018; Liu et al., 2018; Subburathinam et al., 2019; Zeng et al., 2021; Zhang and Ji, 2021). Many of these methods use a dependency parser or a semantic parser to construct a graph capturing global interactions between tokens/spa"
2021.acl-long.488,2020.acl-main.713,1,0.845276,"xifen. Table 6: Examples showing how external knowledge improves the quality of extracted span graphs. Edges represent relations of type Adverse-Effect. Only relations with predicted probabilities of at least 0.5 are shown. propagation problem. To overcome these limitations, many studies have proposed joint models that perform entity extraction and relation extraction simultaneously (Roth and Yih, 2007; Li and Ji, 2014; Li et al., 2017; Zheng et al., 2017; Bekoulis et al., 2018a,b; Wadden et al., 2019; Fu et al., 2019; Luan et al., 2019; Zhao et al., 2020; Wang and Lu, 2020; Li et al., 2020b; Lin et al., 2020). Particularly, span-based joint extraction methods have gained much popularity lately because of their ability to detect overlapping entities. For example, Eberts and Ulges (2020) propose SpERT, a simple but effective span-based model that utilizes BERT as its core. The recent work of Ji et al. (2020) also closely follows the overall architecture of SpERT but differs in span-specific and contextual semantic representations. Despite their impressive performance, these methods are not designed specifically for the biomedical domain, and they do not utilize any external knowledge base. To the be"
2021.acl-long.488,D18-1156,0,0.0487811,"Missing"
2021.acl-long.488,D19-5804,1,0.828698,"n the framework, a global KG for the entire corpus is first constructed, and then a 6255 sentence-level KG is created for each individual sentence in the corpus. Our method of KG construction is more flexible as we directly create a KG for each input text. Furthermore, the work of Huang et al. (2020) only deals with event extraction and assumes that gold-standard entity mentions are provided at inference time. Some previous work has focused on integrating external knowledge into neural architectures for other tasks, such as reading comprehension (Mihaylov and Frank, 2018), question answering (Pan et al., 2019), natural language inference (Sharma et al., 2019), and conversational modeling (Parthasarathi and Pineau, 2018). Different from these studies, our work explicitly emphasizes the benefit of collective inference using global relational information. Many previous studies have also used GNNs for various IE tasks (Nguyen and Grishman, 2018; Liu et al., 2018; Subburathinam et al., 2019; Zeng et al., 2021; Zhang and Ji, 2021). Many of these methods use a dependency parser or a semantic parser to construct a graph capturing global interactions between tokens/spans. However, parsers for specialized bi"
2021.acl-long.488,D18-1073,0,0.0210429,"level KG is created for each individual sentence in the corpus. Our method of KG construction is more flexible as we directly create a KG for each input text. Furthermore, the work of Huang et al. (2020) only deals with event extraction and assumes that gold-standard entity mentions are provided at inference time. Some previous work has focused on integrating external knowledge into neural architectures for other tasks, such as reading comprehension (Mihaylov and Frank, 2018), question answering (Pan et al., 2019), natural language inference (Sharma et al., 2019), and conversational modeling (Parthasarathi and Pineau, 2018). Different from these studies, our work explicitly emphasizes the benefit of collective inference using global relational information. Many previous studies have also used GNNs for various IE tasks (Nguyen and Grishman, 2018; Liu et al., 2018; Subburathinam et al., 2019; Zeng et al., 2021; Zhang and Ji, 2021). Many of these methods use a dependency parser or a semantic parser to construct a graph capturing global interactions between tokens/spans. However, parsers for specialized biomedical domains are expensive to build. KECI does not rely on such expensive resources. 5 Conclusions and Futur"
2021.acl-long.488,D18-1360,0,0.0568168,"Missing"
2021.acl-long.488,N19-1308,0,0.0704474,"f si respectively. xSTART(i) and xEND(i) are the boundary token representations. ˆxi is an attention-weighted sum of the token representations in the span (Lee et al., 2017). φ(si ) is a feature vector denoting the span length. FFNNg is a feedforward network with ReLU activations. Overview 2.3 KECI considers text spans as the basic units for feature extraction and prediction. This design choice allows us to handle nested entity mentions (Sohrab and Miwa, 2018). Also, joint entity and relation extraction can be naturally formulated as the task of extracting a span graph from an input document (Luan et al., 2019). In a span graph, each node represents a (predicted) entity mention, and each edge represents a (predicted) relation between two entity mentions. Given an input document D, KECI first enumerates all the spans (up to a certain length) and embeds them into feature vectors (Sec. 2.2). With these feature vectors, KECI predicts an initial span graph and applies a GCN to integrate initial relational information into each span representation (Sec. 2.3). KECI then uses an entity linker to build a background knowledge graph and applies another GCN to encode each node of the graph (Sec. 2.4). Finally,"
2021.acl-long.488,D19-1005,0,0.0348202,"Missing"
2021.acl-long.488,N10-1123,0,0.0386958,"f Ji et al. (2020) also closely follows the overall architecture of SpERT but differs in span-specific and contextual semantic representations. Despite their impressive performance, these methods are not designed specifically for the biomedical domain, and they do not utilize any external knowledge base. To the best of our knowledge, our work is the first span-based framework that utilizes external knowledge for joint entity and relation extraction from biomedical text. Biomedical event extraction is a closely related task that has also received a lot of attention from the research community (Poon and Vanderwende, 2010; Kim et al., 2013; V S S Patchigolla et al., 2017; Rao et al., 2017; Espinosa et al., 2019; Li et al., 2019; Wang et al., 2020; Huang et al., 2020; Ramponi et al., 2020; Yadav et al., 2020). Several studies have proposed to incorporate external knowledge from domain-specific KBs into neural models for biomedical event extraction. For example, Li et al. (2019) incorporate entity information from Gene Ontology into tree-LSTM models. However, their approach does not explicitly use any external relational information. Recently, Huang et al. (2020) introduce a framework that uses a novel Graph Edg"
2021.acl-long.488,2020.emnlp-main.431,0,0.0633915,"Missing"
2021.acl-long.488,W17-2315,0,0.0148362,"differs in span-specific and contextual semantic representations. Despite their impressive performance, these methods are not designed specifically for the biomedical domain, and they do not utilize any external knowledge base. To the best of our knowledge, our work is the first span-based framework that utilizes external knowledge for joint entity and relation extraction from biomedical text. Biomedical event extraction is a closely related task that has also received a lot of attention from the research community (Poon and Vanderwende, 2010; Kim et al., 2013; V S S Patchigolla et al., 2017; Rao et al., 2017; Espinosa et al., 2019; Li et al., 2019; Wang et al., 2020; Huang et al., 2020; Ramponi et al., 2020; Yadav et al., 2020). Several studies have proposed to incorporate external knowledge from domain-specific KBs into neural models for biomedical event extraction. For example, Li et al. (2019) incorporate entity information from Gene Ontology into tree-LSTM models. However, their approach does not explicitly use any external relational information. Recently, Huang et al. (2020) introduce a framework that uses a novel Graph Edge conditioned Attention Network (GEANet) to utilize domain knowledge"
2021.acl-long.488,D19-1631,0,0.0268141,"pus is first constructed, and then a 6255 sentence-level KG is created for each individual sentence in the corpus. Our method of KG construction is more flexible as we directly create a KG for each input text. Furthermore, the work of Huang et al. (2020) only deals with event extraction and assumes that gold-standard entity mentions are provided at inference time. Some previous work has focused on integrating external knowledge into neural architectures for other tasks, such as reading comprehension (Mihaylov and Frank, 2018), question answering (Pan et al., 2019), natural language inference (Sharma et al., 2019), and conversational modeling (Parthasarathi and Pineau, 2018). Different from these studies, our work explicitly emphasizes the benefit of collective inference using global relational information. Many previous studies have also used GNNs for various IE tasks (Nguyen and Grishman, 2018; Liu et al., 2018; Subburathinam et al., 2019; Zeng et al., 2021; Zhang and Ji, 2021). Many of these methods use a dependency parser or a semantic parser to construct a graph capturing global interactions between tokens/spans. However, parsers for specialized biomedical domains are expensive to build. KECI does"
2021.acl-long.488,D18-1309,0,0.0656798,"Missing"
2021.acl-long.488,D19-1030,1,0.893864,"Missing"
2021.acl-long.488,W17-2340,0,0.0204238,"architecture of SpERT but differs in span-specific and contextual semantic representations. Despite their impressive performance, these methods are not designed specifically for the biomedical domain, and they do not utilize any external knowledge base. To the best of our knowledge, our work is the first span-based framework that utilizes external knowledge for joint entity and relation extraction from biomedical text. Biomedical event extraction is a closely related task that has also received a lot of attention from the research community (Poon and Vanderwende, 2010; Kim et al., 2013; V S S Patchigolla et al., 2017; Rao et al., 2017; Espinosa et al., 2019; Li et al., 2019; Wang et al., 2020; Huang et al., 2020; Ramponi et al., 2020; Yadav et al., 2020). Several studies have proposed to incorporate external knowledge from domain-specific KBs into neural models for biomedical event extraction. For example, Li et al. (2019) incorporate entity information from Gene Ontology into tree-LSTM models. However, their approach does not explicitly use any external relational information. Recently, Huang et al. (2020) introduce a framework that uses a novel Graph Edge conditioned Attention Network (GEANet) to utiliz"
2021.acl-long.488,D19-1585,0,0.0179071,"e she was taking methysergide. #3: TITLE: Acute abdomen due to endometriosis in a premenopausal woman taking tamoxifen. Table 6: Examples showing how external knowledge improves the quality of extracted span graphs. Edges represent relations of type Adverse-Effect. Only relations with predicted probabilities of at least 0.5 are shown. propagation problem. To overcome these limitations, many studies have proposed joint models that perform entity extraction and relation extraction simultaneously (Roth and Yih, 2007; Li and Ji, 2014; Li et al., 2017; Zheng et al., 2017; Bekoulis et al., 2018a,b; Wadden et al., 2019; Fu et al., 2019; Luan et al., 2019; Zhao et al., 2020; Wang and Lu, 2020; Li et al., 2020b; Lin et al., 2020). Particularly, span-based joint extraction methods have gained much popularity lately because of their ability to detect overlapping entities. For example, Eberts and Ulges (2020) propose SpERT, a simple but effective span-based model that utilizes BERT as its core. The recent work of Ji et al. (2020) also closely follows the overall architecture of SpERT but differs in span-specific and contextual semantic representations. Despite their impressive performance, these methods are not"
2021.acl-long.488,2020.emnlp-main.133,0,0.0390834,"in a premenopausal woman taking tamoxifen. Table 6: Examples showing how external knowledge improves the quality of extracted span graphs. Edges represent relations of type Adverse-Effect. Only relations with predicted probabilities of at least 0.5 are shown. propagation problem. To overcome these limitations, many studies have proposed joint models that perform entity extraction and relation extraction simultaneously (Roth and Yih, 2007; Li and Ji, 2014; Li et al., 2017; Zheng et al., 2017; Bekoulis et al., 2018a,b; Wadden et al., 2019; Fu et al., 2019; Luan et al., 2019; Zhao et al., 2020; Wang and Lu, 2020; Li et al., 2020b; Lin et al., 2020). Particularly, span-based joint extraction methods have gained much popularity lately because of their ability to detect overlapping entities. For example, Eberts and Ulges (2020) propose SpERT, a simple but effective span-based model that utilizes BERT as its core. The recent work of Ji et al. (2020) also closely follows the overall architecture of SpERT but differs in span-specific and contextual semantic representations. Despite their impressive performance, these methods are not designed specifically for the biomedical domain, and they do not utilize a"
2021.acl-long.488,2020.louhi-1.10,0,0.0406182,"ations. Despite their impressive performance, these methods are not designed specifically for the biomedical domain, and they do not utilize any external knowledge base. To the best of our knowledge, our work is the first span-based framework that utilizes external knowledge for joint entity and relation extraction from biomedical text. Biomedical event extraction is a closely related task that has also received a lot of attention from the research community (Poon and Vanderwende, 2010; Kim et al., 2013; V S S Patchigolla et al., 2017; Rao et al., 2017; Espinosa et al., 2019; Li et al., 2019; Wang et al., 2020; Huang et al., 2020; Ramponi et al., 2020; Yadav et al., 2020). Several studies have proposed to incorporate external knowledge from domain-specific KBs into neural models for biomedical event extraction. For example, Li et al. (2019) incorporate entity information from Gene Ontology into tree-LSTM models. However, their approach does not explicitly use any external relational information. Recently, Huang et al. (2020) introduce a framework that uses a novel Graph Edge conditioned Attention Network (GEANet) to utilize domain knowledge from UMLS. In the framework, a global KG for the entire co"
2021.acl-long.488,P17-1132,0,0.0234733,"ion. Lr* denotes the binary cross-entropy loss of relation classification. Le1 and Lr1 are loss terms for the initial span graph prediction (Eq. 2 and Eq. 3 of Section 2.3). Le2 and Lr2 are loss terms for the final span graph prediction (Eq. 9 of Section 2.5). We apply a larger weight score to the loss terms Le2 and Lr2 . We train the framework using only ground-truth labels of the entity and relation extraction tasks. We do not make use of any entity linking supervision in this work. where FFNNc is a feedforward network mapping from R2×d → R. Then we compute an additional sentinel vector ci (Yang and Mitchell, 2017; He et al., 2020) and also compute a score αi for it:  ci = FFNNs hi   (7) αi = FFNNc hi , ci 3 where FFNNs is another feedforward network mapping from Rd → Rd . Intuitively, ci records the information of the local context of si , and αi measures the importance of such information. After that, we compute a final knowledge-aware representation fi for each span si as follows: X Z = exp (αi ) + exp (αiz ) vz ∈C(si ) βi = exp (αi )/Z and βij = exp (αij )/Z X fi = βi ci + βij nj (8) vj ∈C(si ) The attention mechanism is illustrated in Figure 3. With the extracted knowledge-aware span represent"
2021.acl-long.488,W02-1010,0,0.151981,"be its AdverseEffect. However, KECI can link the term “endometriosis” to a UMLS entity of semantic type Disease or Syndrome. As a result, the system can correct the term’s type and also predict the right edges for the final span graph. Finally, we also examined the errors made by KECI. One major issue is that MetaMap sometimes fails to return any candidate entity from UMLS for an entity mention. We leave the extension of this work to using multiple KBs as future work. 4 Related Work Traditional pipelined methods typically treat entity extraction and relation extraction as two separate tasks (Zelenko et al., 2002; Zhou et al., 2005; Chan and Roth, 2011). Such approaches ignore the close interaction between named entities and their relation information and typically suffer from the error 6254 Datasets Top 3 types with the lowest avg. attention scores Top 3 types with the highest avg. attention scores BioRelEx Diagnostic Procedure (0.04); Activity (0.05); Plant (0.05) Amino Acid, Peptide, or Protein (0.32); Enzyme (0.32); Molecular Function (0.36) ADE Intellectual Product (0.15); Idea or Concept (0.19); Temporal Concept (0.19) Antibiotic (0.78); Organic Chemical (0.79); Nucleic Acid, Nucleoside, or Nucl"
2021.acl-long.488,2021.textgraphs-1.5,1,0.753535,"rence time. Some previous work has focused on integrating external knowledge into neural architectures for other tasks, such as reading comprehension (Mihaylov and Frank, 2018), question answering (Pan et al., 2019), natural language inference (Sharma et al., 2019), and conversational modeling (Parthasarathi and Pineau, 2018). Different from these studies, our work explicitly emphasizes the benefit of collective inference using global relational information. Many previous studies have also used GNNs for various IE tasks (Nguyen and Grishman, 2018; Liu et al., 2018; Subburathinam et al., 2019; Zeng et al., 2021; Zhang and Ji, 2021). Many of these methods use a dependency parser or a semantic parser to construct a graph capturing global interactions between tokens/spans. However, parsers for specialized biomedical domains are expensive to build. KECI does not rely on such expensive resources. 5 Conclusions and Future Work In this work, we propose a novel span-based framework named KECI that utilizes external domain knowledge for joint entity and relation extraction from biomedical text. Experimental results show that KECI is highly effective, achieving new stateof-the-art results on two datasets: Bio"
2021.acl-long.488,2021.naacl-main.4,1,0.788852,"evious work has focused on integrating external knowledge into neural architectures for other tasks, such as reading comprehension (Mihaylov and Frank, 2018), question answering (Pan et al., 2019), natural language inference (Sharma et al., 2019), and conversational modeling (Parthasarathi and Pineau, 2018). Different from these studies, our work explicitly emphasizes the benefit of collective inference using global relational information. Many previous studies have also used GNNs for various IE tasks (Nguyen and Grishman, 2018; Liu et al., 2018; Subburathinam et al., 2019; Zeng et al., 2021; Zhang and Ji, 2021). Many of these methods use a dependency parser or a semantic parser to construct a graph capturing global interactions between tokens/spans. However, parsers for specialized biomedical domains are expensive to build. KECI does not rely on such expensive resources. 5 Conclusions and Future Work In this work, we propose a novel span-based framework named KECI that utilizes external domain knowledge for joint entity and relation extraction from biomedical text. Experimental results show that KECI is highly effective, achieving new stateof-the-art results on two datasets: BioRelEx and ADE. Theore"
2021.acl-long.488,P17-1113,0,0.02474,"anifestations of retroperitoneal fibrosis while she was taking methysergide. #3: TITLE: Acute abdomen due to endometriosis in a premenopausal woman taking tamoxifen. Table 6: Examples showing how external knowledge improves the quality of extracted span graphs. Edges represent relations of type Adverse-Effect. Only relations with predicted probabilities of at least 0.5 are shown. propagation problem. To overcome these limitations, many studies have proposed joint models that perform entity extraction and relation extraction simultaneously (Roth and Yih, 2007; Li and Ji, 2014; Li et al., 2017; Zheng et al., 2017; Bekoulis et al., 2018a,b; Wadden et al., 2019; Fu et al., 2019; Luan et al., 2019; Zhao et al., 2020; Wang and Lu, 2020; Li et al., 2020b; Lin et al., 2020). Particularly, span-based joint extraction methods have gained much popularity lately because of their ability to detect overlapping entities. For example, Eberts and Ulges (2020) propose SpERT, a simple but effective span-based model that utilizes BERT as its core. The recent work of Ji et al. (2020) also closely follows the overall architecture of SpERT but differs in span-specific and contextual semantic representations. Despite their"
2021.acl-long.488,P05-1053,0,0.238007,"However, KECI can link the term “endometriosis” to a UMLS entity of semantic type Disease or Syndrome. As a result, the system can correct the term’s type and also predict the right edges for the final span graph. Finally, we also examined the errors made by KECI. One major issue is that MetaMap sometimes fails to return any candidate entity from UMLS for an entity mention. We leave the extension of this work to using multiple KBs as future work. 4 Related Work Traditional pipelined methods typically treat entity extraction and relation extraction as two separate tasks (Zelenko et al., 2002; Zhou et al., 2005; Chan and Roth, 2011). Such approaches ignore the close interaction between named entities and their relation information and typically suffer from the error 6254 Datasets Top 3 types with the lowest avg. attention scores Top 3 types with the highest avg. attention scores BioRelEx Diagnostic Procedure (0.04); Activity (0.05); Plant (0.05) Amino Acid, Peptide, or Protein (0.32); Enzyme (0.32); Molecular Function (0.36) ADE Intellectual Product (0.15); Idea or Concept (0.19); Temporal Concept (0.19) Antibiotic (0.78); Organic Chemical (0.79); Nucleic Acid, Nucleoside, or Nucleotide (0.87) Table"
2021.acl-long.489,D19-1371,0,0.0605024,"Missing"
2021.acl-long.489,C10-1018,0,0.0505882,"ramework to capture dependency structures and entity properties from an external knowledge base. More recently, GEANet (Huang et al., 2020) introduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework. The main difference of our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with"
2021.acl-long.489,D13-1184,0,0.0294931,"ependency structures and entity properties from an external knowledge base. More recently, GEANet (Huang et al., 2020) introduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework. The main difference of our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthe"
2021.acl-long.489,D19-1381,0,0.0207709,"mance. probably because the BERT tokenizer breaks these words into pieces de, phosphorylation, encouraging BERT models to learn misleading patterns. 4 Related Work Biomedical Information Extraction A number of previous studies contribute to biomedical event extraction with various techniques, such as dependency parsing (McClosky et al., 2011; Li et al., 2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural models (Espinosa et al., 2019), and multi-turn question answering (Wang et al., 2020b). Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training. DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events. Our model is also in an end-to-end training pipeline, but additionally utilizes fine-grained AMR semantic parsing and external knowledge to improve the performance. Utilization of External Knowledge In terms of utilization of ext"
2021.acl-long.489,2020.findings-emnlp.89,0,0.0814531,"3) and external knowledge graphs. AMR is a semantic representation language that converts the meaning of each input sentence into a rooted, directed, labeled, acyclic graph structure. AMR semantic representation includes PropBank (Palmer et al., 2005) frames, non-core semantic roles, coreference, entity typing and linking, modality, and negation. The nodes in AMR are concepts instead of words, and the edge types are much more fine-grained compared with traditional semantic languages like dependency parsing and semantic role labeling. We train a transformer-based AMR semantic parser (Fernandez Astudillo et al., 2020) on biomedical scientific texts and use it in our biomedical IE model. To better handle long sentences with distant trigger and entity pairs, we use AMR parsing to compress each sentence and to better capture global interactions between tokens. For example, as shown in Figure 1, the Positive Regulation event trigger “changes” is located far away from its arguments CTF, OTF-1, OTF-2 in the original sentence. However, in the AMR graph, such trigger-entity pairs are linked within two hops. Therefore, it will be much easier for the model to identify such kinds of events with the guidance of AMR pa"
2021.acl-long.489,2020.findings-emnlp.114,0,0.554126,"trigger spans. 6264 where W, We are trainable parameters, and f l and σ(·) are a single layer feed-forward neural network and LeakyReLU activation function respectively. Then we obtain the neighborhood information h∗i by the weighted sum of all neighbor features: X l h∗i = αi,j W∗ hlk , k∈Ni where W∗ is a trainable parameter. The updated node feature is calculated by a combination of the original node feature and its neighborhood information, where γ controls the level of message passing between neighbors. hl+1 = hli + γ · h∗i i (2) Note that our edge-conditioned GAT structure is similar to (Huang et al., 2020). The main difference is that (Huang et al., 2020) only uses edge features l , while we for calculating the attention score αi,j use the concatenation of the feature vectors of each edge and its involved pair of nodes. Such a method can better characterize differing importance levels for neighbor nodes, and thus yield better model performance. We select the last layer hL i as the final representation for each entity or trigger. Message Passing Given the knowledge enriched AMR graph G = (V, E) and representation vectors of extracted trigger and entity spans, we initialize the feature vectors fo"
2021.acl-long.489,W11-1802,0,0.0356039,"activation function in the output layer) for role type classificatt = FFN ([τ : τ ]) or tion, where we have yi,j tt i j te yi,j = FFNte ([τi : εj ]). The overall training objective is defined in a multi-task setting, which includes the cross-entropy loss for trigger and argument classification, as well as the binary classification loss LI . X X tt tt te te L = LI − yi,j log yˆi,j − yi,j log yˆi,j . (3) i,j 3 3.1 i,j Experiments Experimental Setup Data Similarly to the recent work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we also conduct experiments on the BioNLP GENIA 2011 (Kim et al., 2011) dataset consisting of both abstracts and main body texts from biomedical scientific papers. Similarly to previous work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we only focus on extracting the core events, which involves Protein entities, 9 fine-grained event types, and 2 event argument types. We do not incorporate event ontology or training data from the newer versions of the BioNLP GENIA shared tasks (e.g., GENIA 2013) to ensure fair comparisons with previous models. The statistics of this dataset are shown in Table 2. The original GENIA dataset Data Split Train Set Dev S"
2021.acl-long.489,N19-1145,1,0.866501,"Missing"
2021.acl-long.489,P11-1163,0,0.104179,"Missing"
2021.acl-long.489,2020.nlpcovid19-acl.18,0,0.0987857,"Missing"
2021.acl-long.489,D19-1585,0,0.0277363,"on procedure is shown in Figure 2. 2.4 Node Identification and Message Passing Contextualized Encoder Given an input sentence S, we use the BERT model pretrained on biomedical scientific texts (Lee et al., 2020) to obtain the contextualized word representations {x1 , x2 , · · · , xN }. If one word is split into multiple pieces by the BERT tokenizer, we take the average of the representation vectors for all pieces as the final word representation. Node Identification After encoding the input sentence using BERT, we first identify the entity and trigger spans as the candidate nodes. Similar to (Wadden et al., 2019), given the contextualized word representations, we first enumerate all possible spans up to a fixed length K, and calculate each span representation according to the concatenation of the left and right endpoints and a trainable feature vector characterizing the span length6 . Specifically, given each span si = [start(i), end(i)], the span representation vector is:   si = xstart(i) , xend(i) , z(si ) , (1) where z(si ) denotes a trainable feature vector that is only determined by the span length. We use separate binary classifiers for each specific entity and trigger type to handle the spans"
2021.acl-long.489,J05-1004,1,0.286254,"Missing"
2021.acl-long.489,2020.louhi-1.10,0,0.443205,"g1 , Nikolaus Parulian1 , Heng Ji1 , Ahmed S. Elsayed2 , Skatje Myers2 , Martha Palmer2 1 University of Illinois at Urbana-Champaign 2 University of Colorado Boulder {zixuan11, nnp2, hengji}@illinois.edu {ahmed.s.elsayed, skatje.myers, martha.palmer}@colorado.edu Abstract entities, relations, and key events. It is an essential task for accelerating practical applications of the results and achievements from scientific research. For example, practical progress on combating COVID-19 depends highly on efficient transmission, assessment and extension of cutting-edge scientific research discovery (Wang et al., 2020a; Lybarger et al., 2020; Möller et al., 2020). In this scenario, a powerful biomedical IE system will be able to create a dynamic knowledge base from the surging number of relevant papers, making it more efficient to get access to the latest knowledge and use it for scientific discovery, as well as diagnosis and treatment of patients. IE from biomedical scientific papers presents two unique and non-trivial challenges. First, the authors of scientific papers tend to compose long sentences, where the event triggers and entity mentions are usually located far away from each other within the sent"
2021.acl-long.489,D19-5804,1,0.720275,"base. More recently, GEANet (Huang et al., 2020) introduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework. The main difference of our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthetic QA training (Reddy et al., 2020). Our COVID-19 dataset will fu"
2021.acl-long.489,N10-1123,0,0.046613,"Protein “RAR beta” Protein “VDR” Table 7: Examples from development set showing how KG enriched AMR graph improves the model performance. probably because the BERT tokenizer breaks these words into pieces de, phosphorylation, encouraging BERT models to learn misleading patterns. 4 Related Work Biomedical Information Extraction A number of previous studies contribute to biomedical event extraction with various techniques, such as dependency parsing (McClosky et al., 2011; Li et al., 2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural models (Espinosa et al., 2019), and multi-turn question answering (Wang et al., 2020b). Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training. DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events. Our model is also in an end-to-end training pipeline, but additionally utilizes fine-grained AMR seman"
2021.acl-long.489,2020.knlp-1.1,0,0.0346393,"our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthetic QA training (Reddy et al., 2020). Our COVID-19 dataset will further advance the field in developing effective IE techniques specifically for the COVID-19 domain. 5 Conclusions and Future Work In this paper, we propose a novel biomedical Information Extraction framework to effectively tackle two unique"
2021.acl-long.489,2020.emnlp-main.431,0,0.0604686,"Missing"
2021.acl-long.489,2021.naacl-main.4,1,0.768984,"classifiers for each specific entity and trigger type to handle the spans with multiple labels. Each binary classifier is a feed-forward neural network with ReLU activation in the hidden layer, which is trained with binary cross-entropy loss jointly with the whole model. In the diagnostic setting of using gold-standard entity mentions, we only employ span enumeration for event trigger identification, and use the gold-standard entity set for the following event extraction steps. Edge-conditioned GAT To fully exploit the information of external knowledge and AMR semantic structure, similar to (Zhang and Ji, 2021), we use an L-layer graph attention network to let the model aggregate neighbor information from the fused graph G = (V, E). We use hli to denote the node feature for vi ∈ V in layer l, and ei,j to represent the edge feature vector for ei,j ∈ E. To update the node feature from l to l + 1, we first calculate the attention score for each neighbor j ∈ Ni based on the concatenation of node features hli , hlj and edge features ei,j .    exp σ f l [Whli : We ei,j : Whlj ] l  , αi,j =P l l l k∈Ni exp σ f [Whi : We ei,k : Whk ] 6 We use different maximum span length K for entity and trigger span"
2021.acl-long.489,W17-2315,0,0.0124794,"KG enriched AMR graph improves the model performance. probably because the BERT tokenizer breaks these words into pieces de, phosphorylation, encouraging BERT models to learn misleading patterns. 4 Related Work Biomedical Information Extraction A number of previous studies contribute to biomedical event extraction with various techniques, such as dependency parsing (McClosky et al., 2011; Li et al., 2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural models (Espinosa et al., 2019), and multi-turn question answering (Wang et al., 2020b). Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training. DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events. Our model is also in an end-to-end training pipeline, but additionally utilizes fine-grained AMR semantic parsing and external knowledge to improve the performance. Utilization"
2021.acl-short.131,2020.acl-main.117,1,0.866357,"istics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1035–1042 August 1–6, 2021. ©2021 Association for Computational Linguistics answer span from this identified paragraph. While previous MRC methods (Chen et al., 2017; Devlin et al., 2019) use ground-truth start and end span positions exclusively as training objectives when extracting answer spans from the context and consider all other positions as incorrect instances equally. However, spans that overlap with the ground-truth should be considered as partially correct. Motivated by Li et al. (2020) which proposes a new optimization criteria based on constructing prior distribution over synonyms for machine translation, we further propose to improve the above base model by considering the start and end positions of ground-truth answer spans as Gaussian-like distributions, instead of single points, and optimize our model using statistical distance. We call this final model, VAULT (VAriable Unified Long Text representation) as it can handle a variable number and lengths of paragraphs at any position with the same unified model structure to handle long texts. To evaluate the performance of"
2021.acl-short.131,2020.coling-industry.9,1,0.841803,"l structure to handle long texts. To evaluate the performance of VAULT, we select the new Natural Questions (NQ, Kwiatkowski et al., 2019) and TechQA (Castelli et al., 2020) datasets. NQ attempts to make Machine Reading Comprehension (MRC) more realistic by providing longer Wikipedia documents as contexts and real user search-engine queries as questions, and aims at avoiding observation bias: high lexical overlap between the question and the answer context which can happen frequently if the question is created after the user sees the paragraph (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Chakravarti et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Murdock et al., 2018). The task introduces the extraction of long answers (henceforth LA; typically paragraphs) besides also requiring short answers (henceforth SA) similar to SQuAD (Rajpurkar et al., 2016). In Figure 1 we examine an example from NQ along with the answers of VAULT and (Zheng et al., 2020). We see that while VAULT can extract answers from the very bottom of a page – if relevant – the existing system suffers from positional bias. It often predicts answers from the first paragraph of Wikipedia (a region which often contains the most rel"
2021.acl-short.131,P17-1171,0,0.373142,"is approach allows us to encode paragraph-level position in the text and teach the model to impute information on each paragraph into the hidden outputs for these tokens that we can exploit to determine in which paragraph the answer resides. We then predict the 1035 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1035–1042 August 1–6, 2021. ©2021 Association for Computational Linguistics answer span from this identified paragraph. While previous MRC methods (Chen et al., 2017; Devlin et al., 2019) use ground-truth start and end span positions exclusively as training objectives when extracting answer spans from the context and consider all other positions as incorrect instances equally. However, spans that overlap with the ground-truth should be considered as partially correct. Motivated by Li et al. (2020) which proposes a new optimization criteria based on constructing prior distribution over synonyms for machine translation, we further propose to improve the above base model by considering the start and end positions of ground-truth answer spans as Gaussian-like"
2021.acl-short.131,P17-1020,0,0.0216472,"al., 2019a) adapt a span extraction model for short answer extraction. (Zheng et al., 2020; Liu et al., 2020) construct complex networks for paragraph-level representation to enhance long answer classification along with span extraction for short answers. In this work, we propose a more light-weight and parallel-efficient way for constructing paragraph-level representation and classification by using longer context and 1036 modeling the negative instance through Gaussian prior optimization. Using the hierarchical nature of a long document for question answering has been previously studied by (Choi et al., 2017), where they use a hierarchical approach to select candidate sentences and extract answers in those candidates. However, due to the limit of input length for large PLMs, existing methods (Alberti et al., 2019b; Zheng et al., 2020; Chakravarti et al., 2020) slice long documents into document pieces and perform prediction for each piece separately. In our work, we show that by modeling longer input with position-aware paragraph representation coupled with Gaussian prior optimization (which is novel for MRC), we can achieve comparable performance using much simpler architecture compared to previo"
2021.acl-short.131,P17-1055,0,0.0315606,"TechQA outperforming a standard fine-tuned model trained on a large PLM such as RoBERTa. To summarize, our contributions include: 1. We introduce a novel and effective yet simple paragraph representation. 2. We introduce soft labels to leverage information from local contexts near ground-truth during training which is novel for MRC. 3. Our model provides similar performance to a SOTA system on NQ while being 16 times faster and also effectively adapts to a new domain: TechQA. 2 Related Work Machine reading comprehension has been widely modeled as cloze-type span extraction (Chen et al., 2017; Cui et al., 2017; Devlin et al., 2019). In NQ, we need to identify answers in two levels, long and short answers. (Alberti et al., 2019a) adapt a span extraction model for short answer extraction. (Zheng et al., 2020; Liu et al., 2020) construct complex networks for paragraph-level representation to enhance long answer classification along with span extraction for short answers. In this work, we propose a more light-weight and parallel-efficient way for constructing paragraph-level representation and classification by using longer context and 1036 modeling the negative instance through Gaussian prior optimiza"
2021.acl-short.131,P17-1147,0,0.0930564,"the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent"
2021.acl-short.131,2020.emnlp-main.550,0,0.0157458,"texts. To evaluate the performance of VAULT, we select the new Natural Questions (NQ, Kwiatkowski et al., 2019) and TechQA (Castelli et al., 2020) datasets. NQ attempts to make Machine Reading Comprehension (MRC) more realistic by providing longer Wikipedia documents as contexts and real user search-engine queries as questions, and aims at avoiding observation bias: high lexical overlap between the question and the answer context which can happen frequently if the question is created after the user sees the paragraph (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Chakravarti et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Murdock et al., 2018). The task introduces the extraction of long answers (henceforth LA; typically paragraphs) besides also requiring short answers (henceforth SA) similar to SQuAD (Rajpurkar et al., 2016). In Figure 1 we examine an example from NQ along with the answers of VAULT and (Zheng et al., 2020). We see that while VAULT can extract answers from the very bottom of a page – if relevant – the existing system suffers from positional bias. It often predicts answers from the first paragraph of Wikipedia (a region which often contains the most relevant information). We e"
2021.acl-short.131,Q19-1026,0,0.151544,"approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent multi-grained information for answer extraction"
2021.acl-short.131,P19-1612,0,0.0111992,"performance of VAULT, we select the new Natural Questions (NQ, Kwiatkowski et al., 2019) and TechQA (Castelli et al., 2020) datasets. NQ attempts to make Machine Reading Comprehension (MRC) more realistic by providing longer Wikipedia documents as contexts and real user search-engine queries as questions, and aims at avoiding observation bias: high lexical overlap between the question and the answer context which can happen frequently if the question is created after the user sees the paragraph (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Chakravarti et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Murdock et al., 2018). The task introduces the extraction of long answers (henceforth LA; typically paragraphs) besides also requiring short answers (henceforth SA) similar to SQuAD (Rajpurkar et al., 2016). In Figure 1 we examine an example from NQ along with the answers of VAULT and (Zheng et al., 2020). We see that while VAULT can extract answers from the very bottom of a page – if relevant – the existing system suffers from positional bias. It often predicts answers from the first paragraph of Wikipedia (a region which often contains the most relevant information). We evaluate our model"
2021.acl-short.131,2020.acl-main.604,0,0.119455,"ce soft labels to leverage information from local contexts near ground-truth during training which is novel for MRC. 3. Our model provides similar performance to a SOTA system on NQ while being 16 times faster and also effectively adapts to a new domain: TechQA. 2 Related Work Machine reading comprehension has been widely modeled as cloze-type span extraction (Chen et al., 2017; Cui et al., 2017; Devlin et al., 2019). In NQ, we need to identify answers in two levels, long and short answers. (Alberti et al., 2019a) adapt a span extraction model for short answer extraction. (Zheng et al., 2020; Liu et al., 2020) construct complex networks for paragraph-level representation to enhance long answer classification along with span extraction for short answers. In this work, we propose a more light-weight and parallel-efficient way for constructing paragraph-level representation and classification by using longer context and 1036 modeling the negative instance through Gaussian prior optimization. Using the hierarchical nature of a long document for question answering has been previously studied by (Choi et al., 2017), where they use a hierarchical approach to select candidate sentences and extract answers"
2021.acl-short.131,2021.ccl-1.108,0,0.0485891,"Missing"
2021.acl-short.131,P18-2124,0,0.060427,"Missing"
2021.acl-short.131,D16-1264,0,0.348374,"eve comparable performance on NQ with a state-of-the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses"
2021.acl-short.131,Q18-1021,0,0.153703,"ex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent multi-grained inform"
2021.acl-short.131,D18-1259,0,0.172173,"NQ with a state-of-the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM. 1 Introduction Machine Reading Comprehension (MRC) has seen great advances in recent years with the rise of pre-trained language models (PLM) (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) and public leaderboards (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hie"
2021.acl-short.131,2020.acl-main.599,1,0.563418,"rds (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Joshi et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019). While some challenges (Rajpurkar et al., 2016, 2018) focus on reading comprehension with shorter contexts, many others ∗ † Work done during an internship at IBM Research AI. Equal contributions. (Welbl et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019; Tanaka et al., 2021) focus on longer contexts that cannot fit into a typical 512 sub-token transformer window. Motivated by this, we focus on reading comprehension with long contexts. One newer approach to this task (Zheng et al., 2020) focuses on modeling document hierarchy to represent multi-grained information for answer extraction. Although this approach creates a strong representation of the text, it suffers from a significant drawback. The graph-based methods (Veliˇckovi´c et al., 2018) are inefficient on parallel hardware, such as GPUs, resulting in slow inference speed (Zhou et al., 2018; Zheng et al., 2020). Motivated by this, in this paper, we propose a reading comprehension model that addresses the above issue and uses a more light-weight, parallelefficient (i.e. efficient on parallel hardware) paragraph represent"
2021.acl-srw.2,2020.webnlg-1.7,0,0.0501032,"Missing"
2021.acl-srw.2,2020.inlg-1.14,0,0.0430816,"data and resources are publicly available for research purpose at: https://github.com/ EagleW/Stage-wise-Fine-tuning International Tennis Federation Telangana Northeast Karnataka sports Governing Body state Acharya Institute of Technology sports Offered Tennis was given the &apos;Technical Campus&apos; by West Arabian Sea All India Council for Technical Education location Mumbai Figure 1: Input RDF Knowledge Graph and T5 (Raffel et al., 2020) have achieved stateof-the-art results on WebNLG dataset due to factual knowledge acquired in the pre-training phase (Harkous et al., 2020; Ribeiro et al., 2020b; Kale, 2020; Chen et al., 2020a). Despite such improvement, PLMs fine-tuned only on the clean (or labeled) data might be more prone to hallucinate factual knowledge (e.g., “Visvesvaraya Technological University” in Table 1). Inspired by the success of domain-adaptive pre-training (Gururangan et al., 2020), we propose a novel two-step fine-tuning mechanism graph-totext generation task. Unlike (Ribeiro et al., 2020b; Herzig et al., 2020; Chen et al., 2020a) which directly fine-tune the PLMs on the training set, we first fine-tune our model over noisy RDF graphs and related article pairs crawled from Wikipe"
2021.acl-srw.2,P19-1483,0,0.0437821,"Missing"
2021.acl-srw.2,N19-1238,0,0.0359069,"Missing"
2021.acl-srw.2,W07-0734,0,0.2471,"Missing"
2021.acl-srw.2,D16-1128,0,0.0655727,"Missing"
2021.acl-srw.2,W17-3518,0,0.366399,"pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes the model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel treelevel embedding method to capture the interdependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset.1 1 Introduction In the graph-to-text generation task (Gardent et al., 2017), the model takes in a complex KG (an example is in Figure 1) and generates a corresponding faithful natural language description (Table 1). Previous efforts for this task can be mainly divided into two categories: sequence-to-sequence models that directly solve the generation task with LSTMs (Gardent et al., 2017) or Transformer (Castro Ferreira et al., 2019); and graph-to-text models (Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) which use a graph encoder to capture the structure of the KGs. Recently, Transformer-based PLMs such as GPT2 (Radford et al., 2019), BART (Lewis"
2021.acl-srw.2,2020.acl-main.703,0,0.193722,"2017), the model takes in a complex KG (an example is in Figure 1) and generates a corresponding faithful natural language description (Table 1). Previous efforts for this task can be mainly divided into two categories: sequence-to-sequence models that directly solve the generation task with LSTMs (Gardent et al., 2017) or Transformer (Castro Ferreira et al., 2019); and graph-to-text models (Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) which use a graph encoder to capture the structure of the KGs. Recently, Transformer-based PLMs such as GPT2 (Radford et al., 2019), BART (Lewis et al., 2020), ∗ This research was conducted during the author’s internship at Salesforce Research. 1 The programs, data and resources are publicly available for research purpose at: https://github.com/ EagleW/Stage-wise-Fine-tuning International Tennis Federation Telangana Northeast Karnataka sports Governing Body state Acharya Institute of Technology sports Offered Tennis was given the &apos;Technical Campus&apos; by West Arabian Sea All India Council for Technical Education location Mumbai Figure 1: Input RDF Knowledge Graph and T5 (Raffel et al., 2020) have achieved stateof-the-art results on WebNLG dataset due"
2021.acl-srw.2,D19-1310,0,0.0211509,"spectively. It also combines relations with the same type together with correct order, e.g., given two death places of a person, the model generates: “died in Sidcup, London” instead of generating two sentences or placing the city name ahead of the area name. 4 Related Work The WebNLG task is similar to Wikibio generation (Lebret et al., 2016; Wang et al., 2018), AMRto-text generation (Song et al., 2018) and ROTOWIRE (Wiseman et al., 2017; Puduppully et al., 2019). Previous methods usually treat the graphto-text generation as an end-to-end generation task. Those models (Trisedya et al., 2018; Gong et al., 2019; Shen et al., 2020) usually first lineralize the knowledge graph and then use attention mechanism to generate the description sentences. While the linearization of input graph may sacrifice the inter-dependency inside input graph, some papers (Ribeiro et al., 2019, 2020a; Zhao et al., 2020) Results with positional embeddings. For the KG with multiple triples, additional positional embeddings help reduce the errors introduced by pro5 We only use BERTScore to evaluate baselines which have results available online. 6 For more examples, please check Appendix for reference. 19 Category Output T5-l"
2021.acl-srw.2,W18-6501,0,0.0157906,"has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset.1 1 Introduction In the graph-to-text generation task (Gardent et al., 2017), the model takes in a complex KG (an example is in Figure 1) and generates a corresponding faithful natural language description (Table 1). Previous efforts for this task can be mainly divided into two categories: sequence-to-sequence models that directly solve the generation task with LSTMs (Gardent et al., 2017) or Transformer (Castro Ferreira et al., 2019); and graph-to-text models (Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) which use a graph encoder to capture the structure of the KGs. Recently, Transformer-based PLMs such as GPT2 (Radford et al., 2019), BART (Lewis et al., 2020), ∗ This research was conducted during the author’s internship at Salesforce Research. 1 The programs, data and resources are publicly available for research purpose at: https://github.com/ EagleW/Stage-wise-Fine-tuning International Tennis Federation Telangana Northeast Karnataka sports Governing Body state Acharya Institute of Technology sports Offered Tennis was given the &apos;Technical Campus&apos; by West Arabian Sea All India Council for Te"
2021.acl-srw.2,2020.acl-main.740,0,0.0431105,"Missing"
2021.acl-srw.2,2020.coling-main.218,0,0.0287517,"Missing"
2021.acl-srw.2,N19-1236,0,0.03137,"Missing"
2021.acl-srw.2,2020.acl-main.398,0,0.0300624,"Missing"
2021.acl-srw.2,P18-1150,0,0.0136327,"Wikipedia fine-tuning helps the model handle unseen relations such as “inOfficeWhileVicePresident”, and “activeYearsStartYear” by stating “His vice president is Atiku Abubakar.” and “started playing in 1995” respectively. It also combines relations with the same type together with correct order, e.g., given two death places of a person, the model generates: “died in Sidcup, London” instead of generating two sentences or placing the city name ahead of the area name. 4 Related Work The WebNLG task is similar to Wikibio generation (Lebret et al., 2016; Wang et al., 2018), AMRto-text generation (Song et al., 2018) and ROTOWIRE (Wiseman et al., 2017; Puduppully et al., 2019). Previous methods usually treat the graphto-text generation as an end-to-end generation task. Those models (Trisedya et al., 2018; Gong et al., 2019; Shen et al., 2020) usually first lineralize the knowledge graph and then use attention mechanism to generate the description sentences. While the linearization of input graph may sacrifice the inter-dependency inside input graph, some papers (Ribeiro et al., 2019, 2020a; Zhao et al., 2020) Results with positional embeddings. For the KG with multiple triples, additional positional embed"
2021.acl-srw.2,P02-1040,0,0.113947,"Missing"
2021.acl-srw.2,P18-1151,0,0.097748,"raph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset.1 1 Introduction In the graph-to-text generation task (Gardent et al., 2017), the model takes in a complex KG (an example is in Figure 1) and generates a corresponding faithful natural language description (Table 1). Previous efforts for this task can be mainly divided into two categories: sequence-to-sequence models that directly solve the generation task with LSTMs (Gardent et al., 2017) or Transformer (Castro Ferreira et al., 2019); and graph-to-text models (Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) which use a graph encoder to capture the structure of the KGs. Recently, Transformer-based PLMs such as GPT2 (Radford et al., 2019), BART (Lewis et al., 2020), ∗ This research was conducted during the author’s internship at Salesforce Research. 1 The programs, data and resources are publicly available for research purpose at: https://github.com/ EagleW/Stage-wise-Fine-tuning International Tennis Federation Telangana Northeast Karnataka sports Governing Body state Acharya Institute of Technology sports Offered Tennis was given the &apos;Technical Campus&apos; b"
2021.acl-srw.2,W18-6502,1,0.935138,"for the relation ri , and 3 for the object oi . BART-base + Wikipedia + Position + Wiki + Position BART-large + Wikipedia + Position + Wiki + Position distill-BART-xsum + Wikipedia + Position + Wiki + Position T5-base + Wikipedia + Position + Wiki + Position T5-large + Wikipedia + Position + Wiki + Position • Tree level ID calculates the distance (the number of relations) from the root which is the source vertex of the RDF graph. Two-step Fine-tuning To get better domain adaptation ability (Gururangan et al., 2020; Herzig et al., 2020), following TaPas and Wikipedia Person and Animal Dataset (Wang et al., 2018), we perform intermediate pre-training by coupling noisy English Wikipedia data with Wikidata triples, both of which are crawled in March 2020. We select 15 related categories (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, WrittenWork, Athlete, Artist, City, MeanOfTransportation, CelestialBody, Politician) that appear in the WebNLG dataset (Gardent et al., 2017) and collect 542,192 data pairs. For each Wikipedia article, we query its corresponding WikiData triples and remove sentences which contain no values in the Wikidata triples to form graph-text pa"
2021.acl-srw.2,2020.tacl-1.2,0,0.0185246,"and an apoapsis of 475426000.0 kilometres. S |11264 Claudiomaccone P| epoch O |2005-11-26; S |Aleksandr Prudnikov P |club O |FC Amkar Perm The chairman of FC Spartak Moscow is Sergey Rodionov. Aleksandr Prudnikov plays for FC Spartak Moscow and manages FC Amkar Perm. [ S |FC Amkar Perm P |manager O |Gadzhi Gadzhiyev; S |Aleksandr Prudnikov P |club O |FC Amkar Perm ] Table 5: System Error Examples. We highlight fabricated facts, [missed relations], incorrect relations, and ground truth relations with different color. use graph encoder such as GCN (Duvenaud et al., 2015) and graph transformer (Wang et al., 2020a; Koncel-Kedziorski et al., 2019) to encode the input graphs. Others (Shen et al., 2020; Wang et al., 2020b) try to carefully design loss functions to control the generation quality. With the development of computation resources, large scale PLMs such as GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) achieve state-ofthe-art results even with simple linearized graph input (Harkous et al., 2020; Chen et al., 2020a; Kale, 2020; Ribeiro et al., 2020b). Instead of directly fine-tuning the PLMs, we propose a two-step finetuning mechanism to get better domain ad"
2021.acl-srw.2,2020.acl-main.101,0,0.0174464,"and an apoapsis of 475426000.0 kilometres. S |11264 Claudiomaccone P| epoch O |2005-11-26; S |Aleksandr Prudnikov P |club O |FC Amkar Perm The chairman of FC Spartak Moscow is Sergey Rodionov. Aleksandr Prudnikov plays for FC Spartak Moscow and manages FC Amkar Perm. [ S |FC Amkar Perm P |manager O |Gadzhi Gadzhiyev; S |Aleksandr Prudnikov P |club O |FC Amkar Perm ] Table 5: System Error Examples. We highlight fabricated facts, [missed relations], incorrect relations, and ground truth relations with different color. use graph encoder such as GCN (Duvenaud et al., 2015) and graph transformer (Wang et al., 2020a; Koncel-Kedziorski et al., 2019) to encode the input graphs. Others (Shen et al., 2020; Wang et al., 2020b) try to carefully design loss functions to control the generation quality. With the development of computation resources, large scale PLMs such as GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) achieve state-ofthe-art results even with simple linearized graph input (Harkous et al., 2020; Chen et al., 2020a; Kale, 2020; Ribeiro et al., 2020b). Instead of directly fine-tuning the PLMs, we propose a two-step finetuning mechanism to get better domain ad"
2021.acl-srw.2,D19-1314,0,0.0196528,"Related Work The WebNLG task is similar to Wikibio generation (Lebret et al., 2016; Wang et al., 2018), AMRto-text generation (Song et al., 2018) and ROTOWIRE (Wiseman et al., 2017; Puduppully et al., 2019). Previous methods usually treat the graphto-text generation as an end-to-end generation task. Those models (Trisedya et al., 2018; Gong et al., 2019; Shen et al., 2020) usually first lineralize the knowledge graph and then use attention mechanism to generate the description sentences. While the linearization of input graph may sacrifice the inter-dependency inside input graph, some papers (Ribeiro et al., 2019, 2020a; Zhao et al., 2020) Results with positional embeddings. For the KG with multiple triples, additional positional embeddings help reduce the errors introduced by pro5 We only use BERTScore to evaluate baselines which have results available online. 6 For more examples, please check Appendix for reference. 19 Category Output T5-large T5-large T5-large +Wiki T5-large +Position Andrew White (born in 2003) is a musician who is associated with the band Kaiser Chiefs and Marry Banilow. He is also associated with the label Polydor Records and is signed to B-Unique Records. S |Aleksandra Kovaˇc P"
2021.acl-srw.2,2020.tacl-1.38,0,0.214606,"search. 1 The programs, data and resources are publicly available for research purpose at: https://github.com/ EagleW/Stage-wise-Fine-tuning International Tennis Federation Telangana Northeast Karnataka sports Governing Body state Acharya Institute of Technology sports Offered Tennis was given the &apos;Technical Campus&apos; by West Arabian Sea All India Council for Technical Education location Mumbai Figure 1: Input RDF Knowledge Graph and T5 (Raffel et al., 2020) have achieved stateof-the-art results on WebNLG dataset due to factual knowledge acquired in the pre-training phase (Harkous et al., 2020; Ribeiro et al., 2020b; Kale, 2020; Chen et al., 2020a). Despite such improvement, PLMs fine-tuned only on the clean (or labeled) data might be more prone to hallucinate factual knowledge (e.g., “Visvesvaraya Technological University” in Table 1). Inspired by the success of domain-adaptive pre-training (Gururangan et al., 2020), we propose a novel two-step fine-tuning mechanism graph-totext generation task. Unlike (Ribeiro et al., 2020b; Herzig et al., 2020; Chen et al., 2020a) which directly fine-tune the PLMs on the training set, we first fine-tune our model over noisy RDF graphs and related article pairs crawle"
2021.acl-srw.2,2020.acl-main.224,0,0.0387302,"is similar to Wikibio generation (Lebret et al., 2016; Wang et al., 2018), AMRto-text generation (Song et al., 2018) and ROTOWIRE (Wiseman et al., 2017; Puduppully et al., 2019). Previous methods usually treat the graphto-text generation as an end-to-end generation task. Those models (Trisedya et al., 2018; Gong et al., 2019; Shen et al., 2020) usually first lineralize the knowledge graph and then use attention mechanism to generate the description sentences. While the linearization of input graph may sacrifice the inter-dependency inside input graph, some papers (Ribeiro et al., 2019, 2020a; Zhao et al., 2020) Results with positional embeddings. For the KG with multiple triples, additional positional embeddings help reduce the errors introduced by pro5 We only use BERTScore to evaluate baselines which have results available online. 6 For more examples, please check Appendix for reference. 19 Category Output T5-large T5-large T5-large +Wiki T5-large +Position Andrew White (born in 2003) is a musician who is associated with the band Kaiser Chiefs and Marry Banilow. He is also associated with the label Polydor Records and is signed to B-Unique Records. S |Aleksandra Kovaˇc P| activeYearsStartYear O |1"
2021.acl-srw.2,2020.acl-main.641,0,0.0650939,"combines relations with the same type together with correct order, e.g., given two death places of a person, the model generates: “died in Sidcup, London” instead of generating two sentences or placing the city name ahead of the area name. 4 Related Work The WebNLG task is similar to Wikibio generation (Lebret et al., 2016; Wang et al., 2018), AMRto-text generation (Song et al., 2018) and ROTOWIRE (Wiseman et al., 2017; Puduppully et al., 2019). Previous methods usually treat the graphto-text generation as an end-to-end generation task. Those models (Trisedya et al., 2018; Gong et al., 2019; Shen et al., 2020) usually first lineralize the knowledge graph and then use attention mechanism to generate the description sentences. While the linearization of input graph may sacrifice the inter-dependency inside input graph, some papers (Ribeiro et al., 2019, 2020a; Zhao et al., 2020) Results with positional embeddings. For the KG with multiple triples, additional positional embeddings help reduce the errors introduced by pro5 We only use BERTScore to evaluate baselines which have results available online. 6 For more examples, please check Appendix for reference. 19 Category Output T5-large T5-large T5-lar"
2021.acl-tutorials.2,P08-1090,0,0.0818516,"tical role in understanding events (Cybulska and Vossen, 2014). This part should last for 35 minutes. Background of Events and Their Representations [30min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic sali"
2021.acl-tutorials.2,D17-1168,1,0.911618,"alization, coreference resolution and prediction of events and their relations, (iii) induction of event processes and properties, and (iv) a wide range of NLP and commonsense understanding tasks that benefit from aforementioned techniques. We will conclude the tutorial by outlining emerging research problems in this area. 1 Introduction Human languages always involve the description of real-world events. Therefore, understanding events plays a critical role in NLP. For example, narrative prediction benefits from learning the causal relations of events to predict what happens next in a story (Chaturvedi et al., 2017a); machine comprehension of documents may involve understanding of events that affect the stock market (Ding et al., 2015), describe natural phenomena (Berant et al., 2014) or identify disease phenotypes (Zhang et al., 2020d). In fact, event understanding also widely finds its important use cases in tasks such as opendomain question answering (Yang et al., 2003), intent prediction (Rashkin et al., 2018), timeline construction (Do et al., 2012), text summarization (Daum´e III and Marcu, 2006) and misinformation detection (Fung et al., 2021). Since events are not just simple, standalone predica"
2021.acl-tutorials.2,2021.acl-long.133,1,0.81067,"Missing"
2021.acl-tutorials.2,glavas-etal-2014-hieve,0,0.0604228,"Missing"
2021.acl-tutorials.2,2020.conll-1.43,1,0.898616,"many efforts have been devoted into modeling event narratives (Peng et al., 2017; Chaturvedi et al., 2017b; Lee and Goldwasser, 2019) such that they can successfully predict missing events in an event process. Besides, another important event understanding angle is conceptualization (Zhang et al., 2020a), which aims at understanding the super-sub relations between a coarse-grained event and a fine-grained event process (Glavaˇs et al., 2014). In this context, the machine could also be expected to generate the event process given a goal (Zhang et al., 2020a), infer the goal given the process (Chen et al., 2020), and capture the recurrence of events in a process (Zhu et al., 2021). Last but not least, event coreference, which links references to the same event together, also plays a critical role in understanding events (Cybulska and Vossen, 2014). This part should last for 35 minutes. Background of Events and Their Representations [30min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk abou"
2021.acl-tutorials.2,P19-1433,0,0.0195631,"ng the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend informa"
2021.acl-tutorials.2,D15-1076,0,0.0239549,"min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based enc"
2021.acl-tutorials.2,P18-1201,1,0.822982,"lined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress"
2021.acl-tutorials.2,P06-1039,0,0.0890017,"Missing"
2021.acl-tutorials.2,D18-1208,1,0.845181,"Missing"
2021.acl-tutorials.2,2020.acl-main.713,1,0.724423,"presentation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and expensive human annotations, which are not feasible on a large sca"
2021.acl-tutorials.2,P15-1155,1,0.800367,"movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present differen"
2021.acl-tutorials.2,Y18-1046,0,0.0145132,"ls (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and ex"
2021.acl-tutorials.2,W17-2701,0,0.0175787,"also interested in large-scale events that unfold over time. Over the past year, we saw many examples of such events, including COVID-19, the vaccine roll-out, the Black Lives Matter movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g.,"
2021.acl-tutorials.2,P19-1413,0,0.0244441,"to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress of events from different angles. For example, many efforts have been devoted into modeling event narratives (Peng et al., 2017; Chaturvedi et al., 2017b; Lee and Goldwasser, 2019) such that they can successfully predict missing events in an event process. Besides, another important event understanding angle is conceptualization (Zhang et al., 2020a), which aims at understanding the super-sub relations between a coarse-grained event and a fine-grained event process (Glavaˇs et al., 2014). In this context, the machine could also be expected to generate the event process given a goal (Zhang et al., 2020a), infer the goal given the process (Chen et al., 2020), and capture the recurrence of events in a process (Zhu et al., 2021). Last but not least, event coreference, which"
2021.acl-tutorials.2,P19-1500,0,0.0234852,"Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present different perspectives on the same event as well as identify salient information that is highlighted many in different ways across the set of input articles. This part is scheduled for 30 minutes • Emmon Bach. The algebra of events. Linguistics and philosophy. 9(1):5-16, 1986. • Nathanael Chambers. Event Schema Induction with a Probabilistic Entity-Driven Model. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process"
2021.acl-tutorials.2,2021.emnlp-main.422,1,0.760995,"tracted eventuality knowledge, we will explain how various prediction tasks, including the completion of an event complex, conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cros"
2021.acl-tutorials.2,N18-2089,0,0.0177113,"t the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for"
2021.acl-tutorials.2,2020.acl-main.230,1,0.883102,"Missing"
2021.acl-tutorials.2,D18-1483,0,0.0226446,"Missing"
2021.acl-tutorials.2,2020.emnlp-main.50,1,0.742388,"es, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress of events from different angles. For example, many efforts have been devoted"
2021.acl-tutorials.2,C14-1114,0,0.0297635,"ents was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present different perspectives on the same event as well as identify salient information that is highlighted many in different ways across the set of input articles. This part is scheduled for 30 minutes • Emmon Bach. The algebra of events. Li"
2021.acl-tutorials.2,P18-1122,1,0.835058,"conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motiv"
2021.acl-tutorials.2,2021.naacl-main.69,1,0.705823,"tracted eventuality knowledge, we will explain how various prediction tasks, including the completion of an event complex, conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cros"
2021.acl-tutorials.2,D19-1405,0,0.0220875,"e will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of prese"
2021.acl-tutorials.2,P18-1043,0,0.0258892,"Therefore, understanding events plays a critical role in NLP. For example, narrative prediction benefits from learning the causal relations of events to predict what happens next in a story (Chaturvedi et al., 2017a); machine comprehension of documents may involve understanding of events that affect the stock market (Ding et al., 2015), describe natural phenomena (Berant et al., 2014) or identify disease phenotypes (Zhang et al., 2020d). In fact, event understanding also widely finds its important use cases in tasks such as opendomain question answering (Yang et al., 2003), intent prediction (Rashkin et al., 2018), timeline construction (Do et al., 2012), text summarization (Daum´e III and Marcu, 2006) and misinformation detection (Fung et al., 2021). Since events are not just simple, standalone predicates, frontier research 2 Outline of Tutorial Content This half-day tutorial presents a systematic overview of recent advances in event-centric NLP technologies. We will begin with motivating this topic with several real-world applications, and introduce the main research problems. Then, we will introduce methods for automated extraction of events as well as their participants, properties and relations fr"
2021.acl-tutorials.2,2021.eacl-main.198,1,0.68319,"past year, we saw many examples of such events, including COVID-19, the vaccine roll-out, the Black Lives Matter movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many"
2021.acl-tutorials.2,2021.naacl-main.4,1,0.738456,"gs (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and expensive human annotations, which are not feasible on a large scale. In this tutorial, we introduce recent 7 2.7 progress on modeling commonsense knowledge with high-or"
2021.acl-tutorials.2,D19-1030,1,0.89034,"Missing"
2021.acl-tutorials.2,2020.acl-main.678,1,0.834145,"d expensive human annotations, which are not feasible on a large scale. In this tutorial, we introduce recent 7 2.7 progress on modeling commonsense knowledge with high-order selectional preference over event knowledge and demonstrates that how to convert relatively cheap event knowledge, which can be easily acquired from raw documents with linguistic patterns, to precious commonsense knowledge defined in ConceptNet (Zhang et al., 2020b). Beyond that, we will also introduce how to automatically acquire other event-centric commonsense knowledge including but not limited to temporal properties (Zhou et al., 2020), intentions (Chen et al., 2020), effects (Sap et al., 2019) and graph schemas (Li et al., 2020c) of events. This part is estimated to be 35 minutes. 2.6 Emerging Research Problems [20min] Event-centric NLP impacts on a wide spectrum of knowledge-driven AI tasks, and is particularly knotted with commonsense understanding. We will conclude the tutorial using 20 minutes by presenting some challenges and potential research topics in applying eventuality knowledge in downstream tasks (e.g., reading comprehension, dialogue generation, and event timeline generation), and grounding eventuality knowle"
2021.acl-tutorials.2,2020.emnlp-main.51,1,0.784661,"er investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequence"
2021.acl-tutorials.2,N15-1112,0,0.062705,"Missing"
2021.crac-1.14,N10-1061,0,0.0906843,"Missing"
2021.crac-1.14,P07-1107,0,0.077803,"ing in a miss detection of the coreferent pair. However, with the help of video, the amodal SMT is able to resolve the coreference using their correlations with similar type of videos in the visual mode. Nevertheless, we observe that introducing the visual mode can sometimes lead to false positives. 4 Related Work Motivated by event coreference resolution for Effect of Multi-mode mechanism From Table low-resource languages, unsupervised event coref4 and Table 5, removing any mode leads to degra- erence resolution algorithms have been prodation to the model, suggesting the importance of posed (Haghighi and Klein, 2007, 2010; Bejan 137 (a) Text SMT (b) Text SMT Document Document A powerful explosion and fire apparently caused by a gas leak at a Paris bakery Saturday injured several people , blew out windows and overturned cars ... Witnesses described the overwhelmingly sound of the blast and people trapped inside nearby buildings. Hundreds of thousands of mourners have turned out in Iran to receive home the remains of Qasem Soleimani, the general killed by a US drone strike in Iraq ... His assassination marked a significant escalation between Iran and the US. Iran ’s Supreme Leader Ayatollah Khamenei, ... T"
2021.crac-1.14,D09-1120,0,0.128709,"Missing"
2021.crac-1.14,2021.findings-emnlp.8,1,0.651623,"initialized the visual centroids using K-means algorithm and the other parameters uniformly on their supports. The model is then trained for 10 EM iterations. We experimented with various number of visual clusters, and the result is shown in Figure 2. The subsequent results are all obtained with 15 clusters. Since the representation of a visual event is contin- 3 Experiments and Results uous, we assume that the visual features of similar events tend to cluster together and can be well mod- 3.1 Data and Experiments Setup eled by a Gaussian distribution with mean µk and The video M2E2 dataset (Anonymous, 2021), invariance σk for each event type k. However, the spired by the image M2E2 dataset (Li et al., 2020), 135 Algorithm 1: Learning with EM Initialization: initialize t0 , q0 , M0 for τ = 1 to T do for each document D do for i = 1 to I do Update counts for p(m0 |m, π) and q(c|i) as in (Ma et al., 2016); for c = 1 to J do ci |vc )p(mi |zci ) Licz = Pp(zp(z ji |vj )p(mi |zji ) j P c(mi , z) + = Jc=1 Licz ∆µz + = z (Licz − p(zci |vc )) vcσ−µ z Update p(m0 |m, π) and q(c|i) parameters as in (Ma et al., 2016); µz = µz + η∆µz Video M2E2 # Train/Test # Event mentions # Event clusters 645/215 4158 647 T"
2021.crac-1.14,P10-1143,0,0.167389,"Lee et al., 2018) by (Cattan et al., 2020), which uses RoBERTa (Liu et al., 2019) to extract contextualized word embeddings. All parameter and optimization settings are made the same as the multimodal supervised model described in Section 2.2. We used ground truth mention spans and within-document antecedent prediction instead of using their cross-document hierarchical clustering algorithm. In addition, we used the following unsupervised models as baselines, including consists of 860 manually annotated documents, each paired with a 1-2 minutes video released by • HDP-LF: re-implementation of (Bejan and Harabagiu, 2010) with a rich set of semantic and authentic news outlets such as British Broadcasting syntactic features; Company (BBC), Voice of American (VoA) and Reuters. The videos are found from Youtube channel with event types as searching keywords such as • DD-CRP: re-implementation of (Yang et al., 2015) with additional pairwise features compared “attack” and “elect”. The textual coreference labels to HDP-LF; are annotated by two annotators with three rounds of adjudication using the Brat interface (Stenetorp et al., 2011) following ACE2005 guideline. More • Text-only SMT: proposed in (Ma et al., 2016)"
2021.crac-1.14,J93-2003,0,0.118126,"dependence assumptions based on (Ma et al., 2016): (6) To fuse visual and linguistic information, we use a special attention mechanism (Bahdanau et al., 2015) similar to the one proposed in (Yu et al., 2019). In particular, the model attends over the visual mention embedding to create a contextualized embedding as additional feature for each textual mention: p(M |V ) = I Y p(πi |i)p(ci |i, πi )· i=1 p(mi |vci , mci , πi ), (12) where Π = (π1 , · · · , πI ) are discrete variables called mode variables. This likelihood resembles that of the classical statistical machine translation (SMT) model (Brown et al., 1993) where the text mentions form the source language sentence the exp(Fv (mi , vj )) αmi ,vj = P (7) entire multimoal document is the target language v exp(Fv (mi , v)) sentence. Therefore, we call the model amodal SMT. In each mode, the model will resolve the The text resolver then utilizes the contextualized embeddings as additional cues to resolve corefer- current mention with a different subset of linguistic features. This is motivated by the observation ence between the mention pairs: that while resolving proper-nominal mention pairs such as “the Great War” and “World War I” mostly βmi = max"
2021.crac-1.14,J13-4004,0,0.021867,"Missing"
2021.crac-1.14,D17-1018,0,0.0193898,"ute mode Fc (mi , mi0 ) Fcv (mi , mi0 ) Figure 1: The proposed unsupervised multimodal coreference resolution model. The bubbles represent the random variables involved in the generative process, the squares represent raw feature inputs and arcs represent the values taken by the antecedent variables πi µk p(mi |mj , πi ) q(ci |i) Table 1: Definitions of key notations secutive word tokens (xstart(i) , · · · , xend(i) ), where start(i) and end(i) are the indices of the start and end token of the span. The mention spans can be extracted either using a binary classifier as done in previous works (Lee et al., 2017, 2018) or by an end-to-end event extraction system such as OneIE (Lin et al., 2020) as in this work. Similarly, each visual mention is a span of consecutive embeddings of video frames (ystart(j) , · · · , yend(j) ) extracted by human annotators. Inspired by (Ma et al., 2016), the coreference relations between events can be represented by a sequence of latent antecedent variables C = (c1 , · · · , cI ) associated with each event mentions, where ci = j means that mention i is the parent of (textual or visual) mention j in the tree formed by mentions that are coreferent. For within-text corefere"
2021.crac-1.14,N18-2108,0,0.0370372,"Missing"
2021.crac-1.14,2020.acl-main.230,1,0.768908,"Missing"
2021.crac-1.14,2020.acl-main.713,1,0.704922,"dal coreference resolution model. The bubbles represent the random variables involved in the generative process, the squares represent raw feature inputs and arcs represent the values taken by the antecedent variables πi µk p(mi |mj , πi ) q(ci |i) Table 1: Definitions of key notations secutive word tokens (xstart(i) , · · · , xend(i) ), where start(i) and end(i) are the indices of the start and end token of the span. The mention spans can be extracted either using a binary classifier as done in previous works (Lee et al., 2017, 2018) or by an end-to-end event extraction system such as OneIE (Lin et al., 2020) as in this work. Similarly, each visual mention is a span of consecutive embeddings of video frames (ystart(j) , · · · , yend(j) ) extracted by human annotators. Inspired by (Ma et al., 2016), the coreference relations between events can be represented by a sequence of latent antecedent variables C = (c1 , · · · , cI ) associated with each event mentions, where ci = j means that mention i is the parent of (textual or visual) mention j in the tree formed by mentions that are coreferent. For within-text coreference, ci ∈ {0, · · · , i − 1} and for crossmedia coreference, ci ∈ {1, · · · , J}. 2."
2021.crac-1.14,2021.ccl-1.108,0,0.0461984,"Missing"
2021.crac-1.14,H05-1004,0,0.36204,"Missing"
2021.crac-1.14,D15-1020,1,0.752397,"e degradation. Frequency for each event type is the number of distinct mention pairs that have at least one of the mention belonging to that event type. and Harabagiu, 2010; Yang et al., 2015; Ma et al., 2016). Notably, (Bejan and Harabagiu, 2010) proposed a hierarchical Dirichlet process (HDP) to capture recurring surface patterns on the global cluster level such as matching trigger lemma, ar- on multimodal event coreference resolution focus guments, neighboring events, etc, with a rich-get- on the supervised setting with specialized datasets. richer mechanism. (Yang et al., 2015) developed (Zhang et al., 2015) proposed a pipeline approach a probabilistic model to combine global features based on a pretrained text-only coreference syswith rich pairwise features by learning a distance tem with additional aligned videos for each event metric between mentions. (Ma et al., 2016) refor- and their visual similarity as visual features for mulated text-only coreference resolution as a trans- cross-document coreference. However, their crosslation process from each mention to its antecedent document model is not applicable to our withinwith different resolution modes. Previous works document coreference setti"
2021.crac-1.14,N16-1116,0,0.0314219,"Missing"
2021.crac-1.14,P14-2006,0,0.011054,"q0 , M0 for τ = 1 to T do for each document D do for i = 1 to I do Update counts for p(m0 |m, π) and q(c|i) as in (Ma et al., 2016); for c = 1 to J do ci |vc )p(mi |zci ) Licz = Pp(zp(z ji |vj )p(mi |zji ) j P c(mi , z) + = Jc=1 Licz ∆µz + = z (Licz − p(zci |vc )) vcσ−µ z Update p(m0 |m, π) and q(c|i) parameters as in (Ma et al., 2016); µz = µz + η∆µz Video M2E2 # Train/Test # Event mentions # Event clusters 645/215 4158 647 Table 3: Dataset statistics. Only non-singleton clusters are counted The CoNLL score is calculated as the average of these three scores and we used the CoNLL 2012 scorer (Pradhan et al., 2014). We also used the more straightforward pairwise F1 score and mean average precision (mAP) score, which score the overlap between the set of predicted coreference links and the gold links. 3.2 Baselines We used the following supervised coreference resolution model as baseline, namely, Figure 2: Coreference scores vs. number of visual clusters used by amodal SMT • Text-only model: an implementation of the state-of-the-art neural coreference resolution model (Lee et al., 2018) by (Cattan et al., 2020), which uses RoBERTa (Liu et al., 2019) to extract contextualized word embeddings. All parameter"
2021.crac-1.14,D12-1113,0,0.0200396,"tly βmi = max αmi ,vj (8) j requires semantic features such as embedding simβmi ,mi0 = max αmi ,vj αmi0 ,vj (9) ilarity, resolving pronoun-nominal mention pairs j such as “the Great War” and “it” relies more on synFcv (mi , mi0 ) = MLPcv ([βmi ; βmi0 ; βmi ,mi0 ]). tactic and discourse features such as part-of-speech (10) tags, parse trees and sentence distance. The need of distinct features for different coreferent types Intuitively, the value of βmi ’s scores how “visual” a is also demonstrated in previous works (Haghighi particular mention is and βmi ,mi0 scores how likely and Klein, 2009; Ratinov and Roth, 2012; Lee et al., the two mentions refer to the same visual event. 2013; Ma et al., 2016). The final text coreference score is then the weighted The overview of our unsupervised model is sum of the textual and viusal context score: shown in Fig. 1. Our unsupervised model has three modes, one visual mode and two textual modes, F (mi , mi0 ) = λv · Fcv (mi , mi0 )+ one called trigger-matching mode and the other called attribute-matching mode. In visual mode, (1 − λv ) · Fc (mi , mi0 ). (11) πi = visual, the antecedent of mention i is reThe score can be then interpreted as the logit of stricted to on"
2021.crac-1.14,W11-1816,0,0.0547719,"Missing"
2021.crac-1.14,M95-1005,0,0.661848,"Missing"
2021.crac-1.14,Q15-1037,0,0.0992628,"edent prediction instead of using their cross-document hierarchical clustering algorithm. In addition, we used the following unsupervised models as baselines, including consists of 860 manually annotated documents, each paired with a 1-2 minutes video released by • HDP-LF: re-implementation of (Bejan and Harabagiu, 2010) with a rich set of semantic and authentic news outlets such as British Broadcasting syntactic features; Company (BBC), Voice of American (VoA) and Reuters. The videos are found from Youtube channel with event types as searching keywords such as • DD-CRP: re-implementation of (Yang et al., 2015) with additional pairwise features compared “attack” and “elect”. The textual coreference labels to HDP-LF; are annotated by two annotators with three rounds of adjudication using the Brat interface (Stenetorp et al., 2011) following ACE2005 guideline. More • Text-only SMT: proposed in (Ma et al., 2016) with features adapted to event coreference resoludetails about this dataset are available in Table 3 tion for each textual mode described in Section as well as (Anonymous, 2021). We follow the 3:1 2. random split of training/test sets by (Anonymous, 2021). Three standard metrics for coreference"
2021.crac-1.14,D19-1516,0,0.0713312,"ference labels are mostly unavailable. Therefore, it is desirable to design an unsupervised coreference resolution algorithm to learn coreference relations without of the need of such labels. Instead of maximizing the likelihood of the coreference labels as in the supervised case, our unsupervised multimodal coreference resolution model instead tries to maximize the conditional likelihood p(M |V ), with independence assumptions based on (Ma et al., 2016): (6) To fuse visual and linguistic information, we use a special attention mechanism (Bahdanau et al., 2015) similar to the one proposed in (Yu et al., 2019). In particular, the model attends over the visual mention embedding to create a contextualized embedding as additional feature for each textual mention: p(M |V ) = I Y p(πi |i)p(ci |i, πi )· i=1 p(mi |vci , mci , πi ), (12) where Π = (π1 , · · · , πI ) are discrete variables called mode variables. This likelihood resembles that of the classical statistical machine translation (SMT) model (Brown et al., 1993) where the text mentions form the source language sentence the exp(Fv (mi , vj )) αmi ,vj = P (7) entire multimoal document is the target language v exp(Fv (mi , v)) sentence. Therefore, w"
2021.emnlp-main.422,D13-1185,0,0.0147787,"on all evaluation tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al.,"
2021.emnlp-main.422,P08-1090,0,0.742122,"event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between par"
2021.emnlp-main.422,chambers-jurafsky-2010-database,0,0.0434858,"Missing"
2021.emnlp-main.422,N13-1104,0,0.0305441,"n tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et"
2021.emnlp-main.422,P16-1154,0,0.011558,"ained the new event ei is an A RREST event, so we add LDC Schema Learning Ontology. three argument nodes for D ETAINEE, JAILOR, and 5 Compared to (Liao et al., 2019), we do not use the posiP LACE respectively. The edges between these ar- tional embedding mask because the newly generated nodes guments and event ei are also added into the graph. have distinct roles. 5206 3.5 Coreferential Argument Generation After updating the node representations, we detect the entity type of each argument, and also predict whether the argument is coreferential to existing entities. Inspired by copy mechanism (Gu et al., 2016), we classify each argument node vj to either a new entity with entity type φ(vj ), or an existing entity node in the previous graph G&lt;i . For example, in Figure 2, the D ETAINEE should be classified to the existing ATTACKER node, while JAILOR node is classified as P ERSON. Namely, p(hei , aj , vj i|ei , aj ) ( p(hei , aj , vj i, g|ei , aj ) if vj is new, = p(hei , aj , vj i, c|ei , aj ) otherwise, where p(hei , aj , vj i, g|ei , aj ) is the generation probability, classifying the new node to its entity type φ(vj ):  p(hei , aj , vj i, g|ei , aj ) = exp(W φ(vj ) v j ) Z The copy probability p"
2021.emnlp-main.422,E12-1034,0,0.0677415,"Missing"
2021.emnlp-main.422,D19-6014,0,0.0172787,"lso overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced f"
2021.emnlp-main.422,2020.findings-emnlp.340,0,0.0275404,"nt. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-dr"
2021.emnlp-main.422,2021.naacl-main.274,1,0.713586,"complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or entities are merged. We consider the isolated events as irrel"
2021.emnlp-main.422,2020.emnlp-main.50,1,0.80141,"tional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et a"
2021.emnlp-main.422,2021.naacl-main.69,1,0.668757,"ns between arguments, so we only compute this metric for the IED dataset. IED Schema Learning Corpus: The same type of complex events may have many variants, which depends on the different types of conditions and participants. In order to evaluate our model’s capability at capturing uncertainty and multiple hypotheses, we decided to dive deeper into one scenario and chose the improvised explosive device (IED) as our case study. We first collected Wikipedia articles that describe 4 types of complex events, i.e., Car-bombing IED, Drone Strikes IED, Suicide IED and General IED. Then we followed (Li et al., 2021) to exploit the external links to collect the additional news documents with the corresponding complex event type. The ground-truth schemas for this IED corpus are created manually, through a schema curation 4.3 Instance Graph Perplexity Evaluation tool (Mishra et al., 2021). Only one human schema graph was created for each complex event type, To evaluate our temporal event graph model, we resulting in 4 schemas. In detail, for each com- compute the instance graph perplexity by predictplex event type, we presented example instance ing the instance graphs in the test set, graphs and the ranked"
2021.emnlp-main.422,L16-1555,0,0.0194662,"l., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs ("
2021.emnlp-main.422,2020.acl-main.713,1,0.793931,"rded as a summary abstraction of instance graphs, capturing the reoccurring structures. 3 3.1 Our Approach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After"
2021.emnlp-main.422,K16-1008,0,0.0141284,"and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the"
2021.emnlp-main.422,W16-1007,0,0.0187693,"between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin et al., 2018; Grover et al."
2021.emnlp-main.422,I17-2007,0,0.0262425,"hat events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applic"
2021.emnlp-main.422,P15-1019,0,0.0538438,"Missing"
2021.emnlp-main.422,D19-1642,0,0.0347696,"Missing"
2021.emnlp-main.422,N15-1119,1,0.748927,"roach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or en"
2021.emnlp-main.422,P17-1178,1,0.891563,"Missing"
2021.emnlp-main.422,L16-1556,0,0.0245624,"iders the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin"
2021.emnlp-main.422,P16-1028,0,0.0157009,"ations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single o"
2021.emnlp-main.422,E14-1024,0,0.0227831,"duction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal depen"
2021.emnlp-main.422,2020.emnlp-main.612,0,0.0293622,"Missing"
2021.emnlp-main.422,2021.naacl-demos.16,1,0.847555,"Missing"
2021.emnlp-main.422,D15-1195,0,0.0450596,"Missing"
2021.emnlp-main.422,N16-1049,0,0.0199393,"l arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudi"
2021.emnlp-main.422,2021.naacl-main.6,1,0.815069,"Missing"
2021.emnlp-main.422,2020.emnlp-main.374,0,0.0992327,"Missing"
2021.emnlp-main.428,C96-1079,0,0.738512,"Missing"
2021.emnlp-main.428,2020.emnlp-main.52,0,0.135305,"s for new event types. After each training stage t, the model needs to detect mentions of all learned event types. We propose a novel knowledge transfer module to leverage rich connections between learned types and new types. Another challenge in lifelong event detection is the naturally imbalanced distribution of event types in natural language as shown in Figure 2. Existing methods (Rebuffi et al., 2017; Castro et al., 2018; Wu et al., 2019; Hou et al., 2019) for class incremental learning usually study relatively balanced classification datasets, and previous attempts (Nguyen et al., 2016; Cao et al., 2020) on incremental learning of event detection ignore this problem by only experimenting on frequent types. 1 Relative Number of Mentions late lifelong event detection by modifying the commonly studied class incremental lifelong learning, where the model incrementally learns to classify more classes with only positive instances. We add a special NA type denoting negative instances that are not event triggers for any types. For example, injured in the sentence “Bob is injured.” is a mention of an injure event, and is is a negative instance. Whenever training on new event types, the new training da"
2021.emnlp-main.428,P15-1017,0,0.221465,"ents 1 φ p(c |x) = P|Ot |o for the combined S S ontology of all seen types, i.e., 1 + j=1 ei Ot = C1 . . . Ct . Throughout this paper, we don’t include type NA when mentioning the term Then the cross-entropy loss is used to train the ontology unless specified. Compared with the tra- model on the current dataset: ditional supervised learning setting, the main difX ference of lifelong learning is that the model is exLC = − log p(y|x). (4) posed to only training data Dt that covers a subset (x,y)∈Dt 5280 Although some of the methods designed explicitly for event detection (Ji and Grishman, 2008; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Yan et al., 2019; Tong et al., 2020) may have better detection performance, this model is more flexible in that (1) by taking event detection as label prediction for text spans, many existing lifelong learning methods for classification become applicable; (2) as described in Section 2.1, this architecture can handle a variety of IE tasks without significant modification. Experience Replay. An exemplar set is kept and updated continually containing training instances for all learned types to remind the model when old training data is no longer available. W"
2021.emnlp-main.428,2020.emnlp-main.129,0,0.0216762,"ances for already learned types. This setting ACE 2005 English (Walker et al., 2006): ACE simulates annotating new types in a different corpus 2005 English corpus contains 33 event types. We from the existing dataset for the old types. This setfind that in the split used by previous work (Lin ting is practical when new types come from another et al., 2020) several event types are missing in the domain, or when we want to add new documents development set and the test set. Hence, we re-split to train the model. We include more details of this the data for better coverage of event types. MAVEN (Wang et al., 2020): This is a general do- setting in the Appendix. We only experiment with main event detection dataset with 168 event types. this setting using the larger MAVEN dataset since we need sufficient training instances for each type. MAVEN covers a wide range of diverse event types We hold out some of them as negative instances for compared with ACE 2005 English dataset, and this diversity makes it a better benchmark for evaluat- other data subsets. ing the correlation and knowledge transfer between Evaluation We use the F1 score to evaluate the old and new event types. Since the original test model’"
2021.emnlp-main.441,N16-1049,0,0.0251969,"ies on event schema induction adopt rule-based approaches (Lehnert et al., 1992; Chinchor et al., 1993) and classification-based methods (Chieu et al., 2003; Bunescu and Mooney, 2004) to induce templates from labeled corpus. Later, unsupervised methods are proposed to leverage relation patterns (Sekine, 2006; Qiu et al., 2008) and coreference chains (Chambers and Jurafsky, 2011) for event schema induction. Typical approaches use probabilistic generative models (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Li et al., 2020, 2021) or ad-hoc clustering algorithms (Huang et al., 2016; Sha et al., 2016) to induce predicate and argument clusters. In particular, (Liu et al., 2019) takes an entity-centric view toward event schema induction. It clusters entities into semantic slots and finds predicates for entity clusters in a post-processing step. (Yuan et al., 2018) studies the event profiling task and includes one module that leverages a Bayesian generative model to cluster hpredicate:role:labeli triplets into event types. These methods typically rely on discrete hand-crafted features derived from bag-of-word text representations and impose strong statistics assumptions; whereas our method us"
2021.emnlp-main.441,P19-1276,0,0.0220825,"92; Chinchor et al., 1993) and classification-based methods (Chieu et al., 2003; Bunescu and Mooney, 2004) to induce templates from labeled corpus. Later, unsupervised methods are proposed to leverage relation patterns (Sekine, 2006; Qiu et al., 2008) and coreference chains (Chambers and Jurafsky, 2011) for event schema induction. Typical approaches use probabilistic generative models (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Li et al., 2020, 2021) or ad-hoc clustering algorithms (Huang et al., 2016; Sha et al., 2016) to induce predicate and argument clusters. In particular, (Liu et al., 2019) takes an entity-centric view toward event schema induction. It clusters entities into semantic slots and finds predicates for entity clusters in a post-processing step. (Yuan et al., 2018) studies the event profiling task and includes one module that leverages a Bayesian generative model to cluster hpredicate:role:labeli triplets into event types. These methods typically rely on discrete hand-crafted features derived from bag-of-word text representations and impose strong statistics assumptions; whereas our method uses pre-trained language models to reduce the feature generation complexity an"
2021.emnlp-main.441,2020.emnlp-main.666,1,0.802433,"Missing"
2021.emnlp-main.441,E17-1010,0,0.0251289,"Missing"
2021.emnlp-main.441,P15-1019,0,0.0197179,"“immunize” and “vaccinate” for Vaccinate type). 6 Related Work Event Schema Induction. Early studies on event schema induction adopt rule-based approaches (Lehnert et al., 1992; Chinchor et al., 1993) and classification-based methods (Chieu et al., 2003; Bunescu and Mooney, 2004) to induce templates from labeled corpus. Later, unsupervised methods are proposed to leverage relation patterns (Sekine, 2006; Qiu et al., 2008) and coreference chains (Chambers and Jurafsky, 2011) for event schema induction. Typical approaches use probabilistic generative models (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Li et al., 2020, 2021) or ad-hoc clustering algorithms (Huang et al., 2016; Sha et al., 2016) to induce predicate and argument clusters. In particular, (Liu et al., 2019) takes an entity-centric view toward event schema induction. It clusters entities into semantic slots and finds predicates for entity clusters in a post-processing step. (Yuan et al., 2018) studies the event profiling task and includes one module that leverages a Bayesian generative model to cluster hpredicate:role:labeli triplets into event types. These methods typically rely on discrete hand-crafted features derived from b"
2021.emnlp-main.441,W16-5706,0,0.0431728,"Missing"
2021.emnlp-main.441,J05-1004,0,0.245902,"et al., 2021) utilize bag-of-word text representations and imassume a set of predefined event types and their pose strong statistical assumptions. Huang et al. corresponding annotations are curated by human (2016) relax those restrictions using a pipelined experts. This annotation process is expensive and approach that leverages extensive lexical and setime-consuming. Besides, those manually-defined mantic resources (e.g., FrameNet (Baker et al., event types often fail to generalize to new do1998), VerbNet (Schuler and Palmer, 2005), and mains. For example, the widely used ACE 2005 PropBank (Palmer et al., 2005)) to discover event event schemas2 do not contain any event type schemas. While being effective, this method is 1 The programs, data and resources are publicly availlimited by the scope of external resources and acable for research purpose at https://github.com/ curacies of its preprocessing tools. Recently, some mickeystroller/ETypeClus. 2 studies (Huang et al., 2018; Lai and Nguyen, 2019; https://www.ldc.upenn.edu/ collaborations/past-projects/ace Huang and Ji, 2020) have used transfer learning to 5427 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pa"
2021.emnlp-main.441,W15-0812,0,0.0241755,", ET YPE C LUS clusters the remaining salient P-O pairs into event types using a latent space generative model. This model jointly embeds P-O pairs into a latent spherical space and performs clustering within this space. By doing so, we can guide the latent space learning with the clustering objective and enable the clustering process to benefit from the well-separated structure of the latent space. We show our ET YPE C LUS framework can save annotation cost and output corpus-specific event types on three datasets. The first two are benchmark datasets ACE 2005 and ERE (Entity Relation Event) (Song et al., 2015). ET YPE C LUS can successfully recover predefined types and identify new event types such as Build in ACE and Bombing in ERE. Furthermore, to test the performance of ET YPE C LUS in new domains, we collect a corpus about the disease outbreak scenario. Results show that ET YPE C LUS can identify many interesting fine-grained event types (e.g., Vaccinate, Test) that align well with human annotations. Contributions. The major contributions of this paper are summarized as follows: (1) A new event type representation is created as a cluster of hpredicate sense, object headi tuples; (2) a novel eve"
2021.emnlp-main.441,P18-2010,0,0.131353,"he men’s suicide bomber training in 2011 in Somalia and association with Beledi, who prosecutors said bombed a government checkpoint in Mogadishu that year. Table 3: Example outputs of ET YPE C LUS discovered event types with their associated sentences in ACE and ERE datasets. The first two types come from ACE and the remaining two are from ERE. The event types with superscript “∇ ” originally do not exist in human-labeled schemas and are discovered by ET YPE C LUS framework. Predicates are in bold and object heads are underlined and in italics. ACE Methods Kmeans sp-Kmeans AggClus Triframes (Ustalov et al., 2018) JCSC (Huang et al., 2016) ET YPE C LUS ERE ARI (std) NMI (std) ACC (std) BCubed-F1 (std) ARI (std) NMI (std) ACC (std) BCubed-F1 (std) 26.27 (1.60) 26.06 (2.12) 24.45 (0.00) 19.35 (6.60) 36.10 (4.96) 40.78 (3.20) 48.02 (1.55) 47.30 (1.65) 45.71 (0.00) 36.38 (4.91) 49.50 (2.70) 57.57 (2.40) 41.57 (3.07) 40.41 (2.46) 41.00 (0.00) — 46.17 (3.64) 48.35 (2.55) 41.33 (1.75) 39.52 (1.42) 40.20 (0.00) 38.91 (2.36) 43.83 (3.17) 51.58 (2.50) 11.17 (1.83) 13.62 (2.14) 6.07 (0.00) 10.89 (2.51) 17.07 (4.40) 24.09 (1.93) 35.10 (2.36) 37.33 (2.25) 29.62 (0.00) 34.94 (2.54) 39.50 (3.97) 49.40 (1.37) 31.65 (1"
2021.emnlp-main.441,D19-1027,0,0.0259244,"w keywords. These methods reduce the annotation efforts but still require all target new types to be given. Recently, some studies (Huang 15 More example outputs are in Appendix Section H. et al., 2018; Lai and Nguyen, 2019; Huang and Ji, 2020) use transfer learning techniques to extend traditional event extraction models to new types without explicitly deriving schemas of new event types. Compared to our study, these methods still require many annotations for a set of seen types and their resulting vector-based event type representations are less human interpretable. Another related work by (Wang et al., 2019) uses GAN to extract events from an open domain corpus. It clusters hentity:location:keyword:datei quadruples related to the same event rather than finds event types. 7 Conclusions and Future Work In this paper, we study the event type induction problem that aims to automatically generate salient event types for a given corpus. We define a novel event type representation as a hpredicate sense, object headi cluster, and propose ET YPE C LUS that can extract and select salient predicates and object heads, disambiguate predicate senses, and jointly embed and cluster P-O pairs in a latent space. E"
2021.emnlp-main.441,2020.emnlp-demos.6,0,0.0185926,"Missing"
2021.emnlp-main.47,2021.acl-long.488,1,0.741931,"model is trained on a parallel corpus of molecules and descriptions. 4 4.1 Methodology Model To accomplish this retrieval task, we need to connect text to molecules. To do so, we build an 3.3 Substructure or Description Retrieval aligned semantic embedding space. Our approach Although the biomedical domain has been more consists of two distinct submodels: a text encoder popular than chemistry (Zheng et al., 2014; Li et al., and a molecule encoder. Both submodels create an 2019; Li and Ji, 2019; Islamaj Do˘gan et al., 2019; embedding in the aligned space, and cosine similarZhang et al., 2021; Lai et al., 2021), information ity is used to rank the embeddings. A description retrieval in chemistry has long been studied and embedding can be compared against a database of is summarized by Krallinger et al. (2017). Most existing molecule embeddings, and this process work has focused on only a single modality: text scales easily using an approximate nearest neighor molecules. Text-based retrieval includes tasks bor search algorithm such as (Johnson et al., 2017). such as finding relevant papers for a chemical or For the text encoder, we use SciBERT (Beltagy reaction and chemical entity recognition. Much e"
2021.emnlp-main.47,N19-1145,1,0.892175,"Missing"
2021.emnlp-main.47,D19-6204,1,0.831591,"retrieval from text, can be considered as a CLIR task which we approach using an interlingual semantic approach. The model is trained on a parallel corpus of molecules and descriptions. 4 4.1 Methodology Model To accomplish this retrieval task, we need to connect text to molecules. To do so, we build an 3.3 Substructure or Description Retrieval aligned semantic embedding space. Our approach Although the biomedical domain has been more consists of two distinct submodels: a text encoder popular than chemistry (Zheng et al., 2014; Li et al., and a molecule encoder. Both submodels create an 2019; Li and Ji, 2019; Islamaj Do˘gan et al., 2019; embedding in the aligned space, and cosine similarZhang et al., 2021; Lai et al., 2021), information ity is used to rank the embeddings. A description retrieval in chemistry has long been studied and embedding can be compared against a database of is summarized by Krallinger et al. (2017). Most existing molecule embeddings, and this process work has focused on only a single modality: text scales easily using an approximate nearest neighor molecules. Text-based retrieval includes tasks bor search algorithm such as (Johnson et al., 2017). such as finding relevant p"
2021.emnlp-main.47,2020.acl-main.230,1,0.854972,"Missing"
2021.emnlp-main.47,S13-2058,0,0.0205721,"t al., 2018; Qu et al., 2019; Kratochvíl, 2019; Goyal et al., 2020). Hybrid approaches have also been attempted; Zhou et al. (2010) replace chemical entities in text with a unique canonical key (thus standardizing synonyms). This also allows them to perform query expansion by including similar molecules from their database. In contrast to this, we perform direct semantic cross-modal retrieval task in our approach, as opposed to just augmenting queries. Work in chemical entity recognition has also incorporated hybrid approaches, mostly as chemical name to structure converters such as ChemSpot (Rocktäschel et al., 2013) and OPSIN (Lowe et al., 2011). 3.4 Cross-Lingual Retrieval Cross-lingual information retrieval (CLIR) is a technique to retrieve documents from a target language given a query in a different source language. Two common strategies are either translating the query into the target language or translating the document corpus into the source language (Zhang and Zhao, 2020). Further, work exists combining these approaches using interlingual semantics, such as via bilingual word embeddings (Vulic and Moens, 2015) or word embeddings and a dictionary (Bhattacharya et al., 2016). Our problem, cross-mod"
2021.emnlp-main.47,2021.acl-long.489,1,0.747663,"mantic approach. The model is trained on a parallel corpus of molecules and descriptions. 4 4.1 Methodology Model To accomplish this retrieval task, we need to connect text to molecules. To do so, we build an 3.3 Substructure or Description Retrieval aligned semantic embedding space. Our approach Although the biomedical domain has been more consists of two distinct submodels: a text encoder popular than chemistry (Zheng et al., 2014; Li et al., and a molecule encoder. Both submodels create an 2019; Li and Ji, 2019; Islamaj Do˘gan et al., 2019; embedding in the aligned space, and cosine similarZhang et al., 2021; Lai et al., 2021), information ity is used to rank the embeddings. A description retrieval in chemistry has long been studied and embedding can be compared against a database of is summarized by Krallinger et al. (2017). Most existing molecule embeddings, and this process work has focused on only a single modality: text scales easily using an approximate nearest neighor molecules. Text-based retrieval includes tasks bor search algorithm such as (Johnson et al., 2017). such as finding relevant papers for a chemical or For the text encoder, we use SciBERT (Beltagy reaction and chemical entity"
2021.emnlp-main.519,D19-5816,1,0.892822,"Missing"
2021.emnlp-main.519,P99-1071,1,0.465211,"is on the size of input event graph. 4 Related Work etc. However, they ignore relations between event arguments, or only use hierarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework."
2021.emnlp-main.519,D19-5602,0,0.0273129,"Missing"
2021.emnlp-main.519,N19-1240,0,0.0414547,"Missing"
2021.emnlp-main.519,N19-1423,0,0.177587,"ences measured by A time-aware optimal transport distance is the inter-similarity of these articles. In these meththen introduced for learning the compression ods, the document representations are limited to model in an unsupervised manner. We show local text features, ignoring the global context of that our approach significantly improves the the news collection. The applications of neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major"
2021.emnlp-main.519,P19-1259,0,0.0422027,"Missing"
2021.emnlp-main.519,P19-1102,0,0.084656,"e of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Processing (NLP) techniques: (1) In contrast to multi-document summarization (MDS) we construct a single event graph from the dealing with tens of documents (Fabbri et al., 2019), input documents, with co-referential entities (e.g., house, mansion in Figure 1) and co1 The programs, data and resources are publicly available referential events (e.g., die, collapsed) merged for research purpose in https://github.com/limanling/ event-graph-summarization. across documents. Our comprehensive event 6443 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6443–6456 c November 7–11, 2021. 2021 Association for Computational Linguistics Input Documents 2009-06-27 There was no sign of foul play in the death of Michael Jackson......A recor"
2021.emnlp-main.519,W04-1017,0,0.0911918,"ptimal transport (OT): We propose a new for- minimal distance with only m events to be kept, a global decision is learned to select salient but mulation of timeline summarization, by selecting also diverse events. The summary graphs are genevent nodes from the input graph to form a smaller summary graph. Under a certain summary size con- erated using a differentiable compression model according to a hyperparameter of compression rate, straint, a summary graph with high coverage has a instead of using annotated timelines. Thus, our small information loss, compared to the one with low coverage (Filatova and Hatzivassiloglou, 2004). objective allows model training in an end-to-end unsupervised way. We constrain the total number of event nodes to be kept in the summary, and optimize the summary (3) Time-aware Gromov-Wasserstein distance: graph to be close to the original graph using opti- The distance between two graphs should capture 6444 the following criteria: i) Semantic relevance: each node first has its initial local context encoded via a pre-trained BERT model and node type embeddings. For example, S TART P OSITION event is not closely related to the T RANSPORT event in Figure 1 though they have temporal dependenc"
2021.emnlp-main.519,C10-1039,0,0.0590717,"erarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timel"
2021.emnlp-main.519,N09-1041,0,0.0241803,"Work etc. However, they ignore relations between event arguments, or only use hierarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracp"
2021.emnlp-main.519,2020.acl-main.457,1,0.840441,"is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timeline Summarization. Due to the lack of to hierarchically generate the timeline. training data, timeline summarization focuses on extractive methods with heuristics (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Ac"
2021.emnlp-main.519,2021.naacl-main.274,1,0.693645,"ion from a node to its mentions v An event node in an event graph e An entity node in an event graph hvi , vl i A temporal ordering edge (vl happens after vi ) hvi , a, ej i An argument edge (the entity ej plays argument role a in event vi ) hej , r, ek i An entity relation edge between ej and ek , and r is the relation type Table 1: List of symbols. We apply OneIE (Lin et al., 2020), a state-ofthe-art Information Extraction (IE) system, to extract entities, relations and events; then perform 2 Method cross-document entity and event coreference res2.1 Overview olution (Pan et al., 2015, 2017; Lai et al., 2021) Our approach aims at finding the graph that has over the document cluster of each timeline topic. minimal distance from the input graph (Filatova We apply (Ning et al., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the be"
2021.emnlp-main.519,2020.acl-main.230,1,0.885492,"Missing"
2021.emnlp-main.519,2020.emnlp-main.50,1,0.76894,"ral attribute accuracy if there is a tie. The events with temporal attributes extracted directly from the context are of highest priority, followed by events having temporal attributes propagated from neighbor events in §2.2, and then the ones using document publication date. 4 https://wwconw.voanews.com 5 https://www.reuters.com following §2.2. 6 We use the ACE event ontology7 , with 7 entity types, 6 relation types, 33 event types, and 22 argument roles. For the (unsupervised) training of our event graph compression model, we use event graphs constructed from VoA news between 2011 and 2017 (Li et al., 2020a). The statistics are shown in Table 2. Dataset Split #Doc #Event #Entity #Rel Timeline17 Input Timeline 4,650 19 74,320 115,585 136,509 974 1,936 1,134 Crisis Input 20,463 325,695 551,228 610,410 Timeline 22 736 1,184 1,309 Timeline100 Input 10,379 178,581 301,132 306,975 Timeline 100 3,296 8,901 23,732 Unlabeled Input 72,576 913,679 381,735 1,046,066 (for OT) Timeline - Table 2: Data statistics, including the number of documents, events, entities, and temporal relations. Evaluation Metrics. We use the conventional metrics for timeline summarization (Martschat and Markert, 2018) to evaluate"
2021.emnlp-main.519,2020.acl-main.713,1,0.86566,"2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Processing (NLP) techniques: (1) In contrast to multi-document summarization (MDS) we construct a single event graph from the dealing with tens of documents (Fabbri et al., 2019), input documents, with co-referential entities (e.g., house, mansion in Figure 1) and co1 The programs, data and resources are publicly available referential events (e.g., die, collapsed) merged for research purpose in https://github.com/limanling/ event-graph-summarization. across documents. Our comprehensive event 6443 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process"
2021.emnlp-main.519,P19-1500,0,0.0578843,"Missing"
2021.emnlp-main.519,P14-5010,0,0.00254608,"l., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the best transport plan that has a minimal distance event, We extract and normalize time expressions between two graphs. To apply optimal transport using publication date (Manning et al., 2014), and to timeline summarization, the key is to design the then apply (Wen et al., 2021) to extract the event distance to evaluate the information loss, and thus temporal attributes from the context. If the tem6445 poral attributes can not be decided according to the context, we propagate the temporal attributes from neighbor events based on their shared arguments (?). After that, we use the document publication date to populate the remaining missing dates. For example, in Figure 1, the date 2009-0625 of the collapse (D IE) event is extracted from context last Thursday, and the date of the unco"
2021.emnlp-main.519,K18-1023,0,0.0660956,"orld datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Pr"
2021.emnlp-main.519,C14-1114,0,0.0646958,"neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction"
2021.emnlp-main.519,D19-1642,0,0.0443163,"Missing"
2021.emnlp-main.519,N15-1119,1,0.81622,"s type w A mapping function from a node to its mentions v An event node in an event graph e An entity node in an event graph hvi , vl i A temporal ordering edge (vl happens after vi ) hvi , a, ej i An argument edge (the entity ej plays argument role a in event vi ) hej , r, ek i An entity relation edge between ej and ek , and r is the relation type Table 1: List of symbols. We apply OneIE (Lin et al., 2020), a state-ofthe-art Information Extraction (IE) system, to extract entities, relations and events; then perform 2 Method cross-document entity and event coreference res2.1 Overview olution (Pan et al., 2015, 2017; Lai et al., 2021) Our approach aims at finding the graph that has over the document cluster of each timeline topic. minimal distance from the input graph (Filatova We apply (Ning et al., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtai"
2021.emnlp-main.519,P17-1178,1,0.766016,"Missing"
2021.emnlp-main.519,2021.textgraphs-1.4,0,0.0817398,"Missing"
2021.emnlp-main.519,D19-5403,0,0.210679,"hey pendency across key stories, which, compared to determine the key dates of events. These standard MDS, poses additional challenges in remethods overlook the events’ intra-structures (arguments) and inter-structures (event-event constructing temporal order. (3) Manual labeling connections). Following a different route, of timeline summaries is costly; thus the labeled we propose to represent the news articles data for model training is very limited. as an event-graph, thus the summarization As a result, previous studies (Martschat and task becomes compressing the whole graph Markert, 2018; Steen and Markert, 2019) usually to its salient sub-graph. The key hypothesis take an unsupervised approach. Specifically, these is that the events connected through shared arguments and temporal order depict the skelemethods first identify the key dates from the pubton of a timeline, containing events that are lication time distribution. Then for each key date semantically related, structurally salient, and and its associated news articles, a summary is gentemporally coherent in the global event graph. erated based on the salient sentences measured by A time-aware optimal transport distance is the inter-similarity o"
2021.emnlp-main.519,2020.acl-main.553,0,0.0178075,"ument. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timeline Summarization. Due to the lack of to hierarchically generate the timeline. training data, timeline summarization focuses on extractive methods with heuristics (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh"
2021.emnlp-main.519,N16-1008,0,0.0607266,"Missing"
2021.emnlp-main.519,2021.naacl-main.6,1,0.500178,"r events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the best transport plan that has a minimal distance event, We extract and normalize time expressions between two graphs. To apply optimal transport using publication date (Manning et al., 2014), and to timeline summarization, the key is to design the then apply (Wen et al., 2021) to extract the event distance to evaluate the information loss, and thus temporal attributes from the context. If the tem6445 poral attributes can not be decided according to the context, we propagate the temporal attributes from neighbor events based on their shared arguments (?). After that, we use the document publication date to populate the remaining missing dates. For example, in Figure 1, the date 2009-0625 of the collapse (D IE) event is extracted from context last Thursday, and the date of the unconscious (I NJURE) event is propagated along with their shared argument Michael Jackson."
2021.emnlp-main.519,J81-4005,0,0.679601,"Missing"
2021.emnlp-main.519,D11-1040,0,0.138735,"ised manner. We show local text features, ignoring the global context of that our approach significantly improves the the news collection. The applications of neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline s"
2021.emnlp-main.519,K17-1045,0,0.0478793,"Missing"
2021.emnlp-main.519,2021.textgraphs-1.5,1,0.837732,"Missing"
2021.emnlp-main.519,P19-1628,0,0.0232982,"e summaries of all selected dates; (2) agree F1 to compute ROUGE only between the summaries which have the same dates; (3) align F1 to first align summaries in the output with those in the reference based on similarity and the distance between their dates, then compute the ROUGE score between aligned summaries. Distant alignments are punished. Baselines. We compare with: (1) (Chieu and Lee, 2004), a typical extractive model based on sentence similarity; and (2) (Martschat and Markert, 2018), the state-of-the-art extractive timeline sumarization model based on submodular functions. (3) PacSum (Zheng and Lapata, 2019), the state-of-the-art unsupervised graph-based ranking summarization baseline, which utilizes BERT to encode sentences for sentence centrality ranking in a sentence graph. We use the publication date of the selected sentence as key dates. (4) SummPip (Zhao et al., 2020), the state-of-the-art unsupervised multi-document summarization baseline, which constructs a sentence graph and performs spectral clustering. After that, a summary is generated for each sentence cluster 6 The preprocessed event graphs are released together with the dataset. 7 https://www.ldc.upenn.edu/collaborations/ past-proj"
2021.emnlp-main.810,W09-3302,0,0.0491436,"1 to M do 8 θ k ← Train with Eq. (5); 9 {wi ← 1 (fi,yi (x; θ k ) &gt; τ )}ni=1 ; 10 θ ENS ← Train with Eq. (6); 11 // Augmentation; 0 12 {x } ← Eq. (7); 13 // Self-training; (0) 14 θ ← θ ENS ; 15 // Train for T iterations; 16 for t ← 0 to T − 1 do 17 y (t+1) ← Eq. (8); 18 θ (t+1) ← Train with Eq. (9); (T ) 19 Return θ = θ ; Dataset # Types # Train # Test CoNLL03 OntoNotes5.0 Wikigold 4 18 4 14,041 59,924 1,142 3,453 8,262 274 Table 1: Dataset statistics with the number of entity types and the number of training/test sequences. follow the pre-processing of (Chiu and Nichols, 2016), and Wikigold (Balasuriya et al., 2009). The dataset statistics are shown in Table 1. All datasets are in English language. 3.2 Compared Methods We compare with a wide range of state-of-the-art distantly-supervised methods and supervised methods. Fully supervised methods use the ground truth training set for model training. Distantlysupervised methods use the distantly-labeled training set obtained as in (Liang et al., 2020). All methods are evaluated on the test set. the distant supervision quality (i.e., compares distantly-labeled results with the ground truth). • Distant RoBERTa: We fine-tune a pre-trained RoBERTa model on dista"
2021.emnlp-main.810,L18-1707,0,0.0232011,", location and organization names) from texts, can be matched to multiple types in the knowledge is a fundamental task in natural language process- bases—such ambiguity cannot be resolved by the ing with a wide range of applications, including context-free matching process. Figure 1 shows that question answering (Khalid et al., 2008), knowl- some “person” mentions may be partially labeled edge base construction (Etzioni et al., 2005), text (or not labeled at all in other cases), and some summarization (Aramaki et al., 2009) and dialog entities with multiple possible types may be mislasystems (Bowden et al., 2018). In recent years, beled. deep neural models (Devlin et al., 2019; Huang Due to the existence of such noise, straightforet al., 2015; Lample et al., 2016; Ma and Hovy, ward application of supervised learning to distantly2016) have achieved enormous success for NER, labeled data will yield deteriorated performance, thanks to their strong representation learning power because neural models have the strong capacity to that accurately captures the entity semantics in tex- fit to the given (noisy) data. Some previous studies tual contexts. However, a common bottleneck of on distantly-supervised NER"
2021.emnlp-main.810,2020.acl-main.194,0,0.0210362,"truth), or require an additional set of manually-labeled data to train a denoising model. Our method addresses the label noise with a noise-robust learning scheme and a self-training step for better generalization, without using any ground truth data. Our study is also related to data augmentation techniques. In NLP, data augmentation is well developed for text classification, by either creating real text sequences (Xie et al., 2020) via back translation (Sennrich et al., 2016) or in the hidden states of the model via perturbations (Miyato et al., 2017) or interpolations by mixing up labels (Chen et al., 2020). However, these techniques cannot be readily used for the NER task. (Dai and Adel, 2020) study a set of simple augmentation methods for the NER task, like synonym replacement, mention replacement or segment shuffling. Nevertheless, these augmentations are context-free which may generate unreasonable sequences or require additional sources like the WordNet. Our proposed augmentation method is unsupervised and contextualized, generating high-quality sequences thanks to the pre-trained knowledge of PLMs and reliably improving model generalization. 5 Conclusion and Future Work function and a nois"
2021.emnlp-main.810,Q16-1026,0,0.033396,"6 // Train for M iterations; 7 for m ← 1 to M do 8 θ k ← Train with Eq. (5); 9 {wi ← 1 (fi,yi (x; θ k ) &gt; τ )}ni=1 ; 10 θ ENS ← Train with Eq. (6); 11 // Augmentation; 0 12 {x } ← Eq. (7); 13 // Self-training; (0) 14 θ ← θ ENS ; 15 // Train for T iterations; 16 for t ← 0 to T − 1 do 17 y (t+1) ← Eq. (8); 18 θ (t+1) ← Train with Eq. (9); (T ) 19 Return θ = θ ; Dataset # Types # Train # Test CoNLL03 OntoNotes5.0 Wikigold 4 18 4 14,041 59,924 1,142 3,453 8,262 274 Table 1: Dataset statistics with the number of entity types and the number of training/test sequences. follow the pre-processing of (Chiu and Nichols, 2016), and Wikigold (Balasuriya et al., 2009). The dataset statistics are shown in Table 1. All datasets are in English language. 3.2 Compared Methods We compare with a wide range of state-of-the-art distantly-supervised methods and supervised methods. Fully supervised methods use the ground truth training set for model training. Distantlysupervised methods use the distantly-labeled training set obtained as in (Liang et al., 2020). All methods are evaluated on the test set. the distant supervision quality (i.e., compares distantly-labeled results with the ground truth). • Distant RoBERTa: We fine-t"
2021.emnlp-main.810,2020.coling-main.343,0,0.228163,"original ones. To further enforce the labelpreserving constraint of the augmented sequence, we (1) sample x0i only from the top-5 terms given by pMLM (ˆ x; θ PRE ) to avoid low-quality replacements, i and (2) require x0i to have the same capitalization and tokenization with xi (i.e., if xi is capitalized or is a subword, so should x0i ). Using PLMs to perform augmentation for NER has the major benefit of being unsupervised and contextualized. Without PLMs, one may still perform augmentation by replacing an entity in the sequence with another of the same type in the distant supervision source (Dai and Adel, 2020). However, such an approach requires prior knowledge about the entity type in the sequence (i.e., it does not work for non-entities or entities not matched with distant labels), and the augmentation is context-free, which may create low-quality and invalid sequences (e.g., it does not fit the context to replace a technology company with a news agency although they both belong to the “organization” entity type). Contextualized Augmentations with PLMs. Many PLMs (Devlin et al., 2019; Lan et al., 2020; Liu et al., 2019) are pre-trained with the masked language modeling (MLM) task on large-scale t"
2021.emnlp-main.810,N19-1423,0,0.195265,"ultiple types in the knowledge is a fundamental task in natural language process- bases—such ambiguity cannot be resolved by the ing with a wide range of applications, including context-free matching process. Figure 1 shows that question answering (Khalid et al., 2008), knowl- some “person” mentions may be partially labeled edge base construction (Etzioni et al., 2005), text (or not labeled at all in other cases), and some summarization (Aramaki et al., 2009) and dialog entities with multiple possible types may be mislasystems (Bowden et al., 2018). In recent years, beled. deep neural models (Devlin et al., 2019; Huang Due to the existence of such noise, straightforet al., 2015; Lample et al., 2016; Ma and Hovy, ward application of supervised learning to distantly2016) have achieved enormous success for NER, labeled data will yield deteriorated performance, thanks to their strong representation learning power because neural models have the strong capacity to that accurately captures the entity semantics in tex- fit to the given (noisy) data. Some previous studies tual contexts. However, a common bottleneck of on distantly-supervised NER directly treat distant applying deep learning models is the acqu"
2021.emnlp-main.810,2021.emnlp-main.813,1,0.652733,"owledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our work is more closely related to distantly3.8 Case Study supervised NER which uses external gazetteers Finally, we perform case study to understand the or knowledge bases to automatically derive enadvantage of RoSTER with a concrete example tity labels. Along this line, different methods in Table 6. We show the prediction of AutoNER, have been proposed to leverage the distant superBOND and RoSTER on a training"
2021.emnlp-main.810,2020.tacl-1.28,0,0.0232953,"the sequence (i.e., it does not work for non-entities or entities not matched with distant labels), and the augmentation is context-free, which may create low-quality and invalid sequences (e.g., it does not fit the context to replace a technology company with a news agency although they both belong to the “organization” entity type). Contextualized Augmentations with PLMs. Many PLMs (Devlin et al., 2019; Lan et al., 2020; Liu et al., 2019) are pre-trained with the masked language modeling (MLM) task on large-scale text corpora carrying general knowledge like the Wikipedia. Previous studies (Jiang et al., 2020; Petroni et al., 2019) have shown that entity-related knowledge can be extracted from a PLM (without any fine-tuning) by querying it via cloze templates Self-Training. The goals of self-training (ST) and gathering the PLM’s MLM outputs. are two-fold: (1) use the model’s high-confidence 10370 ··· O 0 <latexit sha1_base64=&quot;8G0nGLSTs3/pcwh69xFGY55W9fQ=&quot;&gt;AAAB+HicbVC7TsMwFL0pr1IeDTCyWFQIpipBRTBWYmEsEn1IbVQ5jtNadZzIdhAl6pewMIAQK5/Cxt/gtBmg5UiWj865Vz4+fsKZ0o7zbZXW1jc2t8rblZ3dvf2qfXDYUXEqCW2TmMey52NFORO0rZnmtJdIiiOf064/ucn97gOVisXiXk8T6kV4JFjICNZGGtrVgR/zQE0jc2WPs7OhXXPqzhxolbgFqUGB1tD+GgQxSSMqNOFYqb"
2021.emnlp-main.810,2020.acl-main.523,0,0.0248361,"evious studies have explored ing the augmentations (i.e., including or excluding cross lingual knowledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our work is more closely related to distantly3.8 Case Study supervised NER which uses external gazetteers Finally, we perform case study to understand the or knowledge bases to automatically derive enadvantage of RoSTER with a concrete example tity labels. Along this line, different methods in Table 6. We show the predictio"
2021.emnlp-main.810,N16-1030,0,0.0456277,"such ambiguity cannot be resolved by the ing with a wide range of applications, including context-free matching process. Figure 1 shows that question answering (Khalid et al., 2008), knowl- some “person” mentions may be partially labeled edge base construction (Etzioni et al., 2005), text (or not labeled at all in other cases), and some summarization (Aramaki et al., 2009) and dialog entities with multiple possible types may be mislasystems (Bowden et al., 2018). In recent years, beled. deep neural models (Devlin et al., 2019; Huang Due to the existence of such noise, straightforet al., 2015; Lample et al., 2016; Ma and Hovy, ward application of supervised learning to distantly2016) have achieved enormous success for NER, labeled data will yield deteriorated performance, thanks to their strong representation learning power because neural models have the strong capacity to that accurately captures the entity semantics in tex- fit to the given (noisy) data. Some previous studies tual contexts. However, a common bottleneck of on distantly-supervised NER directly treat distant applying deep learning models is the acquisition labels as if ground truth for model training and 1 rely on simple tricks such as"
2021.emnlp-main.810,2020.acl-main.139,0,0.0435576,"Missing"
2021.emnlp-main.810,2021.ccl-1.108,0,0.0539633,"Missing"
2021.emnlp-main.810,P16-1101,0,0.293987,"the created augmentations improve the model’s generalization ability. • On three benchmark datasets, RoSTER outperforms existing distantly-supervised NER approaches by significant margins. 2 Method In this section, we (1) briefly describe how to obtain distantly-labeled data, (2) introduce our noiserobust learning scheme and (3) propose a selftraining method with a new contextualized augmentation generation technique. We assume the pre-trained RoBERTa (Liu et al., 2019) model is used as our backbone model, but our proposed methods can be integrated with other architectures (e.g., LSTM-based (Ma and Hovy, 2016)) as well. 2020) for this step: (1) potential entities are determined via POS tagging and hand-crafted rules, (2) their types are acquired by querying Wikidata using SPARQL (Vrandeˇci´c and Krötzsch, 2014), and (3) additional gazetteers from multiple online resources are used for matching more entities in the corpus. 2.2 Noise-Robust Learning We first overview the common setup for training NER models, and then propose two designs that work jointly for distantly-supervised NER, motivated by the challenges of learning with noisy labels: (1) a new loss function, and (2) noisy label removal. Final"
2021.emnlp-main.810,2020.emnlp-main.724,1,0.862818,"Missing"
2021.emnlp-main.810,P17-1135,0,0.0243666,"man annotation burden when applying deep modWe study the effectiveness of the generated con- els, several studies propose to train NER models textualized augmentations for the self-training step. with weakly/distantly-labeled data. For weaklyWe run the self-training step with and without us- supervised NER, previous studies have explored ing the augmentations (i.e., including or excluding cross lingual knowledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our w"
2021.emnlp-main.810,N19-1250,0,0.0222651,"Missing"
2021.emnlp-main.810,P19-1231,0,0.019775,"Automobile Corporation]ORG and [Ek Chor China Motorcycle]ORG . AutoNER: Shanghai-Ek [Chor]PER is jointly owned by the Shanghai Automobile Corporation and [Ek Chor]PER [China]LOC Motorcycle. BOND: [Shanghai-Ek Chor]PER is jointly owned by the [Shanghai]LOC [Automobile Corporation]ORG and [Ek Chor]PER [China Motorcycle]ORG . RoSTER: [Shanghai-Ek Chor]ORG is jointly owned by the [Shanghai Automobile Corporation]ORG and [Ek Chor China Motorcycle]ORG . Table 6: Case study with RoSTER and baselines. The sentence is from CoNLL03. 2019), formulating the task as a positive-unlabeled learning problem (Peng et al., 2019), and adopting early stopping to prevent the model from overfitting to distant labels (Liang et al., 2020). However, previous methods either do not explicitly address the noise in the distantly-labeled data (i.e., treating them as if they are ground truth), or require an additional set of manually-labeled data to train a denoising model. Our method addresses the label noise with a noise-robust learning scheme and a self-training step for better generalization, without using any ground truth data. Our study is also related to data augmentation techniques. In NLP, data augmentation is well devel"
2021.emnlp-main.810,D19-1250,0,0.149452,"on have been exploited via noise-robust learning, but some tokens may have not been fully leveraged by the model since they are excluded by the noisy label removal step. The self-training step aims to bootstrap on all tokens using the model’s own predictions to improve its generalization ability. Similar selftraining ideas have been explored in classification tasks (Meng et al., 2018, 2019, 2020). (2) The pretrained language model (PLM) has only been used to initialize the NER model for fine-tuning, while PLMs (without fine-tuning) encode factual and relational knowledge through pre-training (Petroni et al., 2019) that may complement the NER model training. The self-training step thus also brings additional pre-trained knowledge for better model generalization by creating contextualized augmentations using a PLM. Figure 2 shows an overview. Given that the MLM task shares high similarity with the NER task (i.e., both leverage the contextual information within the sequence for token-level classification) and that the MLM outputs contain general knowledge acquired during pre-training, we propose to use the pre-trained RoBERTa model (without fine-tuning) θ PRE for creating label-preserving augmentations (i"
2021.emnlp-main.810,P16-1009,0,0.0395807,"wever, previous methods either do not explicitly address the noise in the distantly-labeled data (i.e., treating them as if they are ground truth), or require an additional set of manually-labeled data to train a denoising model. Our method addresses the label noise with a noise-robust learning scheme and a self-training step for better generalization, without using any ground truth data. Our study is also related to data augmentation techniques. In NLP, data augmentation is well developed for text classification, by either creating real text sequences (Xie et al., 2020) via back translation (Sennrich et al., 2016) or in the hidden states of the model via perturbations (Miyato et al., 2017) or interpolations by mixing up labels (Chen et al., 2020). However, these techniques cannot be readily used for the NER task. (Dai and Adel, 2020) study a set of simple augmentation methods for the NER task, like synonym replacement, mention replacement or segment shuffling. Nevertheless, these augmentations are context-free which may generate unreasonable sequences or require additional sources like the WordNet. Our proposed augmentation method is unsupervised and contextualized, generating high-quality sequences th"
2021.emnlp-main.810,D18-1034,0,0.0188475,"rden when applying deep modWe study the effectiveness of the generated con- els, several studies propose to train NER models textualized augmentations for the self-training step. with weakly/distantly-labeled data. For weaklyWe run the self-training step with and without us- supervised NER, previous studies have explored ing the augmentations (i.e., including or excluding cross lingual knowledge transfer from high rethe second term in Eq. (9)) on the Wikigold dataset source languages to low resource languages (Feng and show the results in Figure 3(c). Even with- et al., 2018; Ni et al., 2017; Xie et al., 2018), agout augmentations, the self-training improves the gregating multiple weak labeling functions (Lison model by using high-confidence predictions for et al., 2020; Safranchik et al., 2020) or leveragself-refinement; with augmentations, the model is ing sentence-level labels (Kruengkrai et al., 2020). trained with more data and eventually generalizes Few-shot approaches (Huang et al., 2021) have also better with higher test set performance. Two con- been explored to leverage very few labeled data for crete augmentation examples are shown in Table 5. NER model training. Our work is more closely"
2021.emnlp-main.815,2020.emnlp-main.436,0,0.142806,"s. Event-event temporal relation extraction aims to automatically extract the temporal order given a pair of events and further build a temporal graph. Figure 1 illustrates an example sentence and its temporal graph. There are three events in the sentence, said, identified and run. The temporal relation between said and identified is A FTER, and the temporal relations between said and run and between identified and run are B EFORE. Neural network based methods have achieved promising improvement for temporal relation extraction (Meng et al., 2017; Meng and Rumshisky, 2018; Cheng et al., 2020; Ballesteros et al., 2020; Wen et al., 2021a). They mostly consider the task as pairwise classification. There are also efforts focusing on the global structures, including Markov logical networks and Integer Linear Programming said Before identified run Microsoft (e1, said) it has (e2, identified) three companies for the China program to (e3, run) through June . Figure 1: An example sentence and their temporal relations. In this example, there three different events, and the final extracted graph shows the pair-wise temporal relation extraction results. (ILP) based methods (Bramsen et al., 2006; Chambers and Jurafsky"
2021.emnlp-main.815,W06-1623,0,0.364937,"heng et al., 2020; Ballesteros et al., 2020; Wen et al., 2021a). They mostly consider the task as pairwise classification. There are also efforts focusing on the global structures, including Markov logical networks and Integer Linear Programming said Before identified run Microsoft (e1, said) it has (e2, identified) three companies for the China program to (e3, run) through June . Figure 1: An example sentence and their temporal relations. In this example, there three different events, and the final extracted graph shows the pair-wise temporal relation extraction results. (ILP) based methods (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; Ning et al., 2017, 2018a, 2019; Han et al., 2019). Though achieving great success, event time, an important feature, is often overlooked by previous work. Conceptually, if we know the exact time information for all events, their temporal relations can be naturally derived. For example, if we know that event A happened on Monday while event B happened on Tuesday in the same week, it is obvious that A happened B EFORE B. However, explicit time arguments can be rarely found in text, especially in news articles (Wen et al., 20"
2021.emnlp-main.815,P14-2082,0,0.0201726,"t mentions ei and ej , and their temporal relation ei B EFORE ej , then their predicted time ti and tj should follow ti &lt; tj . Similarly, if their relation is E QUAL, then the distance between their predicted time should be as close as possible. Therefore, we use a margin-based optimization method to constrain our predicted relative event time. We use different margins based on different temporal relations, 3 Lt = 1[r(ei ,ej ) = B EFORE] max (0, 1 − (tj − ti )) We conduct our experiments on MATRES (Ning et al., 2018b). MATRES contains refined annotations on TimeBank (Pustejovsky et al., 2003; Cassidy et al., 2014) and TempEval (UzZaman et al., 2013) (containing AQUAINT and Platinum subsets) documents. We follow the previous work that uses TimeBank and AQUAINT for training and we use Platinum as the test set. We randomly select 21 documents as development set. The detailed statistics can be found in Table 1. + 1[r(ei ,ej ) = A FTER] max (0, 1 − (ti − tj )) + 1[r(ei ,ej ) = E QUAL]|ti − tj |. (5) If ei is B EFORE ej , the above optimization will maximize the distance (tj − ti ) unless it is equal or larger than 1, which follows the constraint ti &lt; tj . On the contrary, If ei is A FTER ej , it will maximi"
2021.emnlp-main.815,Q14-1022,0,0.0195228,"it aggressively classifies tion extraction correlates with relative event time relations as B EFORE or A FTER labels. However, prediction, while the second case shows that our because it cannot handle VAGUE, it’s low in pre- model can utilize the event-pair representation and 10434 classify relation as VAGUE rather than completely depending on predicted timestamps. 4 Related Work Earlier efforts on temporal relation extraction focus on global inference using methods such as Integer Linear Programming (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; Chambers et al., 2014). Recently, neural network-based methods have also achieved promising improvement (Meng et al., 2017; Meng and Rumshisky, 2018; Ning et al., 2018a, 2019; Han et al., 2019; Wang et al., 2020; Cheng et al., 2020). Especially, Goyal and Durrett (2019) use LSTM to encode Timex for temporal relation extraction, Ballesteros et al. (2020) jointly train event time arguments extraction from ACE2005. The most related work is (Leeuwenberg and Moens, 2018), which proposes to predict relative event time and uses the comparison of relative timestamps as temporal relations. Our work focuses on jointly traini"
2021.emnlp-main.815,D08-1073,0,0.128088,"Missing"
2021.emnlp-main.815,2020.findings-emnlp.121,0,0.131003,"e evolution of events. Event-event temporal relation extraction aims to automatically extract the temporal order given a pair of events and further build a temporal graph. Figure 1 illustrates an example sentence and its temporal graph. There are three events in the sentence, said, identified and run. The temporal relation between said and identified is A FTER, and the temporal relations between said and run and between identified and run are B EFORE. Neural network based methods have achieved promising improvement for temporal relation extraction (Meng et al., 2017; Meng and Rumshisky, 2018; Cheng et al., 2020; Ballesteros et al., 2020; Wen et al., 2021a). They mostly consider the task as pairwise classification. There are also efforts focusing on the global structures, including Markov logical networks and Integer Linear Programming said Before identified run Microsoft (e1, said) it has (e2, identified) three companies for the China program to (e3, run) through June . Figure 1: An example sentence and their temporal relations. In this example, there three different events, and the final extracted graph shows the pair-wise temporal relation extraction results. (ILP) based methods (Bramsen et al., 2"
2021.emnlp-main.815,P19-1433,0,0.0187275,"e event-pair representation and 10434 classify relation as VAGUE rather than completely depending on predicted timestamps. 4 Related Work Earlier efforts on temporal relation extraction focus on global inference using methods such as Integer Linear Programming (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; Chambers et al., 2014). Recently, neural network-based methods have also achieved promising improvement (Meng et al., 2017; Meng and Rumshisky, 2018; Ning et al., 2018a, 2019; Han et al., 2019; Wang et al., 2020; Cheng et al., 2020). Especially, Goyal and Durrett (2019) use LSTM to encode Timex for temporal relation extraction, Ballesteros et al. (2020) jointly train event time arguments extraction from ACE2005. The most related work is (Leeuwenberg and Moens, 2018), which proposes to predict relative event time and uses the comparison of relative timestamps as temporal relations. Our work focuses on jointly training relative time prediction and temporal relation extraction and utilizes relative timestamps as features via the Stack-Propagation framework. 5 Conclusions and Future Work In this paper, we leverage relative event time prediction that can ground e"
2021.emnlp-main.815,D19-1041,0,0.0206244,"sentation and 10434 classify relation as VAGUE rather than completely depending on predicted timestamps. 4 Related Work Earlier efforts on temporal relation extraction focus on global inference using methods such as Integer Linear Programming (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; Chambers et al., 2014). Recently, neural network-based methods have also achieved promising improvement (Meng et al., 2017; Meng and Rumshisky, 2018; Ning et al., 2018a, 2019; Han et al., 2019; Wang et al., 2020; Cheng et al., 2020). Especially, Goyal and Durrett (2019) use LSTM to encode Timex for temporal relation extraction, Ballesteros et al. (2020) jointly train event time arguments extraction from ACE2005. The most related work is (Leeuwenberg and Moens, 2018), which proposes to predict relative event time and uses the comparison of relative timestamps as temporal relations. Our work focuses on jointly training relative time prediction and temporal relation extraction and utilizes relative timestamps as features via the Stack-Propagation framework. 5 Conclusions and Future Work In this paper, we leverage relative event time prediction that can ground e"
2021.emnlp-main.815,P18-1122,0,0.0809781,"ge models, dental supervision to constrain the predicted time. 10432 For example, given two event mentions ei and ej , and their temporal relation ei B EFORE ej , then their predicted time ti and tj should follow ti &lt; tj . Similarly, if their relation is E QUAL, then the distance between their predicted time should be as close as possible. Therefore, we use a margin-based optimization method to constrain our predicted relative event time. We use different margins based on different temporal relations, 3 Lt = 1[r(ei ,ej ) = B EFORE] max (0, 1 − (tj − ti )) We conduct our experiments on MATRES (Ning et al., 2018b). MATRES contains refined annotations on TimeBank (Pustejovsky et al., 2003; Cassidy et al., 2014) and TempEval (UzZaman et al., 2013) (containing AQUAINT and Platinum subsets) documents. We follow the previous work that uses TimeBank and AQUAINT for training and we use Platinum as the test set. We randomly select 21 documents as development set. The detailed statistics can be found in Table 1. + 1[r(ei ,ej ) = A FTER] max (0, 1 − (ti − tj )) + 1[r(ei ,ej ) = E QUAL]|ti − tj |. (5) If ei is B EFORE ej , the above optimization will maximize the distance (tj − ti ) unless it is equal or larger"
2021.emnlp-main.815,D18-1155,0,0.0283905,"Missing"
2021.emnlp-main.815,2021.ccl-1.108,0,0.0936716,"Missing"
2021.emnlp-main.815,P18-1049,0,0.0926592,"tant task to understand the evolution of events. Event-event temporal relation extraction aims to automatically extract the temporal order given a pair of events and further build a temporal graph. Figure 1 illustrates an example sentence and its temporal graph. There are three events in the sentence, said, identified and run. The temporal relation between said and identified is A FTER, and the temporal relations between said and run and between identified and run are B EFORE. Neural network based methods have achieved promising improvement for temporal relation extraction (Meng et al., 2017; Meng and Rumshisky, 2018; Cheng et al., 2020; Ballesteros et al., 2020; Wen et al., 2021a). They mostly consider the task as pairwise classification. There are also efforts focusing on the global structures, including Markov logical networks and Integer Linear Programming said Before identified run Microsoft (e1, said) it has (e2, identified) three companies for the China program to (e3, run) through June . Figure 1: An example sentence and their temporal relations. In this example, there three different events, and the final extracted graph shows the pair-wise temporal relation extraction results. (ILP) based method"
2021.emnlp-main.815,D17-1092,0,0.148613,"rdering is an important task to understand the evolution of events. Event-event temporal relation extraction aims to automatically extract the temporal order given a pair of events and further build a temporal graph. Figure 1 illustrates an example sentence and its temporal graph. There are three events in the sentence, said, identified and run. The temporal relation between said and identified is A FTER, and the temporal relations between said and run and between identified and run are B EFORE. Neural network based methods have achieved promising improvement for temporal relation extraction (Meng et al., 2017; Meng and Rumshisky, 2018; Cheng et al., 2020; Ballesteros et al., 2020; Wen et al., 2021a). They mostly consider the task as pairwise classification. There are also efforts focusing on the global structures, including Markov logical networks and Integer Linear Programming said Before identified run Microsoft (e1, said) it has (e2, identified) three companies for the China program to (e3, run) through June . Figure 1: An example sentence and their temporal relations. In this example, there three different events, and the final extracted graph shows the pair-wise temporal relation extraction r"
2021.emnlp-main.815,D17-1108,0,0.017293,"sk as pairwise classification. There are also efforts focusing on the global structures, including Markov logical networks and Integer Linear Programming said Before identified run Microsoft (e1, said) it has (e2, identified) three companies for the China program to (e3, run) through June . Figure 1: An example sentence and their temporal relations. In this example, there three different events, and the final extracted graph shows the pair-wise temporal relation extraction results. (ILP) based methods (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; Ning et al., 2017, 2018a, 2019; Han et al., 2019). Though achieving great success, event time, an important feature, is often overlooked by previous work. Conceptually, if we know the exact time information for all events, their temporal relations can be naturally derived. For example, if we know that event A happened on Monday while event B happened on Tuesday in the same week, it is obvious that A happened B EFORE B. However, explicit time arguments can be rarely found in text, especially in news articles (Wen et al., 2021b). Leeuwenberg and Moens (2018) propose to predict the relative timeline and directly"
2021.emnlp-main.815,P18-1212,0,0.2769,"8; Yoshikawa et al., 2009; Do et al., 2012; Ning et al., 2017, 2018a, 2019; Han et al., 2019). Though achieving great success, event time, an important feature, is often overlooked by previous work. Conceptually, if we know the exact time information for all events, their temporal relations can be naturally derived. For example, if we know that event A happened on Monday while event B happened on Tuesday in the same week, it is obvious that A happened B EFORE B. However, explicit time arguments can be rarely found in text, especially in news articles (Wen et al., 2021b). Leeuwenberg and Moens (2018) propose to predict the relative timeline and directly compare the relative timestamps of events to derive their temporal relations. Although showing promising performance, those predicted timestamps do not consider information from event pairs and cannot handle the uncertain temporal boundary of an event expressed in text to predict relations such as VAGUE. In this paper, we follow the idea of Leeuwenberg and Moens (2018) to predict the relative event time for temporal relation extraction, where the relative time is a real number indicating the relative position of the event in the timeline."
2021.emnlp-main.815,D19-1214,1,0.843417,"nsider them as addi-Relative-Event-Time-to-Enhance-Event-Ev ent-Temporal-Relation-Extraction tional features and incorporate them into training 10431 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10431–10437 c November 7–11, 2021. 2021 Association for Computational Linguistics the temporal relation classifier. We propose a joint model with Stack-Propagation framework (Zhang and Weiss, 2016) to connect relative event time prediction and temporal relation extraction, which can exploit the explicit features from the former task for the latter task (Qin et al., 2019). Our model directly uses the output of relative event time prediction as the input for temporal relation classification so that the classification can benefit from both representations of event pairs and the predicted relative event time. Because we do not break the differentiability between two tasks, the training objective for one task can also promote another task. Similarly to Leeuwenberg and Moens (2018), we adapt margin-based optimization to constrain the distance between two relative event time values, given their temporal relation. Our experiments show that the relative event time pre"
2021.emnlp-main.815,S13-2001,0,0.0279641,"poral relation ei B EFORE ej , then their predicted time ti and tj should follow ti &lt; tj . Similarly, if their relation is E QUAL, then the distance between their predicted time should be as close as possible. Therefore, we use a margin-based optimization method to constrain our predicted relative event time. We use different margins based on different temporal relations, 3 Lt = 1[r(ei ,ej ) = B EFORE] max (0, 1 − (tj − ti )) We conduct our experiments on MATRES (Ning et al., 2018b). MATRES contains refined annotations on TimeBank (Pustejovsky et al., 2003; Cassidy et al., 2014) and TempEval (UzZaman et al., 2013) (containing AQUAINT and Platinum subsets) documents. We follow the previous work that uses TimeBank and AQUAINT for training and we use Platinum as the test set. We randomly select 21 documents as development set. The detailed statistics can be found in Table 1. + 1[r(ei ,ej ) = A FTER] max (0, 1 − (ti − tj )) + 1[r(ei ,ej ) = E QUAL]|ti − tj |. (5) If ei is B EFORE ej , the above optimization will maximize the distance (tj − ti ) unless it is equal or larger than 1, which follows the constraint ti &lt; tj . On the contrary, If ei is A FTER ej , it will maximize the distance (ti − tj ), which fo"
2021.emnlp-main.815,2020.emnlp-main.51,0,0.148118,"l relation 2019). 2) LSTM+T EM P ROB+ILP: LSTM-based extraction are based on contextualized representa- method incorporating pretrained language model tions from the pretrained language model, we adopt embedding, commensense prior (T EM P ROB) and Stack-Propagation framework to connect these two ILP (Ning et al., 2019). 3) Joint Constrained Learntasks while preserving the differentiability. ing: A constrained learning based optimization for Specifically, besides the event-pair contextual- joint event temporal and hierarchy relation extracized representation that the baseline method uses tion (Wang et al., 2020). 4) Self-Training: Multifor pair-wise temporal relation classification, we task self-training on temporal relation extraction usfurther incorporate their predicted relative event ing additional time annotation from ACE2005 and time into classification, the original TimeBank (Ballesteros et al., 2020). We use RoBERTa-large as our pretrained lanai,j = FFN2 (tanh(FFN1 ([ci,j ; ti ; tj ]))). (6) guage model. Our best model is optimized using During training, we use cross-entropy objective AdamW for 30 epochs with learning rate between for temporal relation classification, {1e-5, 2e-5} for both pr"
2021.emnlp-main.815,2021.naacl-main.6,1,0.77449,"Missing"
2021.emnlp-main.815,P09-1046,0,0.181564,"al., 2021a). They mostly consider the task as pairwise classification. There are also efforts focusing on the global structures, including Markov logical networks and Integer Linear Programming said Before identified run Microsoft (e1, said) it has (e2, identified) three companies for the China program to (e3, run) through June . Figure 1: An example sentence and their temporal relations. In this example, there three different events, and the final extracted graph shows the pair-wise temporal relation extraction results. (ILP) based methods (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; Ning et al., 2017, 2018a, 2019; Han et al., 2019). Though achieving great success, event time, an important feature, is often overlooked by previous work. Conceptually, if we know the exact time information for all events, their temporal relations can be naturally derived. For example, if we know that event A happened on Monday while event B happened on Tuesday in the same week, it is obvious that A happened B EFORE B. However, explicit time arguments can be rarely found in text, especially in news articles (Wen et al., 2021b). Leeuwenberg and Moens (2018) propose to predict"
2021.emnlp-main.815,P16-1147,0,0.0513615,"Missing"
2021.emnlp-tutorials.3,P17-1017,0,0.0148689,"etry generation. However, in many downstream applications such as news summarization, counter-argument narrative generation, and knowledge base description, the generation process needs to be guided by certain level of knowledge such as sentiment (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu"
2021.emnlp-tutorials.3,D19-1187,0,0.0164504,"e-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overview [20 mins] At the beginning of the tutorial we will motivate the task of knowledge-driven NLG by showing a large variety of applications (e.g., KD and KS) in academia and industry which have been mentioned in the Introduction. We will present examples about the shortcomings of pure Seq2Seq or language models as well as the opportunities of using knowledge to enrich the generation. We categorize the input source knowledge and rel"
2021.emnlp-tutorials.3,P16-1154,0,0.0195403,"ng, representing time and number, duplicate removal, augmenting massive pre-trained language models with external commonsense and background knowledge, and developing effective automatic evaluation metrics, and rigorous and efficient human evaluation procedures. We will provide pointers to resources, including data sets, software and on-line demos. 3 Full reading list: https://github.com/wyu97/KENLG-Reading Small reading list: • Survey: KENLG (Yu et al., 2020) • General learning and NLG frameworks (1) Seq2Seq (Bahdanau et al., 2015), (2) Transformer (Vaswani et al., 2017), (3) Copy mechanism (Gu et al., 2016); • Semantic knowledge for enhancing NLG (4) Topic (Xing et al., 2017), (5) Sentiment (Hu et al., 2017), (6) Emotion (Zhou et al., 2018a); • Structured knowledge for enhancing NLG (7) Wikipedia KB (Liu et al., 2018b), (8) Sports Tables (Wiseman et al., 2017), (9) Commonsense KG (Zhou et al., 2018b), (10) Scientific KG (Koncel et al., 2019). Diversity Considerations The topic to be presented is of great interest to diverse group of audience from academics and industry. We will cover a broad diversity of methods and applications in many languages and domains. In particular, enriching modeling an"
2021.emnlp-tutorials.3,D18-1435,1,0.894754,"Missing"
2021.emnlp-tutorials.3,P19-1197,0,0.0282931,"certain level of knowledge such as sentiment (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2"
2021.emnlp-tutorials.3,N18-2101,0,0.023574,"t (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overvi"
2021.emnlp-tutorials.3,N19-1236,0,0.042255,"Missing"
2021.emnlp-tutorials.3,N19-1238,0,0.0202373,"n-line demos. 3 Full reading list: https://github.com/wyu97/KENLG-Reading Small reading list: • Survey: KENLG (Yu et al., 2020) • General learning and NLG frameworks (1) Seq2Seq (Bahdanau et al., 2015), (2) Transformer (Vaswani et al., 2017), (3) Copy mechanism (Gu et al., 2016); • Semantic knowledge for enhancing NLG (4) Topic (Xing et al., 2017), (5) Sentiment (Hu et al., 2017), (6) Emotion (Zhou et al., 2018a); • Structured knowledge for enhancing NLG (7) Wikipedia KB (Liu et al., 2018b), (8) Sports Tables (Wiseman et al., 2017), (9) Commonsense KG (Zhou et al., 2018b), (10) Scientific KG (Koncel et al., 2019). Diversity Considerations The topic to be presented is of great interest to diverse group of audience from academics and industry. We will cover a broad diversity of methods and applications in many languages and domains. In particular, enriching modeling and learning with external knowledge, as the core topic in this tutorial, is particularly helpful for low-resource language modeling where no large data are available. We have a diverse instructor team across multiple institutions (ND, UIUC, UCSD, and Salesforce Inc.) with varying seniority (ranging from junior/senior PhD students to assista"
2021.emnlp-tutorials.3,P19-1195,0,0.0562748,"Missing"
2021.emnlp-tutorials.3,D19-1013,0,0.0191107,"eam applications such as news summarization, counter-argument narrative generation, and knowledge base description, the generation process needs to be guided by certain level of knowledge such as sentiment (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-suppo"
2021.emnlp-tutorials.3,P19-1479,0,0.0170669,", 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overview [20 mins] At the beginning of the tutorial we will motivate the task of knowledge-driven NLG by showing a large variety of applications (e.g., KD and KS) in academia and industry which have been mentioned in the Introduction. We will present examples about the shortcomings of pure Seq2Seq or language models as well as the opportunities of using knowledge to enrich the generation. We categorize the input source knowledge and related advanced machine learning technologies in Figure 1. We will present the"
2021.emnlp-tutorials.3,P18-1150,0,0.0235964,"tive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overview [20 mins] At the beginning of the tutorial we will motivate the task of knowledge-driven NLG by"
2021.emnlp-tutorials.3,P18-1138,0,0.157091,"er, in many downstream applications such as news summarization, counter-argument narrative generation, and knowledge base description, the generation process needs to be guided by certain level of knowledge such as sentiment (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), kn"
2021.emnlp-tutorials.3,2020.acl-main.640,0,0.0170934,"divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overview [20 mins] At the beginning of the tutorial we will motivate the task of knowledge-driven NLG by showing a large variety of applicatio"
2021.emnlp-tutorials.3,D19-1406,0,0.0126721,"plied in many real-world applications, including dialogue systems, biography generation, technical paper draft generation, and multimedia news summarization. Neural language models have achieved impressive successes at automatic NLG, especially on creative writing such as story completion and poetry generation. However, in many downstream applications such as news summarization, counter-argument narrative generation, and knowledge base description, the generation process needs to be guided by certain level of knowledge such as sentiment (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019)"
2021.emnlp-tutorials.3,2021.emnlp-main.412,1,0.750842,"eative writing such as story completion and poetry generation. However, in many downstream applications such as news summarization, counter-argument narrative generation, and knowledge base description, the generation process needs to be guided by certain level of knowledge such as sentiment (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or vi"
2021.emnlp-tutorials.3,P19-1191,1,0.902011,"Missing"
2021.emnlp-tutorials.3,2020.acl-main.184,0,0.0235074,"on (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overview [20 mins] At the beginning of the tutorial we will motivate the task of knowledge-driven NLG by showing a large variety of applications (e.g., KD and KS) in academia and industry which have been mentioned in the Introduction. We will present examples about the shortcomings of pure Seq2Seq or language models as well as the opportunities of using knowledge to enrich the generation. We categorize the input source knowledge and related advanced machine"
2021.emnlp-tutorials.3,2020.acl-main.101,0,0.0283484,"ription, the generation process needs to be guided by certain level of knowledge such as sentiment (Hu et al., 2017), topic (Xing et al., 2017), and style (Tikhonov et al., 2019). The usage of supportive knowledge in NLG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et a"
2021.emnlp-tutorials.3,D18-1433,1,0.844212,"tion (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overview [20 mins] At the beginning of the tutorial we will motivate the task of knowledge-driven NLG by showing a large variety of applications (e.g., KD and KS) in academia and industry which have been mentioned in the Introduction. We will present examples about the shortcomings of pure Seq2Seq or language models as well as the opportunities of using k"
2021.emnlp-tutorials.3,D19-1548,0,0.0215925,"LG can be roughly divided into the following two levels: (1) knowledge description (KD), which transforms structured data into unstructured text, such as topic-to-text (Dong et al., 2021; Yu et al., 2021), knowledge base description (Gardent et al., 2017; Liu et al., 2018a; Qin et al., 2019; Zeng et al., 2021), table-to-text generation (Liu et al., 2018b; Moryossef et al., 2019; Wang et al., 2020) and its variants in low-resource (Ma et al., 2019) and multi-lingual setting (Kaffee et al., 2018), datato-text (Wiseman et al., 2017; Puduppully et al., 2019), and graph-to-text (Song et al., 2018; Zhu et al., 2019; Yao et al., 2020); (2) knowledge synthesis (KS), which obtain knowledge from external knowledge resources (e.g, knowledge base) and integrate it into text generation, such as image or video caption generation (Whitehead et al., 2018; Lu et al., 2018), knowledge graph-supported dialogue generation (Liu et al., 2019; Zhang et al., 2020), knowledge-guided comment generation (Li et al., 2019), and scientific paper generation (Wang 2 2.1 Brief Tutorial Outline Motivation and Overview [20 mins] At the beginning of the tutorial we will motivate the task of knowledge-driven NLG by showing a large va"
2021.findings-acl.195,D19-5304,0,0.024143,"s demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1 1 Introduction Speech-to-text translation (ST) takes speech input in a source language and outputs text utterance in a target language. It has many real-world applications, including automatic video captioning, simultaneous translation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce e"
2021.findings-acl.195,P18-1163,0,0.0279739,"Missing"
2021.findings-acl.195,N19-1202,0,0.0291476,"Missing"
2021.findings-acl.195,W19-6603,0,0.02628,"Missing"
2021.findings-acl.195,N16-1109,0,0.159703,"anslation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce errors. However, it remains a challenge for end-to-end ST to catch up with their cascaded counterparts in performance. We argue that the root cause is the gap between the two modalities, speech and text. Although they both encode human languages, they are dissimilar in both coding attributes (pitch, volume, and intonation versus words, affixes, and punctuation) and length (thousands of time frames versus t"
2021.findings-acl.195,E09-1030,0,0.0544836,"r MT data. However, they both lack pivotal modules in model design to semantically bridge the gap between audio and text, and could thus suffer from modality mismatch in representations. Cascaded ST The cascaded method is a more long-standing trend in ST (Sperber et al., 2017; Jan et al., 2018). To alleviate its innate problem of error propagation, Cheng et al. (2018, 2019) introduce synthetic ASR-related errors and perturbations. On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based los"
2021.findings-acl.195,2020.acl-demos.34,0,0.190402,"erive a novel bi-modal contrastive training task to learn an alignment between semantic memories of two modalities. Finally, Chimera achieves a new state-of-the-art performance on the MuST-C benchmark and demonstrates its efficacy in learning modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al."
2021.findings-acl.195,L18-1001,0,0.0250668,"s. MuST-C contains translations from English (EN) to 8 languages: Dutch (NL), French (FR), German (DE), Italian (IT), Portuguese (PT), Romanian (RO), Russian (RU), and Spanish (ES). With each pair consisting of at least 385 hours of audio recordings, to the best of our knowledge, MuST-C is currently the largest speech translation dataset available for each language pair. It includes data from English TED talks with manual transcripts and translations at the sentence level. We use the dev and tst-COMMON sets as our development and test data, respectively. Augmented LibriSpeech Dataset (En-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation t"
2021.findings-acl.195,2020.coling-main.314,0,0.288628,"ing modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al., 2019) and model adaptation (Di Gangi et al., 2020), have also been shown to be effective. Among these attempts, (Indurthi et al., 2019; Le et al., 2020; Liu et al., 2020) are most related to ours, as they also attempt to train mode"
2021.findings-acl.195,W18-6309,0,0.0126882,". On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here"
2021.findings-acl.195,2006.iwslt-papers.1,0,0.0870071,"Liu et al., 2020) are most related to ours, as they also attempt to train models on ASR or MT data. However, they both lack pivotal modules in model design to semantically bridge the gap between audio and text, and could thus suffer from modality mismatch in representations. Cascaded ST The cascaded method is a more long-standing trend in ST (Sperber et al., 2017; Jan et al., 2018). To alleviate its innate problem of error propagation, Cheng et al. (2018, 2019) introduce synthetic ASR-related errors and perturbations. On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based a"
2021.findings-acl.195,N19-4009,0,0.0504683,"× X X X X × × × × × × X × × × × × × × × X × × X X 22.7 22.9 22.4 23.6 23.1 22.1 25.2 22.3 25.6 27.1 • 32.9 32.8 31.6 33.5 34.1 34.5 34.3 35.0 35.6 15.3 15.8 14.7 15.2 15.8 16.7 17.4 27.2 28.0 26.9 28.1 28.7 30.2 30.6 22.7 23.8 23.0 24.2 24.2 24.0 25.0 21.9 21.9 21.0 22.9 22.4 23.2 24.0 28.1 28.0 26.3 30.0 29.3 29.7 30.2 27.3 27.4 24.9 27.6 28.2 28.5 29.2 Table 1: Main results on tst-COMMON subset on all 8 languages in MuST-C dataset. “Speech” denotes unlabeled audio data. • : the result uses a mixed WMT14+OpenSubtitles data for MT pre-training. EN-DE Among the baselines, † shows results from Ott et al. (2019), ‡ from Inaguma et al. (2020), ? from Zhang et al. (2020a), ♦ from Le et al. (2020), ] from Liu et al. (2020), [ from Indurthi et al. (2019), and ◦ from Pino et al. (2019). ∗ shows results of a simple baseline model by combining a Wav2Vec2 module (Baevski et al., 2020) and a Transformer model, which could be viewed as the “no external data” version of Chimera. External Data Speech ASR MT Model ∗ EN-FR W2V2-T TCEN † LSTM ‡ AFS ◦ Multilingual ? Transformer ⊥ Curiculum ⊥ COSTT [ LUT ♦ STAST ] X × × × × × × × × × × × X × X X X × X X × × X × × × × X × × 6.4 17.1 17.0 17.2 17.6 17.7 18.0 18.2 18.3"
2021.findings-acl.195,P02-1040,0,0.109641,"plit from words, and normalized. Non-print punctuation is removed. The sentences are then tokenized with Moses tokenizer 5 . We filter out samples whose number of source or target tokens is over 250 and whose ratio of source and target text lengths is outside range [2/3, 3/2]. For sub-wording, we use a unigram sentencepiece6 model with a dictionary size of 10000. On each translation direction, The sentencepiece model is learned on all text data from both ST and MT corpora. The dictionary is shared across MT and ST and across source and target languages. The performance is evaluated with BLEU (Papineni et al., 2002) using sacreBLEU 7 . We average 5 https://github.com/mosessmt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl 6 https://github.com/google/sentencepiece 7 https://github.com/mjpost/sacrebleu, with configuration 2218 S.S. Projection Decoder EN-DE EN-FR MT Contrastive EN-DE EN-FR Fixed Fixed Fixed Fixed 25.6 24.3 24.2 23.8 35.0 34.3 33.4 33.1 X X × × X × X × 25.6 25.0 24.7 25.1 35.0 34.6 34.6 34.6 Table 4: Performance of Mem-16 Chimera when freezing different modules in fine-tuning. S.S. Projection is abbreviation for shared semantic projection. “Fixed” indicates that weights in this mo"
2021.findings-acl.195,2020.autosimtrans-1.2,0,0.0353742,"on the MuST-C benchmark and demonstrates its efficacy in learning modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al., 2019) and model adaptation (Di Gangi et al., 2020), have also been shown to be effective. Among these attempts, (Indurthi et al., 2019; Le et al., 2020; Liu et al., 2020) a"
2021.findings-acl.195,N19-1285,0,0.0281362,"Missing"
2021.findings-acl.195,L16-1147,0,0.0153501,"tively. Augmented LibriSpeech Dataset (En-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16"
2021.findings-acl.195,D17-1145,0,0.0950134,"a +1.9 BLEU margin. Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1 1 Introduction Speech-to-text translation (ST) takes speech input in a source language and outputs text utterance in a target language. It has many real-world applications, including automatic video captioning, simultaneous translation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has"
2021.findings-acl.195,2020.iwslt-1.8,0,0.0534939,"Missing"
2021.findings-acl.195,P19-1115,0,0.0277419,"Missing"
2021.findings-acl.195,W19-4305,0,0.0177039,"-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is"
2021.findings-acl.195,W18-3023,0,0.0162589,"n (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is the transcript sequence and yi is the translation sequence in the target language. As"
2021.findings-acl.195,2020.findings-emnlp.230,0,0.170183,"n-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16 Chimera × × × × × × X X X X × ×"
2021.findings-acl.195,2020.acl-main.148,0,0.0607551,"n-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16 Chimera × × × × × × X X X X × ×"
2021.findings-acl.195,P19-1649,0,0.0304663,"Missing"
2021.findings-acl.195,2020.acl-main.150,0,0.0133604,"errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is the transcript sequence and yi is the translation sequence in the target language. As a benefit of shared semantic projection, Chimera is able to leverage large-scale MT training corpo"
2021.findings-acl.195,W97-0400,0,0.769807,"Missing"
2021.findings-acl.195,2020.acl-main.344,0,0.130173,"ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce errors. However, it remains a challenge for end-to-end ST to catch up with their cascaded counterparts in performance. We argue that the root cause is the gap between the two modalities, speech and text. Although they both encode human languages, they are dissimilar in both coding attributes (pitch, volume, and intonation versus words, affixes, and punctuation) and length (thousands of time frames versus tens of words). This issue is further coupled with the rel"
2021.findings-acl.356,P06-1060,0,0.0910227,"nodes and edge types. “[s]” or [SEP] is a virtual edge type, representing the end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this"
2021.findings-acl.356,D10-1033,0,0.0154093,"Missing"
2021.findings-acl.356,C16-1239,0,0.0527608,"Missing"
2021.findings-acl.356,N07-1015,0,0.0256623,"e end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all pos"
2021.findings-acl.356,P17-1085,0,0.0397803,"Missing"
2021.findings-acl.356,2020.acl-main.521,0,0.0282496,"s fail to capture interrelations between relation types for different pairs of mentions. Another approach is to treat the joint information extraction task as a table filling problem (Zhang et al., 2017; Wang and Lu, 2020), and generate twodimensional tables with a Multi-Dimensional Recurrent Neural Network (Graves et al., 2007). This can capture interrelations among entities and relations, but the space complexity grows quadratically with respect to the length of the input text, making this approach impractical for long sequences. Some attempts, such as Seq2RDF (Liu et al., 2018) and IMoJIE (Kolluru et al., 2020), leverage the power of Seq2seq models (Cho et al., 2014) 4066 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4066–4078 August 1–6, 2021. ©2021 Association for Computational Linguistics to capture the interrelations among mentions and types with first-order complexity, but they all use a pre-defined vocabulary for mention prediction, which largely depends on the distribution of the target words and will not be able to handle unseen out-of-vocabulary words. To solve these problems, we propose a first-order approach that invertibly maps the target graph to an a"
2021.findings-acl.356,P14-1038,1,0.811113,"-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all possible entity pairs in a document, and the relation type is a null value for most of th"
2021.findings-acl.356,D14-1198,1,0.88614,"ties. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-ofthe-art on the joint entity and relation extraction task.1 1 Figure 1: We represent directed multigraphs as alternating sequences of nodes (blue) and edges (orange). Here, the graph is traversed by Breadth First Search (BFS) with an ascending ordering of nodes and edge types. “[s]” or [SEP] is a virtual edge type, representing the end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al.,"
2021.findings-acl.356,N19-1308,0,0.0184322,"elation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on Wadden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while O NEIE uses feature engineered templates; Moreover, O NEIE needs to do pairwise classification for relation extraction,"
2021.findings-acl.356,P16-1105,0,0.0213187,"(e.g., military bases) can be both locations and organizations, or facilities. 5 Related Work NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on Wadden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relati"
2021.findings-acl.356,D14-1200,0,0.0294044,"l part-of United Nations). Such mistakes could be avoided by consulting a knowledge base such as DBpedia (Bizer et al., 2009) or by performing entity linking. Inherent ambiguity Many examples have inherent ambiguity, e.g. European Union can be typed as organization or political entity, while some entities (e.g., military bases) can be both locations and organizations, or facilities. 5 Related Work NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relation"
2021.findings-acl.356,N19-4009,0,0.0289626,"to our hybrid span decoder during training. We set the maximum span length, m = 16, the hidden size of our model, dm = 256, and the number of the decoder blocks, N = 12. Even though theoretically the beamsearch should help us reduce the exposure bias, we do not observe any performance gain during grid search of the beam size and the length penalty on the validation set (detailed grid search setting is in Appendix A). Thus we set a vanilla beam size of 1 and the length penalty of 1, and leave this theoryexperiment contradiction for future research. Our model is built with the FAIRSEQ toolkit (Ott et al., 2019) for efficient distributed training and all the experiments are conducted on two NVIDIA TITAN X GPUs. 4 https://catalog.ldc.upenn.edu/ LDC2006T06 4072 IE Models PointerNet (Katiyar and Cardie, 2017) SpanRE (Dixit and Al-Onaizan, 2019) Dygie++ (Wadden et al., 2019) OneIE (Lin et al., 2020) TabSeq (Wang and Lu, 2020) HySPA (ours) w/ RoBERTa w/ ALBERT Space Complexity O(n) O(n) O(n) O(n) O(n2 ) Time Complexity O(n2 ) O(n2 ) O(n2 ) O(n2 ) O(n) O(n) O(n) NER 82.6 86.0 88.6 88.8 89.5 88.9 89.9 RE 55.9 62.8 63.4 67.5 67.6 68.2 68.0 Table 1: Joint NER and RE F1 scores of the IE models on the ACE05 tes"
2021.findings-acl.356,D14-1162,0,0.0850407,"U |] U ˆ = [Q1 , ..., Q|Q |] Q where ⊕ means the concatenation operator beˆ U ˆ, Q ˆ are the lists of the tween two lists, and R, type names in the sets R, U, Q, respectively (e.g. ˆ = [“Geopolitics”, “Person”, ...]). Note that the Q concatenation order between the lists of type names can be arbitrary as long as it is kept consistent throughout the whole model. Then, as in the embedding part of the table-sequence encoder (Wang and Lu, 2020), for each type, vi , we embed the label tokens of the types with the contextualized word embedding from a pre-trained language model, the GloVe embedding (Pennington et al., 2014) and the character embedding, E1 = ContextualizedEmbed(v), ∈ R E2 = GloveEmbed(v), ∈ R vi ∈ v, we take the average of these token vectors as the representation of vi and freeze its update during training. More details of the embedding pipeline can be found in Appendix A. This embedding pipeline is also used to embed the words in the input text, x. Unlike the pipeline for the type embedding, we represent the word as the contextualized embedding of its first sub-token from the pre-trained Language Model (LM, e.g. BERT (Devlin et al., 2018)), and finetune the LM in an end-to-end fashion. After ob"
2021.findings-acl.356,P11-1053,0,0.0335572,"e, representing the end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it nee"
2021.findings-acl.356,P15-1150,0,0.0265745,"done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on Wadden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while O NEIE uses feature engineered templates;"
2021.findings-acl.356,D19-1585,0,0.220401,"Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all possible entity pairs in a document, and the relation type is a null value for most of the cases due to the sparsity of relations between entities. Also, pairw"
2021.findings-acl.356,2020.emnlp-main.133,0,0.0335547,"Missing"
2021.findings-acl.356,D16-1137,0,0.0468413,"Missing"
2021.findings-acl.356,P18-1047,0,0.0123911,"adden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while O NEIE uses feature engineered templates; Moreover, O NEIE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities. While several Seq2Seq-based models (Zhang et al., 2020; Zeng et al., 2018, 2020; Wei et al., 2019; Zhang et al., 2019) have been proposed to generate triples (i.e., node-edge-node), our model is fundamentally different from them in that: (1) it is generating a BFS/DFS traversal of the target graph, which captures dependencies between nodes and edges and has a shorter target sequence, (2) we model the nodes as the spans in the text, which is independent of the vocabulary, so even if the tokens of the nodes are rare or unseen words, we can still generate spans on them based on the context information. 6 Conclusion In this work, we propose the Hybrid Span Generation ("
2021.findings-acl.356,D17-1182,0,0.0276987,"Missing"
2021.findings-acl.356,D19-1392,0,0.041141,"Missing"
2021.findings-emnlp.120,N19-1423,0,0.00892113,"to identify the relation type between each given pair of entities. Similar to entity classification, we also divide all the entity-entity relations into training, validation, and testing sets and use a two-layer feed-forward neural network for relation classification, and use the accuracy as the evaluation metric. There are totally 18 relation types in ACE-2005 dataset and 9 types in WikiEvents. 4.3 Baselines and Implementation Details any events, and the entity node embeddings are randomly initialized before training. To show the influence of incorporating the pretrained language model BERT (Devlin et al., 2019), in CompGCNBERT, we use the bert-base-uncased model checkpoint5 to initialize entity node embeddings before conducting message passing using CompGCN. For our event-enhanced models, EventKE represents our proposed event-enhanced knowledge graph embedding model. In addition, to show the effects of each individual part of our model, we also introduce two model variants for ablation study. EventKE-w/o-templink denotes the model without information aggregation on event-event temporal links. In EventKE-w/o-events, instead of using annotated events or event extraction results, we randomly initialize"
2021.findings-emnlp.120,Y14-1039,0,0.0231772,"to the city. Event Structure Ranking Change Conflict:Attack “combat ” Rank: PHYS:Located place attacker “Republican Guard” target attacker tar get “Israel” “Republican Guard” ? ORG-AFF Membership “Al-Rantissi” ? 1 “Baghdad” “Baghdad” Conflict:Attack “retaliated ” Israel retaliated on Hamas, namely Al-Rantissi, it missed him and killed civilians. Task Input Rank: 45 Rank: 3 “Hamas” “Al-Rantissi” “Hamas” Rank: 117 Table 4: Qualitative examples from the validation set demonstrating how events can help on KG completion. projection matrix decomposition (Ji et al., 2015, 2016), relational mappings (Fan et al., 2014), multiple relation projections (Do et al., 2018), Gaussian distributions (He et al., 2015; Xiao et al., 2016), neural networks (Yang et al., 2015; Trouillon et al., 2016), 2D convolutions (Dettmers et al., 2018; Jiang et al., 2019), and rotations (Sun et al., 2019). However, such translation-based methods model each knowledge triple individually without considering the global context for entities in the KG. To solve this problem, another line of work attempts to model the KG as a unified heterogeneous graph and use graph neural networks to capture the global interactions between entity nodes,"
2021.findings-emnlp.120,P16-1025,1,0.876602,"Missing"
2021.findings-emnlp.120,P18-1201,1,0.900238,"Missing"
2021.findings-emnlp.120,P15-1067,0,0.0137682,"can Guard, some of whom may have withdrawn into the city. Event Structure Ranking Change Conflict:Attack “combat ” Rank: PHYS:Located place attacker “Republican Guard” target attacker tar get “Israel” “Republican Guard” ? ORG-AFF Membership “Al-Rantissi” ? 1 “Baghdad” “Baghdad” Conflict:Attack “retaliated ” Israel retaliated on Hamas, namely Al-Rantissi, it missed him and killed civilians. Task Input Rank: 45 Rank: 3 “Hamas” “Al-Rantissi” “Hamas” Rank: 117 Table 4: Qualitative examples from the validation set demonstrating how events can help on KG completion. projection matrix decomposition (Ji et al., 2015, 2016), relational mappings (Fan et al., 2014), multiple relation projections (Do et al., 2018), Gaussian distributions (He et al., 2015; Xiao et al., 2016), neural networks (Yang et al., 2015; Trouillon et al., 2016), 2D convolutions (Dettmers et al., 2018; Jiang et al., 2019), and rotations (Sun et al., 2019). However, such translation-based methods model each knowledge triple individually without considering the global context for entities in the KG. To solve this problem, another line of work attempts to model the KG as a unified heterogeneous graph and use graph neural networks to captur"
2021.findings-emnlp.120,N19-1103,0,0.0242162,"ad” Conflict:Attack “retaliated ” Israel retaliated on Hamas, namely Al-Rantissi, it missed him and killed civilians. Task Input Rank: 45 Rank: 3 “Hamas” “Al-Rantissi” “Hamas” Rank: 117 Table 4: Qualitative examples from the validation set demonstrating how events can help on KG completion. projection matrix decomposition (Ji et al., 2015, 2016), relational mappings (Fan et al., 2014), multiple relation projections (Do et al., 2018), Gaussian distributions (He et al., 2015; Xiao et al., 2016), neural networks (Yang et al., 2015; Trouillon et al., 2016), 2D convolutions (Dettmers et al., 2018; Jiang et al., 2019), and rotations (Sun et al., 2019). However, such translation-based methods model each knowledge triple individually without considering the global context for entities in the KG. To solve this problem, another line of work attempts to model the KG as a unified heterogeneous graph and use graph neural networks to capture the global interactions between entity nodes, such as (Nickel et al., 2016; Schlichtkrull et al., 2018; Zhang et al., 2019; Vashishth et al., 2020; Hu et al., 2020; Shui and Karypis, 2020). Recent studies also focus on using additional information to improve KG embedding, such"
2021.findings-emnlp.120,2021.eacl-main.72,0,0.0240749,"rn low-dimensional stand a real-world entity, we usually consider not entity and relation embeddings that are informative only its static facts and properties, but more imporand scalable to use for many downstream applica- tantly, the dynamic events associated with the entity tions, such as information retrieval (Yang, 2020), as well. Based on this motivation, we hypothesize recommendation systems (Sun et al., 2020), ma- that events are essential for the model to underchine reading comprehension (Qiu et al., 2019), stand entities and relations more comprehensively and query-answering systems (Kacupaj et al., 2021; and improve the quality of KG embeddings, which 1 can be beneficial for multiple knowledge-related Data and source codes are made publicly available at https://github.com/zhangzx-uiuc/EventKE. downstream tasks. 1389 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1389–1400 November 7–11, 2021. ©2021 Association for Computational Linguistics In Information Extraction (IE), an event usually consists of an event trigger (a word or phrase that most directly indicates an event occurrence) and a set of event arguments (the entities participating in the event with diffe"
2021.findings-emnlp.120,2021.naacl-main.274,1,0.743449,"he most recent event-event temporal relation extraction system (Wen et al., 2021b) to obtain the event-event temporal relations. However, gold-standard event annotations for real-world KGs are usually expensive or even intractable. To demonstrate that our proposed model can also be applied to KGs without gold-standard event annotations, we choose another large-scale dataset, WikiEvents (Li et al., 2021), which contains news articles from Wikipedia references about complex events. We use the most recent crossdocument event extraction (Lin et al., 2020; Li et al., 2021), coreference resolution (Lai et al., 2021) and temporal tracking system (Wen et al., 2021a) to automatically extract events from the news articles. The statistics of the extracted KGs from the two datasets are shown in Table 1. X   L L viL , ri,j = − log σs CONV viL , ri,j , v+ X Experiments Dataset Entities Rels Events Args ACE-2005 7,376 7,441 3,071 5,587 WikiEvents 104,942 151,253 39,092 112,972 Table 1: The statistics of the extracted knowledge graphs from the two datasets, where Rels and Args denote the number of entity-entity relations and event argument links respectively.  ∈V − 3 The circular correlation φ(a, b) between"
2021.findings-emnlp.120,2020.emnlp-main.50,1,0.850286,"Missing"
2021.findings-emnlp.120,2021.naacl-main.69,1,0.713516,"G embeddings. We perform automatic entity linking (Wu et al., 2020) to link the entities to Wikidata (dumped in August 2019), and merge the entities with the same Wikidata entries into one single node. We use the most recent event-event temporal relation extraction system (Wen et al., 2021b) to obtain the event-event temporal relations. However, gold-standard event annotations for real-world KGs are usually expensive or even intractable. To demonstrate that our proposed model can also be applied to KGs without gold-standard event annotations, we choose another large-scale dataset, WikiEvents (Li et al., 2021), which contains news articles from Wikipedia references about complex events. We use the most recent crossdocument event extraction (Lin et al., 2020; Li et al., 2021), coreference resolution (Lai et al., 2021) and temporal tracking system (Wen et al., 2021a) to automatically extract events from the news articles. The statistics of the extracted KGs from the two datasets are shown in Table 1. X   L L viL , ri,j = − log σs CONV viL , ri,j , v+ X Experiments Dataset Entities Rels Events Args ACE-2005 7,376 7,441 3,071 5,587 WikiEvents 104,942 151,253 39,092 112,972 Table 1: The statistics of"
2021.findings-emnlp.120,2020.acl-main.713,1,0.768814,"ith the same Wikidata entries into one single node. We use the most recent event-event temporal relation extraction system (Wen et al., 2021b) to obtain the event-event temporal relations. However, gold-standard event annotations for real-world KGs are usually expensive or even intractable. To demonstrate that our proposed model can also be applied to KGs without gold-standard event annotations, we choose another large-scale dataset, WikiEvents (Li et al., 2021), which contains news articles from Wikipedia references about complex events. We use the most recent crossdocument event extraction (Lin et al., 2020; Li et al., 2021), coreference resolution (Lai et al., 2021) and temporal tracking system (Wen et al., 2021a) to automatically extract events from the news articles. The statistics of the extracted KGs from the two datasets are shown in Table 1. X   L L viL , ri,j = − log σs CONV viL , ri,j , v+ X Experiments Dataset Entities Rels Events Args ACE-2005 7,376 7,441 3,071 5,587 WikiEvents 104,942 151,253 39,092 112,972 Table 1: The statistics of the extracted knowledge graphs from the two datasets, where Rels and Args denote the number of entity-entity relations and event argument links respe"
2021.findings-emnlp.120,N19-1308,0,0.0502525,"Missing"
2021.findings-emnlp.120,2020.findings-emnlp.105,0,0.0431309,"2019). However, such translation-based methods model each knowledge triple individually without considering the global context for entities in the KG. To solve this problem, another line of work attempts to model the KG as a unified heterogeneous graph and use graph neural networks to capture the global interactions between entity nodes, such as (Nickel et al., 2016; Schlichtkrull et al., 2018; Zhang et al., 2019; Vashishth et al., 2020; Hu et al., 2020; Shui and Karypis, 2020). Recent studies also focus on using additional information to improve KG embedding, such as entity type information (Niu et al., 2020) and text information (Wang et al., 2021). Comparison and Discussion Our approach is highly related to the global event network embedding model (Zeng et al., 2021), however, there are a few essential differences. First, (Zeng et al., 2021) focuses on learning the embedding vectors for events to use for event-centric downstream tasks such as event type classification, argument extraction, and event coreference resolution. Our model focus on using event information to improve entity and relation understanding in knowledge graphs. For model design, (Zeng et al., 2021) treats the entity and event"
2021.findings-emnlp.120,D19-1602,0,0.028249,"graph representa- representations. If we think of how humans undertion learning, which aims to learn low-dimensional stand a real-world entity, we usually consider not entity and relation embeddings that are informative only its static facts and properties, but more imporand scalable to use for many downstream applica- tantly, the dynamic events associated with the entity tions, such as information retrieval (Yang, 2020), as well. Based on this motivation, we hypothesize recommendation systems (Sun et al., 2020), ma- that events are essential for the model to underchine reading comprehension (Qiu et al., 2019), stand entities and relations more comprehensively and query-answering systems (Kacupaj et al., 2021; and improve the quality of KG embeddings, which 1 can be beneficial for multiple knowledge-related Data and source codes are made publicly available at https://github.com/zhangzx-uiuc/EventKE. downstream tasks. 1389 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1389–1400 November 7–11, 2021. ©2021 Association for Computational Linguistics In Information Extraction (IE), an event usually consists of an event trigger (a word or phrase that most directly indicates"
2021.findings-emnlp.120,2020.acl-main.412,0,0.0404829,"and eventevent relations to fuse the event information into KG embeddings. Experimental results on real-world datasets demonstrate that events can greatly improve the quality of the KG embeddings on multiple downstream tasks.1 Event Arguments Relations Winning Championship league winning team Lakers team Event Temporal Relations before league lead player Kobe Bryant winner citizen U.S. located after Awarded for films award Oscar passenger Los Angeles place before after Air Crash A helicopter crashed aircraft Figure 1: An example of representing an eventenhanced KG as a heterogeneous network. Saxena et al., 2020). Typical KG embedding models, such as (Bordes et al., 2013; Dettmers et al., 2018; Sun et al., 2019), usually learn the model parameters by maximizing pre-defined score functions on ground-truth triples. One major limitation of such methods is that each knowledge triple is modeled locally and independently, without considering the global contextual information of KGs. To solve this problem, another line of approaches (Schlichtkrull et al., 2018; Vashishth et al., 2020) manages to model KGs as heterogeneous net1 Introduction works, and design message passing among entities Knowledge graph (KG)"
2021.findings-emnlp.120,D19-1585,0,0.0417824,"Missing"
2021.findings-emnlp.120,2021.textgraphs-1.5,1,0.743389,"is problem, another line of work attempts to model the KG as a unified heterogeneous graph and use graph neural networks to capture the global interactions between entity nodes, such as (Nickel et al., 2016; Schlichtkrull et al., 2018; Zhang et al., 2019; Vashishth et al., 2020; Hu et al., 2020; Shui and Karypis, 2020). Recent studies also focus on using additional information to improve KG embedding, such as entity type information (Niu et al., 2020) and text information (Wang et al., 2021). Comparison and Discussion Our approach is highly related to the global event network embedding model (Zeng et al., 2021), however, there are a few essential differences. First, (Zeng et al., 2021) focuses on learning the embedding vectors for events to use for event-centric downstream tasks such as event type classification, argument extraction, and event coreference resolution. Our model focus on using event information to improve entity and relation understanding in knowledge graphs. For model design, (Zeng et al., 2021) treats the entity and event nodes as a unified network, and adopts a relational GCN module to encode the entire network. On the contrary, we regard the entities and events as two separate set"
2021.findings-emnlp.120,2021.naacl-main.6,1,0.765523,"cal challenge of applying event information to KG embedding is the scarcity of highquality event annotations, since it is usually expensive to acquire manual event annotations that are relevant to KG entities. Fortunately, as the event extraction techniques have become mature recently, we can manage to use event extraction systems to extract our desired events from natural language texts with high quality. In this paper, in addition to the gold-standard event annotations such as those in the ACE-2005 dataset2 , we also use a state-of-theart cross-document event extraction and tracking system (Wen et al., 2021a) to obtain events from news articles. This shows that our proposed EventKE can also be widely applicable to traditional KG datasets without manually-labeled events. We evaluate our trained KG embeddings on three typical tasks: knowledge graph completion, entity classification and relation classification, and the results demonstrate that the event information greatly improves knowledge graph representations. Our contributions can be summarized as follows: • We propose to incorporate event information into KG representation learning, and design a novel and effective attention-based bipartite i"
2021.findings-emnlp.120,2020.emnlp-main.519,0,0.0319708,"riple (s, r, t), the score function can be computed by CONV(s, r, t) = f (vec (f ([s, r] ∗ ω)) W) t, where s and r denote the 2D reshaping for vectors and ∗ denotes the convolution operator. 4 4.1 v+ ∈V + − v− log 1 − L σs CONV viL , ri,j , v− Dataset We first evaluate our model on the Automatic Content Extraction (ACE 2005) dataset,4 which provides document-level entity, relation, and event annotations. We use this dataset because it has gold-standard event annotations, which is convenient to evaluate the impact of incorporating events into KG embeddings. We perform automatic entity linking (Wu et al., 2020) to link the entities to Wikidata (dumped in August 2019), and merge the entities with the same Wikidata entries into one single node. We use the most recent event-event temporal relation extraction system (Wen et al., 2021b) to obtain the event-event temporal relations. However, gold-standard event annotations for real-world KGs are usually expensive or even intractable. To demonstrate that our proposed model can also be applied to KGs without gold-standard event annotations, we choose another large-scale dataset, WikiEvents (Li et al., 2021), which contains news articles from Wikipedia refer"
2021.findings-emnlp.120,P16-1219,0,0.0610028,"Missing"
2021.findings-emnlp.120,P19-1522,0,0.0401597,"Missing"
2021.findings-emnlp.140,E17-1104,0,0.0440201,"Missing"
2021.findings-emnlp.140,P15-2049,0,0.0727123,"Missing"
2021.findings-emnlp.140,K19-1049,0,0.0213027,"ring this question by investigating the most basic aspects. Word Order Permutation We analyze whether BERT models fine-tuned for biomedical EL even consider one of the most fundamental properties of a sequence - the word order. In this probing experiment, we first train an EL model on the original (unshuffled) training set of a dataset. We then evaluate the model on the development set under the condition that the tokens of each mention/entityname are shuffled. General Approach A general approach to EL is to train an encoder ? that encodes mentions and entity names into the same vector space (Gillick et al., 2019) (Figure 1). Before inference, we use ? to pre-compute embeddings for all the entity names in the KB. During inference, mentions are also en- Attention Scope Restriction The self-attention coded by ? and entities are retrieved using a simple mechanism of BERT makes each token in the input distance function such as cosine similarity. In this directly interact with every other token (Vaswani 1632 et al., 2017). As a result, the attention operation is quadratic to the input length. To analyze whether direct connections between distant tokens are crucial for biomedical EL, we conduct experiments w"
2021.findings-emnlp.140,2020.findings-emnlp.114,0,0.0513517,"Missing"
2021.findings-emnlp.140,D17-1191,0,0.0413261,"Missing"
2021.findings-emnlp.140,P19-1356,0,0.0175723,"table for a wide range of real-world settings. ?(?) = SAPBERTCLS (?? ) ?(?? ) = SAPBERTCLS (??? ) ∀ ?? ∈  (1) where SAPBERTCLS returns the final hidden state corresponding to the [CLS] token. Since SAPBERT was pre-trained on almost 12M pairs of synonyms, it can be directly used without further fine-tuning on the target task’s training data. However, for several datasets, the performance can still be improved by training with task-specific supervision. 2.2 Probing Experiments Previous studies have shown that BERT can encode a wide range of syntactic and semantic features (Tenney et al., 2019; Jawahar et al., 2019). However, it is unknown to what extent existing BERT models for biomedical EL utilize such rich linguistic signals. We take the first step towards answering this question by investigating the most basic aspects. Word Order Permutation We analyze whether BERT models fine-tuned for biomedical EL even consider one of the most fundamental properties of a sequence - the word order. In this probing experiment, we first train an EL model on the original (unshuffled) training set of a dataset. We then evaluate the model on the development set under the condition that the tokens of each mention/entity"
2021.findings-emnlp.140,D14-1181,0,0.0456685,"d to every other token at the last layer. 2.3 ResCNN for Biomedical Entity Linking As to be discussed in Section 3, the performance of existing BERT models only changes slightly when the input word order is shuffled or when the attention scope is limited. These observations suggest that a simpler model that mainly focuses on capturing local interactions may perform as well as SOTA BERT-based models. A natural candidate that exhibits the desired properties is the convolutional neural network (CNN) architecture. CNNs have been empirically shown to be quite effective in capturing local features (Kim, 2014). Furthermore, CNNs typically use fewer parameters than Transformer-based models because of their sparse connectivity and weight sharing properties. To this end, we introduce a simple but effective CNN with residual connections (ResCNN) for biomedical EL. Given an input text (e.g., a query mention or an entity name), ResCNN computes a vector representation for the input through several layers. Figure 3: Encoding block of ResCNN. then transform each token into an initial vector representation by re-using the first embedding layer of PubMedBERT (Gu et al., 2020). This operation is very similar t"
2021.findings-emnlp.140,2021.naacl-main.274,1,0.745773,"s BERT-based SOTA systems on three of the datasets. It is worth noting that SAPBERT (Liu et al., 2020) was pre-trained on almost 12M pairs of UMLS synonyms. Without such pre-training, our lightweight models still match the performance of SAPBERT. Inference Time Table 4 shows the speed of various models on CPU and on GPU. Compared to SAPBERT, our model is about 3 to 4 times faster on GPU and about 15 to 20 times faster on CPU. It takes less time to run our model on CPU than running SAPBERT on GPU. These results demonstrate the efficiency of our proposed model. 4 et al., 2020; Wen et al., 2021; Lai et al., 2021a; Li et al., 2020). Acknowledgement This research is based upon work supported by the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, NSF No. 2034562, Agriculture and Food Research Initiative (AFRI) grant no. 202067021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, and U.S. DARPA KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied,"
2021.findings-emnlp.140,2021.acl-long.488,1,0.741944,"s BERT-based SOTA systems on three of the datasets. It is worth noting that SAPBERT (Liu et al., 2020) was pre-trained on almost 12M pairs of UMLS synonyms. Without such pre-training, our lightweight models still match the performance of SAPBERT. Inference Time Table 4 shows the speed of various models on CPU and on GPU. Compared to SAPBERT, our model is about 3 to 4 times faster on GPU and about 15 to 20 times faster on CPU. It takes less time to run our model on CPU than running SAPBERT on GPU. These results demonstrate the efficiency of our proposed model. 4 et al., 2020; Wen et al., 2021; Lai et al., 2021a; Li et al., 2020). Acknowledgement This research is based upon work supported by the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, NSF No. 2034562, Agriculture and Food Research Initiative (AFRI) grant no. 202067021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, and U.S. DARPA KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied,"
2021.findings-emnlp.140,2020.coling-main.56,1,0.819874,"Missing"
2021.findings-emnlp.140,2020.nlpcovid19-2.1,0,0.0376751,"he rated knowledge base (KB). For example, given the attention scope is restricted. Based on these obsersentence “The average NH3 concentrations were vations, we propose an effective convolutional neulow.”, the mention NH3 should be linked to the en- ral network with residual connections (ResCNN) tity KB:Ammonia. Biomedical EL is an important for the task. Because of the sparse connectivity research problem, with applications in many down- and weight sharing properties, ResCNN has a small stream tasks, such as biomedical question answer- number of parameters and is highly efficient. Expering (Lee et al., 2020), information retrieval, and iments on five datasets show that the performance of ResCNN is comparable to the state-of-the-art 1 The code is publicly available at https://github.com/ laituan245/rescnn_bioel (SOTA) BERT-based models while having about 1631 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1631–1639 November 7–11, 2021. ©2021 Association for Computational Linguistics Figure 2: Attention scope restriction. In this example, the window size of the limited attention head is 3. Figure 1: An illustration of the adopted approach to EL. In this example, the cl"
2021.findings-emnlp.140,2020.acl-main.713,1,0.818604,"Missing"
2021.findings-emnlp.140,2021.acl-short.72,0,0.0795343,"Missing"
2021.findings-emnlp.140,D14-1162,0,0.116461,"than Transformer-based models because of their sparse connectivity and weight sharing properties. To this end, we introduce a simple but effective CNN with residual connections (ResCNN) for biomedical EL. Given an input text (e.g., a query mention or an entity name), ResCNN computes a vector representation for the input through several layers. Figure 3: Encoding block of ResCNN. then transform each token into an initial vector representation by re-using the first embedding layer of PubMedBERT (Gu et al., 2020). This operation is very similar to using traditional word embeddings such as GloVe (Pennington et al., 2014), and so it can be carried out efficiently. We keep the embedding layer fixed and do not tune its parameters during training. An advantage of WordPiece tokenization is that a relatively small vocabulary (e.g., 30,000 wordpieces) is sufficient to model large, naturally-occurring corpora. In contrast, the vocabulary size of traditional word embeddings is typically much larger. Encoding Layer Our encoding layer consists of several encoding blocks (Figure 3). Each block has multiple convolutional filters of varying window sizes (Kim, 2014). Each filter is followed by an ReLU activation. We also em"
2021.findings-emnlp.140,P19-1317,0,0.0468182,"Missing"
2021.findings-emnlp.140,2021.emnlp-main.230,0,0.093768,"Missing"
2021.findings-emnlp.140,P19-1355,0,0.0298083,"the entity linking performance only models can achieve state-of-the-art results on many changes slightly when the input word order is biomedical EL datasets, they are computationally shuffled or when the attention scope is limexpensive and contain about 110M parameters. ited to a fixed window size. From these obEven though there are scientific labs that have a servations, we propose an efficient convolulot of computing resources, many researchers still tional neural network with residual connechave minimal access to large-scale computational tions for biomedical entity linking. Because power (Strubell et al., 2019). Therefore, it is of of the sparse connectivity and weight sharing properties, our model has a small number of practical importance to provide a more scalable soparameters and is highly efficient. On five publution for biomedical entity linking. Furthermore, lic datasets, our model achieves comparable the factors contributing to the success of these large or even better linking accuracy than the stateBERT-based models remain unclear. And thus, it of-the-art BERT-based models while having is not known whether the over-parameterization is about 60 times fewer parameters. 1 needed to achieve com"
2021.findings-emnlp.140,2020.acl-main.335,0,0.0194489,"igrams instead of unigrams. Therefore, SAPBERT is highly insensitive to word-order randomization. These results agree with recent studies on general-domain BERT models (Pham et al., 2020; Sinha et al., 2021). Table 1 also shows that the performance of SAPBERT only changes slightly when the attention scope is limited. Data and Experimental Setup We experiment across five different datasets: NCBI (Dogan et al., Probing Results (BIOSYN) We have also ex2014), BC5CDR-c and BC5CDR-d (Li et al., perimented with BERT models trained on the 2016), MedMentions (Mohan and Li, 2019), and BIOSYN framework (Sung et al., 2020). We diCOMETA (Basaldella et al., 2020). For each rectly use the trained BERT models downloaded dataset, we follow the data split by Liu et al. (2020). from https://github.com/dmis-lab/BioSyn. Table It is worth highlighting that even though the five 2 shows the results of our conducted probing exdatasets can all be categorized as “biomedical periments with BIOSYN. Note that the authors of datasets”, they have very different characteristics. BIOSYN only provided the trained checkpoints for For example, while MedMentions was constructed NCBI-d, BC5CDR-d, and BC5CDR-c. Overall, by annotating scie"
2021.findings-emnlp.140,P19-1452,0,0.0259311,"on is general and suitable for a wide range of real-world settings. ?(?) = SAPBERTCLS (?? ) ?(?? ) = SAPBERTCLS (??? ) ∀ ?? ∈  (1) where SAPBERTCLS returns the final hidden state corresponding to the [CLS] token. Since SAPBERT was pre-trained on almost 12M pairs of synonyms, it can be directly used without further fine-tuning on the target task’s training data. However, for several datasets, the performance can still be improved by training with task-specific supervision. 2.2 Probing Experiments Previous studies have shown that BERT can encode a wide range of syntactic and semantic features (Tenney et al., 2019; Jawahar et al., 2019). However, it is unknown to what extent existing BERT models for biomedical EL utilize such rich linguistic signals. We take the first step towards answering this question by investigating the most basic aspects. Word Order Permutation We analyze whether BERT models fine-tuned for biomedical EL even consider one of the most fundamental properties of a sequence - the word order. In this probing experiment, we first train an EL model on the original (unshuffled) training set of a dataset. We then evaluate the model on the development set under the condition that the tokens"
2021.findings-emnlp.140,2021.naacl-demos.16,1,0.657711,"s than the previous BERT-based SOTA systems on three of the datasets. It is worth noting that SAPBERT (Liu et al., 2020) was pre-trained on almost 12M pairs of UMLS synonyms. Without such pre-training, our lightweight models still match the performance of SAPBERT. Inference Time Table 4 shows the speed of various models on CPU and on GPU. Compared to SAPBERT, our model is about 3 to 4 times faster on GPU and about 15 to 20 times faster on CPU. It takes less time to run our model on CPU than running SAPBERT on GPU. These results demonstrate the efficiency of our proposed model. 4 et al., 2020; Wen et al., 2021; Lai et al., 2021a; Li et al., 2020). Acknowledgement This research is based upon work supported by the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, NSF No. 2034562, Agriculture and Food Research Initiative (AFRI) grant no. 202067021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, and U.S. DARPA KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either exp"
2021.findings-emnlp.140,2020.emnlp-main.519,0,0.0332616,"osest neighbor to the source mention is the entity name anilines. Therefore, this mention should be linked to the entity C0003038. 60 times fewer parameters. 2 Methods In the following sections, we will first describe some preliminaries relating to the formulation of the EL problem and a general approach for the task (Sec. 2.1). We will then go into details about our probing experiments in Sec. 2.2. We will describe the design of our ResCNN model in Sec. 2.3. 2.1 work, we adopt this general approach, because it is more efficient and simpler than the two-stage retrieval and re-ranking systems (Wu et al., 2020). Several recent SOTA methods for biomedical EL also follow this approach. For example, Liu et al. (2020) models ? using SAPBERT, a BERT model pretrained on UMLS synonyms: Preliminaries Problem Formulation Given an entity mention ? from a biomedical text and a knowledge base (KB) consisting of ? entities  = {?1 , ?2 , ..., ?? }, the task is to find the entity ?? ∈  that ? refers to. We assume that each entity is associated with a primary name and a list of alternative names. We denote the set of all names in the KB as  = {?1 , ?2 , ..., ?? }, where ? is the number of names. We use ?? and ??"
2021.findings-emnlp.140,2021.acl-long.489,1,0.819301,"Missing"
2021.findings-emnlp.8,P98-1013,0,0.556106,"(Miech et al., 2019, 2020; Chen et al., 2021). In the task such as weakly supervised grounding also tries to find a common space for text and visual where we can find the correct region given a text query (Akbari et al., 2019; Zhang et al., 2020; Gupta et al., 2020). These works usually learn in a weakly supervised We tackle this task in a two-stage manner: first 75 manner where human-annotated image/video caption pairs were given. In our multimodal event coreference resolution task, we try to learn in a self-supervised manner where only the video and its ASR were given. lies on the FrameNet (Baker et al., 1998) ontology derived from text which defines frames for each verb, along with semantic roles of arguments. Seminal work by Yatskar et al. (2016) introduced the SituNet dataset of images labeled with visual verbs and argument roles. Follow-up approaches have leveraged structured prediction mechanisms (Li et al., 2017; Suhail and Sigal, 2019) and attention (Cooray et al., 2020) to further improve performance on SituNet. Pratt et al. (2020) extend SituNet with bounding box annotations of event arguments and introduce a model for localizing event arguments in images. None of these target the video do"
2021.findings-emnlp.8,P18-1201,1,0.844818,"Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a sampling-based method for jointly extracting events and arguments. Other methods have attempted to leverage zero-shot learning (Huang et al., 2018) and weak supervision (Chen et al., 2017) to further improve performance on both event and event argument extraction. Multimodal Event Extraction. Some prior work has leveraged multimodal information for text-only event extraction. Zhang et al. (2017) propose a method which learns to transfer visual knowledge from multimodal resources to text-only documents to improve event extraction. Tong et al. (2020) supplement existing event detection benchmarks with image data and show significant performance gains by leveraging multimodal information for trigger disambiguation. Most relevant to our work"
2021.findings-emnlp.8,P17-1038,0,0.0177764,"al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a sampling-based method for jointly extracting events and arguments. Other methods have attempted to leverage zero-shot learning (Huang et al., 2018) and weak supervision (Chen et al., 2017) to further improve performance on both event and event argument extraction. Multimodal Event Extraction. Some prior work has leveraged multimodal information for text-only event extraction. Zhang et al. (2017) propose a method which learns to transfer visual knowledge from multimodal resources to text-only documents to improve event extraction. Tong et al. (2020) supplement existing event detection benchmarks with image data and show significant performance gains by leveraging multimodal information for trigger disambiguation. Most relevant to our work is Li et al. (2020)’s method which intro"
2021.findings-emnlp.8,P08-1030,1,0.649172,"o semantic role labeling dataset and task, where the target is to extract events and generate language description for arguments. Unlike Sadhu et al. (2021), we propose to extract multimodal events and localize arguments, where components in the extracted event frame may appear in either modality. Text Event Extraction. Recognizing and extracting events in text is an important information extraction problem that has been thoroughly studied. Both document-level (Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goa"
2021.findings-emnlp.8,P15-1017,0,0.0309621,"odality. Text Event Extraction. Recognizing and extracting events in text is an important information extraction problem that has been thoroughly studied. Both document-level (Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a samplin"
2021.findings-emnlp.8,2020.acl-main.230,1,0.748435,"Missing"
2021.findings-emnlp.8,2021.naacl-main.69,1,0.753108,"or perform multimodal event extraction. More related to our work is Sadhu et al. (2021), which introduces the video semantic role labeling dataset and task, where the target is to extract events and generate language description for arguments. Unlike Sadhu et al. (2021), we propose to extract multimodal events and localize arguments, where components in the extracted event frame may appear in either modality. Text Event Extraction. Recognizing and extracting events in text is an important information extraction problem that has been thoroughly studied. Both document-level (Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically le"
2021.findings-emnlp.8,N16-1034,0,0.0223154,"e components in the extracted event frame may appear in either modality. Text Event Extraction. Recognizing and extracting events in text is an important information extraction problem that has been thoroughly studied. Both document-level (Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument"
2021.findings-emnlp.8,2020.acl-main.713,1,0.144156,"mation extraction problem that has been thoroughly studied. Both document-level (Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a sampling-based method for jointly extracting events and arguments. Other methods have attempted to l"
2021.findings-emnlp.8,2020.emnlp-main.128,0,0.023036,"(Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a sampling-based method for jointly extracting events and arguments. Other methods have attempted to leverage zero-shot learning (Huang et al., 2018) and weak supervision (Chen et al"
2021.findings-emnlp.8,D19-1068,0,0.0364699,"Missing"
2021.findings-emnlp.8,P18-4009,0,0.0151401,"deo domain as we do or perform multimodal event extraction. More related to our work is Sadhu et al. (2021), which introduces the video semantic role labeling dataset and task, where the target is to extract events and generate language description for arguments. Unlike Sadhu et al. (2021), we propose to extract multimodal events and localize arguments, where components in the extracted event frame may appear in either modality. Text Event Extraction. Recognizing and extracting events in text is an important information extraction problem that has been thoroughly studied. Both document-level (Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) t"
2021.findings-emnlp.8,2020.aacl-main.21,0,0.0345878,"utional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a sampling-based method for jointly extracting events and arguments. Other methods have attempted to leverage zero-shot learning (Huang et al., 2018) and weak supervision (Chen et al., 2017) to further improve performance on both event and event argument extraction. Multimodal Event Extraction. Some prior work has leveraged multimodal information for text-only event extraction. Zhang et al. (2017) propose a method which learns to transfer visual knowledge from multimodal resources to text-only documents to improve event extraction. Tong et al. (2020) supplement existing event detectio"
2021.findings-emnlp.8,D19-1584,0,0.0119036,"roblem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a sampling-based method for jointly extracting events and arguments. Other methods have attempted to leverage zero-shot learning (Huang et al., 2018) and weak supervision (Chen et al., 2017) to further improve performance on both event and event argument extraction. Multimodal Event Extraction. Some prior work has leveraged multimodal information for text-only event extraction. Zhang et al. (2017) propose a method which learns to transfer"
2021.findings-emnlp.8,2021.naacl-main.4,1,0.745892,"ting events in text is an important information extraction problem that has been thoroughly studied. Both document-level (Yang et al., 2018; Li et al., 2021) and sentence-level (Zeng et al., 2018) methods have been proposed. Classic work by Ahn (2006) and Ji and Grishman (2008) leverage manually designed features for the task and formulate event extraction as a classification problem. More recent event extraction methods have leveraged neural models such as recurrent networks (Nguyen et al., 2016; Sha et al., 2018), convolutional networks (Chen et al., 2015), graph networks (Liu et al., 2019; Zhang and Ji, 2021), joint neural model (Lin et al., 2020a), conditioned generation (Li et al., 2021) and transformers (Liu et al., 2020) to automatically learn task-relevant features. A related line of work has focused on the problem of event argument extraction, where the goal is to predict event argument roles of entities in text to fill the roles of predicted event frames. Wang et al. (2019) propose a hierarchical event argument extraction model leveraging modular networks to exploit argument role concept correlation. Wang et al. (2020) propose a sampling-based method for jointly extracting events and argume"
2021.findings-emnlp.8,2020.emnlp-demos.6,0,0.0837239,"Missing"
2021.naacl-demos.16,D14-1148,0,0.0290568,". We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very fe"
2021.naacl-demos.16,C16-1201,0,0.0607333,"Missing"
2021.naacl-demos.16,N13-1073,0,0.043681,"luster, we apply these patterns as highprecision patterns before two statistical temporal ordering models separately. The schema matching algorithm will select the best matching from two graphs as the final instantiated schema results. Because the annotation for non-English data can be expensive and time-consuming, the temporal event tracking component has only been trained on English input. To extend the temporal event tracking capability to cross-lingual setting, we apply Google Cloud neural machine translation 6 to translate Spanish documents into English and apply the FastAlign algorithm (Dyer et al., 2013) to obtain word alignment. 2.6 Cross-media Information Grounding and Fusion Visual event and argument role extraction: Our goal is to extract visual events along with their argument roles from visual data, i.e., images and videos. In order to train event extractor from visual data, we have collected a new dataset called Video M2E2 which contains 1,500 video-article pairs by searching over YouTube news channels using 18 event primitives related to visual concepts as search keywords. We have extensively annotated the the videos and sampled key frames for annotating bounding boxes of argument rol"
2021.naacl-demos.16,2020.acl-main.718,0,0.068856,"Missing"
2021.naacl-demos.16,D19-5102,0,0.0208286,"arch purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Pr"
2021.naacl-demos.16,glavas-etal-2014-hieve,0,0.0312373,"l cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: ht"
2021.naacl-demos.16,R13-2011,0,0.025761,"ross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeli"
2021.naacl-demos.16,D19-1041,0,0.0480812,"Missing"
2021.naacl-demos.16,N19-1085,0,0.0191037,"oend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE sy"
2021.naacl-demos.16,W18-3101,0,0.0195422,"ly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who di"
2021.naacl-demos.16,N16-1056,0,0.030098,"nologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of even"
2021.naacl-demos.16,W18-5620,0,0.0245812,"ts described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but the current, first-generati"
2021.naacl-demos.16,L16-1545,0,0.0202604,"rstand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but th"
2021.naacl-demos.16,2021.naacl-main.274,1,0.542222,"e postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents, we create N (N2−1) pairs of documents and treat each pair as a single “mega-document”. We apply our model to each mega-document and, at the end, aggregate the predictions across all megadocuments to extract the coreference clusters. Finally, we also apply a simple heuristic rule that prevents two entity mentions from being merged together if they are linked to different entities with high confidence. Our event coreference resolution method (Lai et al., 2021) is similar to entity coreference resolution, while incorporating additional symbolic features such as the event type information. If the input documents are all about one specific complex event, we apply some schema-guided heuristic rules to further refine the predictions of the neural event coreference resolution model. For example, in a bombing schema, there is typically only one bombing event. Therefore, in a document cluster, if there are two event mentions of type bombing and they have several arguments in common, these two mentions will be considered as coreferential. 2.5 Cross-document"
2021.naacl-demos.16,D17-1018,0,0.0120578,"5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that app"
2021.naacl-demos.16,2020.acl-main.703,0,0.0204692,"racted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments. Our model is based on BART (Lewis et al., 2020), which is an encoder-decoder language model. To utilize the encoder-decoder LM for argument extraction, we construct an input sequence of hsi template hsih/sidocument h/si. All argument names (arg1, arg2 etc.) in the template are replaced by a special placeholder token hargi. This model is trained in an end-to-end fashion by directly optimizing the generation probability. To align the extracted arguments back to the document, we adopt a simple postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents"
2021.naacl-demos.16,N19-4019,1,0.796479,"ence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide ra"
2021.naacl-demos.16,2020.acl-demos.11,1,0.922974,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2020.emnlp-main.50,1,0.880674,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2021.naacl-main.69,1,0.764758,"chemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments."
2021.naacl-demos.16,2020.acl-main.713,1,0.86722,"und the extracted knowledge elements onto our extracted graph via cross-media event coreference resolution (Section 2.6). Finally, our system selects the schema from a schema repository that best matches the extracted IE graph and merges these two graphs (Section 2.7). Our system can extract 24 types of entities, 46 types of relations and 67 types of events as defined in the DARPA KAIROS3 ontology. times for each detected words, as well as potential alternative transcriptions. Then from the speech recognition results and text input, we extract entity, relation, and event mentions using OneIE (Lin et al., 2020), a stateof-the-art joint neural model for sentence-level information extraction. Given a sentence, the goal of this module is to extract an information graph G = (V, E), where V is the node set containing entity mentions and event triggers and E is the edge set containing entity relations and event-argument links. We use a pre-trained BERT encoder (Devlin et al., 2018) to obtain contextualized word representations for the input sentence. Next, we adopt separate conditional random field-based taggers to identify entity mention and event trigger spans from the sentence. We represent each span,"
2021.naacl-demos.16,2021.ccl-1.108,0,0.0312676,"Missing"
2021.naacl-demos.16,L16-1631,0,0.0207744,"ences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolutio"
2021.naacl-demos.16,P17-1009,0,0.0621125,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.253,0,0.0765544,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.344,0,0.0324985,"match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columb"
2021.naacl-demos.16,D17-1108,1,0.83481,"on. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolution by grounding the extracted visual poral attributes through shared arguments. Furtherentities to text. We are the first to perform cross- more, we take advantage of the schema repository media joint event extraction and coreference reso- knowledge by using the frequent temporal"
2021.naacl-demos.16,P18-1212,1,0.895436,"Missing"
2021.naacl-demos.16,D19-1642,1,0.844444,"Missing"
2021.naacl-demos.16,P18-1122,1,0.846572,"oss-document Temporal Event Ordering Based on the event coreference resolution component described above, we group all mentions into clusters. Next we aim to order events along a timeline. We follow Zhou et al. (2020) to design a component for temporal event ordering. Specifically, we further pre-train a T5 model (Raffel et al., 2020) with distant temporal ordering supervision signals. These signals are acquired through two set of syntactic patterns: 1) before/after keywords in text and 2) explicit date and time mentions. We take such a pre-trained temporal T5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time"
2021.naacl-demos.16,P17-1178,1,0.849941,"he audio signal. It cific global feature. We compute the global feature returns the transcription with starting and ending score as uf , where u is a learnable weight vec3 https://www.darpa.mil/program/knowledge-di tor. Finally, we use a beam search-based decoder rected-artificial-intelligence-reasoning-overto generate the information graph with the highest schemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple argument"
2021.naacl-demos.16,W15-0812,0,0.0200332,"11 139 1,213 Videos 31 Table 4: Data statistics for schema matching corpus (LDC2020E39). Schema-guided Information Extraction. The Category Extracted Schema Instantiated Events Steps Steps performance of each component is shown in Table 3. We evaluate the end-to-end perfor# 3,180 1,738 958 mance of our system on a complex event corTable 5: Results of schema matching. pus (LDC2020E39), which contains multi-lingual multi-media document clusters. The data statistics are shown in Table 4. We train our mention ex3.3 Qualitative Analysis traction component on ACE 2005 (Walker et al., 2006) and ERE (Song et al., 2015); document- Figure 2 illustrates a subset of examples for the best matched results from our end-to-end system. We level argument exraction on ACE 2005 (Walker 7 et al., 2006) and RAMS (Ebner et al., 2020); corefLDC2017E03 8 erence component on ACE 2005 (Walker et al., LDC2017E52 137 Extracted Graph Old Bailey A court in British legal history Max Hill Manchester Communicator Place JudgeCourt JudgeCourt Place JudgeCourt Broadcast ReleaseParole Resident ... ... ChargeIndict TrialHearing Defendant Sentence ArrestJailDetain Defendant Defendant Detainee Defendant ReleaseParole Defendant Salman Abedi"
2021.naacl-demos.16,W02-2024,0,0.567788,"Missing"
2021.naacl-demos.16,D19-1585,0,0.0252013,".. ... ChargeIndict TrialHearing Defendant Defendant Sentence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et"
2021.naacl-demos.16,W12-4501,0,0.0999034,"Missing"
2021.naacl-demos.16,2021.naacl-main.6,1,0.690677,"Missing"
2021.naacl-demos.16,P18-4009,0,0.0278418,"lized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a compl"
2021.naacl-demos.16,N18-5009,1,0.831121,"Missing"
2021.naacl-demos.8,W19-1909,0,0.0217165,"Missing"
2021.naacl-demos.8,2020.emnlp-demos.18,0,0.241329,"ung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated"
2021.naacl-demos.8,D19-1371,0,0.0246983,"89178)] To address the role of angiotensin in lung injury, there is an ongoing clinical trial to examine whether losartan treatment affects outcomes in COVID-19 associated ARDS (NCT04312009). [PMID:32439915 (PMC7242178)] Losartan was also the molecule chosen in two trials recently started in the United States by the University of Minnesota to treat patients with COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from te"
2021.naacl-demos.8,W17-2307,0,0.0224213,"posing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated with Evidence amlodipine and benazepril, and chronic back pain. Sentences [PMID:32081428 (PMC7092824)] On the other hand, many ACE inhibitors are cu"
2021.naacl-demos.8,W13-2001,0,0.0375856,"sults, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Disease"
2021.naacl-demos.8,N19-1145,1,0.895006,"Missing"
2021.naacl-demos.8,Q17-1008,0,0.0252127,"s Factor, and Interleukin-10. We see all of these connections in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many l"
2021.naacl-demos.8,2020.bionlp-1.22,0,0.0299439,"Missing"
2021.naacl-demos.8,D19-6204,1,0.833702,"ctions in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and"
2021.naacl-demos.8,2020.acl-demos.11,1,0.877148,"2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., biomedical scientists) to identify specific drug candidates and molecular targets that are relevant in their biomedical and clinical research aims. The use of our know"
2021.naacl-demos.8,W19-5006,0,0.0162201,"ults, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19"
2021.naacl-demos.8,2020.acl-main.230,1,0.885325,"Missing"
2021.naacl-demos.8,D19-1410,0,0.0136688,"nowledge elements in each path in the KG. Each edge is assigned a salience score by aggregating the scores of paths passing through it. In addition to knowledge elements, we also present related sentences and source information as evidence. We use BioBert (Lee et al., 2020), a pre-trained language model to represent each sentence along with its left and right neighboring sentences as local contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evid"
2021.naacl-demos.8,2020.bionlp-1.21,0,0.0201708,"igation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease"
2021.naacl-demos.8,D18-1230,1,0.819154,"Missing"
2021.naacl-demos.8,2020.bionlp-1.3,0,0.0196518,"h COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., bio"
2021.naacl-demos.8,W16-3104,0,0.0689349,"Missing"
2021.naacl-demos.8,P19-1191,1,0.889692,"Missing"
2021.naacl-demos.8,C14-1149,1,0.831567,"Missing"
2021.naacl-demos.8,2020.acl-demos.8,1,0.900567,"cal contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evidence from a background corpora of COVID-19. 4 4.1 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 CATB-10270 CATB-1418 CATB-1674 CATB-16A CATB-16D2 CATB-1852 CATB1874 CATB-2744 CATB-3098 CATB-348 CATB-3483 CATB-5880 CATB-84 CATB912 CATD CATHY CATK"
2021.naacl-main.112,N18-1150,0,0.0287353,"need for handling long documents via abstractive summarization. To that end, extract-then-abstract methods are proposed. For example, Pilault et al. (2020) first extract relevant sentences and then rewrite them into paper abstracts. Our work is in line with building end-to-end abstractive summarization models for long input. Cohan et al. (2018) design a hierarchical encoder to read different sections separately, and then use combined attentions over words and sections to generate the summary. Multiple agents are created to read segments separately, and then collaboratively write an abstract (Celikyilmaz et al., 2018). However, both work truncates articles to 2K words. Although efficient encoder attentions have been studied in Zaheer et al. (2020) for abstractive summarization, at most 3K tokens can be consumed by their models. Our H EPOS encoderdecoder attention are able to process more than 10K tokens, significantly improving summary informativeness and faithfulness. 8 Conclusion posed APESsrc being the stronger of the two. After inspection, we find that human-written summaries contain paraphrases or acronyms that APES cannot capture via strict lexical matching. For instance, for the question “Diabetes m"
2021.naacl-main.112,W19-4828,0,0.0312332,""" and “care""). 2019), which share a similar theoretical foundation as global tokens; and kernel methods over attentions require training models from scratch (Choromanski et al., 2020; Katharopoulos et al., 2020). 3 Encoder-decoder Attention with Head-wise Positional Strides (Hepos) The efficient design of encoder-decoder attentions with head-wise positional strides (H EPOS) allows models to consume longer sequences. Concretely, our design is motivated by two observations: (1) Attention heads are redundant (Voita et al., 2019). (2) Any individual head rarely attends to several tokens in a row (Clark et al., 2019). Therefore, as illustrated in Fig. 1, H EPOS uses separate encoderdecoder heads on the same layer to cover different subsets of source tokens at fixed intervals. Each head starts at a different position, and all heads collectively attend to the full sequence. Given a stride size of sh , for the h-th head, its attention value between decoder query qj (at step j) and encoder key vector ki (for the i-th input token) can be formulated as: ( ahji = softmax(qj ki ), 0 if (i − h) mod sh = 0 otherwise (1) In H EPOS attention, each query token attends to n/sh tokens per head, yielding a memory complex"
2021.naacl-main.112,N19-4009,0,0.0438152,"Missing"
2021.naacl-main.112,2020.acl-main.703,0,0.62286,"with fewer unfaithful errors. 1 Introduction Long documents, such as scientific papers and government reports, often discuss substantial issues at length, and thus are time-consuming to read, let alone to comprehend. Generating abstractive summaries can help readers quickly grasp the main topics, yet prior work has mostly focused on short texts (containing hundreds of words), e.g., news articles (Gehrmann et al., 2018; Liu and Lapata, 2019; Zhang et al., 2019). Model training efficiency and summary quality present a pair of challenges for long document summarization. State-of-the-art systems (Lewis et al., 2020; Zhang et al., 2019) are built upon Transformer (Vaswani et al., 2017), which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents 1 . Solutions have been proposed to reduce 1 For instance, to fine-tune BART on documents of 10K the calculation of encoder self-attentions (Wang et al., 2020c; Zaheer et al., 2020) by selectively attending to neighboring tokens (Beltagy et al., 2020; Child et al., 2019) or relevant words (Kitaev et al., 2020; Tay et al., 2020a). Yet, these methods do not apply"
2021.naacl-main.112,W04-1013,0,0.20411,"new entific publications; B ILL S UM (Kornilova and Eisummary-worthy bigrams appear in the later half delman, 2019), a collection of congressional bills; of the articles, showing a more even distribution. and B IG PATENT (Sharma et al., 2019), a corpus of A similar trend is observed on unigrams. However, 4 B ILL S UM has the shortest documents among the www.gao.gov 5 crsreports.congress.gov five datasets. 1422 5 Summary Evaluation with Cloze QA This work aims to evaluate whether processing more text improves both informativeness and faithfulness of abstractive summaries. In addition to ROUGE (Lin, 2004) and human evaluation, we extend existing QA-based metric (Eyal et al., 2019) and consider an entailment-based scorer. Entailment-based Evaluation. We further consider FactCC (Kryscinski et al., 2020), which evaluates factual consistency of a system summary by predicting an entailment score between the source and the summary. We reproduce their method on our datasets. Additional details for implementing the evaluation models and the entity extraction models are given in Appendix B. QA-based Evaluation. We present a new faithfulness evaluation metric by extending the APES score (Eyal et al., 20"
2021.naacl-main.112,2020.acl-main.713,1,0.773036,"Missing"
2021.naacl-main.112,D19-1387,0,0.0238134,"her ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors. 1 Introduction Long documents, such as scientific papers and government reports, often discuss substantial issues at length, and thus are time-consuming to read, let alone to comprehend. Generating abstractive summaries can help readers quickly grasp the main topics, yet prior work has mostly focused on short texts (containing hundreds of words), e.g., news articles (Gehrmann et al., 2018; Liu and Lapata, 2019; Zhang et al., 2019). Model training efficiency and summary quality present a pair of challenges for long document summarization. State-of-the-art systems (Lewis et al., 2020; Zhang et al., 2019) are built upon Transformer (Vaswani et al., 2017), which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents 1 . Solutions have been proposed to reduce 1 For instance, to fine-tune BART on documents of 10K the calculation of encoder self-attentions (Wang et al., 2020c; Zaheer et al., 2020) by sel"
2021.naacl-main.112,2020.emnlp-main.748,0,0.348369,"both GovReport and Informativeness PubMed, as shown in Table 3. Within learnable patterns, Sinkhorn attention consistently obtains better We investigate whether processing more words generates more informative summaries. ROUGE scores. Moreover, combining techniques in fixed patterns is more effective than simply us- Comparisons include recent top-performing abing window-based sparse attentions, though with stractive models: PEGASUS (Zhang et al., 2019), an increased memory cost. a large pre-trained summarization model with For encoder-decoder attentions, H EPOS consis- truncated inputs; TLM (Pilault et al., 2020), tently yields higher ROUGE scores than Linformer DANCER (Gidiotis and Tsoumakas, 2020), and on both datasets, using either full or Sinkhorn en- SEAL (Zhao et al., 2020), all of them using hybrid coder. Notably, coupled with a Sinkhorn attention, extract-then-abstract methods; and B IG B IRD (Zaour model’s performance matches the variant using heer et al., 2020), which combines sliding window, 1424 Results. Overall, models that read more text obtain higher ROUGE scores, according to results on GovReport and PubMed in Table 4. First, different encoder variants with full encoder-decoder attenti"
2021.naacl-main.112,C08-1087,0,0.075765,"nd human evaluation. We further show that our new cloze QA metric better correlates with human judgment than prior faithfulness evaluation metrics. 7 Acknowledgements Additional Related Work This research is supported in part by Oracle for Summarizing long inputs has been investigated in Research Cloud Credits, National Science Foundamany domains, including books (Mihalcea and Ceylan, 2007), patents (Trappey et al., 2009), tion through Grant IIS-1813341, and by the Office movie scripts (Gorinski and Lapata, 2015), and sci- of the Director of National Intelligence (ODNI), entific publications (Qazvinian and Radev, 2008). Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The However, the datasets are often too small to train views and conclusions contained herein are those neural models. Cohan et al. (2018) publish two of the authors and should not be interpreted as large-scale datasets by collecting articles from necessarily representing the official policies, either AR X IV and P UB M ED . Popular methods rely on extractive summarizers that identify salient sen- expressed or implied, of ODNI, IARPA, or the U.S. tences based on positional information (Dong et al., Gov"
2021.naacl-main.112,P19-1212,1,0.831188,"Missing"
2021.naacl-main.112,P19-1032,0,0.0286764,"exities and numbers of newly learned parameters in Table 1. 2.1 Fixed Patterns Fixed patterns are used to limit the scope of attentions. In our experiments, in addition to windowbased attentions, we also combine them with global tokens, stride patterns, or random attentions. Sliding window attentions (Beltagy et al., 2020) aim to capture the local context, which is critical for language understanding (Liu* et al., 2018; Child et al., 2019). Concretely, each query token attends to w/2 neighboring tokens on both left and right, yielding a memory complexity of O(nw). Adaptive span is proposed by Sukhbaatar et al. (2019) to learn attention windows at different layers. This is implemented by learning a masking function for each head independently. In practice, the adaptive span attention has a complexity of O(nw), ˆ where w ˆ is the maximum values of predicted spans for all heads. Besides, it introduces O(1) new parameters for learning spans. Transformer models are built upon multi-head attentions in multiple layers. The attention is calcuT √ lated as Attention(Q, K, V) = softmax( QK )V, dk where Q, K, and V are query, key, and value matrices, each consisting of n vectors for a document Global tokens (Beltagy"
2021.naacl-main.112,P19-1580,0,0.0307304,"(“Job"" and “home"") in the input, heads 2 and 4 look at the second and fourth words (“in"" and “care""). 2019), which share a similar theoretical foundation as global tokens; and kernel methods over attentions require training models from scratch (Choromanski et al., 2020; Katharopoulos et al., 2020). 3 Encoder-decoder Attention with Head-wise Positional Strides (Hepos) The efficient design of encoder-decoder attentions with head-wise positional strides (H EPOS) allows models to consume longer sequences. Concretely, our design is motivated by two observations: (1) Attention heads are redundant (Voita et al., 2019). (2) Any individual head rarely attends to several tokens in a row (Clark et al., 2019). Therefore, as illustrated in Fig. 1, H EPOS uses separate encoderdecoder heads on the same layer to cover different subsets of source tokens at fixed intervals. Each head starts at a different position, and all heads collectively attend to the full sequence. Given a stride size of sh , for the h-th head, its attention value between decoder query qj (at step j) and encoder key vector ki (for the i-th input token) can be formulated as: ( ahji = softmax(qj ki ), 0 if (i − h) mod sh = 0 otherwise (1) In H EPO"
2021.naacl-main.112,2020.acl-main.450,0,0.354432,"es (Gehrmann et al., 2018; Liu and Lapata, 2019; Zhang et al., 2019). Model training efficiency and summary quality present a pair of challenges for long document summarization. State-of-the-art systems (Lewis et al., 2020; Zhang et al., 2019) are built upon Transformer (Vaswani et al., 2017), which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents 1 . Solutions have been proposed to reduce 1 For instance, to fine-tune BART on documents of 10K the calculation of encoder self-attentions (Wang et al., 2020c; Zaheer et al., 2020) by selectively attending to neighboring tokens (Beltagy et al., 2020; Child et al., 2019) or relevant words (Kitaev et al., 2020; Tay et al., 2020a). Yet, these methods do not apply to encoder-decoder attentions in summarization models since they collaborate and dynamically pinpoint salient content in the source as the summary is decoded. Truncation is commonly used to circumvent the issue. However, training on curtailed content further aggravates “hallucination” in existing abstractive models (Maynez et al., 2020). We argue that summarizing long documents (e.g., with t"
2021.naacl-main.112,D19-1298,0,0.144343,"Missing"
2021.naacl-main.274,P15-1017,0,0.0851188,"Missing"
2021.naacl-main.274,W09-3208,1,0.881814,"rly describes the event, vir- not actually happen (i.e., its modality attribute is tually all previous approaches employ features re- OTHER). Therefore, our model should be able to avoid the mistake if it utilizes additional symbolic lated to event triggers in one form or another. To features such as the modality attribute in this case. achieve better performance, many methods also There are several previous methods that use conneed to use a variety of additional symbolic featextual embeddings together with type-based or tures such as event types, attributes, and arguments (Chen et al., 2009; Chen and Ji, 2009; Zhang et al., argument-based information (Lu et al., 2020; Yu 2015; Sammons et al., 2015; Lu and Ng, 2016; et al., 2020). For example, Lu et al. (2020) proposes a new mechanism to better exploit event type inChen and Ng, 2016; Duncan et al., 2017). Previous formation for coreference resolution. Despite their neural methods (Nguyen et al., 2016; Choubey and Huang, 2017; Huang et al., 2019) also use non- impressive performance, these methods are specific to one particular type of additional information. contextual word embeddings such as word2vec 1 In this paper, we propose general and effecti"
2021.naacl-main.274,D19-1610,1,0.829458,"t straightforward way to build the final pair representation fij of mi and mj is to simply concatenate the trigger-based representation and all the feature-based representations together: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (4) However, this approach is not always optimal. First, as the symbolic features are predicted, they can be noisy and contain errors. The performance of most symbolic feature predictors is far from perfect (Table 2). Also, depending on the specific context, some features can be more useful than others. Inspired by studies on gated modules (Lin et al., 2019; Lai et al., 2019), we propose ContextDependent Gated Module (CDGM), which uses a gating mechanism to extract information from the input symbolic features selectively (Figure 1). Given two mentions mi and mj , we use their trigger feature vector tij as the main controlling con(u) ti = ei X j=si xj ei − si + 1 text to compute the filtered representation hij : (1) (u) hij 3492 (u)  = CDGM(u) tij , hij (5) Figure 1: Overall architecture of our mention-pair encoder, which uses CDGMs to incorporate symbolic features. where u ∈ {1, 2, . . . , K}. More specifically: (u) (u)  (u)  gij = σ FFNNg tij , hij (u) (u) ("
2021.naacl-main.274,D17-1018,0,0.0441632,"ersa. Finally, after using CDGMs to distill symbolic features, the final pair representation fij of mi and mj can be computed as follows: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (8) And the coreference score s(i, j) of mi and mj is: s(i, j) = FFNNa (fij ) where FFNNa is a mapping from R(K+1)×p → R. 2.4 Training and Inference Algorithm 1: Noise Addition for Symbolic Features Input: Document D Hyperparameters: {1 , 2 , · · · , K } for i = 1 . . . k do for u = 1 . . . K do (u) With prob. u , replace ci by (u) cˆi ∼ Uniform(Nu ) end end Training We use the same loss function as in (Lee et al., 2017). Also, notice that the training accuracy of a feature predictor is typically much higher than its accuracy on the dev/test set (Table 2). If we simply train our model without any regularization, our CDGMs will rarely come across noisy symbolic features during training. Therefore, to encourage our CDGMs to actually learn to distill reliable signals, we also propose a simple but effective noisy training method. Before passing a training data batch to the model, we randomly add noise to the predicted features. More specifically, for each document D in the batch, we go through every symbolic feat"
2021.naacl-main.274,2020.acl-main.713,1,0.626665,"d indices of its trigger by si and ei respectively. We assume the mentions are ordered based on si (i.e., If i ≤ j then si ≤ sj ). We also assume each mi has K (predicted) cat(1) (2) (K) egorical features {ci , ci , . . . , ci }, with each (u) ci ∈ {1, 2, . . . , Nu } taking one of Nu different discrete values. Table 2 lists the symbolic features we consider in this work. The definitions of the features and their possible values are in ACE and Rich ERE guidelines (LDC, 2005; Mitamura et al., 2016). The accuracy scores of the symbolic feature predictors are also shown in Table 2. We use OneIE (Lin et al., 2020) to identify event mentions along with their subtypes. For other symbolic features, we train a joint classification model based on SpanBERT. The appendix contains more details. 2.2 Dataset Single-Mention Encoder Given a document D, our model first forms a contextualized representation for each input token using a Transformer encoder (Joshi et al., 2020). Let X = (x1 , ..., xn ) be the output of the encoder, where xi ∈ Rd . Then, for each mention mi , its trigger’s representation ti is defined as the average of its token embeddings: where FFNNt is a feedforward network mapping from R3×d → Rp ,"
2021.naacl-main.274,P19-1016,1,0.846259,"Rp . Now, the most straightforward way to build the final pair representation fij of mi and mj is to simply concatenate the trigger-based representation and all the feature-based representations together: (1) (2) (K) fij = [tij , hij , hij , . . . , hij ] (4) However, this approach is not always optimal. First, as the symbolic features are predicted, they can be noisy and contain errors. The performance of most symbolic feature predictors is far from perfect (Table 2). Also, depending on the specific context, some features can be more useful than others. Inspired by studies on gated modules (Lin et al., 2019; Lai et al., 2019), we propose ContextDependent Gated Module (CDGM), which uses a gating mechanism to extract information from the input symbolic features selectively (Figure 1). Given two mentions mi and mj , we use their trigger feature vector tij as the main controlling con(u) ti = ei X j=si xj ei − si + 1 text to compute the filtered representation hij : (1) (u) hij 3492 (u)  = CDGM(u) tij , hij (5) Figure 1: Overall architecture of our mention-pair encoder, which uses CDGMs to incorporate symbolic features. where u ∈ {1, 2, . . . , K}. More specifically: (u) (u)  (u)  gij = σ FFNNg"
2021.naacl-main.274,L16-1631,0,0.207491,"ches employ features re- OTHER). Therefore, our model should be able to avoid the mistake if it utilizes additional symbolic lated to event triggers in one form or another. To features such as the modality attribute in this case. achieve better performance, many methods also There are several previous methods that use conneed to use a variety of additional symbolic featextual embeddings together with type-based or tures such as event types, attributes, and arguments (Chen et al., 2009; Chen and Ji, 2009; Zhang et al., argument-based information (Lu et al., 2020; Yu 2015; Sammons et al., 2015; Lu and Ng, 2016; et al., 2020). For example, Lu et al. (2020) proposes a new mechanism to better exploit event type inChen and Ng, 2016; Duncan et al., 2017). Previous formation for coreference resolution. Despite their neural methods (Nguyen et al., 2016; Choubey and Huang, 2017; Huang et al., 2019) also use non- impressive performance, these methods are specific to one particular type of additional information. contextual word embeddings such as word2vec 1 In this paper, we propose general and effective The code is publicly available at https://github.com/ laituan245/eventcoref. methods for incorporating a"
2021.naacl-main.274,P17-1009,0,0.159707,"Missing"
2021.naacl-main.274,D16-1038,0,0.128683,"Missing"
2021.naacl-main.274,D14-1162,0,0.0903407,"bservations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016.1 ... we are seeing these soldiers {head out}ev1 ... ... these soldiers were set to {leave}ev2 in January ... ev1 (Movement:Transport): Modality = ASSERTED ev2 (Movement:Transport): Modality = OTHER Table 1: An example of using the modality attribute to improve event coreference resolution. (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). With the recent remarkable success of language models such as BERT (Devlin et al., 2019) and SpanBERT (Joshi et al., 2020), one natural question is whether we can simply use these models for coreference resolution without relying on any additional features. We argue that it is still highly beneficial to utilize symbolic features, especially when they are clean and have complementary information. Table 1 shows an example in the ACE 2005 dataset, where our baseline SpanBERT model 1 Introduction incorrectly predicts the highlighted event mentions to be coreferential. The event triggers are sema"
2021.naacl-main.4,D18-1307,0,0.0135668,"; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract entities, events, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-IE model for illustrati"
2021.naacl-main.4,P19-1136,0,0.0346849,"Missing"
2021.naacl-main.4,P19-1024,0,0.0226133,"onment for a blasphemy conviction, police said Sunday. AMR-IE outputs Movement:Transport “AIG” “insurer” “Japan” “AIG” “insurer” “Japan” Table 5: Examples from ACE05-E test set that illustrates how AMR parsing can improve the performance of joint IE. Note that due to the limitation of space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building"
2021.naacl-main.4,2020.findings-emnlp.114,0,0.217535,"AMR parsing can improve the performance of joint IE. Note that due to the limitation of space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018"
2021.naacl-main.4,P16-1025,1,0.818343,"er understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-IE model for illustration in Table 5. 5 Related Work Some recent efforts have incorporated dependency parsing trees into neural networks for event extraction (Li et al., 2019) and relation extraction (Miwa and Bansal, 2016; Pouran Ben Veyseh et al., 2020). For semantic role labeling (SRL), (Stanovsky and Dagan, 2016) manages to exploit the similarity between SRL and open domain IE by creating a mapping between two tasks. (Huang et al., 2016, 2018) employ AMR as a more concise input format for their IE models, but they decompose each AMR into triples to capture the local contextual information between nodes and edges, while the node information is not disseminated in a global graph topology. (Rao et al., 2017) proposes a subgraph matching based method to extract biomedical events from AMR graphs, while (Li et al., 2020) uses an additional GCN based encoder for obtaining better word representations. Besides, graph neural networks are also widely 46 encoder in OneIE, our proposed framework leverages a semantic graph aggregator to i"
2021.naacl-main.4,P18-1201,1,0.877629,"Missing"
2021.naacl-main.4,W13-2322,0,0.0330431,"formation extraction, we propose an intuitive and effective framework to utilize information from semantic parsing to jointly extract an in1 The programs are publicly available for research purpose at https://github.com/zhangzx-uiuc/AMR-IE. 39 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–49 June 6–11, 2021. ©2021 Association for Computational Linguistics formation network composed of entities, relations, event triggers and their arguments. We adopt Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which contains rich semantic structures with fine-grained node and edge types as our input semantic graphs. Compared with previous IE models, our proposed model mainly consists of the following two novel components. AMR-Guided Graph Encoding. The AMR graph topology can directly inform the IE model some global inter-dependencies among knowledge elements, even if they are located far away in the original sentence. Such a property makes it easier for the IE model to capture some non-local longdistance connections for relation and event argument role labeling. We design a semantic graph aggregato"
2021.naacl-main.4,P17-1085,0,0.0238309,", such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract entities, events, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from th"
2021.naacl-main.4,D18-1156,0,0.0758883,"Missing"
2021.naacl-main.4,W11-1802,0,0.0877543,"pes, 33 event types, and 22 event argument roles. ERE-EN We also adopt another dataset EREEN from the Deep Exploration and Filtering of Test (DEFT) program, which includes more recent news articles and political reviews. We extract 17,108 sentences from datasets LDC2015E29, LDC2015E68, and LDC2015E78. Following (Lin et al., 2020), we keep 7 entity types, 5 relation types, 38 event types, and 20 argument roles. GENIA To further prove that our proposed model is generalizable to other specific domains, we also evaluate our model on biomedical event extraction datasets BioNLP Genia 2011 and 2013 (Kim et al., 2011, 2013). We ignore all of the trigger-trigger links (nested event structures) and merge all repeated event triggers into unified information networks to make them compatible for comparison with previous models. Since the test sets are blind and not available for merging the annotations, we evaluate the model performance on the official development sets instead. Details of dataset statistics are shown in Table 2. 4 #Sents Table 2: Dataset statistics. Experiments 4.1 Split ?#,$ Figure 3: An illustration of ordered decoding, where τ1 and τ2 are identified triggers while each εi,j is identified en"
2021.naacl-main.4,2021.ccl-1.108,0,0.0401748,"Missing"
2021.naacl-main.4,P12-2010,0,0.0162532,"that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019)"
2021.naacl-main.4,2020.acl-main.721,0,0.0239572,"e to the limitation of space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity e"
2021.naacl-main.4,N19-1308,0,0.0315734,"ations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract entities, events, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-IE model for illustration in Table 5. 5 Related Work Some rec"
2021.naacl-main.4,N19-1145,1,0.822444,"Missing"
2021.naacl-main.4,P16-1105,0,0.0292099,"vents, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-IE model for illustration in Table 5. 5 Related Work Some recent efforts have incorporated dependency parsing trees into neural networks for event extraction (Li et al., 2019) and relation extraction (Miwa and Bansal, 2016; Pouran Ben Veyseh et al., 2020). For semantic role labeling (SRL), (Stanovsky and Dagan, 2016) manages to exploit the similarity between SRL and open domain IE by creating a mapping between two tasks. (Huang et al., 2016, 2018) employ AMR as a more concise input format for their IE models, but they decompose each AMR into triples to capture the local contextual information between nodes and edges, while the node information is not disseminated in a global graph topology. (Rao et al., 2017) proposes a subgraph matching based method to extract biomedical events from AMR graphs, while (Li et al"
2021.naacl-main.4,2020.acl-main.230,1,0.750786,"Missing"
2021.naacl-main.4,2020.acl-main.715,0,0.205146,". Movement:Transport “airlifting” north troop A Pakistani court in central Punjab province has sentenced a Christian man to life imprisonment for a blasphemy conviction, police said Sunday. AMR-IE outputs Movement:Transport “AIG” “insurer” “Japan” “AIG” “insurer” “Japan” Table 5: Examples from ACE05-E test set that illustrates how AMR parsing can improve the performance of joint IE. Note that due to the limitation of space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale"
2021.naacl-main.4,P14-1038,1,0.763288,"res of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract entities, events, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we selec"
2021.naacl-main.4,N19-1082,0,0.0210104,"the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract enti"
2021.naacl-main.4,D14-1198,1,0.804748,"ARG2 son “Scott Peterson” wife Information Network die-01 Life:Die Justice:Sentence “murdering” “faces” Place “house” Agent Defendant LOC Victim Victim “Laci” PER “their” PER PER-SOC “son” “wife” PER PER PER-SOC “Scott Peterson” “he” PER PER Figure 1: Comparison of the AMR graph generated from pre-trained AMR parser and information network from IE for the same sentence from ACE05: Scott Peterson now faces death penalty because of murdering his wife Laci and their unborn son at their house. Introduction Information extraction (IE) aims to extract structured knowledge as an information network (Li et al., 2014) from unstructured natural language texts, while semantic parsing attempts to construct a semantic graph to summarize the meaning of the input text. Since both of them focus on extracting the main information from a sentence, the output information networks and semantic graphs have a lot in common in terms of node and edge semantics. In an example shown in Figure 1, many knowledge elements in the information network can be perfectly matched to certain nodes in the semantic graph with similar semantic meanings. Moreover, these two types of graphs may also be similar with regard to network topol"
2021.naacl-main.4,W17-2315,0,0.0805743,"parsing trees into neural networks for event extraction (Li et al., 2019) and relation extraction (Miwa and Bansal, 2016; Pouran Ben Veyseh et al., 2020). For semantic role labeling (SRL), (Stanovsky and Dagan, 2016) manages to exploit the similarity between SRL and open domain IE by creating a mapping between two tasks. (Huang et al., 2016, 2018) employ AMR as a more concise input format for their IE models, but they decompose each AMR into triples to capture the local contextual information between nodes and edges, while the node information is not disseminated in a global graph topology. (Rao et al., 2017) proposes a subgraph matching based method to extract biomedical events from AMR graphs, while (Li et al., 2020) uses an additional GCN based encoder for obtaining better word representations. Besides, graph neural networks are also widely 46 encoder in OneIE, our proposed framework leverages a semantic graph aggregator to incorporate information from fine-grained AMR semantics and enforce global interactions in the encoding phase. In addition, instead of a simple left-to-right sequential decoder, we creatively use the AMR hierarchy to decide the decoding order of knowledge elements. Both the"
2021.naacl-main.4,N16-1033,0,0.0265062,"l., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract entities, events, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-IE model for illustration in Table 5. 5 Related Work Some recent efforts have incorporated dependency parsing trees into neur"
2021.naacl-main.4,P19-1423,0,0.0225051,"nt IE. Note that due to the limitation of space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and jo"
2021.naacl-main.4,2020.emnlp-main.127,0,0.0318379,"space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and"
2021.naacl-main.4,D16-1252,0,0.0297335,"r makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-IE model for illustration in Table 5. 5 Related Work Some recent efforts have incorporated dependency parsing trees into neural networks for event extraction (Li et al., 2019) and relation extraction (Miwa and Bansal, 2016; Pouran Ben Veyseh et al., 2020). For semantic role labeling (SRL), (Stanovsky and Dagan, 2016) manages to exploit the similarity between SRL and open domain IE by creating a mapping between two tasks. (Huang et al., 2016, 2018) employ AMR as a more concise input format for their IE models, but they decompose each AMR into triples to capture the local contextual information between nodes and edges, while the node information is not disseminated in a global graph topology. (Rao et al., 2017) proposes a subgraph matching based method to extract biomedical events from AMR graphs, while (Li et al., 2020) uses an additional GCN based encoder for obtaining better word representations. Besides"
2021.naacl-main.4,N19-1306,0,0.141325,"finally select the graph with the highest global score g(G) in step K as the output. #Ents #Events #Rels ACE05-E Train Dev Test 17,172 923 832 29,006 2,451 3,017 4,202 450 403 4,664 560 636 ERE-EN Train Dev Test 14,736 1,209 1,163 39,501 3,369 3,295 6,208 525 551 5,054 408 466 Genia’11 Train Dev 9,583 3,499 12,058 4,842 5,854 1,933 513 117 Genia’13 Train Dev 2,992 3,341 3,794 4,542 1,776 1,821 46 34 4.2 Experimental Setup We adopt the most recent joint IE models DyGIE++ (Wadden et al., 2019) and OneIE (Lin et al., 2020) as baselines in our experiments, and use the same evaluation metrics as (Zhang et al., 2019b; Wadden et al., 2019; Lin et al., 2020) to report the F1-Score for each IE subtask. Entity: An extracted entity mention is correct only if both the predicted word span (ai , bi ) and entity type ei match a reference entity mention. Event Trigger: An event trigger is correctly identified (Trg-I) if the predicted span (pi , qi ) matches a reference trigger. It is correctly classified (Trg-C) if the predicted event type ti also matches the reference trigger. Event Argument: A predicted event argument a ) is correctly identified (Arg-I) if (τ , ε ) (τi , εj , li,j i j matches a reference event a"
2021.naacl-main.4,P19-1131,0,0.0161752,"document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract entities, events, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-IE model for illustration in Table 5. 5 R"
2021.naacl-main.4,D18-1244,0,0.0213039,"tenced a Christian man to life imprisonment for a blasphemy conviction, police said Sunday. AMR-IE outputs Movement:Transport “AIG” “insurer” “Japan” “AIG” “insurer” “Japan” Table 5: Examples from ACE05-E test set that illustrates how AMR parsing can improve the performance of joint IE. Note that due to the limitation of space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some re"
2021.naacl-main.4,P17-1113,0,0.0338207,"(Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint model to extract entities, events, and relations based on span graph propagation, while OneIE (Lin et al., 2020) further makes exploits global features to facilitate the model to capture more global interactions. Compared with the flat In order to further understand how our proposed AMR guided encoding and AMR conditioned decoding method help to improve the performance, we select typical examples from the output of our AMR-"
2021.naacl-main.4,2020.findings-emnlp.326,0,0.163412,". Movement:Transport “airlifting” north troop A Pakistani court in central Punjab province has sentenced a Christian man to life imprisonment for a blasphemy conviction, police said Sunday. AMR-IE outputs Movement:Transport “AIG” “insurer” “Japan” “AIG” “insurer” “Japan” Table 5: Examples from ACE05-E test set that illustrates how AMR parsing can improve the performance of joint IE. Note that due to the limitation of space, we only show a subset of each ARM graph that are most relevant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale"
2021.naacl-main.4,P19-1128,0,0.0168195,"ant for generating the correct IE outputs. 4.5 Qualitative Analysis used for event extraction (Liu et al., 2018; Veyseh et al., 2020; Balali et al., 2020; Zhang et al., 2021) and relation and entity extraction (Zhang et al., 2018; Fu et al., 2019; Guo et al., 2019; Sun et al., 2020). Graph neural networks also demonstrate effectiveness to encode other types of intrinsic structures of a sentence, such as knowledge graph (Zhang et al., 2019a; Huang et al., 2020), document-level relations (Sahu et al., 2019; Lockard et al., 2020; Zeng et al., 2020), and selfconstructed graphs (Kim and Lee, 2012; Zhu et al., 2019; Qian et al., 2019; Sahu et al., 2020). However, all these approaches focus on single IE tasks while can not scale to extracting a joint information network with entities, relations, and events. There are some recent efforts that focus on building joint neural models for performing multiple IE tasks simultaneously, such as joint entity and relation extraction (Li and Ji, 2014; Katiyar and Cardie, 2017; Zheng et al., 2017; Bekoulis et al., 2018; Sun et al., 2019; Luan et al., 2019) and joint event and entity extraction (Yang and Mitchell, 2016). DyGIE++ (Wadden et al., 2019) designs a joint mo"
2021.naacl-main.4,D19-1585,0,0.494439,"nce, we propose a new hierarchical decoding method. We use AMR parsing as a condition to decide the order of decoding knowledge elements, where the nodes and edges are determined in a tree-like order based on the semantic graph hierarchy. Experiment results on multiple datasets show that our proposed model significantly outperforms state-of-the-art on all IE subtasks. 2 to be ground-truth but are generated by pretrained AMR parsers. Therefore, we do not incorporate additional information and our problem settings are identical to typical joint information extraction approaches such as DyGIE++ (Wadden et al., 2019) and OneIE (Lin et al., 2020). Given an input sentence S = {w1 , w2 , · · · , wN }, we formulate our problem of joint information extraction as follows. Entity Extraction Entity extraction aims to identify word spans as entity mentions and classify them into pre-defined entity types. Given the set of entity types E, the entity extraction task is to output a collection E of entity mentions: E = {εi = (ai , bi , ei ) |ai 6 bi , ei ∈ E} where ai , bi ∈ {1, 2, · · · , N } denote the starting and ending indices of the extracted entity mentions, and ei represents the entity type in a type set E. For"
2021.naacl-main.69,2020.acl-main.703,0,0.400601,"final output. that describes the event with hargi placeholders. The generated output is a filled template where placeholders are replaced by concrete arguments. An example of the unfilled template from the ontology and the filled template for the event type Transaction.ExchangeBuySell 5 can be seen in Figure 2. Notably, one template per event type is given in the ontology, and does not require further human curation as opposed to the question designing process in question answering (QA) models (Du and Cardie, 2020; Feng et al., 2020). Our base model is an encoder-decoder language model (BART (Lewis et al., 2020), T5 (Raffel et al., 2020). The generation process models the conditional probability of selecting a new token given the previous tokens and the input to the encoder. p(x |c) = |x| Y p (xi |x&lt;i , c) (1) ground truth sequence is the filled template where the placeholder token is replaced by the argument span whenever possible. In the case where there are multiple arguments for the same slot, we connect the arguments with the word “and&quot;. The generation probability is computed by taking the dot product between the decoder output and the embeddings of tokens from the input.  p(xi = w|x&lt;i , c, t)"
2021.naacl-main.69,2020.emnlp-main.724,1,0.736281,"labeling and our model is an adaptation of TapNet (Yoon et al., 2019; Hou et al., 2020), which was designed for few-shot classification and later extended to Conditional Random Field (CRF) models. Compared with (Hou et al., 2020), we do not collapse the entries of the transition matrix, making it possible for our model to learn different probabilities for each event type. Since our model takes class keywords as input, we refer to this model as TAP K EY. For each event type, we first obtain a class representation vector ck based on given keywords using the masked category prediction method in (Meng et al., 2020). This class representation vector is an average over the BERT vector representations of the keywords, with some filtering applied to remove ambiguous occurrences. Details of the filtering process are included in Appendix A. Following the linear-chain CRF model, the probability of a tagged sequence is: log p(y|h; θ) ∝ X i ϕ(yi |hi ) + X ψ(yi |yi−1 , hi ) (5) i hi is the output of the embedding network (in our case, BERT-large) corresponding to xi . The label space for yi is the set of IO tags. We choose to use this simplified tagging scheme because it has fewer parameters and the fact that con"
2021.naacl-main.69,2020.emnlp-main.373,0,0.0345908,"Missing"
2021.naacl-main.69,N19-1423,0,0.0433191,"Missing"
2021.naacl-main.69,2020.acl-main.713,1,0.894878,"USTICE. 13 known and only annotation for these event types will be seen. We used two settings for selecting known types: 10 most frequent events types and 8 event types, one from each parent type of the event ontology. The evaluation is done on the complete set of event types. We refer the reader to Appendix C for implementation details and hyperparameter settings. 4.1 Datasets In addition to our dataset W IKI E VENTS, we also report the performance on the Automatic Content Extraction (ACE) 2005 dataset10 and the Roles Across Multiple Sentences (RAMS) dataset11 . We follow preprocessing from (Lin et al., 2020; Wadden et al., 2019) for the ACE dataset. 12 Statistics of the ACE data splits can be found in Table 3. RAMS (Ebner et al., 2020) is a recently released dataset with cross-sentence argument annotation. A 5-sentence window is provided for each event trigger and the closest argument span is annotated for each role. We follow the official data splits from Version 1.0. 4.2 Our experiments fall under three settings: (1) document-level event argument extraction; (2) document-level informative argument extraction and (3) zero-shot event extraction. For document-level event argument extraction we fo"
2021.naacl-tutorials.3,D17-1209,0,0.0598168,"Missing"
2021.naacl-tutorials.3,2020.findings-emnlp.255,1,0.779255,"e the tutorial self-contained. For trainees interested in reading important studies before the tutorial, we recommend the following papers regarding GNNs (Kipf and Welling, 2016; Li et al., 2015; Hamilton et al., 2017), automatic graph construction for NLP (Bastings et al., 2017; Chen et al., 2020b,a), joint text and knowledge representation learning (Feng et al., 2020; Lin et al., 2020), modeling directed graphs (Xu et al., 2018; Chen et al., 2020b) and heterogeneous graphs (Bastings et al., 2017; Chen et al., 2020c), and GNN based encoder-decoder models (Xu et al., 2018; Chen et al., 2020b; Li et al., 2020). 5 Diversity Yu Chen is a Research Scientist at Facebook AI. He got his PhD degree in Computer Science from Rensselaer Polytechnic Institute. His research interests lie at the intersection of Machine Learning (Deep Learning), and Natural Language Processing, with a particular emphasis on the fast-growing field of Graph Neural Networks and their applications in various domains. His work has been published in top-ranked conferences including but not limited to NeurIPS, ICML, ICLR, AAAI, IJCAI, NAACL, KDD, WSDM, ISWC, and AMIA. He was the recipient of the Best Student Paper Award of AAAI DLGMA’2"
2021.naacl-tutorials.3,2020.acl-main.713,1,0.712552,"https://sites.google.com/a/ email.wm.edu/teddy-lfwu/. IV. (20 minutes) Hands-on Demonstration 1. A Brief Overview of the Graph4NLP Library 2. Live Demo V. (10 minutes) Conclusion and Open Directions 4 Reading List We aim to make the tutorial self-contained. For trainees interested in reading important studies before the tutorial, we recommend the following papers regarding GNNs (Kipf and Welling, 2016; Li et al., 2015; Hamilton et al., 2017), automatic graph construction for NLP (Bastings et al., 2017; Chen et al., 2020b,a), joint text and knowledge representation learning (Feng et al., 2020; Lin et al., 2020), modeling directed graphs (Xu et al., 2018; Chen et al., 2020b) and heterogeneous graphs (Bastings et al., 2017; Chen et al., 2020c), and GNN based encoder-decoder models (Xu et al., 2018; Chen et al., 2020b; Li et al., 2020). 5 Diversity Yu Chen is a Research Scientist at Facebook AI. He got his PhD degree in Computer Science from Rensselaer Polytechnic Institute. His research interests lie at the intersection of Machine Learning (Deep Learning), and Natural Language Processing, with a particular emphasis on the fast-growing field of Graph Neural Networks and their applications in various do"
2021.textgraphs-1.5,W09-3208,1,0.803824,"representations of event mentions and its arguments. Graph Representation Methods. Skipgram (Mikolov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as"
2021.textgraphs-1.5,W09-4303,1,0.847544,"averaged contextual representations of event mentions and its arguments. Graph Representation Methods. Skipgram (Mikolov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Josh"
2021.textgraphs-1.5,P10-1143,0,0.369636,"0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summary. For fair comparison, we train the same framework with the following graph representation learning methods. Event Coreference Resolution. Besides existing methods (Bejan and Harabagiu, 2010b; Liu et al., 2014) we implement the model architecture (Lee et al., 2017) that has achieved the current state-of-the-art results in entity coreference resolution (Joshi et al., 2019) and cross-document event Results and Analysis Dataset We construct corpus-level graphs for training, development, and test sets from the English subset of Automatic Content Extraction (ACE) 2005 dataset2 . We follow the pre-processing steps in (Lin et al., 2020) and show the dataset statistics in Table 1. 2 https://www.ldc.upenn.edu/collaborations/ past-projects/ace 47 Node Article ACE train dev test 521 30 40 E"
2021.textgraphs-1.5,P18-1045,0,0.0130917,"18; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger"
2021.textgraphs-1.5,P18-1198,0,0.0302608,"eight sharing between different relation types. Multiple Views. The structure of event networks can be viewed in multiple different perspectives. For example, when entity-entity relations are masked out, an event network degenerates to pieces of isolated events and only local neighborhood will be observed. The advantage of separate modeling 3.3 Topology Learning To capture neighborhood information, we train the graph encoder with relation discrimination loss to 45 learn the graph topology. X X X LT = ( E[log Dr (yi , yj )] i + r∈R j∈Nir X X r∈R linguistic properties (Hewitt and Manning, 2019; Conneau et al., 2018). The task of event network embedding requires the embedded distributional node representations to preserve semantic proximity, local neighborhood and global neighborhood. Accordingly, we intrinsically evaluate the semantics preservation with node typing and assess the local neighborhood preservation with event argument role classification. We also apply the node embeddings to a downstream task, event coreference resolution, to extrinsically evaluate the global neighborhood preservation. Node Typing and Event Argument Role Classification are conducted under the same evaluation setting: given t"
2021.textgraphs-1.5,W16-1004,1,0.882797,"Missing"
2021.textgraphs-1.5,N19-1423,0,0.208811,"work denoted as G = {V, E}, where V and E are node and edge sets, respectively. Each node vi = hai , bi , si , li i ∈ V represents an event or entity mention, where ai and bi are the start and end word indices in sentence si , and li is the node type label. Each edge eij = hi, j, lij i ∈ E represents an event-entity or entity-entity relation, where i and j are indices of the involved nodes and lij is the edge type label. In this work, we initialize the semantic representation of each node vi with an m-dimensional attribute vector xi derived from sentence context using a pretrained BERT model (Devlin et al., 2019). Semantic Proximity (Gao and Huang, 2018). Given an event network G = {V, E}, the semantic proximity of node vi and node vj is determined by We design Event Network Structural Probes, an evaluation framework including a series of structural probing tasks, to check the model’s capability to implicitly incorporate event network structures. In this work, the learned node embeddings are intrinsically evaluated with node typing and event argument role classification tasks, and applied to the downstream task of event coreference resolution. Experimental results on the augmented Automatic Content Ex"
2021.textgraphs-1.5,P14-2082,0,0.027139,"ntity relation. In this example, Execution event and Set Fire event are connected through two paths, which tell the story of angry protesters revenge the death of Nimral-Nimr against Saudi Arabia by attacking its embassy. event network. In Figure 1, a good representation of the Set Fire event should involve the Execution event because the latter clarifies the grievance motivating the former. We further enrich event representations by introducing more context from the entire event network. Compared with other methods to connect events (e.g., with eventevent relations (Pustejovsky et al., 2003; Cassidy et al., 2014; Hong et al., 2016; Ikuta et al., 2014; O’Gorman et al., 2016)), our representation of each event grounded in an event network is semantically richer. the event representations. GENE and its variants significantly outperform the baseline methods on various tasks. In summary, our contributions are: • We formalize the task of event network embedding and accordingly propose a novel unsupervised learning framework, which trains the multi-view graph encoder with topology and semantics learning losses. • We design a series of incrementally structural probing tasks, including node typing, argument r"
2021.textgraphs-1.5,W16-1701,1,0.893696,"Missing"
2021.textgraphs-1.5,D19-1495,0,0.0237863,"d event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns distributed event representations using supervised multi-task le"
2021.textgraphs-1.5,Y00-1012,0,0.398674,"ncluding trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety"
2021.textgraphs-1.5,I17-1010,0,0.023199,"models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns di"
2021.textgraphs-1.5,W14-2903,0,0.0300513,"Missing"
2021.textgraphs-1.5,2020.tacl-1.5,0,0.285549,"2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj"
2021.textgraphs-1.5,D19-1588,0,0.0127311,"i et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is"
2021.textgraphs-1.5,W19-3311,0,0.0233777,"oint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns distributed event repr"
2021.textgraphs-1.5,N19-1419,0,0.0184702,"can be seen as a way of weight sharing between different relation types. Multiple Views. The structure of event networks can be viewed in multiple different perspectives. For example, when entity-entity relations are masked out, an event network degenerates to pieces of isolated events and only local neighborhood will be observed. The advantage of separate modeling 3.3 Topology Learning To capture neighborhood information, we train the graph encoder with relation discrimination loss to 45 learn the graph topology. X X X LT = ( E[log Dr (yi , yj )] i + r∈R j∈Nir X X r∈R linguistic properties (Hewitt and Manning, 2019; Conneau et al., 2018). The task of event network embedding requires the embedded distributional node representations to preserve semantic proximity, local neighborhood and global neighborhood. Accordingly, we intrinsically evaluate the semantics preservation with node typing and assess the local neighborhood preservation with event argument role classification. We also apply the node embeddings to a downstream task, event coreference resolution, to extrinsically evaluate the global neighborhood preservation. Node Typing and Event Argument Role Classification are conducted under the same eval"
2021.textgraphs-1.5,2021.naacl-main.274,1,0.730225,"lov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2"
2021.textgraphs-1.5,D12-1045,0,0.0247054,"et al., 2019) jointly model nodes and edges. Attributed network embedding approaches (Gao and Huang, 2018; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (To"
2021.textgraphs-1.5,K16-1008,0,0.01633,"y features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns distributed event representations using supervised multi-task learning, while our framework is based on unsupervised learning. Network Embedding. Our work falls into the scope of unsupervised learning for heterogeneous attributed network embeddings. Heterogeneous network embedding methods (Chang et al., 2015; 7 Conclusions and"
2021.textgraphs-1.5,D17-1018,0,0.042921,"by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summary. For fair comparison, we train the same framework with the following graph representation learning methods. Event Coreference Resolution. Besides existing methods (Bejan and Harabagiu, 2010b; Liu et al., 2014) we implement the model architecture (Lee et al., 2017) that has achieved the current state-of-the-art results in entity coreference resolution (Joshi et al., 2019) and cross-document event Results and Analysis Dataset We construct corpus-level graphs for training, development, and test sets from the English subset of Automatic Content Extraction (ACE) 2005 dataset2 . We follow the pre-processing steps in (Lin et al., 2020) and show the dataset statistics in Table 1. 2 https://www.ldc.upenn.edu/collaborations/ past-projects/ace 47 Node Article ACE train dev test 521 30 40 Event Entity Event-Entity 4,353 494 424 3,688 667 750 7,888 938 897 Edge Ent"
2021.textgraphs-1.5,W16-5706,0,0.0359262,"Missing"
2021.textgraphs-1.5,P17-1178,1,0.809683,"out of 238 labels. Each label consists of an event type and an argument role type as defined in ACE. For example, the argument role label “Justice:Arrest-Jail:Agent"" can only be correctly selected when the event node implies the type “Justice:Arrest-Jail"" and the entity node implies its role being the “Agent"". Compared to the traditional argument role labeling procedure, this setting skips the step of mention identification, which has been done in network construction process. The performance is reported with multi-label classification Micro F1 score. 4.3 We perform automatic entity linking (Pan et al., 2017) to link entities to Wikipedia. Entity nodes linked to the same Wikipedia entity are merged into one node. We further retrieve entity-entity relations from Wikidata and enrich the event network with these connections, such as the part-whole relation between Tehran and Iran in Figure 1. We also add narrative event-event relations by connecting every pair of events within one document as edges in the graph. 5.2 Non-Graph Event Representation Methods. Mention-based method represents events with contextual representations inferred by BERT (Devlin et al., 2019). Tuple-based method uses the averaged"
2021.textgraphs-1.5,2020.emnlp-main.50,1,0.79706,"Missing"
2021.textgraphs-1.5,D16-1038,0,0.0360844,"Missing"
2021.textgraphs-1.5,2020.acl-main.713,1,0.736969,"n, we train the same framework with the following graph representation learning methods. Event Coreference Resolution. Besides existing methods (Bejan and Harabagiu, 2010b; Liu et al., 2014) we implement the model architecture (Lee et al., 2017) that has achieved the current state-of-the-art results in entity coreference resolution (Joshi et al., 2019) and cross-document event Results and Analysis Dataset We construct corpus-level graphs for training, development, and test sets from the English subset of Automatic Content Extraction (ACE) 2005 dataset2 . We follow the pre-processing steps in (Lin et al., 2020) and show the dataset statistics in Table 1. 2 https://www.ldc.upenn.edu/collaborations/ past-projects/ace 47 Node Article ACE train dev test 521 30 40 Event Entity Event-Entity 4,353 494 424 3,688 667 750 7,888 938 897 Edge Entity-Entity Original Wiki* 6,856 7,040 723 853 796 1,543 Event-Event Narrative* Coref 70,992 912 12,572 144 6,154 121 Table 1: Statistics for the enhanced ACE 2005 dataset. Wiki and Narrative are enriched event-event relations. GENE w/ LT ) have a significant drop of performance on this task, while our proposed model has the best performance because of the similarity and"
2021.textgraphs-1.5,liu-etal-2014-supervised,0,0.0315778,"Missing"
2021.textgraphs-1.5,L16-1631,0,0.0153208,"s arguments. Graph Representation Methods. Skipgram (Mikolov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The"
2021.textgraphs-1.5,P17-1009,0,0.0172825,"Attributed network embedding approaches (Gao and Huang, 2018; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) propo"
2021.textgraphs-1.5,H05-1004,0,0.206904,"event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summary. For fair compari"
2021.textgraphs-1.5,M95-1005,0,0.392176,"article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summar"
2021.textgraphs-1.5,W13-5408,0,0.184046,"ing, argument role classification, and event coreference resolution. 1 1 Introduction Understanding events is a fundamental human activity. Our minds represent events at various granularity and abstraction levels, which allows us to quickly access and reason about related scenarios. A typical event mention includes an event trigger (the word or phrase that most clearly expresses an event occurrence) and its arguments (i.e., participants in events). The lexical embedding of a trigger is usually not sufficient, because the type of an event often depends on its arguments (Ritter and Rosen, 2000; Xu and Huang, 2013; Weber et al., 2018). For example, the support verb “get” may indicate a Transfer.Ownership event (“Ellison to spend $10.3 billion to get his company.”) or a Movement.Transport event (“Airlines are getting flyers to destinations on time more often.”). In Figure 1, the event type triggered by “execution” is Life.Die instead of project implementation. However, such kind of atomic event representation is 1 Our code is released at https://github.com/ pkuzengqi/GENE 42 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 42–53 June 11,"
2021.textgraphs-1.5,D15-1020,1,0.831804,"imited ability in entity coreference resolution. In some failing cases, GENE model does not link two events because some of their connecting arguments are expressed as pronouns. This limitation is inherited from the upstream event extraction. 6 Dong et al., 2017; Wang et al., 2019) jointly model nodes and edges. Attributed network embedding approaches (Gao and Huang, 2018; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introduc"
C10-2058,S07-1012,0,0.0166562,"ated documents and Wikipedia, and related events to recover and predict some implicit time arguments (Filatova and Hovy, 2001; Mani et al., 2003; Mann, 2007; Eidelman, 2008; Gupta and Ji, 2009). 2.2 Coreference Resolution One of the key challenges for information fusion is cross-document entity coreference – precise clustering of mentions into correct entities. There are two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The recent research has been mainly promoted in the web people search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 509 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a “conflict-attack” event, then they are unlikely to be connected by a “part-whole” relation; “Mahmoud Abbas” and “Abu Mazen” are likely to be coreferential if they get involved in the same “life-born” event. Some prior work (Ji et al., 2005; Jing et al., 2007"
C10-2058,W99-0201,0,0.0490975,"contradictory fact pairs from the Web appear consistent, and that requires background knowledge to predict. Assessing event coreference is essential: for texts to contradict, they must refer to the same event. Event coreference resolution is more challenging than entity coreference because each linking decision needs to be made based upon the overall similarity of the event trigger and multiple arguments. Hasler and Orasan (2009) further found that in many cases even coreferential even arguments are not good indicators for event coreference. Earlier work on event coreference resolution (e.g. Bagga and Baldwin, 1999) was limited to several MUC scenarios. Recent work (Chen et al., 2009) focus on much wider coverage of event types defined in ACE. The methods from the knowledge fusion community (e.g. Appriou et al., 2001; Gregoire, 2006) mostly focus on resolving conflicts rather than identifying them (i.e. inconsistency problem rather than ambiguity). These approaches allow the conflicts to be resolved in a straightforward way but they rely on the availability of meta-data (e.g., distribution of weights between attributes, probability assignment etc.). However, it is not always clear where to get this meta-"
C10-2058,D08-1029,0,0.0561755,"Missing"
C10-2058,P08-2045,0,0.0285463,"Missing"
C10-2058,P03-1042,0,0.0271738,"exts. (2) Run source language IE on the source language texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithm’s assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annotated corpora which are enough for reasonable extraction performance); (2) the"
C10-2058,P09-1068,0,0.0600569,"Missing"
C10-2058,W09-2209,1,0.825989,"ce language IE on the source language texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithm’s assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annotated corpora which are enough for reasonable extraction performance); (2) the two views are condi"
C10-2058,W09-4303,1,0.840412,"background knowledge to predict. Assessing event coreference is essential: for texts to contradict, they must refer to the same event. Event coreference resolution is more challenging than entity coreference because each linking decision needs to be made based upon the overall similarity of the event trigger and multiple arguments. Hasler and Orasan (2009) further found that in many cases even coreferential even arguments are not good indicators for event coreference. Earlier work on event coreference resolution (e.g. Bagga and Baldwin, 1999) was limited to several MUC scenarios. Recent work (Chen et al., 2009) focus on much wider coverage of event types defined in ACE. The methods from the knowledge fusion community (e.g. Appriou et al., 2001; Gregoire, 2006) mostly focus on resolving conflicts rather than identifying them (i.e. inconsistency problem rather than ambiguity). These approaches allow the conflicts to be resolved in a straightforward way but they rely on the availability of meta-data (e.g., distribution of weights between attributes, probability assignment etc.). However, it is not always clear where to get this meta-data. The event attributes such as Modality, Polarity, Genericity and"
C10-2058,W09-4300,0,0.185953,"Missing"
C10-2058,P08-3003,0,0.0265119,"Missing"
C10-2058,W04-3247,0,0.0394632,"rding the real extent of the event or providing evidence corroborating the text part. Current state-of-the-art information fusion approaches can be divided into two groups: formal “top-down” methods from the generic knowledge fusion community and quantitative “bottom-up” techniques from the applied Semantic Web community (Appriou et al., 2001; Gregoire, 2006). Both approaches have their limitations. It will be beneficial to combine both types of approaches so that the fusion decision can be made depending on the type of problem and the amount of domain information it possesses. Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. Magalhaes et al. (2008) described a semantic similarity metric based on key word vectors for multi-media fusion. Iria and Magalhaes (2009) exploited information across different parts of a multimedia document to improve document classification. It is important to go beyond key words and attempt representing the documents by the semantic facts identified by IE. One possible solution is to exploit the linkage information. Specifically, coreference resolution methods should be applied to"
C10-2058,2007.mtsummit-papers.24,0,0.0176926,"amework using cross-lingual information projection. They demonstrated that this framework is particularly effective for a challenging IE task which is situated at the end of a pipeline and thus suffers from the errors propagated from upstream processing and has low-performance baseline. 3.2 Translation Challenges Because the facts are aggregated from multiple languages, the translation errors will bring us great challenges. However, in order to extend cross-lingual information fusion techniques to more language pairs, we can start from the much more scalable task of “information” translation (Etzioni et al., 2007). The additional processing may take the form of machine translation (MT) of extracted facts such as names and events. IE tasks performed notably worse on machine translated texts than on texts originally written in English, and error analysis indicated that a major cause was the low quality of name translation (Ji et al., 2009b). Traditional MT systems focus on the overall fluency and accuracy of the translation but fall short in their ability to translate certain informationally critical words. In particular, it appears that better entity name translation can substantially improve cross-ling"
C10-2058,W01-1313,0,0.102761,"Missing"
C10-2058,P05-1045,0,0.019247,"y of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a “conflict-attack” event, then they are unlikely to be connected by a “part-whole” relation; “Mahmoud Abbas” and “Abu Mazen” are likely to be coreferential if they get involved in the same “life-born” event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction – part of one event mention contradicts part of another, and redundancy – part of one event mention conveys the same content as (or is entailed by) part of another. Once these central relations are identified they will pro"
C10-2058,P09-2093,1,0.881543,"Missing"
C10-2058,H05-1003,1,0.878394,"search task (Artiles et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 509 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a “conflict-attack” event, then they are unlikely to be connected by a “part-whole” relation; “Mahmoud Abbas” and “Abu Mazen” are likely to be coreferential if they get involved in the same “life-born” event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction – part of one event mentio"
C10-2058,P08-1030,1,0.905563,"Missing"
C10-2058,W09-3107,1,0.919867,"ing Markov Logic Networks (Richardson and Domingos, 2006), a statistical relational learning language, to model these global inference rules more declaratively. Markov Logic will make it possible to compactly specify probability distributions over the complex relational inferences. It can capture non-deterministic (soft) rules that tend to hold among facts but do not have to. Exploiting this approach will also provide greater flexibility to incorporate additional linguistic and world knowledge into inference. The information fused across documents can be represented as an information network (Ji, 2009) in which entities can be viewed as vertices on the graph and they can be connected by some type of static relationship (e.g. those attributes defined in NIST TAC-KBP task (McNamee and Dang, 2009)), or as a temporal chain linking dynamic events (e.g. Bethard and Martin, 2008; Chambers and Jurafsky, 2009; Ji et al., 2009a). The latter representation is more attractive because business or international affairs analysts often review many news reports to track people, companies, and government activities and trends. The query logs from the commercial search engines show that there is a fair number"
C10-2058,P07-1131,0,0.12588,"les et al., 2007) such as (Balog et al., 2008), ACE2008 such as (Baron and Freedman, 509 2008) and NIST TAC KBP (McNamee and Dang, 2009) evaluations. Interestingly, the quality of information can often be improved by the fused fact network itself, which can be called as self-boosting of information fusion. For example, if two GPE entities are involved in a “conflict-attack” event, then they are unlikely to be connected by a “part-whole” relation; “Mahmoud Abbas” and “Abu Mazen” are likely to be coreferential if they get involved in the same “life-born” event. Some prior work (Ji et al., 2005; Jing et al., 2007) demonstrated the effectiveness of using semantic relations to improve entity coreference resolution; while (Downey et al., 2005; Sutton and McCallum, 2004; Finkel et al., 2005; Mann, 2007) experimented with information fusion of relations across multiple documents. The TextRunner system (Banko et al., 2007) can collapse and compress redundant facts extracted from multiple documents based on coreference resolution (Yates and Etzioni, 2009), semantic similarity computation and normalization. Two relations are central for event fusion: contradiction – part of one event mention contradicts part o"
C10-2058,N03-2019,0,0.0353542,"Missing"
C10-2058,N07-1042,0,0.456594,"re than 50 different intervening strings (e.g. “killed many people on a”, “’s attack on a”, “blew apart a”, “blew himself up on a”, “drove his explosives-laden car into a”, “had rigged the”, “set off a bomb on a”, etc.), but the ACE1 training corpora only cover about 1/3 of these expressions. Several recent studies have stressed the benefits of using information redundancy on estimating the correctness of the IE output (Downey et 1 http://www.itl.nist.gov/iad/mig/tests/ace/ 508 al., 2005), improving disease event extraction (Yangarber, 2006), Message Understanding Conference event extraction (Mann, 2007; Patwardhan and Riloff, 2009) and ACE event extraction (Ji and Grishman, 2008). This approach is based on the premise that many facts will be reported multiple times from different sources in different forms. This may occur both within the same document and within a cluster of topically related and successive documents. Therefore, by aggregating similar facts across documents and conducting statistical global inference by favoring interpretation consistency, enhanced extraction performance can be achieved with heterogeneous data than uniform data. The underlying hypothesis of cross-document i"
C10-2058,N09-1065,0,0.0204213,". The event attributes such as Modality, Polarity, Genericity and Tense (Sauri et al., 2006) will play an important role in event coreference resolution because two event mentions cannot be coreferential if any of the attributes conflict with each other. Such attempts have been largely neglected in the prior research due to the low weights of attribute labeling in the ACE scoring metric. (Chen et al., 2009) demonstrated that simple automatic event attribute labeling can significantly improve event coreference resolution. In addition, some very recent work including (Nicolae and Nicolae, 2006; Ng, 2009; Chen et al., 2009) found that graph-cut based clustering can improve coreference resolution. The challenge lies in computing the affinity matrix. 3 Cross-Lingual Information Fusion Cross-lingual comparable corpora are also prevalent now because almost all the influential events can be reported in multi-languages at the first time, but probably in different aspects. Therefore, linked fact networks can be constructed and lots of research tasks can benefit from such structures. Since the two networks are similar in structure but not homogeneous, we can do alignment and translation which may adv"
C10-2058,D09-1016,0,0.0222857,"Missing"
C10-2058,C02-1070,0,0.0220476,"e texts into target language, and then run target language IE on the translated texts. (2) Run source language IE on the source language texts, and then use machine translation (MT) word alignments to translate (project) extracted information into target languages. Regardless of the different architectures, both pipelines are facing the following challenges from extraction and translation. 3.1 Extraction Challenges Some recent fusion work focus on cross-lingual interaction and inference to improve both sides synchronously, beyond the parallel comparisons of cross-lingual IE pipelines in (e.g. Riloff et al., 2002). One of such examples is on cross-lingual co-training (e.g. Cao et al., 2003; Chen and Ji, 2009). In co-training (Blum and Mitchell, 1998), the uncertainty of a classifier is defined as the portion of instances on which it cannot make classification decisions. Exchanging tagged data in bootstrapping can help reduce the uncertainties of classifiers. The cross-lingual fusion process satisfies the co-training algorithm’s assumptions about two views (in this case, two languages): (1) the two views are individually sufficient for classification (IE systems in both languages were learned from annot"
C10-2058,W04-2401,0,0.0106635,"oss-sentence third pronoun resolution by exploiting gender and animacy knowledge discovery methods. The processing methods of text and other media are typically organized as a pipeline architecture of processing stages (e.g. from pattern recognition, to information fusion, and to summarization). Each of these stages has been studied separately and quite intensively over the past decade. It’s critical to move away from approaches that make chains of independent local decisions, and instead toward methods that make multiple decisions jointly using global information. Joint inference techniques (Roth and Yih, 2004; Ji et al., 2005; McCallum, 2006) can transform the integration of multi-media into a benefit by reducing the errors in individual stages. In doing so, we can take advantage (among other properties) of the coherence of a discourse: that a correct analysis of a text discourse reveals a large number of connections from the image information in its context, and so (in general) a more tightly connected analysis is more likely to be correct. For example, prior work has demonstrated the benefit of jointly modeling name tagging and n-best hypotheses, ASR lattices or word confusion networks (Hakkani-"
C10-2058,R09-1032,1,\N,Missing
C10-2058,N06-1011,0,\N,Missing
C10-2058,D08-1002,0,\N,Missing
C10-2072,P07-1126,0,0.0218369,"oss-media information extraction focused on one single domain (e.g. eGovernment (Amato et al., 2010); soccer game (Pazouki and Rahmati, 2009)) and structured/semi-structured texts (e.g. product catalogues (Labsky et al., 2005)). Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. We expand the task to the more general news domain including unstructured texts and use cross-media inference to enhance extraction performance. Some recent work has exploited analysis of associated texts to improve image annotation (e.g. Deschacht and Moens, 2007; Feng and Lapata, 2008). Some recent research demonstrated crossmodal integration can provide significant gains in improving the richness of information. For example, Oviatt et al. (1997) showed that speech and pen-based gestures can provide complementary capabilities because basic subject, verb, and object constituents almost always are spoken, whereas those describing locative information invariably are written or gestured. However, not much work demonstrated an effective method of using video/image annotation to improve text extraction. Our experiments provide some case studies in this new"
C10-2072,P08-1032,0,0.0276522,"ction focused on one single domain (e.g. eGovernment (Amato et al., 2010); soccer game (Pazouki and Rahmati, 2009)) and structured/semi-structured texts (e.g. product catalogues (Labsky et al., 2005)). Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. We expand the task to the more general news domain including unstructured texts and use cross-media inference to enhance extraction performance. Some recent work has exploited analysis of associated texts to improve image annotation (e.g. Deschacht and Moens, 2007; Feng and Lapata, 2008). Some recent research demonstrated crossmodal integration can provide significant gains in improving the richness of information. For example, Oviatt et al. (1997) showed that speech and pen-based gestures can provide complementary capabilities because basic subject, verb, and object constituents almost always are spoken, whereas those describing locative information invariably are written or gestured. However, not much work demonstrated an effective method of using video/image annotation to improve text extraction. Our experiments provide some case studies in this new direction. Our work can"
C10-2072,Y09-1024,1,0.793204,"oy cross-media inference methods to reduce uncertainty. We will demonstrate this approach on a case study of gender detection for persons. Automatic gender detection is crucial to many natural language processing tasks such as pronoun reference resolution (Bergsma, 2005). Gender detection for last names has proved challenging; Gender for nominals can be highly ambiguous in various contexts. Unfortunately most state-of-the-art approaches discover gender information without considering specific contexts in the document. The results were stored either as a knowledge base with probabilities (e.g. Ji and Lin, 2009) or as a static gazetteer (e.g. census data). Furthermore, speech recognition normally performs poorly on names, which brings more challenges to gender detection for mis-spelled names. We consider two approaches as our baselines. The first baseline is to discover gender knowledge from Google N-grams using specific lexical patterns (e.g. “[mention] and his/her/its/their”) (Ji and Lin, 2009). The other baseline is a gazetteer matching approach based on census data including person names and gender information, as used in typical text IE systems. We introduce the third method based on male/female"
C10-2072,W97-1401,0,0.0998252,"catalogues (Labsky et al., 2005)). Saggion et al. (2004) described a multimedia extraction approach to create composite index from multiple and multi-lingual sources. We expand the task to the more general news domain including unstructured texts and use cross-media inference to enhance extraction performance. Some recent work has exploited analysis of associated texts to improve image annotation (e.g. Deschacht and Moens, 2007; Feng and Lapata, 2008). Some recent research demonstrated crossmodal integration can provide significant gains in improving the richness of information. For example, Oviatt et al. (1997) showed that speech and pen-based gestures can provide complementary capabilities because basic subject, verb, and object constituents almost always are spoken, whereas those describing locative information invariably are written or gestured. However, not much work demonstrated an effective method of using video/image annotation to improve text extraction. Our experiments provide some case studies in this new direction. Our work can also be considered as an extension of global background inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. Extensive research has been done on video c"
C10-2072,lin-etal-2010-new,1,0.882268,"Missing"
C10-2072,J05-4003,0,0.0427435,"Missing"
C10-2072,P08-1030,1,0.929484,"ferent models are trained on a set of different modalities (e.g., the color moments, wavelet textures, and edge histograms), and the predictions made by these classifiers are combined together with a hierarchical linearlyweighted fusion strategy across different modalities and classifiers. 2.2 3 3.1 Fusion Mono-lingual System Overview Multi-media Document ASR Speech Texts Text Information Extraction Entities/ Relations/Events Videos/Images Partitioning Video Concept Extraction Concepts Multi-level Concept Fusion Global Inference Text Information Extraction We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program 1 to process texts and automatic speech recognition output. The pipeline includes name tagging, nominal mention tagging, coreference resolution, time expression extraction and normalization, relation extraction and event extraction. Entities 1 Mono-lingual Information and Inference Enhanced Concepts/Entities Relations/Events Figure 1. Mono-lingual Cross-Media Information Fusion and Inference Pipeline Figure 1 depicts the general procedure of our mono-lingual information fusion and inference http://www.nist.gov/speech/tests/ace/ 631"
C10-2072,W09-3107,1,0.897677,"Missing"
C12-1028,D11-1071,1,0.382971,"Missing"
C12-1028,D07-1074,0,0.455014,"Missing"
C12-1028,I11-1113,0,0.365426,"Missing"
C12-1028,P11-1095,0,0.251052,"Missing"
C12-1028,D11-1011,0,0.297099,"Missing"
C12-1028,D12-1082,0,0.0455474,"Missing"
C12-1028,I11-1029,0,0.135765,"Missing"
C12-1028,D09-1025,0,0.208337,"Missing"
C12-1028,D12-1113,0,0.107928,"Missing"
C12-1028,P11-1138,0,0.40145,"Missing"
C12-1028,I11-1063,0,0.0928366,"Missing"
C12-1028,C10-1150,0,0.0330919,"Missing"
C12-1076,C10-1034,0,0.0603871,"a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we describe the motivational aspects and hypotheses in this work, which we aim to prove. Hypothesi"
C12-1076,I11-1042,0,0.0868099,"old δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we describe the motivational aspects and hypotheses in this work, which we aim to prove. Hypothesis 1: Informative twee"
C12-1076,P04-3020,0,0.0190467,"homogeneous networks, which is defined as follows: s(vi ) = (1 − d) + d ∗ X v j ∈I n(vi ) w ji s(v j ) X w jk (1) vk ∈Out(v j ) where vi is a vertex with s(vi ) as the ranking score, I n(vi ) as the set of incoming edges, and Out(vi ) as the set of outgoing edges; w i j is the weight for the edge between two vertices vi and v j . An edge exists between two vertices that represent text units when their computed shared content (cosine similarity) exceeds or equals a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the wi"
C12-1076,D11-1061,0,0.0804207,"represent text units when their computed shared content (cosine similarity) exceeds or equals a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we descr"
C12-1076,W04-3252,0,\N,Missing
C12-1076,J92-4003,0,\N,Missing
C14-1148,P12-2046,0,0.0306742,"ies simple rules to predicative nominal titles with explicit time information (e.g., “former President”), the second filters and re-labels four-tuples based on entity lifespan (sec. 5.3), and the third adds four-tuples based on mentions of relations other than, but temporally linked to, the query relation (sec. 5.4). We then discuss results and identify remaining challenges for aggregating temporal information across relation mentions (sec. 6 and 7). A Glossary of selected terms can be found in the appendix. 2 Related Work The most similar work on temporal relation information aggregation are Wang et al. (2012), who use an Integer Linear Programming framework to enforce the validity of induced temporal relation information as well as enforce inter-relation constraints, and Dylla et al. (2013), who collect temporal information about relations, mostly about start and end times, using a temporal probabilistic data base framework to aggregate and enforce constraints based on relation argument existence. All TSF systems we are aware of have used either max-constrain or Validity-Ensured Incremental max-constrain aggregation algorithms (Surdeanu, 2013; Ji et al., 2011), which we cover in section 4. None we"
C14-1149,R13-1051,0,0.0579518,"Missing"
C14-1149,P14-1038,1,0.758714,"rases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and subtype refer to the role played by the entity in the world, the latter being more fine-grain"
C14-1149,P13-1008,1,0.422759,"tity mentions, phrases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and subtype refer to the role played by the entity in the world, the latter bei"
C14-1149,de-marneffe-etal-2006-generating,0,0.00418774,"nd [Mays, per: age, 50]. Formally, a knowledge graph is an annotated graph of entity mentions, phrases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and su"
C14-1149,P04-3020,0,0.0192256,"between ri and tk when response ri is provided by system tk . Credibility Initialization Each source is represented as a combination of publication venue and genre. The credibility scores of sources S are initialized uniformly as n1 , where n is the number of sources. Given the set of systems T = {t1 , . . . , tl }, we initialize their credibility scores c0 (t) based on their interactions on the predicted responses. Suppose each system ti generates a set of responses Rti . The similarity between two systems ti and tj is defined as similarity(ti , tj ) = |Rti ∩Rtj | log (|Rti |)+log (|Rtj |) (Mihalcea, 2004). Then we construct a weighted undirected graph G = hT, Ei, where T (G) = {t1 , . . . , tl } and E(G) = {hti , tj i}, hti , tj i = similarity(ti , tj ), and apply the TextRank algorithm (Mihalcea, 2004) on G to obtain c0 (t). We got negative results by initializing system credibility scores uniformly. We also got negative results by initializing system credibility scores using system metadata, such as the algorithms and resources the system used at each step, its previous performance in benchmark tests, and the confidence values it produced for its responses. We found the quality of an SF syst"
C14-1149,P09-1113,0,0.0716101,"Mays amod nsubj {GPE.Population-Center.NAM, FL-USA} 【 Per:place_of_death】 aux Tampa {Death-Trigger} prep_in had located_in nn prep_of died sleep prep_at home poss {FAC.Building-Grounds.NOM} poss June,28 his {PER.Individual.PRO, Mays} {06/28/2009, TIME-WITHIN} 【 per:date_of_death】 Figure 2: Knowledge Graph Example. number of trigger phrases for each slot type by mapping various knowledge bases, including Wikipedia Infoboxes, Freebase (Bollacker et al., 2008), DBPedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007), into the Gigaword corpus3 and Wikipedia articles via distant supervision (Mintz et al., 2009)4 . Each intermediate node in the knowledge graph that matches a trigger phrase is then assigned a corresponding semantic category. For example, “died” in Fig. 2 is labeled a Death-Trigger. 4.3 Knowledge Graph-Based Verification We design linguistic indicators in terms of the properties of nodes and paths that are likely to be bear on the response’s veracity. Formally, a path consists of the list of nodes and links that must be traversed along a route from a query node to a slot filler node. Node indicators contribute information about a query entity or slot filler node in isolation, that may"
C14-1149,C10-1099,0,0.021436,"Missing"
C16-1045,P11-1061,0,0.0733188,"2016). Contrasting results between two different strategies in Sec 2.3, HAI shows consistent improvement across different languages while post-processing strategy fails in Turkish. Table 2 shows that, in Turkish, the precision of annotations from the second step outperforms the first step, which means in this case the assumption of the post-processing strategy is not valid. 4 Related Work Most of related methods for annotation projection are based on word alignment results. Kim et al. (2010) employed both heuristic method and alignment correction with alignment dictionary of entity mentions. Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Wang and Manning (2014) project model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries in word alignment projection. 6 You can download the code of LSTM-CRF tagger from https://github.com/glample/tagger 468 Wang et al. (2013) also proposed a method to joint train word alignment and bilingual name tagging, which involves training process of word alignment. But this joint method is based on two assumptions. F"
C16-1045,K16-1018,0,0.0366378,"ode of LSTM-CRF tagger from https://github.com/glample/tagger 468 Wang et al. (2013) also proposed a method to joint train word alignment and bilingual name tagging, which involves training process of word alignment. But this joint method is based on two assumptions. First, it requires a readily-trained name tagger in each languages. Even more, both taggers need to have competitive strengths so that they can correct each other. Unfortunately, in the case of low resource languages, no competitive name tagger is available. One of most recent works linked to annotation projection was proposed by Fang and Cohn (2016) for the task of part of speech tagging (POS). Their work interestingly combined both gold annotations and projected ones by learning a global corrective matrix between gold annotation and projected annotation on IL side. The limitation is that gold annotation set on IL side must exist, which is not always the case especially in an incident language. 5 Conclusion and Future Work We introduce a weakly-supervised framework for entity annotation projection. Our model takes original English and IL bitexts as inputs and does not heavily depend on word alignment results. Experiment results show that"
C16-1045,C10-1064,0,0.135457,"tations. Fig. 1 shows an example of entity projection with word alignment results from English to Turkish. On English side, Voice of America Radio and Congo are tagged as an organization (ORG) and a location (LOC) respectively by an English name tagger. The dashed lines represent word alignment results generated by a word alignment tool. Following alignment results, we can project labels to Amerikann Sesi and Congo on Turkish side automatically. A major problem of this framework is that it suffers from noises produced by word alignment errors. Thus, some de-noising methods have been proposed (Kim et al., 2010; Wang and Manning, 2014). Though promising, this framework has several disadvantages. One shortcoming is that it totally depends on word alignment results. In this case, noise propagation from word alignment errors is heavily troublesome. To alleviate this problem, there are post-processing methods for annotation correction (Kim et al., 2010) and soft expectations to make use of more probabilistic information inside word alignment results (Wang and Manning, 2014). Although post-processing correction is efficient to filter out wrong labels, it can hardly find back labels which have been lost i"
C16-1045,W03-0428,0,0.0496254,", c2 , ..., cp i ), cj ∈ {0, 1, ..., VLc − 1}, where p is the number of characters in word Xi , and VLc is the number of different characters in language L. Embeddings for each word Then, we project these input signals from high dimensional space of token id into dense vector spaces using look-up tables. Thus, we have L xL viL = Wword i , L ∈ {E, IL} L i cL vjLi = Wchar j , L ∈ {E, IL} E E vit = Wtag ti L L , W E are look-up tables for English/IL word, English/IL character, and English tag ,Wchar where Wword tag respectively. Since character level information is helpful for name tagging task (Klein et al., 2003), we combine the word level embedding and character level embedding together by following Lample et al. (2016). For each work token, we derive a vector from the corresponding character embedding sequence using bidirectional LSTM networks as following: Li Li LC Li vfLi = LSTM LC f or (v )[p − 1], vb = LSTM back (v )[0] LC where vLi = (v1Li , v2Li , ..., vpLi ), LSTM LC f or (·) and LSTM back (·) are the forward and backward long short-term memory recurrent networks (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) to encode vLi . The output of LSTM (·) is a list of LSTM-RNN hidden layers. Then, we"
C16-1045,N16-1030,0,0.463751,"revious methods. 2.2 Name Tagging on Parallel Text In the previous section, a high-confidence annotation set is automatically generated. Using this annotation set with bitext inputs, we could train a bitext name tagger for name annotation projection. Figure 463 2 shows the structure of this neural-based model. Here we utilize the flexibility of recurrent neural networks to label IL sequences with the information flowing from English side. For both English and IL side, we employ bi-directional LSTM networks to handle sequence inputs of varied lengths. Basically, our model follows the recipe of Lample et al. (2016), with several extensions designed for this task. First, there exist two sets of embedding and recurrent layers in order to handle inputs from both English and IL side. Second, to combine these two parts of signals, there are connections between two bi-directional LSTMs where the zero step of hidden layers on IL side is initialized by the last step of hidden layers on English side. Third, there are not only word and character level information, but tag sequences involved in the input signal, on both English and IL sides. Details of the bitext name tagger will be introduced in the rest of this"
C16-1045,P14-5010,0,0.00687516,"e lost during the first step. Besides, in Sec 2.3, we provide two different strategies to correct some errors made by the second step and give further improvements on the quality of projection. 2.1 High-confidence Annotation Projection In order to supervise the training of the bitext name tagger, we need to firstly generate a high-quality training set out of the entire bitext. A naive way of projecting names from English to IL is to follow word alignment results 2 . Since word alignment task is not perfectly solved 3 and the performance could 1 In our experiment, we use the Stanford NER tool (Manning et al., 2014). Here we use GIZA++ (Och and Ney, 2003) 3 The poor alignment occurs especially when the parallel corpora is not enough (low resource issue) or the quality of bitext is poor. 2 462 BORG IORG IORG O CRF layer O IL LSTM layer IL inputs Word embedding Character layer Amerikaʼnın Sesi Radyosuʼnun ... yardım sağlıyor English LSTM layer English inputs Word embedding Character layer Tag embedding A foundation ... in Congo Figure 2: The framework of the bitext name tagger. become much worse when considering the projection over low frequency phrases such as names, here we add some rules for name search"
C16-1045,J03-1002,0,0.0333889,"nses/by/4.0/ License details: http:// 461 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 461–470, Osaka, Japan, December 11-17 2016. A foundation supported by [Voice of America Radio] provides aid to rape victims in the [Congo] [Amerikaʼnın Sesi Radyosuʼnun] desteklediği bir vakıf [Kongoʼdaki] tecavüz mağdurlarına yardım sağlıyor Figure 1: Errors of annotation projection with word alignment results using English and Turkish bitext. another aspect, since word alignment task is mainly designed for the machine translation task (Och and Ney, 2003), it is not specifically tuned for other NLP tasks such as annotation projection. Another disadvantage is that this framework depends on full-sequence word alignment where each alignment pair in the sequence will be taken into account for annotation projection. Then, the entity projection could always be disrupted by alignment errors on low-frequency word pairs and outliers, especially when the quality of bitexts cannot be guaranteed in low-resource languages. For intuition, in Fig. 1, the overall word alignment quality seems to be acceptable. But for entity projection, Amerikann Sesi Radyosun"
C16-1045,W04-2905,0,0.0572416,"ion ... in Congo Figure 2: The framework of the bitext name tagger. become much worse when considering the projection over low frequency phrases such as names, here we add some rules for name searching before using word alignment results. For each labeled name in a English sentence, we search for the corresponding IL name on the IL side of the bitext as follows: 1. Firstly, if the Levenshtein distance between an IL word sequence and the English name is close enough, the word sequence will be labeled with the English name tag. 2. The second choice of measuring the similarity is to use Soundex (Raghavan and Allan, 2004). 3. If previous steps can not find the corresponding name in IL, a word-to-word translation table is used. The table is derived from GIZA++ and we only keep top 5 most credible translations for each word. And an exact word-to-word translation of entity names is carried out for string matching. After searching steps, we use word alignment results to project names that has not been found in IL. Here we follow constraints that the number of words in a name should be the same between English and IL mentions, and words in the projected name should be contiguous. Finally, we only keep those sentenc"
C16-1045,Q14-1005,0,0.4137,"ows an example of entity projection with word alignment results from English to Turkish. On English side, Voice of America Radio and Congo are tagged as an organization (ORG) and a location (LOC) respectively by an English name tagger. The dashed lines represent word alignment results generated by a word alignment tool. Following alignment results, we can project labels to Amerikann Sesi and Congo on Turkish side automatically. A major problem of this framework is that it suffers from noises produced by word alignment errors. Thus, some de-noising methods have been proposed (Kim et al., 2010; Wang and Manning, 2014). Though promising, this framework has several disadvantages. One shortcoming is that it totally depends on word alignment results. In this case, noise propagation from word alignment errors is heavily troublesome. To alleviate this problem, there are post-processing methods for annotation correction (Kim et al., 2010) and soft expectations to make use of more probabilistic information inside word alignment results (Wang and Manning, 2014). Although post-processing correction is efficient to filter out wrong labels, it can hardly find back labels which have been lost in the word alignment step"
C16-1045,P13-1106,0,0.160229,"or annotation projection are based on word alignment results. Kim et al. (2010) employed both heuristic method and alignment correction with alignment dictionary of entity mentions. Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Wang and Manning (2014) project model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries in word alignment projection. 6 You can download the code of LSTM-CRF tagger from https://github.com/glample/tagger 468 Wang et al. (2013) also proposed a method to joint train word alignment and bilingual name tagging, which involves training process of word alignment. But this joint method is based on two assumptions. First, it requires a readily-trained name tagger in each languages. Even more, both taggers need to have competitive strengths so that they can correct each other. Unfortunately, in the case of low resource languages, no competitive name tagger is available. One of most recent works linked to annotation projection was proposed by Fang and Cohn (2016) for the task of part of speech tagging (POS). Their work intere"
C16-1045,N01-1026,0,0.131351,"an generate better annotations projected from English-IL parallel corpora. The performance of IL name tagger can also be improved significantly by training on the newly projected IL annotation set. 1 Introduction Annotation projection task aims to deal with low resource issues where human annotations are limited or unavailable in incident languages or domains. Since supervised learning algorithms can not work without annotation sets, annotation projection methods could automatically generate annotations from another language or domain where rich annotation sets are available, such as English. Yarowsky and Ngai (2001) proposed a method of using parallel text with word alignment results to project annotations. Fig. 1 shows an example of entity projection with word alignment results from English to Turkish. On English side, Voice of America Radio and Congo are tagged as an organization (ORG) and a location (LOC) respectively by an English name tagger. The dashed lines represent word alignment results generated by a word alignment tool. Following alignment results, we can project labels to Amerikann Sesi and Congo on Turkish side automatically. A major problem of this framework is that it suffers from noises"
C16-1045,N16-1029,1,0.746262,"Missing"
D11-1071,C10-1032,0,0.18114,"profile (attribute-value pairs) for each query. •Maxent (f5 ): a pointwise ranker implemented using OpenNLP Maxent toolkit4 which is based on maximum entropy model. •SVM (f6 ): a pointwise ranker implemented using SV M light (Joachims, 1999). •SVM ranking (f7 ): a pairwise ranker implemented using SV M rank (Joachims, 2006). •ListNet (f8 ): a listwise ranker presented in (Cao et al., 2007). The four supervised rankers apply exactly the same set of features except that SVM ranking (f7 ) needs to double expand the feature vector. The features are categorized into three levels, surface features (Dredze et al., 2010; Zheng et al., 2010), document features (Dredze et al., 2010; Zheng et al., 2010), and profiling features (entity slots that are extracted by the slot filling toolkit (Chen et al., 2011)). 4.4 MiCR for Entity Linking We convert the collaborator searching problem into a clustering problem, i.e., for a given query q in the task, we retrieve at most K = 300 documents from the large corpus C, each of which contains q.string; we then apply a clustering algorithm to generate clusters over the documents, and form query collaborators (excluding q.text) from the cluster that contains q.text. We experi"
D11-1071,P08-1067,0,0.0128617,"ve ranking, namely, micro collaborative ranking (MiCR), macro collaborative ranking (MaCR) and micro-macro collaborative ranking (MiMaCR). Experiments on entity linking task show that our proposed scheme is indeed effective and promising. 1 Introduction Many natural language processing tasks can be formalized as a ranking problem, namely to rank a collection of candidate “objects” with respect to a “query”. For example, intensive studies were devoted to parsing in which multiple possible parsing trees or forests are ranked with respect to a sentence (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), machine translation in which multiple translation hypotheses are ranked with respect to a source sentence (Och, 2002; Shen et al., 2005), anaphora resolution in which multiple antecedents are ranked with respect to an anaphora (Yang et al., 2008), and question answering in which multiple possible answers are ranked with respect to a question (Ravichandran et al., 2003). Previous studies mainly focused on improving the ranking performance using one stand-alone learning algorithm on isolated queries. Although a wide range of learning algorithms (unsupervised, supervised or semi-supervised) is"
D11-1071,W03-1209,0,0.0112048,"n of candidate “objects” with respect to a “query”. For example, intensive studies were devoted to parsing in which multiple possible parsing trees or forests are ranked with respect to a sentence (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), machine translation in which multiple translation hypotheses are ranked with respect to a source sentence (Och, 2002; Shen et al., 2005), anaphora resolution in which multiple antecedents are ranked with respect to an anaphora (Yang et al., 2008), and question answering in which multiple possible answers are ranked with respect to a question (Ravichandran et al., 2003). Previous studies mainly focused on improving the ranking performance using one stand-alone learning algorithm on isolated queries. Although a wide range of learning algorithms (unsupervised, supervised or semi-supervised) is available, each with its strengths and weaknesses, there 771 Heng Ji Computer Science Department Queens College and Graduate Center City University of New York hengji@cs.qc.cuny.edu is not a learning algorithm that can work best on all types of data. In such a situation, it would be desirable to build a “collaborative” model by integrating multiple models. Such an idea f"
D11-1071,P07-1059,0,0.0193326,"rformance of many problems, in which classification is the most intensively studied (Rokach, 2009). The other situation is related with isolated queries handled by learning algorithms. The single query may not be formulated with the best terms or the query itself may not contain comprehensive information required for a highperformance ranking algorithm. Therefore, techniques of query expansion or query reformulation can be introduced and previous research has shown the effectiveness of those techniques in such applications as information retrieval and question answering (Manning et al., 2008; Riezler et al., 2007). Nevertheless, previous research normally considers query reformulation as a new query for the ranking system, it would be more desirable to form a largerscale “collaborative” group for the query and make a unified decision based on the group. Inspired from human collaborative learning in which two or more people form a group and accomplish work together, we propose a new ranking scheme, collaborative ranking, which aims to imitate human collaborative learning and enhance system ranking performance. The main idea is to seek collaborations for each query from two levels: (1) query-level: searc"
D11-1071,J08-3002,0,0.0570362,"ntroduction Many natural language processing tasks can be formalized as a ranking problem, namely to rank a collection of candidate “objects” with respect to a “query”. For example, intensive studies were devoted to parsing in which multiple possible parsing trees or forests are ranked with respect to a sentence (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), machine translation in which multiple translation hypotheses are ranked with respect to a source sentence (Och, 2002; Shen et al., 2005), anaphora resolution in which multiple antecedents are ranked with respect to an anaphora (Yang et al., 2008), and question answering in which multiple possible answers are ranked with respect to a question (Ravichandran et al., 2003). Previous studies mainly focused on improving the ranking performance using one stand-alone learning algorithm on isolated queries. Although a wide range of learning algorithms (unsupervised, supervised or semi-supervised) is available, each with its strengths and weaknesses, there 771 Heng Ji Computer Science Department Queens College and Graduate Center City University of New York hengji@cs.qc.cuny.edu is not a learning algorithm that can work best on all types of dat"
D11-1071,C10-1145,0,0.19813,"Missing"
D11-1071,N10-1072,0,0.0721746,"lue pairs) for each query. •Maxent (f5 ): a pointwise ranker implemented using OpenNLP Maxent toolkit4 which is based on maximum entropy model. •SVM (f6 ): a pointwise ranker implemented using SV M light (Joachims, 1999). •SVM ranking (f7 ): a pairwise ranker implemented using SV M rank (Joachims, 2006). •ListNet (f8 ): a listwise ranker presented in (Cao et al., 2007). The four supervised rankers apply exactly the same set of features except that SVM ranking (f7 ) needs to double expand the feature vector. The features are categorized into three levels, surface features (Dredze et al., 2010; Zheng et al., 2010), document features (Dredze et al., 2010; Zheng et al., 2010), and profiling features (entity slots that are extracted by the slot filling toolkit (Chen et al., 2011)). 4.4 MiCR for Entity Linking We convert the collaborator searching problem into a clustering problem, i.e., for a given query q in the task, we retrieve at most K = 300 documents from the large corpus C, each of which contains q.string; we then apply a clustering algorithm to generate clusters over the documents, and form query collaborators (excluding q.text) from the cluster that contains q.text. We experimented the following"
D11-1071,N04-1023,0,\N,Missing
D11-1071,P05-1022,0,\N,Missing
D14-1129,J03-3002,0,0.252781,"sites. Another parameter can be easily adjusted to achieve the best performance. Therefore, our method can be used in other websites with different styles, without much effort to optimize these parameters. 2 Related Work A large amount of literature has been published on parallel resource mining from the web. According to the existing form of the parallel resource on the Internet, related work can be categorized as follows: Mining from bilingual websites Most existing web mining systems aimed at mining bilingual resource from the bilingual websites, such as PTMiner (Nie et al., 1999), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTI (Chen et al., 2004). PTMiner uses search engines to pinpoint the candidate sites that are likely to contain parallel pages, and then uses the collected URLs as seeds to further crawl each web site for more URLs. Web page pairs are extracted based on manually defined URL pattern matching, and further filtered according to several criteria. STRAND uses a search engine to search for multilingual websites and generated candidate page pairs based on manually created substitution rules. Then, it filters some candidate pairs by analyzing the HTML pages. PTI crawls"
D14-1129,C10-2034,1,0.713251,"en for each bilingual website, it extracts parallel pages based on their content. Mining from bilingual web pages Parallel/bilingual resources may exist not only in two parallel monolingual web pages, but also in single bilingual web pages. Jiang et al. (2009) used an adaptive pattern-based method to mine interesting bilingual data based on the observation that bilingual data usually appears collectively following similar patterns. They found that bilingual web pages are a promising source of up-to-date bilingual terms/sentences which cover many domains and application scenarios. In addition, Feng et al. (2010) proposed a new method 1217 to automatically acquire bilingual web pages from the result pages of a search engine. Mining from comparable corpus Several attempts have been made to extract parallel resources from comparable corpora. Zhao et al. (2002) proposed a robust, adaptive approach for mining parallel sentences from a bilingual comparable news collection. In their method, sentence length models and lexiconbased models were combined under a maximum likelihood criterion. Smith et al. (2010) found that Wikipedia contains a lot of comparable documents, and adopted a ranking model to select pa"
D14-1129,N10-1063,0,0.0224972,"up-to-date bilingual terms/sentences which cover many domains and application scenarios. In addition, Feng et al. (2010) proposed a new method 1217 to automatically acquire bilingual web pages from the result pages of a search engine. Mining from comparable corpus Several attempts have been made to extract parallel resources from comparable corpora. Zhao et al. (2002) proposed a robust, adaptive approach for mining parallel sentences from a bilingual comparable news collection. In their method, sentence length models and lexiconbased models were combined under a maximum likelihood criterion. Smith et al. (2010) found that Wikipedia contains a lot of comparable documents, and adopted a ranking model to select parallel sentence pairs from comparable documents. Bharadwaj et al. (2011) used a SVM classifier with some new features to identify parallel sentences from Wikipedia. 3 Iterative Link-based Parallel Web Pages Mining As mentioned, the basic idea of our method is that the similarity of two pages can be inferred from their neighbors. This idea is illustrated in Figure 1. A D C E B ? A’ D’ C’ B’ E’ Figure 1 Illustration of the link-based method In Figure 1, A, B, C, D and E are some pages in the sam"
D14-1129,P09-1098,0,0.0798181,"yed an important role in multilingual Natural Language Processing, especially in Machine Translation (MT) and Crosslingual Information Retrieval(CLIR). However, it’s time-consuming to build parallel corpora manually. Some existing parallel corpora are subject to subscription or license fee and thus not freely available, while others are domain-specific. Therefore, a lot of previous research has focused on automatically mining parallel corpora from the web. In the past decade, there have been extensive studies on parallel resource extraction from the web (e.g., Chen and Nie, 2000; Resnik 2003; Jiang et al., 2009) and many effective Web mining systems have been developed such as STRAND, PTMiner, BITS and WPDE. For most of these mining systems, there is a typical parallel resource mining strategy which involves three steps: (1) locate the bilingual websites (2) identify parallel web pages from these bilingual websites and (3) extract bilingual resources from the parallel web pages. In this paper, we focus on the step (2) which is regarded as the core of the mining system (Chunyu, 2007). Estimating the translation similarity of two pages is the most basic and key problem in this step. Previous approaches"
D14-1129,A00-1004,0,\N,Missing
D14-1129,1999.mtsummit-1.79,0,\N,Missing
D14-1129,P06-1062,0,\N,Missing
D14-1186,C14-1035,0,0.0603901,"Missing"
D14-1186,P06-1085,0,0.305601,"ghbors. In a text processing pipeline of WS, TE and KE, it is obvious that imprecise WS results will make the overall system performance unsatisfying. At the same time, we can hardly make use of domain-level and document-level information collected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2.1 G0 0 G1 1 Hw 2 H wm 3 Hwm1  Hwmi w mji  N mj m Hw Nm M |"
D14-1186,C12-1098,1,0.839502,"llected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels? Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation (Goldwater et al., 2006; Goldwater et al., 2009). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora (Ren et al., 2008; Xu et al., 2008; Zhang et al., 2010; Li et al., 2012; Chang et al., 2014). Inspired by such existing work, we propose a four-level DP based model, 1774 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774–1778, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2.1 G0 0 G1 1 Hw 2 H wm 3 Hwm1  Hwmi w mji  N mj m Hw Nm M |V | Figure 1: DP-4 Model named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documen"
D14-1186,P09-1012,0,0.200969,"Qatar. 2014 Association for Computational Linguistics 2.1 G0 0 G1 1 Hw 2 H wm 3 Hwm1  Hwmi w mji  N mj m Hw Nm M |V | Figure 1: DP-4 Model named DP-4, to adapt to three levels: corpus, domain and document. In our model, various DPs are designed to reflect the smoothed word distributions in the whole corpus, different domains and different documents. Same as the DP based segmentation models, our model can be easily used as a semi-supervised framework, through exerting on the corpus level the word distributions learned from the available segmentation results. Referring to the work of Mochihashi et al. (2009), we conduct word segmentation using a sentence-wise Gibbs sampler, which combines the Gibbs sampling techniques with the dynamic programming strategy. During the sampling process, the importance values of segmented words are measured in domains and documents respectively, and words, terms and keywords are jointly learned. 2 DP-4 Model Goldwater et al. (2006) applied the HDP model on the word segmentation task. In essence, Goldwater’s model can be viewed as a bigram language model with a unigram back-off. With the language model, word segmentation is implemented by a character-based Gibbs samp"
D14-1186,W06-0127,0,0.0362503,"eness of our method. 1 •Ï (heparinoid) Ç(thrombocytopenia) (have) (with) sû(relation). This is a correctly segmented Chinese sentence. The document containing the example sentence mainly talks about the property of ”{ • (heparinoid)” which can be regarded as one keyword of the document. At the same time, the word @ •Ï Ç(thrombocytopenia) appears frequently in the disease domain and can be treated as a domain-specific term. However, for such a simple sentence, current segmentation tools perform poorly. The segmentation result with the state-of-the-art Conditional Random Fields (CRFs) approach (Zhao et al., 2006) is as follows: @ •(blood platelet) Ï (reduction) Ç(symptom) {(of same kind) •(liver) Introduction For Chinese language which does not contain explicitly marked word boundaries, word segmentation (WS) is usually the first important step for many Natural Language Processing (NLP) tasks including term extraction (TE) and keyword extraction (KE). Generally, Chinese terms and keywords can be regarded as words which are representative of one domain or one document respectively. Previous work of TE and KE normally used the pipelined approaches which first conducted WS and then extracted important wo"
D14-1198,P11-1056,0,0.114042,"Missing"
D14-1198,chang-manning-2012-sutime,0,0.0071302,"3.1 • An entity mention is correct if its entity type (7 in total) and head offsets are correct. • A relation is correct if its type (6 in total) and the head offsets of its two arguments are correct. • An event trigger is correct if its event subtype (33 in total) and offsets are correct. • An argument link is correct if its event subtype, offsets and role match those of any of the reference argument mentions. Rank 1 2 3 4 5 6 7 8 9 10 In this paper we focus on entity arguments while disregard values and time expressions because they can be most effectively extracted by handcrafted patterns (Chang and Manning, 2012). 3.2 Results Based on the results of our development set, we trained all models with 21 iterations and chose the beam size to be 8. For the k-best MIRA updates, we set k as 3. Table 3 compares the overall performance of our approaches and baseline methods. Discussions Feature Study Feature Frame=Killing Die Frame=Travel Transport Physical(Artifact, Destination) w1 =“home” Transport Frame=Arriving Transport ORG-AFF(Person, Entity) Lemma=charge Charge-Indict Lemma=birth Be-Born Physical(Artifact,Origin) Frame=Cause harm Injure Weight 0.80 0.61 0.60 0.59 0.54 0.48 0.45 0.44 0.44 0.43 Table 4: To"
D14-1198,P04-1015,0,0.0284067,"b is the beam size, and m is the sentence length. The actual execution time is much shorter, especially when entity type constraints are applied. 2.2 Parameter Estimation For each training instance (x, y), the structured perceptron algorithm seeks the assignment with the highest model score: z = argmax f (x, y 0 ) · w y 0 ∈Y(x) and then updates the feature weights by using: wnew = w + f (x, y) − f (x, z) We relax the exact inference problem by the aforementioned beam-search procedure. The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate (Collins and Roark, 2004), an instance of violation-fixing methods (Huang et al., 2012). In the rest of this paper, we override y and z to denote prefixes of structures. In addition to the simple perceptron update, we also apply k-best MIRA (McDonald et al., 2005), an online large-margin learning algorithm. During each update, it keeps the norm of the change to feature weights w as small as possible, and forces the margin between y and the k-best candidate z greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem: min kwnew − wk s.t. wnew f (x, y) − wnew f (x, z) ≥ L(y, z) ∀z ∈ best"
D14-1198,W02-1001,0,0.0242356,"d joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relation"
D14-1198,N12-1015,0,0.0158919,"ion time is much shorter, especially when entity type constraints are applied. 2.2 Parameter Estimation For each training instance (x, y), the structured perceptron algorithm seeks the assignment with the highest model score: z = argmax f (x, y 0 ) · w y 0 ∈Y(x) and then updates the feature weights by using: wnew = w + f (x, y) − f (x, z) We relax the exact inference problem by the aforementioned beam-search procedure. The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate (Collins and Roark, 2004), an instance of violation-fixing methods (Huang et al., 2012). In the rest of this paper, we override y and z to denote prefixes of structures. In addition to the simple perceptron update, we also apply k-best MIRA (McDonald et al., 2005), an online large-margin learning algorithm. During each update, it keeps the norm of the change to feature weights w as small as possible, and forces the margin between y and the k-best candidate z greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem: min kwnew − wk s.t. wnew f (x, y) − wnew f (x, z) ≥ L(y, z) ∀z ∈ bestk (x, w) We employ the following three loss functions for comp"
D14-1198,P08-1030,1,0.435611,"Missing"
D14-1198,P14-1038,1,0.942874,"ty mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algor"
D14-1198,P13-1008,1,0.8719,"erdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based"
D14-1198,P05-1012,0,0.091829,"nts (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perceptron framework with a segment-based beam-search algorithm to construct the information networks (Collins, 2002; Li et al., 2013; Li and Ji, 2014). In addition to the perceptron update, we also apply k-best MIRA (McDonald et al., 2005), which refines the perceptron update in three aspects: it is flexible in using various loss functions, it is a large-margin approach, and it can use mulitple candidate structures to tune feature weights. In an information network, we can capture the interactions among multiple nodes by learning joint features during training. In addition to the cross-component dependencies studied in (Li et al., 2013; Li and Ji, 2014), we are able to capture interactions between relations and events. For example, in Figure 1, if we know that the Person mention “Asif Mohammed Hanif ” is an Attacker of the Atta"
D14-1198,D11-1001,0,0.13364,"ons, relations and events from unstructured texts, and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argumen"
D14-1198,W09-1406,0,0.170416,"and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We apply a structured perc"
D14-1198,P13-1161,0,0.0160849,"rom unstructured texts, and these three subtasks are closely interdependent: entity mentions are core components of relations and events, and the extraction of relations and events can help to accurately recognize entity mentions. In addition, the theory of eventualities (D¨olling, 2011) suggested that relations can be viewed as states that events start from and result in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model. Some recent research attempted to jointly model multiple IE subtasks (e.g., (Roth and Yih, 2007; Riedel and McCallum, 2011; Yang and Cardie, 2013; Riedel et al., 2009; Singh et al., 2013; Li et al., 2013; Li and Ji, 2014)). For example, Roth and Yih (2007) conducted joint inference over entity mentions and relations; Our previous work jointly extracted event triggers and arguments (Li et al., 2013), and entity mentions and relations (Li and Ji, 2014). However, a single model that can extract all of them has never been studied so far. For the first time, we uniformly represent the IE output from each sentence as an information network, where entity mentions and event triggers are nodes, relations and event-argument links are arcs. We ap"
D14-1198,P03-2030,0,\N,Missing
D15-1020,P10-1143,0,0.552051,"Missing"
D15-1020,P13-1008,1,0.871345,"Missing"
D15-1020,D14-1198,1,0.909543,"Missing"
D15-1020,W09-3208,1,0.94049,"casts. The CC is either generated by automatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empir"
D15-1020,P14-5010,0,0.00802599,"Missing"
D15-1020,W09-4303,1,0.952905,"her generated by automatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empirical Methods in Natu"
D15-1020,D12-1045,0,0.53257,"tomatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Proce"
D15-1077,D11-1072,0,\N,Missing
D15-1077,C10-1032,0,\N,Missing
D15-1077,E06-1002,0,\N,Missing
D15-1077,D09-1025,0,\N,Missing
D15-1077,P13-1024,1,\N,Missing
D15-1077,P11-1138,0,\N,Missing
D15-1077,C12-1028,1,\N,Missing
D15-1077,P13-1128,0,\N,Missing
D15-1077,D07-1074,0,\N,Missing
D15-1077,D11-1011,0,\N,Missing
D15-1077,D11-1071,1,\N,Missing
D15-1077,I11-1113,0,\N,Missing
D15-1078,P11-1115,1,0.806341,"web link structure (Balog et al, 2008b; You et al, 2011; Blanco et al, 2011; Neumayer et al, 2012; Bron et al, 2013).  Slot Filling The goal of slot filling is to seek and extract the concrete instances (fillers) specific to multiple entity attributes (slots) from a large-scale textual data set (Ji et al., 2010 and 2011; Surdeanu, 2013; Surdeanu and Ji, 2014). The quality of the fillers largely depends on the performance of entity archiving and information extraction. Related studies on archiving mainly employed traditional retrieval techniques, including query expansion and string matching (Ji and Grishman, 2011). A few studies involved document ranking and prioritizing by using probability model (Byrne and Dunnion, 2010; Roth et al, 2014) and statistical language model (Chrupala et al, 2010). For filler extraction, great efforts were made to generate effective patterns and structure perceptrons by supervised learning and reasoning (Chen et al, 2010; Grishman and Min, 2010; Gao et al, 2010; Surdeanu et al, 2011; Louis et al, 2011; Kisiel et al, 2013). And effective feature selection and distant-supervision based classifiers have been explored (Lehmann et al, 2010; Artiles et al, 2011; Sun et al, 2011;"
D15-1078,N04-1043,0,0.0223523,"for FM to recall a larger number of relevant documents. It is helpful to minimize the loss of relevant documents before proceeding to CEA. To improve EM, we focus on identifying the common words that completely match the full name of the target entity. The words normally are elusive and easily treated as a correct entity name, called deceptive name, see that in (1). (1) Countrywide Financial &lt;ORG&gt; True: Countrywide Financial Corporation. Deceptive: Bank of America purchased the failing countrywide financial for $4.1 billion. To reduce EM errors caused by deceptive names, we use name tagging (Miller et al, 2004) to distinguish deceptive names and true names. Further, we filter the documents that are mistakenly retrieved based on the match between a deceptive name and the full-entity-name based query Q. We leverage an Alternate Name Table (ANT) for query expansion. ANT is a mapping table between entity name and alternate name. An alternate name is either generated according to the naming conventions (Burman et al, 2011), such as abbreviation, suffixation and revivification. Some alternative names were extracted from knowledge base through redirect links (Nia et al, 2014), such as nicknames in Wikipedi"
D15-1078,D13-1003,0,0.0977547,"A few studies involved document ranking and prioritizing by using probability model (Byrne and Dunnion, 2010; Roth et al, 2014) and statistical language model (Chrupala et al, 2010). For filler extraction, great efforts were made to generate effective patterns and structure perceptrons by supervised learning and reasoning (Chen et al, 2010; Grishman and Min, 2010; Gao et al, 2010; Surdeanu et al, 2011; Louis et al, 2011; Kisiel et al, 2013). And effective feature selection and distant-supervision based classifiers have been explored (Lehmann et al, 2010; Artiles et al, 2011; Sun et al, 2011; Roth and Klakow, 2013; Roth et al, 2014). Recently active learning (Angeli et al, 2014), truth-finding (Yu et al, 2014) as well, scanning (Yu et al., 2015) and ensemble learning (Viswanathan et al., 2015) were introduced to this field.  Brief Summary In all, entity search concentrates on the analysis of a single specific aspect of an entity, which is of interest or related to a domain. In the expert finding task, only academic careers of person entities (potential experts) are of concern in entity-document relevance determination. By contrast, for the sake of comprehensive understanding of an entity, entity archi"
D15-1078,C14-1149,1,0.829911,"2010; Roth et al, 2014) and statistical language model (Chrupala et al, 2010). For filler extraction, great efforts were made to generate effective patterns and structure perceptrons by supervised learning and reasoning (Chen et al, 2010; Grishman and Min, 2010; Gao et al, 2010; Surdeanu et al, 2011; Louis et al, 2011; Kisiel et al, 2013). And effective feature selection and distant-supervision based classifiers have been explored (Lehmann et al, 2010; Artiles et al, 2011; Sun et al, 2011; Roth and Klakow, 2013; Roth et al, 2014). Recently active learning (Angeli et al, 2014), truth-finding (Yu et al, 2014) as well, scanning (Yu et al., 2015) and ensemble learning (Viswanathan et al., 2015) were introduced to this field.  Brief Summary In all, entity search concentrates on the analysis of a single specific aspect of an entity, which is of interest or related to a domain. In the expert finding task, only academic careers of person entities (potential experts) are of concern in entity-document relevance determination. By contrast, for the sake of comprehensive understanding of an entity, entity archiving necessarily 666 takes multiple and diverse aspects into account, such as a person’s career, f"
D15-1078,N15-1126,1,0.842292,"Missing"
D15-1081,C12-1028,1,0.850897,"rentiating power of our system on the KB relations. It is worth mentioning that there exists a large number of well established ontologies for different sub-domains of Earth Science. SWEET ontologies1 , for example, widely capture Earth and Environmental terminologies. By adopting these ontologies, we will be able to considerably improve our domain EL performance, and the benefits of EL in the domain will further get revealed. 4.5 tures from the source documents in order to precisely select collaborator mentions for collective inference. These features include topic modeling (Xu et al., 2012; Cassidy et al., 2012), relation constraint (Cheng and Roth, 2013), coreferential chaining (Nguyen et al., 2012; Huang et al., 2014), and dependency restriction (Ling et al., 2014). Some recent work utilized multi-layer linguistic analysis integration to capture contextual properties for better mention collection (Pan et al., 2015). While many of these approaches have been proved to be effective, the dependency on deep linguistic knowledge makes it difficult to migrate them to a new language or domain. In contrast to these methods, we establish a very loose setting for the mention selection, and rely on the quantif"
D15-1081,D13-1184,0,0.118323,"ations. It is worth mentioning that there exists a large number of well established ontologies for different sub-domains of Earth Science. SWEET ontologies1 , for example, widely capture Earth and Environmental terminologies. By adopting these ontologies, we will be able to considerably improve our domain EL performance, and the benefits of EL in the domain will further get revealed. 4.5 tures from the source documents in order to precisely select collaborator mentions for collective inference. These features include topic modeling (Xu et al., 2012; Cassidy et al., 2012), relation constraint (Cheng and Roth, 2013), coreferential chaining (Nguyen et al., 2012; Huang et al., 2014), and dependency restriction (Ling et al., 2014). Some recent work utilized multi-layer linguistic analysis integration to capture contextual properties for better mention collection (Pan et al., 2015). While many of these approaches have been proved to be effective, the dependency on deep linguistic knowledge makes it difficult to migrate them to a new language or domain. In contrast to these methods, we establish a very loose setting for the mention selection, and rely on the quantified information computed from the structured"
D15-1081,W06-3306,0,0.0134087,"ting time to link all the mentions in a document is O(nm · nc · nnc · nnm ), where nm is the number of linkable mentions in the document, nc is the number of candidates for each mention, and nnc is the number of neighbor nodes of a candidate, and nnm is the number of neighbors of a mention. 5 There is a limited amount of research work in the literature that focused solely on domainspecific EL (Zheng et al., 2014). In the biomedical domain, a few studies have been found on EL-related tasks such as scientific name discovery (Akella et al., 2012), gene name normalization (Hirschman et al., 2005; Fang et al., 2006; Dai et al., 2010), biomedical named entity recognition (Usami et al., 2011; Van Landeghem et al., Related Work In recent years, collective inference methods for EL have become increasingly popular. Many efforts have been devoted to encoding linguistic fea1 2 http://sweet.jpl.nasa.gov https://lucene.apache.org/ 702 2012) and concept mention extraction (Tsai et al., 2013). The baseline system (Zheng et al., 2014) in this paper is the work most similar to ours in a sense of collectively aligning mentions to structured KBs. However, our system differs by integrating a context similarity ranking"
D15-1081,H92-1045,0,0.489312,"the first ranking step, we examine the candidates without the context and prefers those with higher importance in the KB. Equation (2) computes the salience score Sa (c) for a candidate c: Sa (c) = X H(r) r∈R(c),et ∈Et (r) Sa (et ) L(et ) (2) where R(c) is the relation set for c in the KB; H(r) is given by Equation (1); Et (r) is the tail entity set with c being the head entity and r being the connecting relation in the KB; L(et ) denotes the 1 http://wiki.dbpedia.org http://bioportal.bioontology.org 3 http://www.obofoundry.org 2 696 course according to the one sense per discourse assumption (Gale et al., 1992), but for simplicity we heuristically set wm to be 7-sentence wide as a hyperparameter. Two mention vertices will be connected via a dashed edge if they are coreferential but are not located in the same context window. Here we determine the coreference by performing substring matching and abbreviation expansion. The dashed edge indicates the outof-context coreferential mention together with its neighbors will be indirectly included in Gm as extended context to later facilitate the candidate graph collective validation. Note that all of these loose settings comply with our intention of generati"
D15-1081,N15-1119,1,0.916153,"orporating these domain ontologies, we can easily increase the entity coverage and reduce noise for deploying EL in various new domains. In order to make the most of the KB structure, the mention context should be matched against the KB such that the relevant KB information can be extracted. A collective way of aligning co-occurred mentions to the KB graph has been proved to be a successful strategy to better represent the source context (Pennacchiotti and Pantel, 2009; Fern´andez et al., 2010; Cucerzan, 2011; Han et al., 2011; Ratinov et al., 2011; Dalton and Dietz, 2013; Zheng et al., 2014; Pan et al., 2015). We take a further step to consider quantitatively differentiating entity relations in the KB in order to evaluate entity candidates more precisely. Meanwhile, we jointly validate these candidates by aligning them back to the source context and integrating multiple ranking results. This novel EL framework deeply exploits the KB structure with a light weight representation of the source context, and thus enables a smooth migration to new languages and domains. The main novel contributions of this paper are summarized as follows: 1) We design an unsupervised EL algorithm, namely, Quantified Col"
D15-1081,D09-1025,0,0.0197187,"omain ontology repositories such as BioPortal2 and OBO Foundry3 which provide significantly more domain knowledge than general KBs for EL to leverage. By incorporating these domain ontologies, we can easily increase the entity coverage and reduce noise for deploying EL in various new domains. In order to make the most of the KB structure, the mention context should be matched against the KB such that the relevant KB information can be extracted. A collective way of aligning co-occurred mentions to the KB graph has been proved to be a successful strategy to better represent the source context (Pennacchiotti and Pantel, 2009; Fern´andez et al., 2010; Cucerzan, 2011; Han et al., 2011; Ratinov et al., 2011; Dalton and Dietz, 2013; Zheng et al., 2014; Pan et al., 2015). We take a further step to consider quantitatively differentiating entity relations in the KB in order to evaluate entity candidates more precisely. Meanwhile, we jointly validate these candidates by aligning them back to the source context and integrating multiple ranking results. This novel EL framework deeply exploits the KB structure with a light weight representation of the source context, and thus enables a smooth migration to new languages and"
D15-1081,P11-1138,0,0.130681,"more domain knowledge than general KBs for EL to leverage. By incorporating these domain ontologies, we can easily increase the entity coverage and reduce noise for deploying EL in various new domains. In order to make the most of the KB structure, the mention context should be matched against the KB such that the relevant KB information can be extracted. A collective way of aligning co-occurred mentions to the KB graph has been proved to be a successful strategy to better represent the source context (Pennacchiotti and Pantel, 2009; Fern´andez et al., 2010; Cucerzan, 2011; Han et al., 2011; Ratinov et al., 2011; Dalton and Dietz, 2013; Zheng et al., 2014; Pan et al., 2015). We take a further step to consider quantitatively differentiating entity relations in the KB in order to evaluate entity candidates more precisely. Meanwhile, we jointly validate these candidates by aligning them back to the source context and integrating multiple ranking results. This novel EL framework deeply exploits the KB structure with a light weight representation of the source context, and thus enables a smooth migration to new languages and domains. The main novel contributions of this paper are summarized as follows: 1)"
D15-1081,P14-1036,1,0.844871,"ell established ontologies for different sub-domains of Earth Science. SWEET ontologies1 , for example, widely capture Earth and Environmental terminologies. By adopting these ontologies, we will be able to considerably improve our domain EL performance, and the benefits of EL in the domain will further get revealed. 4.5 tures from the source documents in order to precisely select collaborator mentions for collective inference. These features include topic modeling (Xu et al., 2012; Cassidy et al., 2012), relation constraint (Cheng and Roth, 2013), coreferential chaining (Nguyen et al., 2012; Huang et al., 2014), and dependency restriction (Ling et al., 2014). Some recent work utilized multi-layer linguistic analysis integration to capture contextual properties for better mention collection (Pan et al., 2015). While many of these approaches have been proved to be effective, the dependency on deep linguistic knowledge makes it difficult to migrate them to a new language or domain. In contrast to these methods, we establish a very loose setting for the mention selection, and rely on the quantified information computed from the structured KB to collectively evaluate and validate the entity candidates. S"
D15-1081,W11-0208,0,0.0156387,", where nm is the number of linkable mentions in the document, nc is the number of candidates for each mention, and nnc is the number of neighbor nodes of a candidate, and nnm is the number of neighbors of a mention. 5 There is a limited amount of research work in the literature that focused solely on domainspecific EL (Zheng et al., 2014). In the biomedical domain, a few studies have been found on EL-related tasks such as scientific name discovery (Akella et al., 2012), gene name normalization (Hirschman et al., 2005; Fang et al., 2006; Dai et al., 2010), biomedical named entity recognition (Usami et al., 2011; Van Landeghem et al., Related Work In recent years, collective inference methods for EL have become increasingly popular. Many efforts have been devoted to encoding linguistic fea1 2 http://sweet.jpl.nasa.gov https://lucene.apache.org/ 702 2012) and concept mention extraction (Tsai et al., 2013). The baseline system (Zheng et al., 2014) in this paper is the work most similar to ours in a sense of collectively aligning mentions to structured KBs. However, our system differs by integrating a context similarity ranking and a candidate validation to conduct a two-way collective inference with be"
D15-1081,I11-1029,0,0.151857,"hile many of these approaches have been proved to be effective, the dependency on deep linguistic knowledge makes it difficult to migrate them to a new language or domain. In contrast to these methods, we establish a very loose setting for the mention selection, and rely on the quantified information computed from the structured KB to collectively evaluate and validate the entity candidates. Since the KB is relatively universal to languages and domains, our approach inherently is language and domain independent. Recent cross-lingual EL approaches can be divided into two types. The first type (McNamee et al., 2011; Cassidy et al., 2011; McNamee et al., 2012; Guo et al., 2012; Miao et al., 2013) translated entity mentions and source documents from the new language into English and then ran English mono-lingual EL to link to English KB. The second type (Monahan et al., 2011; Fahrni et al., 2011; Fahrni et al., 2012; Monahan and Carpenter, 2012; Clarke et al., 2012; Fahrni et al., 2013) developed EL systems on the new language and used cross-lingual KB links to map the link results back to English KB. While the bottleneck of the former method usually is on translation errors, the latter approach heavily r"
D16-1106,D13-1160,0,0.075922,"Missing"
D16-1106,chang-manning-2012-sutime,0,0.0296917,"us on the those events identified by the system which are relevant to the main fields in the GVDB schema.4 We map the arguments of these events onto the corresponding database fields, e.g. the agent of the event corresponds to the GVDB’s shooter name. Since the system identifies multiple such events per article, we count it as correct as long as one argument correctly matches the corresponding value in the GVDB (e.g. the system is correct as long as one extracted event has an agent which matches the GVDB’s shooter name for that article). In addition, we run the Stanford CoreNLP TimeEx system (Chang and Manning, 2012) over the articles in order to identify the time of the reported incident. We report the system’s performance using both exact match against the gold annotation (“strict”) as well as an approximate match, in which the system is correct if it is either a substring or a superstring of the gold annotation. E.g. if the victim name is Sean Bolton, the approximate metric will count both Bolton and Officer Sean Bolton as correct. While performance is high for certain structured types of information, like dates and times, fields like victim and shooter name are much less reliably identified. Furthermo"
D16-1106,D11-1142,0,0.0254719,"n, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem. 1 Although these technological achievements are profound, often times we as researchers apply them to somewhat trivial settings like learning about the latest Hollywood divorces (Wijaya et al., 2015) or learning silly facts about the world, like that hwhite suites, will never go out of, stylei (Fader et al., 2011). In this paper, we call the attention of the NLP community to one particularly good use case of our current technology, which could have profound policy implications: gun violence research. Introduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards unde"
D16-1106,P13-1008,1,0.794198,"mation Table 1: Current contents of the GVDB. Size and level of annotation is continually growing. See Forthcoming Extensions. Current Baselines To establish a baseline level of performance, we run an off-the-shelf information 3 See supplementary material for all extracted information and screenshots. Figure 2: Annotation interface associates structured information (e.g. the time of day when the shooting occurred) with a specific span of text in the article. extraction system on the 7,366 articles and measure precision and recall for identifying key information about the incidents. We use the Li et al. (2013) systems, which identifies a range of entities and events. We focus on the those events identified by the system which are relevant to the main fields in the GVDB schema.4 We map the arguments of these events onto the corresponding database fields, e.g. the agent of the event corresponds to the GVDB’s shooter name. Since the system identifies multiple such events per article, we count it as correct as long as one argument correctly matches the corresponding value in the GVDB (e.g. the system is correct as long as one extracted event has an agent which matches the GVDB’s shooter name for that a"
D16-1106,N10-1021,0,0.103628,"Missing"
D16-1106,W15-1205,0,0.017789,"s. The National Violent Death Registry System, arguably the most organized effort, receives data from only 16 states. Most large-scale epidemiological studies sample information from only 100 Emergency Departments. across the text of thousands of web pages. Replacing expensive, manual data entry with automated processing is exactly the type of problem that NLP is made to solve. In fact, the recent application of NLP tools to social science problems has generated a flurry of exciting and encouraging results. NLP has made novel contributions to the way scientists measure everything from income (Preoctiuc-Pietro et al., 2015b) to mental health (Preoctiuc-Pietro et al., 2015a; Schwartz et al., 2016; Choudhury et al., 2016), disease (Santillana et al., 2015; Ireland et al., 2015; Eichstaedt et al., 2015), and the quality of patient care (Nakhasi et al., 2016; Ranard et al., 2016). Text mining has promise for the study of gun violence, too (Bushman et al., 2016). However, most questions about gun violence are not easily answered using shallow analyses like topic models or word clusters. Epidemiologists want to know, for example, does gun ownership lead to increases in gun violence? Or, is there evidence of contagion"
D16-1106,N13-1008,0,0.0109243,"ssion as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers across the country, data abounds. We argue that this is the exac"
D16-1106,W09-2417,0,0.0216846,"roduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers"
D16-1106,N16-1008,0,0.0243649,"n language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers across the country, data abounds. We argue that this is the exact type of information that NLP is designed to organ"
D16-1106,D15-1059,0,0.0129558,"t from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem. 1 Although these technological achievements are profound, often times we as researchers apply them to somewhat trivial settings like learning about the latest Hollywood divorces (Wijaya et al., 2015) or learning silly facts about the world, like that hwhite suites, will never go out of, stylei (Fader et al., 2011). In this paper, we call the attention of the NLP community to one particularly good use case of our current technology, which could have profound policy implications: gun violence research. Introduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is"
D16-1144,D15-1103,0,0.175276,"em as multi-class classification following the type mutual exclusion assumption (i.e., one type per mention) (Nadeau and Sekine, 2007). Recent work has focused on a much larger set of fine-grained types (Yosef et al., 2012; Ling and Weld, 2012). As the type mutual exclusion assumption no longer holds, they cast the problem as multilabel multi-class (hierarchical) classification problems (Gillick et al., 2014; Yosef et al., 2012; Ling and Weld, 2012). Embedding techniques are also recently applied to jointly learn feature and type representations (Yogatama et al., 2015; Dong et al., 2015). Del Corro et al. (2015) proposed an unsupervised method to generate context-aware candidates types, and subsequently select the most appropriate type. Gillick et al. (2014) discuss the label noise issue in fine-grained typing and propose three pruning heuristics. However, these heuristics aggressively delete training examples and may suffer from low recall (see Table. 4). In the context of distant supervision, label noise issue has been studied for other information extraction tasks such as relation extraction (Takamatsu et al., 2012). In relation extraction, label noise is introduced by the false positive textual m"
D16-1144,E14-4040,0,0.0180677,"Missing"
D16-1144,P05-1045,0,0.0126774,"D consists of a set of extracted entity mentions M = {mi }N i=1 , the context (e.g., sentence, paragraph) of each mention {ci }N i=1 , and the candiN date type sets {Yi }i=1 for each mention. We repre N sent D using a set of triples D = (mi , ci , Yi ) i=1 . Problem Description. For each test mention, we aim to predict the correct type-path in Y based on the mention’s context. More specifically, the test set T is defined as a set of mention-context pairs (m, c), where mentions in T (denoted as Mt ) are extracted from their sentences using existing extractors such as named entity recognizer (Finkel et al., 2005). We denote the gold type-path for a test mention m as Y ∗ . This work focuses on learning a typing model from the noisy training corpus D, and estimating Y ∗ from Y for each test mention m (in set Mt ), based on mention m, its context c, and the learned model. Framework Overview. At a high level, the AFET framework (see also Fig. 2) learns low-dimensional representations for entity types and text features, and infers type-paths for test mentions using the learned embeddings. It consists of the following steps: Knowledge Base 1. Extract text features for entity mentions in training set M and t"
D16-1144,P08-1030,1,0.634711,".. author actor Candidate Type Set (Sub-tree) singer ... Knowledge Bases Distant Supervision Entity: Arnold Schwarzenegger Figure 1: Current systems may detect Arnold Schwarzenegger in sentences S1-S3 and assign the same types to all (listed within braces), when only some types are correct for context (blue labels within braces). Assigning types (e.g., person, organization) to mentions of entities in context is an important task in natural language processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) wor"
D16-1144,D12-1082,0,0.0335026,"Missing"
D16-1144,P14-5010,0,0.0110603,"o similar sets of entities should be more related to each other than those assigned to quite different entities (Jiang et al., 2015) (e.g., actor is Feature Head Token POS Character Word Shape Length Context Brown Cluster Dependency Description Syntactic head token of the mention Tokens in the mention Part-of-Speech tag of tokens in the mention All character trigrams in the head of the mention Word shape of the tokens in the mention Number of tokens in the mention Unigrams/bigrams before and after the mention Brown cluster ID for the head token (learned using D) Stanford syntactic dependency (Manning et al., 2014) associated with the head token Example “HEAD Turing” “Turing”, “Machine” “NN” “:tu”, “tur”, ..., “ng:” “Aa” for “Turing” “2” “CXT B:Maserati ,”, “CXT A:and the” “4 1100”, “8 1101111”, “12 111011111111” “GOV:nn”, “GOV:turing” Table 2: Text features used in this paper. “Turing Machine” is used as an example mention from “The band’s former drummer Jerry Fuchs—who was also a member of Maserati, Turing Machine and The Juan MacLean—died after falling down an elevator shaft.”. more related to director than to author in the left column of Fig. 3). Thus, type correlation between yk and yk0 (denoted as"
D16-1144,P09-1113,0,0.0309695,"thete politician artist ... business man ... author actor Candidate Type Set (Sub-tree) singer ... Knowledge Bases Distant Supervision Entity: Arnold Schwarzenegger Figure 1: Current systems may detect Arnold Schwarzenegger in sentences S1-S3 and assign the same types to all (listed within braces), when only some types are correct for context (blue labels within braces). Assigning types (e.g., person, organization) to mentions of entities in context is an important task in natural language processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies"
D16-1144,W09-1119,0,0.0432782,"age processing (NLP). The extracted entity type information can serve as primitives for relation extraction (Mintz et al., 2009) and event extraction (Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) work on a much larger set of fine-grained types (usually over 100) which form a tree-structured hierarchy (see the blue region of Fig. 1). Fine-grained typing allows one mention to have multiple types, which together constitute a type-path (not necessarily ending in a leaf node) in the given type hierarchy, depending on the local context (e.g., sentence). Consider the example in Fig. 1, “Arnold Schwarzenegger” could be labeled as {person, businessm"
D16-1144,P12-1076,0,0.047099,"learn feature and type representations (Yogatama et al., 2015; Dong et al., 2015). Del Corro et al. (2015) proposed an unsupervised method to generate context-aware candidates types, and subsequently select the most appropriate type. Gillick et al. (2014) discuss the label noise issue in fine-grained typing and propose three pruning heuristics. However, these heuristics aggressively delete training examples and may suffer from low recall (see Table. 4). In the context of distant supervision, label noise issue has been studied for other information extraction tasks such as relation extraction (Takamatsu et al., 2012). In relation extraction, label noise is introduced by the false positive textual matches of entity pairs. In entity typing, however, label noise comes from the assignment of types to entity mentions without considering their contexts. The forms 1377 of distant supervision are different in these two problems. Recently, (Ren et al., 2016b) has tackled the problem of label noise in fine-grained entity typing, but focused on how to generate a clean training set instead of doing entity typing. Partial label learning (PLL) (Zhang, 2014; Nguyen and Caruana, 2008; Cour et al., 2011) deals with the pr"
D16-1144,P15-2048,0,0.476766,"didate type set of each mention, all KB types of its KB-linked entity. However, existing distant supervision methods encounter the following limitations when doing automatic fine-grained typing. • Noisy Training Labels. Current practice of distant supervision may introduce label noise to training data since it fails to take a mention’s local contexts into account when assigning type labels (e.g., see Fig. 1). Many previous studies ignore the label noises which appear in a majority of training mentions (see Table. 1, row (1)), and assume all types obtained by distant supervision are “correct” (Yogatama et al., 2015; Ling and Weld, 2012). The noisy labels may mislead the trained models and cause negative effect. A few systems try to denoise the training corpora using simple pruning heuristics such as deleting mentions with conflicting types (Gillick et al., 2014). However, such strategies significantly reduce the size of training set (Table 1, rows (2a-c)) and lead to performance degradation (later shown in our experiments). The larger the target type set, the more severe the loss. • Type Correlation. Most existing methods (Yogatama et al., 2015; Ling and Weld, 2012) treat every type label in a training"
D16-1144,C12-2133,0,0.474884,"(Ji and Grishman, 2008), and assists a wide range of downstream applications including knowledge base (KB) completion (Dong et al., 2014), question answering (Lin et al., 2012) and entity recommendation (Yu et al., 2014). While ∗ person ... Noisy Training Examples Introduction 1 product Equal contribution. Codes and datasets used in this paper can be downloaded at https://github.com/shanzhenren/AFET. traditional named entity recognition systems (Ratinov and Roth, 2009; Nadeau and Sekine, 2007) focus on a small set of coarse types (typically fewer than 10), recent studies (Ling and Weld, 2012; Yosef et al., 2012) work on a much larger set of fine-grained types (usually over 100) which form a tree-structured hierarchy (see the blue region of Fig. 1). Fine-grained typing allows one mention to have multiple types, which together constitute a type-path (not necessarily ending in a leaf node) in the given type hierarchy, depending on the local context (e.g., sentence). Consider the example in Fig. 1, “Arnold Schwarzenegger” could be labeled as {person, businessman} in S3 (investment). But he could also be labeled as {person, politician} in S1 or {person, artist, actor} in S2. Such fine-grained type represe"
D17-1005,P14-1091,0,0.0127696,"d on reliabilities of labeling functions, which are calculated with their proficient subsets’ representations. Then, these inferred true labels would serve as supervision for all components, including context representation, true label discovery and relation extraction. Besides, the context representation bridges relation extraction with true label dis2 Preliminaries In this section, we would formally define relation extraction and heterogeneous supervision, including the format of labeling functions. 47 fc vi zc li 2.1 Relation Extraction Here we conduct relation extraction in sentencelevel (Bao et al., 2014). For a sentence d, an entity mention is a token span in d which represents an entity, and a relation mention is a triple (e1 , e2 , d) which consists of an ordered entity pair (e1 , e2 ) and d. And the relation extraction task is to categorize relation mentions into a given set of relation types R, or Not-Target-Type (None) which means the type of the relation mention does not belong to R. oc,i o∗c ρc,i Si sc,i ti Table 1: Notation Table. into a set of relation types. Intuitively, errors of annotations (O) come from mismatch of contexts, e.g., in Fig. 1, λ1 annotates c1 and c2 with ’true’ lab"
D17-1005,J92-4003,0,0.206259,"scovery model by maximizing JT . 50 features and conduct the text features’ representation learning. After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c . Kind Pattern KB Wiki-KBP #Types #LF 13 147 7 7 NYT #Types #LF 16 115 25 26 Table 4: Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information 3.5 Relation Type Inference CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets. Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2 . All these features are shared with all compared methods in our experiments. We now discuss the strategy of performing type inference for Cu . As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl . Additionally, not like other relation types in R, None does not have a coherent semantic meaning. Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our relation extractor predict it as None, or (2) the entropy of p(.|zc ) over R exceeds a pre-defined t"
D17-1005,P07-1073,0,0.0521594,"he discriminative module in label level, while the discriminative module influences the true label discovery module by selecting a feature subset. Although delicately designed, it fails to make full use of the connection between these modules, i.e., not refine the context representation for classifier. Thus, its discriminative module might suffer from the overwhelming size of text features. 5.1 Relation Extraction Relation extraction aims to detect and categorize semantic relations between a pair of entities. To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances). Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus. Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is"
D17-1005,P14-5010,0,0.00210619,"f ρc,i = δ(oc,i = o∗c ). Thus, we would first infer o∗c = argmaxo∗c JT , then train the true label discovery model by maximizing JT . 50 features and conduct the text features’ representation learning. After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c . Kind Pattern KB Wiki-KBP #Types #LF 13 147 7 7 NYT #Types #LF 16 115 25 26 Table 4: Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information 3.5 Relation Type Inference CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets. Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2 . All these features are shared with all compared methods in our experiments. We now discuss the strategy of performing type inference for Cu . As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl . Additionally, not like other relation types in R, None does not have a coherent semantic meaning. Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our r"
D17-1005,P09-1113,0,0.942466,"belongs to λi ’s proficient subset relation type embedding for ri ∈ R 2. Text feature embeddings are utilized to calculate relation mention embeddings (see Fig. 2); 3. With relation mention embeddings, true labels are inferred by calculating labeling functions’ reliabilities in a context-aware manner (see Fig. 1); 4. Inferred true labels would ‘supervise’ all components to learn model parameters (see Fig. 1). We now proceed by introducing these components of the model in further details. 3.1 Modeling Relation Mention As shown in Table 2, we extract abundant lexical features (Ren et al., 2016; Mintz et al., 2009) to characterize relation mentions. However, this abundance also results in the gigantic dimension of original text features (∼ 107 in our case). In The REH ESSION Framework Here, we present REH ESSION, a novel framework to infer true labels from automatically generated noisy labels, and categorize unlabeled instances 48 Feature Entity mention (EM) head Entity Mention Token Tokens between two EMs Part-of-speech (POS) tag Collocations Entity mention order Entity mention distance Body entity mentions numbers Entity mention context Brown cluster (learned on D) Description Syntactic head token of"
D17-1005,C10-1099,0,0.0117953,"or machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities. Then, the second strategy is to only feed the inferred true labels, referred as ‘TD’. For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score (Bunescu and Mooney, 2005; Bach and Badaskar, 2007) are employed. Notice that both relation extraction and relation classificat"
D17-1005,D15-1205,0,0.0199289,"nts, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation mentions annotated as None, relation mentions with conflicting annotations and conflicts involving None learning with Perceptron algorithm. BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities. Then, the second strategy is to"
D17-1005,P11-1055,0,0.170271,"well. Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation mentions annotated as None, relation mentions with conflicting annotations and conflicts involving None learning with Perceptron algorithm. BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011) models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model. CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor. Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as ‘NL’. Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively up"
D17-1005,C14-1220,0,0.0507984,"on to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type. Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor potential to detect more types of relations in a more precise way. Word embedding has demonstrated great potential in capturing semantic meaning (Mikolov et al., 2013), and achieved great success in a wide range of NLP tasks like relation extraction (Zeng et al., 2014; Takase and Inui, 2016; Nguyen and Grishman, 2015). In our model, we employed the embedding techniques to represent context information, and reduce the dimension of text features, which allows our model to generalize better. 6 Conclusion and Future Work In this paper, we propose REH ESSION, an embedding framework to extract relation under heterogeneous supervision. When dealing with heterogeneous supervisions, one unique challenge is how to resolve conflicts generated by different labeling functions. Accordingly, we go beyond the “source consistency assumption” in prior works and leverage con"
D17-1005,N13-1008,0,0.165874,"0.3325 0.3360 0.1964 0.5645 0.2914 0.3499 0.3923 0.3107 0.5368 0.3879 0.5726 0.4792 0.3677 0.4933 0.4208 Relation Classification NYT Wiki-KBP Accuracy Accuracy 0.6598 0.6226 0.6905 0.5000 0.7954 0.6355 0.7059 0.6484 0.7033 0.5419 0.6485 0.6935 0.7059 0.6355 0.6292 0.5032 0.7570 0.6452 0.6061 0.6613 0.6803 0.5645 0.6409 0.6890 0.8381 0.7277 Table 5: Performance comparison of relation extraction and relation classification Dataset Total Number of RM RM annotated as None RM with conflicts Conflicts involving None Wiki-KBP 225977 100521 32008 30559 NYT 530767 356497 58198 38756 Universal Schemas (Riedel et al., 2013) is proposed to unify different information by calculating a low-rank approximation of the annotations O. It can serve as an alternative of the Investment method, i.e., selecting the relation type with highest score in the low-rank approximation as the true type. But it doesnt explicitly model noise and not fit our scenario very well. Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4. Indeed, it performs similarly to the Investment method. Table 6: Number of relation mentions (RM), relation m"
D17-1005,D15-1174,0,0.0208116,"he context representation for classifier. Thus, its discriminative module might suffer from the overwhelming size of text features. 5.1 Relation Extraction Relation extraction aims to detect and categorize semantic relations between a pair of entities. To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances). Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus. Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type. Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor pot"
D17-1055,W02-1011,0,0.0265986,"ument dv . 3 Model In this section, we firstly introduce the generative process of the LDST model. Then we present the inference algorithm for estimating the model parameters. 3.1 Related work Sentiment analysis is widely applied in many fields, such as business intelligence, politics, sociology. The papers by Pang and Lee (Pang and Lee, 2008) and Liu (Liu, 2012) described most of the existing techniques for sentiment analysis and opinion mining, which could be categorized into lexicon-based approaches (Kennedy and Inkpen, 2006; Turney, 2002; Yang et al., 2014a,b) and corpus-based approaches (Pang et al., 2002; Yang et al., 2015; Wan, 2009). Recently, researchers have turned their attention to exploring sentiment analysis on the social media posts of individuals during natural disasters and emergencies (Beigi et al., 2016; Buscaldi and Hernandez-Farias, 2015; Caragea et al., 2014; Kryvasheyeu et al., 2015; Mandel et al., 2012; Shalunts et al., 2014). For example, a sentiment analysis system is applied for Italian to a set of tweets during the Genoa flooding (Buscaldi and Hernandez-Farias, 2015). They attempted to identify trending topics, toponym and sentiments that might be relevant from a disaste"
D17-1055,P02-1053,0,0.0070065,"written in location v. Ndv is the number of words in location document dv . 3 Model In this section, we firstly introduce the generative process of the LDST model. Then we present the inference algorithm for estimating the model parameters. 3.1 Related work Sentiment analysis is widely applied in many fields, such as business intelligence, politics, sociology. The papers by Pang and Lee (Pang and Lee, 2008) and Liu (Liu, 2012) described most of the existing techniques for sentiment analysis and opinion mining, which could be categorized into lexicon-based approaches (Kennedy and Inkpen, 2006; Turney, 2002; Yang et al., 2014a,b) and corpus-based approaches (Pang et al., 2002; Yang et al., 2015; Wan, 2009). Recently, researchers have turned their attention to exploring sentiment analysis on the social media posts of individuals during natural disasters and emergencies (Beigi et al., 2016; Buscaldi and Hernandez-Farias, 2015; Caragea et al., 2014; Kryvasheyeu et al., 2015; Mandel et al., 2012; Shalunts et al., 2014). For example, a sentiment analysis system is applied for Italian to a set of tweets during the Genoa flooding (Buscaldi and Hernandez-Farias, 2015). They attempted to identify trendin"
D17-1055,P09-1027,0,0.0390129,"firstly introduce the generative process of the LDST model. Then we present the inference algorithm for estimating the model parameters. 3.1 Related work Sentiment analysis is widely applied in many fields, such as business intelligence, politics, sociology. The papers by Pang and Lee (Pang and Lee, 2008) and Liu (Liu, 2012) described most of the existing techniques for sentiment analysis and opinion mining, which could be categorized into lexicon-based approaches (Kennedy and Inkpen, 2006; Turney, 2002; Yang et al., 2014a,b) and corpus-based approaches (Pang et al., 2002; Yang et al., 2015; Wan, 2009). Recently, researchers have turned their attention to exploring sentiment analysis on the social media posts of individuals during natural disasters and emergencies (Beigi et al., 2016; Buscaldi and Hernandez-Farias, 2015; Caragea et al., 2014; Kryvasheyeu et al., 2015; Mandel et al., 2012; Shalunts et al., 2014). For example, a sentiment analysis system is applied for Italian to a set of tweets during the Genoa flooding (Buscaldi and Hernandez-Farias, 2015). They attempted to identify trending topics, toponym and sentiments that might be relevant from a disaster management perspective. Howev"
D17-1055,W12-2104,0,0.136802,"nt or topic distribution of the documents but also the sentiments towards specific topics. For example, a person may be happy with that the disaster passed away, but at the meanwhile he/she may be unsatisfied with the post-disaster relief. Second, most existing sentiment-topic models ignore the temporal evolution of topics and sentiment in a time-variant data corpus such as the Twitter stream. There are strong evidences which indicate that people’s attitudes toward a disaster will gradually change over time with the distribution of emergency supplies (Beigi et al., 2016; Caragea et al., 2014; Mandel et al., 2012). Third, people in different places tend to have different opinions towards particular topics. This motivated us to find the influence of specific topics and the relationship between different topics in different regions, which may improve people’s awareness to help themselves during disasters. We propose a location-based dynamic sentiment-topic model (LDST) which generalizes latent Dirichlet allocation (LDA) (Blei et al., 2003), by jointly modeling topic, sentiment, time and geolocation information. After learning the LDST model, we can identify the topics and sentiments held by people in dif"
D17-1055,P14-2069,1,0.782615,"ation v. Ndv is the number of words in location document dv . 3 Model In this section, we firstly introduce the generative process of the LDST model. Then we present the inference algorithm for estimating the model parameters. 3.1 Related work Sentiment analysis is widely applied in many fields, such as business intelligence, politics, sociology. The papers by Pang and Lee (Pang and Lee, 2008) and Liu (Liu, 2012) described most of the existing techniques for sentiment analysis and opinion mining, which could be categorized into lexicon-based approaches (Kennedy and Inkpen, 2006; Turney, 2002; Yang et al., 2014a,b) and corpus-based approaches (Pang et al., 2002; Yang et al., 2015; Wan, 2009). Recently, researchers have turned their attention to exploring sentiment analysis on the social media posts of individuals during natural disasters and emergencies (Beigi et al., 2016; Buscaldi and Hernandez-Farias, 2015; Caragea et al., 2014; Kryvasheyeu et al., 2015; Mandel et al., 2012; Shalunts et al., 2014). For example, a sentiment analysis system is applied for Italian to a set of tweets during the Genoa flooding (Buscaldi and Hernandez-Farias, 2015). They attempted to identify trending topics, toponym a"
D17-1055,N15-1057,1,0.809666,"In this section, we firstly introduce the generative process of the LDST model. Then we present the inference algorithm for estimating the model parameters. 3.1 Related work Sentiment analysis is widely applied in many fields, such as business intelligence, politics, sociology. The papers by Pang and Lee (Pang and Lee, 2008) and Liu (Liu, 2012) described most of the existing techniques for sentiment analysis and opinion mining, which could be categorized into lexicon-based approaches (Kennedy and Inkpen, 2006; Turney, 2002; Yang et al., 2014a,b) and corpus-based approaches (Pang et al., 2002; Yang et al., 2015; Wan, 2009). Recently, researchers have turned their attention to exploring sentiment analysis on the social media posts of individuals during natural disasters and emergencies (Beigi et al., 2016; Buscaldi and Hernandez-Farias, 2015; Caragea et al., 2014; Kryvasheyeu et al., 2015; Mandel et al., 2012; Shalunts et al., 2014). For example, a sentiment analysis system is applied for Italian to a set of tweets during the Genoa flooding (Buscaldi and Hernandez-Farias, 2015). They attempted to identify trending topics, toponym and sentiments that might be relevant from a disaster management perspe"
D17-1274,P16-1072,0,0.168613,"ler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry, all of Bluff City. the shortest dependency path between Kenny and Troy is: “Troy ←conj Perry ←conj Kenny”, which 2589 Figure 2: Overview of the Architecture. does not include the most indicative words: sons and dau"
D17-1274,N15-1133,0,0.021171,"nd Ji, 2016; Yu et al., 2016). Some work (Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Hong et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b; Ma et al., 2015) also attempted to validate slot types by combining results from multiple systems. Our work is also related to dependency path based relation extraction. The effectiveness of dependency features for relation classification has been reported in some previous work (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007; Neville and Jensen, 2003; Ebrahimi and Dou, 2015; Xu et al., 2015; Ji et al., 2014). Liu et al. (2015), Cai et al. (2016) and Xu et al. (2015) applied CNN, bidirectional recurrent CNN and LSTM to CONLL relation extraction and demonstrated that the most important information has been included within the shortest paths between entities. Considering that the indicative words may not be included by the shortest dependency path between query and candidate filler, we enrich it to a regularized dependency graph by adding more contexts. 2595 7 Conclusions and Future Work In this work, we discussed the unique challenges of slot filling compared with"
D17-1274,P05-1053,0,0.453655,"ions to measure the relatedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry"
D17-1274,W09-2415,0,0.0742998,"Missing"
D17-1274,N16-1097,0,0.031215,"Missing"
D17-1274,N07-1015,0,0.2235,"latedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry, all of Bluff City. th"
D17-1274,P15-2047,1,0.951334,"nto one of the 41 types or none. Most previous studies have treated SF in the same way as within-sentence relation extraction tasks in ACE 1 or SemEval (Hendrickx et al., 2009). They created training data based on crowd-sourcing or distant supervision, and then trained a multi-class classifier or multiple binary classifiers for each slot type based on a set of hand-crafted features. Although Deep Neural Networks (DNN) such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have achieved state-of-the-art results on within-sentence relation extraction (Zeng et al., 2014; Liu et al., 2015; Santos et al., 2015; Nguyen and Grishman, 2015; Yang et al., 2016; Wang et al., 2016), there are limited studies on SF using DNN. Adel and Sch¨utze (2015) and Adel et al. (2016) exploited DNN for SF but did not achieve comparable results as traditional methods. In this paper we aim to answer the following questions: What is the difference between SF and ACE/SemEval relation extraction task? How can we make DNN work for SF? We argue that SF is different and more challenging than traditional relation extraction. First, a query and its candidate filler are usually separated by much wider contex"
D17-1274,D14-1164,0,0.0241327,"milarity scores between column i in F with all slot type vectors, and max{S[i, :]} is the max value among all similarity scores for column i in F . Figure 5: Global Attention. We apply local attention to each convolution output of each subgraph, then take the concatenation of three flattened attentive pooling outputs to a fully connected layer and generate a robust feature representation. Similarly, another feature representation is generated based on global attention. We concatenate these two features to the softmax layer to get the predicted types. 5 Experiments 5.1 Data For model training, Angeli et al. (2014) created some high-quality clean annotations for SF based on crowd-sourcing2 . In addition, Adel et al. (2016) automatically created a larger size of noisy training data based on distant supervision, including about 1,725,891 positive training instances for 41 slot types. We manually assessed the correctness of candidate filler identification and their slot type annotation, and extracted a subset of their noisy annotations and combined it with the clean annotations. Ultimately, we obtain 23,993 positive and 3,000 negative training instances for all slot types. We evaluate our approach in two s"
D17-1274,P14-5010,0,0.00350556,"of birth. Entity types can be inferred by enriching query and filler related contexts. For example, in the following sentence: E3. Merkelquery died in the southern German city of Passauf iller in 1967. we can determine the slot type as city related by incorporating rich contexts (e.g., “city”). To tackle these problems, we propose to regularize the dependency graph, incorporating the shortest dependency path between query and candidate filler, as well as their rich contextual words. Given a sentence s including a query q and candidate filler f , we first apply the Stanford Dependency Parser (Manning et al., 2014) to generate all dependent word pairs: hgovernor, dependenti, then discover the shortest dependency path between query and candidate filler based on BreadthFirst-Search (BFS) algorithm. The regularized dependency graph includes words on the shortest dependency path, as well as words which can be connected to query and filler within n hops. In our experiments, we set n = 1. Figure 3 shows the dependency parsing output for E1 mentioned in Section 1, and the regularized dependency graph with the bold circled nodes. We can see that, the most indicative trigger owns can be found in both the shortes"
D17-1274,H05-1091,0,0.538904,"attention: We use prelearned slot type representations to measure the relatedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf ille"
D17-1274,W15-1506,0,0.0896444,"Missing"
D17-1274,D16-1007,0,0.0268318,"Missing"
D17-1274,D16-1201,0,0.104931,"a wide range of features and patterns, especially for slot types that are in the longtail of the quite skewed distribution of slot fills (Ji et al., 2011a). Previous work has mostly focused on compensating the data needs by constructing patterns (Sun et al., 2011; Roth et al., 2014b), automatic annotation by distant supervision (Surdeanu et al., 2011; Roth et al., 2014a; Adel et al., 2016), and constructing trigger lists for unsupervised dependency graph mining (Yu and Ji, 2016; Yu et al., 2016). Some work (Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Hong et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b; Ma et al., 2015) also attempted to validate slot types by combining results from multiple systems. Our work is also related to dependency path based relation extraction. The effectiveness of dependency features for relation classification has been reported in some previous work (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007; Neville and Jensen, 2003; Ebrahimi and Dou, 2015; Xu et al., 2015; Ji et al., 2014). Liu et al. (2015), Cai et al. (2016) and Xu et al. (2015) applied CNN, bidirectional recurren"
D17-1274,Q16-1019,0,0.0829586,"Missing"
D17-1274,C14-1149,1,0.898473,"Missing"
D17-1274,P15-1061,0,0.0480725,"Missing"
D17-1274,P16-1005,1,0.927171,"er 7–11, 2017. 2017 Association for Computational Linguistics Here, Arcandor and KaDeWe are far separated and it’s difficult to determine the slot type as org:subsidiaries based on the raw wide contexts. Figure 1: Comparison of the Percentage by the # of Words between two entity mentions in ACE05 and SemEval-2010 Task 8 relations, and between query and slot filler in KBP2013 Slot Filling. In addition, compared with relations defined in ACE (18 types) and SemEval (9 types), slot types are more fine-grained and heavily rely on indicative contextual words for disambiguation. Yu et al. (2015) and Yu and Ji (2016) demonstrate that many slot types can be specified by contextual trigger words. Here, a trigger is defined as the word which is related to both the query and candidate filler, and can indicate the type of the target slot. Considering E1 again, owns is a trigger word between Arcandor and KaDeWe, which can indicate the slot type as org:subsidiaries. Most previous work manually constructed trigger lists for each slot type. However, for some slot types, the triggers can be implicit and ambiguous. To address the above challenges, we propose the following new solutions: • To compress wide contexts,"
D17-1274,N15-1126,1,0.901313,"Missing"
D17-1274,C14-1220,0,0.204393,"Missing"
D17-1274,P05-1052,0,0.311731,"ned slot type representations to measure the relatedness of each input bigram with each slot type via a transformation matrix. These two attention mechanisms will guide the pooling step to select the information which is related to query and filler and can indicate slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kenn"
D17-1274,P15-1018,0,0.024862,"SF is the lack of labeled data to generalize a wide range of features and patterns, especially for slot types that are in the longtail of the quite skewed distribution of slot fills (Ji et al., 2011a). Previous work has mostly focused on compensating the data needs by constructing patterns (Sun et al., 2011; Roth et al., 2014b), automatic annotation by distant supervision (Surdeanu et al., 2011; Roth et al., 2014a; Adel et al., 2016), and constructing trigger lists for unsupervised dependency graph mining (Yu and Ji, 2016; Yu et al., 2016). Some work (Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Hong et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b; Ma et al., 2015) also attempted to validate slot types by combining results from multiple systems. Our work is also related to dependency path based relation extraction. The effectiveness of dependency features for relation classification has been reported in some previous work (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007; Neville and Jensen, 2003; Ebrahimi and Dou, 2015; Xu et al., 2015; Ji et al., 2014). Liu et al. (2015), Cai et al. (2016) and Xu et al"
D17-1274,P16-1123,0,0.0357496,"Missing"
D17-1274,D15-1206,0,0.168904,"te slot type. 3 3.1 Regularized Dependency Graph based CNN Regularized Dependency Graph Dependency parsing based features, especially the shortest dependency path between two entities, have been proved to be effective to extract the most important information for identifying the relation between two entities (Bunescu and Mooney, 2005; Zhao and Grishman, 2005; GuoDong et al., 2005; Jiang and Zhai, 2007). Several recent studies also explored transforming a dependency path into a sequence and applied Neural Networks to the sequence for relation classification (Liu et al., 2015; Cai et al., 2016; Xu et al., 2015). However, for SF, the shortest dependency path between query and candidate filler is not always sufficient to infer the slot type due to two reasons. First, the most indicative words may not be included in the path. For example, in the following sentence: E2. Survivors include two sons and daughters-inlaw, Troyf iller and Phyllis Perry, Kennyquery and Donna Perry, all of Bluff City. the shortest dependency path between Kenny and Troy is: “Troy ←conj Perry ←conj Kenny”, which 2589 Figure 2: Overview of the Architecture. does not include the most indicative words: sons and daughters for their p"
D18-1023,P17-1042,0,0.0533221,"versity, CIFAR Global Scholar kyunghyun.cho@nyu.edu 3 Didi Labs and University of Southern California knight@isi.edu Abstract (MT) is only available for dozens of dominant languages. In this paper we aim to construct a multilingual common semantic space where words in multiple languages are mapped into a distributed, language-agnostic semantic continuous space, so that resources and knowledge can be shared across languages. Previous multilingual embedding methods align the semantic distributions of words from multiple languages within the common semantic space. Though several recent attempts (Artetxe et al., 2017, 2018; Conneau et al., 2017) have shown that it is possible to extract multilingual word embedding from a pair of potentially unaligned corpora in multiple languages, we claim that it is necessary to impose more constraints to preserve linguistic properties and facilitate downstream NLP tasks, such as cross-lingual IE, and MT. We find that words also can be clustered through explicit (e.g., sharing affixes of certain linguistic functions) or implicit clues (e.g., sharing neighbors from monolingual word embedding) and such clusters should also be consistent across multiple languages. To do so,"
D18-1023,P18-1073,0,0.02517,"Missing"
D18-1023,P15-1027,0,0.0335093,"ow-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word se"
D18-1023,N16-1030,0,0.0149937,"n (PER), Location (LOC), Organization (ORG), and Geo-Political Entities (GPE). We experiment with two sets of languages. The first set Amh+Tig consists of Amharic and Tigrinya. Both languages share the same Ge’ez script and descend from the proto-Semitic language family. The other set Eng+Uig+Tur consists of one high-resource language (English), one mediumresource language (Turkish) and one low-resource language (Uighur). It also consists of two distinct language scripts: English and Turkish use Latin script while Uighur uses Arabic script. We use an LSTM-CRF architecture (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016) for name tagging. It takes only word embedding as input and predict a tag for each word. Table 5 shows the statistics of training, development, and test sets for each language released by Linguistic Data Consortium (LDC).15 For each language pair, we combine the bilingual aligned words extracted from Wiktionary and monolingual dictionaries based on identical strings.16 We evaluate the quality from several aspects: Impact of Bilingual Dictionary Size In order to show the impact of the size of bilingual lexicons, we use three languages as a case study, and gradually reduce t"
D18-1023,C16-1171,0,0.0188394,"ers. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all monolingual word embeddings into a common semantic space based on word-level as well as clu"
D18-1023,N15-1028,0,0.0226675,"ate-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another b"
D18-1023,W15-1521,0,0.173801,"the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for comm"
D18-1023,P16-1101,0,0.0607558,"C), Organization (ORG), and Geo-Political Entities (GPE). We experiment with two sets of languages. The first set Amh+Tig consists of Amharic and Tigrinya. Both languages share the same Ge’ez script and descend from the proto-Semitic language family. The other set Eng+Uig+Tur consists of one high-resource language (English), one mediumresource language (Turkish) and one low-resource language (Uighur). It also consists of two distinct language scripts: English and Turkish use Latin script while Uighur uses Arabic script. We use an LSTM-CRF architecture (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016) for name tagging. It takes only word embedding as input and predict a tag for each word. Table 5 shows the statistics of training, development, and test sets for each language released by Linguistic Data Consortium (LDC).15 For each language pair, we combine the bilingual aligned words extracted from Wiktionary and monolingual dictionaries based on identical strings.16 We evaluate the quality from several aspects: Impact of Bilingual Dictionary Size In order to show the impact of the size of bilingual lexicons, we use three languages as a case study, and gradually reduce the size of the lexic"
D18-1023,W17-2617,0,0.0411181,"Missing"
D18-1023,D16-1136,0,0.0372422,"antic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advan"
D18-1023,E17-1084,0,0.0871444,"A, MultiSkip, and MultiCross)12 . MultiCluster (Ammar et al., 2016b) groups multilingual words into clusters based on bilingual dictionaries and forces all the words from various languages within one cluster share the same embedding. MultiCCA (Ammar et al., 2016b; Faruqui and Dyer, 2014) uses CCA to estimate linear projections for each pair of languages. MultiSkip is an extension of the multilingual skip-gram model (Luong et al., 2015), which requires parallel data. MultiCross is an approach to unify bilingual word embeddings into a shared semantic space using post hoc linear transformations (Duong et al., 2017). Table 2 lists the hyper-parameters used in the experiments. 512 512 20 1, 2, 3 500 0.5 Adadelta Table 2: Hyper-parameters. X OR = L(HlRi , HlRj ) , {li ,lj }∈A where W is the same as the W used in Section 3.3 for each language. We finally optimize the sum of the losses by finding the parameters 0 θ = {Wl , bl , bl , Ul , b∗l , CNNl , bR l }, where l denotes a specific language: Oθ = OW + ON + Ochar + OR 4 Experiments 4.1 Experiment Setup 4.2 Previous work (Ammar et al., 2016b; Duong et al., 2017) evaluated multilingual word embeddings on a series of intrinsic (e.g., monolingual and crossling"
D18-1023,E14-1049,0,0.1096,"ificantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and c"
D18-1023,H93-1061,0,0.177294,"h, French, Hungarian, Italian, Swedish) respectively. The monolingual data for each language is the combination of the Leipzig Corpora Collection9 and Europarl.10 The bilingual dictionaries are the same as those used in Ammar et al. (2016b).11 Intrinsic Evaluation: QVEC In order to evaluate the quality of multilingual embeddings, we adopt QVEC (Tsvetkov et al., 2015) as the intrinsic evaluation measure. It evaluates the quality of word embeddings based on the alignment of distributional word vectors to linguistic feature vectors extracted from manually crafted lexical resources, e.g., SemCor (Miller et al., 1993). For each word, each dimension of its linguistic feature vector defines the probability of that word belongs to a supersense (e.g., NN.FOOD) which is summarized from WordNet (Fellbaum, 1998). QVEC is computed as QVEC = Pmax j aij ≤1 D X P X r(xi , sj ) × aij , i=1 j=1 where x ∈ RD×1 denotes a distributional word vector and s ∈ RP ×1 denotes a linguistic word vector. D and P denote the sizes of vectors respectively. aij = 1 iff xi is aligned to sj , otherwise aij = 0. r(xi , sj ) is the Pearson’s correlation between xi and sj . QVEC-CCA (Ammar et al., 2016b) is extended from QVEC by using CCA"
D18-1023,N16-1091,0,0.0625978,"Missing"
D18-1023,P15-1119,0,0.0308306,"nt between clusters for common semantic space construction. We evaluate our approach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individu"
D18-1023,W17-2619,0,0.0200454,"linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3"
D18-1023,N18-5009,1,0.881187,"Missing"
D18-1023,N16-1072,0,0.0330789,"roach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the la"
D18-1023,D15-1243,0,0.120914,"/ -ler), Somali (-o)). Linguists have created a wide variety of linguistic property knowledge bases, which are readily available for thousands of languages. For example, the CLDR (Unicode Common Locale Data Repository)2 includes closed word classes and affixes indicating various linguistic properties. We propose to take advantage of these languageuniversal resources to create clusters, where the words within one cluster share the same linguistic property, and build alignment between clusters for common semantic space construction. We evaluate our approach on monolingual and multilingual QVEC (Tsvetkov et al., 2015) tasks, which measure the quality of word embeddings based on the alignment of the embeddings to linguistic feature vectors extracted from manually crafted linguistic resources, as well as an extrinsic evaluation on name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et a"
D18-1023,P17-1179,0,0.0840215,"Missing"
D18-1023,P16-1024,0,0.0317905,"cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all m"
D18-1023,D17-1207,0,0.120181,"Missing"
D18-1023,P15-2118,0,0.0245502,"works (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introduce multiple cluster-level alignments and design a new cluster consistent CorrNet to align both words and clusters. Another branch of approaches for multilingual word embeddings are based on parallel or comparable data, such as parallel sentences (AP Chandar et al., 2014; Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014; Schwenk et al., 2017), phrase translations (Duong et al., 2016) and comparable documents (Vulic and Moens, 2015). Moreover, to reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine"
D18-1023,N16-1156,0,0.121959,"n name tagging for low-resource languages. Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploite"
D18-1023,D13-1141,0,0.0354099,"o reduce the need of bilingual alignment, several approaches have been designed to learn cross-lingual embeddings based on a small seed dictionary (Vulic and Korhonen, 2016; Zhang et al., 2016; Artetxe et al., 2017), or even with no supervision (Cao et al., 2016; Zhang et al., 2017d,c; Conneau et al., 2017; Artetxe et al., 2018). However, such methods are still limited to bilingual word embedding learning and remaining to be explored for common semantic space construction. 3 2 Related Work 3.1 Multilingual word embeddings have advanced many multilingual NLP tasks, such as machine translation (Zou et al., 2013; Mikolov et al., 2013b; Madhyastha and Espa˜na-Bonet, 2017), de2 Approach Overview Figure 1 shows the overview of our neural architecture. We project all monolingual word embeddings into a common semantic space based on word-level as well as cluster-level alignments and learn the transformation functions. First, on cldr.unicode.org 251 Figure 1: Architecture Overview. In each monolingual semantic space, the words within solid rectangle denote a neighbor based cluster and the words within dotted rectangle denote a linguistic property based cluster. where Hl1 ∈ R|Vl1 |×h and Hl2 ∈ R|Vl2 |×h are"
D18-1023,N15-1104,0,0.0397766,". Experiments demonstrate that our framework is effective at capturing linguistic properties and significantly outperforms state-of-the-art multi-lingual embedding learning methods. pendency parsing (Guo et al., 2015; Ammar et al., 2016a), and name tagging (Zhang et al., 2017a; Tsai and Roth, 2016; Zhang et al., 2018; Cheung et al.; Zhang et al., 2017b; Feng et al., 2017). Using bilingual aligned words, previous methods project multiple monolingual embeddings into a shared semantic space using linear mappings (Mikolov et al., 2013b; Rothe et al., 2016; Zhang et al., 2016; Baroni et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016b; Faruqui and Dyer, 2014; Lu et al., 2015). Compared with CCA, which only optimizes the correlation for each individual pair of languages, linear mapping based methods can jointly optimize all the languages in the common semantic space. We focus on learning linear mappings to construct the common semantic space and adopt correlational neural networks (CorrNet) (Chandar et al., 2016; Rajendran et al., 2015) as the basic model. In contrast to previous work which only exploited monolingual word semantics, we introdu"
D18-1023,Q16-1031,0,\N,Missing
D18-1125,P17-1110,0,0.0894867,"Missing"
D18-1125,I17-2072,0,0.171838,"3/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limited human knowledge to capture all genre-agnostic features. As depicted in Figure 1, where red rectangles are features shared by two genres, and blue and green triangles are source and target genre features respectively, Feature Engineering only captures a portion of the genre-agnostic features. Fu et al. (2017), depicted as Feature Projection, applies a domain adversarial neural network to automatically project the source and target genre features into one unified feature space. However, it unnecessarily introduces genre-specific features which undermine the overall performance. To address these problems, we propose a genreseparation network, which consists of two separate Convolutional Neural Networks (CNNs) to automatically separates genre-specific and genreagnostic features for each genre, which is depicted as Genre Separation Network in Figure 1. To avoid information loss during feature encoding"
D18-1125,D15-1205,0,0.205408,"ities and their contexts, a supervised model trained in one genre suffers from dramatical performance decrease when applied to a new genre, due to the distinct contexts among various genres. Previous studies (Plank and Moschitti, 2013; Nguyen and Grishman, 2014, 2015; Yu et al., ∗ Feature Projection source feature Genre Separation Network target feature Figure 1: Comparison of Genre Separation Methods. Introduction *Corresponding author We make all cleaned codes and resources publicly available at https://github.com/Garym713/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limited human knowledge to capture all genre-agnostic features. As depicted in Figure 1, where red rectangles are features shared by two genres, and blue and green triangles are source and target genre features respectively, Feature Engineering only captures a portion of the genre-agnostic features. Fu et al. (2017), depicted as Feature Projection, applies a domain adversarial neural networ"
D18-1125,D17-1274,1,0.848663,"e sentence (s, e1 , e2 , r) where s = [w1 , ..., wm ], for each word wik , we generate a multi-type embedding: v˜i = [vi , pi , p˜i , ti , t˜i , ci , ηi ] where vi denotes a pretrained word embedding. pi and p˜i are position embeddings (Al-Badrashiny et al., 2017) indicating the distance from wi to e1 and e2 respectively. ti and t˜i are entity type embedding (Ren et al., 2016; Huang et al., 2016) of e1 and e2 . ci is the chunking embedding, and ηi is a binary digit indicating whether the word is within the shortest dependency path between e1 and e2 (Bunescu and Mooney, 2005; Liu et al., 2015; Huang et al., 2017). All these embeddings except pre-trained word embedding are randomly initialized and optimized during training. Thus the input layer is a sequence of word representations V = {˜ v1 , v˜2 , ..., v˜n }. We then apply the convolution weights W to each sliding n-gram phrase gj 0 with a biased vector b, i.e., gj = tanh(W · V ) + b. 0 All n-gram representations gj are further used to get an overall vector representation f by maxpooling. Once we obtain fsp , fsc , fsp and fsc , we compute the difference loss: Ldif f = ||fsp> · fsc + ftp> · ftc ||F2 where ||.||F2 represents the squared Frobenius norm"
D18-1125,P17-1001,0,0.0830369,"Missing"
D18-1125,P15-2047,1,0.860361,"lly, given a source sentence (s, e1 , e2 , r) where s = [w1 , ..., wm ], for each word wik , we generate a multi-type embedding: v˜i = [vi , pi , p˜i , ti , t˜i , ci , ηi ] where vi denotes a pretrained word embedding. pi and p˜i are position embeddings (Al-Badrashiny et al., 2017) indicating the distance from wi to e1 and e2 respectively. ti and t˜i are entity type embedding (Ren et al., 2016; Huang et al., 2016) of e1 and e2 . ci is the chunking embedding, and ηi is a binary digit indicating whether the word is within the shortest dependency path between e1 and e2 (Bunescu and Mooney, 2005; Liu et al., 2015; Huang et al., 2017). All these embeddings except pre-trained word embedding are randomly initialized and optimized during training. Thus the input layer is a sequence of word representations V = {˜ v1 , v˜2 , ..., v˜n }. We then apply the convolution weights W to each sliding n-gram phrase gj 0 with a biased vector b, i.e., gj = tanh(W · V ) + b. 0 All n-gram representations gj are further used to get an overall vector representation f by maxpooling. Once we obtain fsp , fsc , fsp and fsc , we compute the difference loss: Ldif f = ||fsp> · fsc + ftp> · ftc ||F2 where ||.||F2 represents the s"
D18-1125,P14-2012,0,0.0508339,"rpose 1 . 1 Feature Input Feature Output Feature Engineering shared feature Relation extraction aims to identify and categorize the semantic relation between two entity mentions based on the contexts within the sentence. Supervised learning approaches have shown to be effective on this task. However, as relation extraction highly depends on information about entities and their contexts, a supervised model trained in one genre suffers from dramatical performance decrease when applied to a new genre, due to the distinct contexts among various genres. Previous studies (Plank and Moschitti, 2013; Nguyen and Grishman, 2014, 2015; Yu et al., ∗ Feature Projection source feature Genre Separation Network target feature Figure 1: Comparison of Genre Separation Methods. Introduction *Corresponding author We make all cleaned codes and resources publicly available at https://github.com/Garym713/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limited human knowledge to captur"
D18-1125,P13-1147,0,0.0572814,"y available for research purpose 1 . 1 Feature Input Feature Output Feature Engineering shared feature Relation extraction aims to identify and categorize the semantic relation between two entity mentions based on the contexts within the sentence. Supervised learning approaches have shown to be effective on this task. However, as relation extraction highly depends on information about entities and their contexts, a supervised model trained in one genre suffers from dramatical performance decrease when applied to a new genre, due to the distinct contexts among various genres. Previous studies (Plank and Moschitti, 2013; Nguyen and Grishman, 2014, 2015; Yu et al., ∗ Feature Projection source feature Genre Separation Network target feature Figure 1: Comparison of Genre Separation Methods. Introduction *Corresponding author We make all cleaned codes and resources publicly available at https://github.com/Garym713/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limite"
D18-1125,D16-1144,1,0.829068,"ncoder respectively. To separate fsp from fsc and separate ftp from ftc , we introduce a difference loss following previous studies (Bousmalis et al., 2016; Liu et al., 2017). More details will be elaborated below. Formally, given a source sentence (s, e1 , e2 , r) where s = [w1 , ..., wm ], for each word wik , we generate a multi-type embedding: v˜i = [vi , pi , p˜i , ti , t˜i , ci , ηi ] where vi denotes a pretrained word embedding. pi and p˜i are position embeddings (Al-Badrashiny et al., 2017) indicating the distance from wi to e1 and e2 respectively. ti and t˜i are entity type embedding (Ren et al., 2016; Huang et al., 2016) of e1 and e2 . ci is the chunking embedding, and ηi is a binary digit indicating whether the word is within the shortest dependency path between e1 and e2 (Bunescu and Mooney, 2005; Liu et al., 2015; Huang et al., 2017). All these embeddings except pre-trained word embedding are randomly initialized and optimized during training. Thus the input layer is a sequence of word representations V = {˜ v1 , v˜2 , ..., v˜n }. We then apply the convolution weights W to each sliding n-gram phrase gj 0 with a biased vector b, i.e., gj = tanh(W · V ) + b. 0 All n-gram representations"
D18-1125,N15-1155,0,0.0834558,"Missing"
D18-1271,C16-1171,0,0.0159595,"nu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates,"
D18-1271,D14-1092,1,0.890876,"Missing"
D18-1271,D12-1025,1,0.84361,"s languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsuperv"
D18-1271,D14-1061,1,0.825032,"ast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsupervised, effective, in"
D18-1271,N07-2008,0,0.0207658,"0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and"
D18-1271,P98-1069,0,0.239043,"slation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel co"
D18-1271,D16-1075,1,0.854091,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,C16-1309,1,0.941152,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,N13-1056,0,0.0349868,"Missing"
D18-1271,W09-3107,1,0.81454,"ed alignment, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of doc"
D18-1271,D15-1015,0,0.0135832,"10; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods,"
D18-1271,P06-1103,0,0.0425456,"i and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast"
D18-1271,W02-0902,0,0.0611221,"tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are"
D18-1271,W11-2125,0,0.0196081,"guage knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Ro"
D18-1271,N16-1132,0,0.031503,"Missing"
D18-1271,P08-1088,0,0.0159272,"d (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of t"
D18-1271,D14-1198,1,0.826094,"nt, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of documents where these"
D18-1271,W11-2206,1,0.807265,"y for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and"
D18-1271,D11-1006,0,0.0252557,", 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By"
D18-1271,J05-4003,0,0.0814633,"ments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016;"
D18-1271,D14-1162,0,0.0810082,"ge. Given that our approach is unsupervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been suppo"
D18-1271,E09-1091,0,0.0212751,"lignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the"
D18-1271,C10-1124,0,0.0211684,"eans it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining"
D18-1271,P15-2118,0,0.0147857,"s and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narro"
D18-1271,N18-1202,0,0.0218069,"pervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been supported by the U.S. DARPA AIDA Pro"
D18-1271,P99-1067,0,0.288414,"ing and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. Howev"
D18-1271,N04-1033,0,0.054922,"37 documents. We removed stopwords, conducted lemmatization and name tagging for the English stream, and did word segmentation and name tagging for the Chinese stream using the Stanford CoreNLP toolkit (Manning et al., 2014). 4 Due to the upper bound of Conf (Gc , Ge ), the algorithm must terminate after several iterations. We detected bursts and constructed the BINets5 for the Chinese and English stream based on (Ge et al., 2016a). The constructed Chinese BINet has 7,360 nodes and 33,892 edges while the English one has 8,852 nodes and 85,125 edges. Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation. Among the 7,360 nodes in the Chinese BINet, 2,281 nodes need to be deciphered since their words are not in the bi-lingual lexicon. 4.1.2 Evaluation Setting We evaluate our approach in an end-to-end fashion. For a node c in the Chinese BINet, we choose the node e∗ which has the highest score as c’s counterpart in the English BINet: e∗ = arg max Score(c, e) e∈Cand(c) We rank the aligned node pairs by the score and manually evaluate the quality of the top K pairs. A pair hc,ei is annotated as correct if e is a cor"
D18-1271,P11-1002,0,0.0330427,"n (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that ou"
D18-1271,P10-1115,0,0.0274153,"rger than that used in our experiment and they are endlessly updated. 2503 10 The alignments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Ki"
D18-1271,W02-2026,0,0.132146,"Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for genera"
D18-1271,P17-1179,0,0.0253021,"Missing"
D18-1271,D17-1207,0,0.0444246,"Missing"
D18-1271,C04-1089,0,0.0807467,"ingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpor"
D18-1271,N10-1063,0,0.0216071,"endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et a"
D18-1271,P06-1010,0,0.229533,"Chinese BINet, its candidate nodes in the English BINet can be derived as: Cand(c) = {e|P(e) ∩ P(c) 6= ∅} where e ∈ Ve , and P(c) and P(e) are the burst periods of c and e respectively. 3.3 Candidate Verification For the candidate list for c (i.e., Cand(c)), we need to verify each node e ∈ Cand(c) and choose the most probable one as c’s counterpart. Formally, we define Score(c, e) as the credibility score of e being the correct counterpart of c and propose the following novel clues for verification. Pronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e ∈ Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: 1 Sp ∈ [0, 1] ∝ LD where LD is the normalized (by e’s length) Levenshtein edit distance between c’s pinyin3 string and e’s word string. Translation For a node e ∈ Cand(c), it is possible that e’s word exists or partially exists in the bi-lingual lexicon. We can exploit the translation clue to verify if e is c’s counterpar"
D18-1271,D12-1003,0,0.0225396,"De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across la"
D18-1433,P16-1014,0,0.0345829,"objects into captions (Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel Knowledgeaware Video Description"
D18-1433,W04-3250,0,0.0136134,"se and diverse descriptions, though it negatively affects the entity incorporation performance. The video alone is insufficient to generate the correct entities (Table 2). In Figure 5a, the VD baseline generates the correct event, but generates the incorrect location “Kabul”. We observe that when the visual evidence is ambiguous, this model may fail to generate the correct events and entities. For example, if a video depicts the destruction of buildings after a hurricane, then the VD baseline 7 8 This criterion is used for computing precision and recall. Found via paired bootstrap resampling (Koehn, 2004). may mistakenly describe the video as an explosion since the visual evidence is similar. The article-only baseline tends to mention the correct entities as shown in Figure 5a, where the description is generally on topic but provides some irrelevant information. Indeed, this model can generate descriptions unrelated to the video itself. In Figure 5b, the article-only baseline’s description contains some correct entities (e.g., “Colombia”), but is not focused on the announcement depicted in the video. As See et al. (2017) discuss, this model can be more extractive than abstractive, copying many"
D18-1433,P13-1008,1,0.940267,"events that appear in the video’s description, but these may not be specific to the video content. For example, in Figure 2b, the video discusses the “heightened security” and does not depict the arrest directly. Topically related news documents capture background knowledge about the attack that led to the “heightened security” as well as the arrest, but they may not describe the actual video content, which displays some of the increased security measures. Thus, we propose to retrieve topically related news documents from which we seek to extract named entities (Pan et al., 2017) and events (Li et al., 2013) likely relevant to the video. We then propose to use this knowledge in the generation process through an entity pointer network, which learns to dynamically incorporate extracted entities into the description, and through a new knowledge gate, which conditions the generator on the extracted event and entity types. We include the video content in the generation by learning video representations using a spatio-temporal hierarchical attention that spatially attends to regions of each frame and temporally attends to different frames. We call the combination of these generation components the Know"
D18-1433,W04-1013,0,0.0415132,"Missing"
D18-1433,D16-1031,0,0.0534703,"ns by encoding information from the preceding and subsequent frames (Yao et al., 2015). We use a LSTM decoder, which applies a temporal attention (Bahdanau et al., 2015) to the frame representations at each step. To generate each word, the decoder computes its hidden state, adjusts this hidden state with the knowledge gate output at the current time step, and determines the most probable word by utilizing the entity pointer network to decide whether to generate a named entity or vocabulary word. Pointer networks are effective at incorporating out-of-vocabulary (OOV) words in output sequences (Miao and Blunsom, 2016; See et al., 2017). In previous research, OOV words may appear in the input sequence, in which case they are copied into the output. Analogously, in our approach, named entities can be considered as OOV words that are from a separate set instead of the input sequence. In the following equations, where appropriate, we omit bias terms for brevity. Encoder. The input to the encoder is a sequence of video frames, {F1 , ..., FN }. First, we extract frame-level features by applying a Convolutional Neural Network (CNN) (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Ioffe and Szegedy, 2015;"
D18-1433,K16-1028,0,0.0223229,"(Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel Knowledgeaware Video Description network, which can utili"
D18-1433,N04-1019,0,0.0681349,"beddings and compute entity embeddings. For visual features, we use the Conv3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the"
D18-1433,P11-1020,0,0.281605,"Description network. The model learns to incorporate entities found in the topically related documents into the description via an entity pointer network and the generation procedure is guided by the event and entity types from the topically related documents through a knowledge gate, which is a gating mechanism added to the model’s decoder that takes a one-hot vector of these types. We evaluate our approach on the new dataset of news videos we have collected, establishing the first benchmark for this dataset as well as proposing a new metric to evaluate these descriptions. 1 a) Description (Chen and Dolan, 2011): A man is talking. b) Human Description: Senior army ofﬁcer and Zimbabwe Defence Forces' spokesperson, Major General S. B. Moyo, assures the public that President Robert Mugabe and his family are safe and denies that the military is staging a coup. Figure 1: Comparison of machine (a) and human (b) generated descriptions.1 Introduction Video captioning is a challenging task that seeks to automatically generate a natural language description of the content of a video. Many video captioning efforts focus on learning video representations that model the spatial and temporal dynamics of the videos"
D18-1433,D17-1238,0,0.0476198,"Missing"
D18-1433,P17-1178,1,0.924882,"contain the named entities or events that appear in the video’s description, but these may not be specific to the video content. For example, in Figure 2b, the video discusses the “heightened security” and does not depict the arrest directly. Topically related news documents capture background knowledge about the attack that led to the “heightened security” as well as the arrest, but they may not describe the actual video content, which displays some of the increased security measures. Thus, we propose to retrieve topically related news documents from which we seek to extract named entities (Pan et al., 2017) and events (Li et al., 2013) likely relevant to the video. We then propose to use this knowledge in the generation process through an entity pointer network, which learns to dynamically incorporate extracted entities into the description, and through a new knowledge gate, which conditions the generator on the extracted event and entity types. We include the video content in the generation by learning video representations using a spatio-temporal hierarchical attention that spatially attends to regions of each frame and temporally attends to different frames. We call the combination of these g"
D18-1433,W14-3348,0,0.0367102,"Missing"
D18-1433,P16-1154,0,0.0754805,"., 2017), incorporate novel objects into captions (Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel K"
D18-1433,P17-1117,1,0.841562,"em mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et al., 2015; Yu et al., 2017). We move towards using datasets with captions that have specific knowledge rather than generic 3998 Model Article-only VD VD+Entity Pointer VD+Knowledge Gate Entity Pointer+Knowledge Gate KaVD METEOR ROUGE-L Entity F1 Auto-Entity F1 Event F1 Auto-Event F1 8.6 9.1 9.7 9.8 10."
D18-1433,D17-1103,1,0.825738,"em mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et al., 2015; Yu et al., 2017). We move towards using datasets with captions that have specific knowledge rather than generic 3998 Model Article-only VD VD+Entity Pointer VD+Knowledge Gate Entity Pointer+Knowledge Gate KaVD METEOR ROUGE-L Entity F1 Auto-Entity F1 Event F1 Auto-Event F1 8.6 9.1 9.7 9.8 10."
D18-1433,N18-2102,1,0.823313,"v3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the F1 score of the named entities in the generated description compared to"
D18-1433,N15-1173,0,0.0413973,"te with this metric. We observe discrepancies between the manual and automatic event metrics, in part, due to errors in the automated extraction and the addition of more test points. For example, in the generated sentence, “Hundreds of people are to take to the streets of...”, the event extraction system mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et"
D18-1433,D15-1199,0,0.025523,"Missing"
D18-1433,D17-1239,0,0.0255935,"atures, we use the Conv3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the F1 score of the named entities in the genera"
D18-1433,P17-1099,0,0.383734,"on from the preceding and subsequent frames (Yao et al., 2015). We use a LSTM decoder, which applies a temporal attention (Bahdanau et al., 2015) to the frame representations at each step. To generate each word, the decoder computes its hidden state, adjusts this hidden state with the knowledge gate output at the current time step, and determines the most probable word by utilizing the entity pointer network to decide whether to generate a named entity or vocabulary word. Pointer networks are effective at incorporating out-of-vocabulary (OOV) words in output sequences (Miao and Blunsom, 2016; See et al., 2017). In previous research, OOV words may appear in the input sequence, in which case they are copied into the output. Analogously, in our approach, named entities can be considered as OOV words that are from a separate set instead of the input sequence. In the following equations, where appropriate, we omit bias terms for brevity. Encoder. The input to the encoder is a sequence of video frames, {F1 , ..., FN }. First, we extract frame-level features by applying a Convolutional Neural Network (CNN) (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Ioffe and Szegedy, 2015; Szegedy et al., 201"
D18-1433,K17-1044,0,0.0998129,"ding, xt−1 . The final decoder hidden state is determined after the knowledge gate computation. The motivation for the knowledge gate is that it biases the model to generate sentences that contain specific knowledge relevant to the video and topically related documents, acting as a kind of coverage mechanism (Tu et al., 2016). For example, given the retrieved event types in Figure 3, the knowledge gate encourages the decoder to generate the event trigger “coup” due to the presence of the “Attack” event type. Inspired by the gating mechanisms from natural language generation (Wen et al., 2015; Tran and Nguyen, 2017), the knowledge gate, gt , is given by gt = σ (Wg,v [xt−1 , vt ] + Wg,sˆst ) (7) kt = gt kt−1 (8) where all W are learned parameters and [xt−1 , vt ] is the concatenation of these two vectors. This gating step determines the amount of the entity and event type features contained in kt−1 to carry to the next step. With the updated kt , we compute the decoder hidden state, st , as st = ˆst + (ot tanh (Ws,k kt )) (9) where ot is the output gate of the LSTM and Ws,k is a learned parameter. Our next step is to generate the next word. The model needs to produce named entities (e.g., “S. B. Moyo” and"
D18-1433,N16-1174,0,0.0505958,"0 corresponds to an event or entity type (e.g., “Arrest-Jail” event type or “President” entity type), so the j th element, k (j) , is 1 if the entity or event type is found in the related documents and 0 otherwise. k0 serves as the initial knowledge gate vector of the decoder (Section 2.2). The entity embeddings give the model access to semantic representations of the entities, while the knowledge gate vector aids the generation process by providing the model with the event and entity types. 2.2 KaVD Network Our model learns video representations using hierarchical, or multi-level, attention (Yang et al., 2016; Qin et al., 2017). The encoder is comprised of a spatial attention (Xu et al., 2015) and bidirectional Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) temporal encoder. The spatial attention allows the model to attend to different locations of each frame (Figure 4), yielding frame representations 3994 S. B. Moyo g Output Distribution gin p genP v + (1 − p gen)P e st p gen vt ct Temporal Attention gin vt S. Mu ga be st Vocab Distribution B. M Zim oyo ba bw e as su r froes m Spatial Attention g hN sta h1 Entity Distribution Knowledge Gate Vector S. B. Moyo sta General"
D18-1433,P16-1008,0,0.0352559,"where st−1 is the previous decoder hidden state and atime is another scoring function. This yields a single, spatio-temporally attentive video representation, vt . We then compute an intermediate hidden state, ˆst , by applying the decoder LSTM 3995 to st−1 , vt , and previous word embedding, xt−1 . The final decoder hidden state is determined after the knowledge gate computation. The motivation for the knowledge gate is that it biases the model to generate sentences that contain specific knowledge relevant to the video and topically related documents, acting as a kind of coverage mechanism (Tu et al., 2016). For example, given the retrieved event types in Figure 3, the knowledge gate encourages the decoder to generate the event trigger “coup” due to the presence of the “Attack” event type. Inspired by the gating mechanisms from natural language generation (Wen et al., 2015; Tran and Nguyen, 2017), the knowledge gate, gt , is given by gt = σ (Wg,v [xt−1 , vt ] + Wg,sˆst ) (7) kt = gt kt−1 (8) where all W are learned parameters and [xt−1 , vt ] is the concatenation of these two vectors. This gating step determines the amount of the entity and event type features contained in kt−1 to carry to the n"
D18-1433,D16-1204,0,0.0955565,"b) Human Description: Senior army ofﬁcer and Zimbabwe Defence Forces' spokesperson, Major General S. B. Moyo, assures the public that President Robert Mugabe and his family are safe and denies that the military is staging a coup. Figure 1: Comparison of machine (a) and human (b) generated descriptions.1 Introduction Video captioning is a challenging task that seeks to automatically generate a natural language description of the content of a video. Many video captioning efforts focus on learning video representations that model the spatial and temporal dynamics of the videos (Yao et al., 2015; Venugopalan et al., 2016; Yu et al., 2017). Although the language generation component within this task is of great importance, less work has been done to enhance the contextual knowledge conveyed by the descriptions. The descriptions generated by previous methods tend to be “generic”, describing To address this problem, we collect a news video dataset, where each video is accompanied by meta-data (e.g., tags and date) and a natural language description of the content in, and/or context around, the video. We create an approach to this task that is motivated by two observations. First, the video content alone is insuf"
D18-1435,W14-3348,0,0.0828395,"esentative social media platform. We use the same keywords as for template generator training to retrieve multi-modal data with Creative Commons license9 , for social and sports events. We choose the images that already have news-style descriptions from users and manually confirm they are well-aligned. In total, we collect 2,594 images for evaluation. For each image, we also obtain the tags (30,148 totally) and meta-data, such as EXIF and geotag data, when they are available. Evaluation Metrics We use three standard image captioning evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), REOUGE (Lin, 2004) and CIDEr (Vedantam et al., 2015), to evaluate the quality of both the generated templates and generated captions. BLEU is a metric based on correlations at the sentence level. METEOR is a metric with recall weighted higher than precision and it takes into account stemming as well as synonym matching. ROUGE is proposed for evaluation of summarization and relies 7 4018 8 9 https://www.flickr.com/ https://creativecommons.org/licenses/ highly on recall. CIDEr metric downweights the n-grams common in all image captions, which are similar to tf-idf. Since the goal of this task"
D18-1435,P12-1038,0,0.0279696,"protest, which is not related to the image. One potential improvement is to incorporate the information from associated tags, such as the number of tags and the named entity types related to the tags, as features during template caption generation to make generated templates dynamically change according to the context. 6 Related Work The goal of image captioning is to automatically generate a natural language sentence given an image. (Kulkarni et al., 2013; Yang et al., 2011; Mitchell et al., 2012) perform object recognition in images and fill hand-made templates with the recognized objects. (Kuznetsova et al., 2012, 2014) retrieve similar images, parse associated captions into phrases, and compose them into new sentences. Due to the use of static, handmade templates, these approaches are unable to generate a variety of sentence realizations, which can result in poorly generated sentences and requires one to manually create more templates to extend the generation. Our approach overcomes this by dynamically generating the output. More recent work utilizes neural networks and applies an encoder-decoder model (Cho et al., 2014). Vinyals et al. (2015) use a CNN to encode images into a fixed size vector repre"
D18-1435,Q14-1028,0,0.0151988,"n, which are not aligned with the key content in images; (3) remove captions with less than 10 tokens because they tend to be not informative enough. The average length of the news image captions is 37 tokens. Compression: The goal of compression is to make news captions short and aligned with images as much as possible by keeping informa4015 tion related to objects and concepts in the images, which are usually subjects, verbs and objects in sentences. In this paper, we propose a simple but efficient compression method based on dependency parsing. We do not use other complicated compressions (Kuznetsova et al., 2014) because our simple method achieves comparative results on image caption dataset. We first apply the Stanford dependency parser (De Marneffe and Manning, 2008) on preprocessed captions. Then, we traverse the parse tree from the root (e.g.‘pours’) via <governor, grammatical relations, dependent> triples using breadth-first search. We decide to keep a dependent or not based on its grammatical relation with the governor. Based on our observations, among the 50 grammatical relations in the Stanford dependency parser, we keep the dependents that have the following grammatical relations with their g"
D18-1435,P14-5010,0,0.00839071,"es using breadth-first search. We decide to keep a dependent or not based on its grammatical relation with the governor. Based on our observations, among the 50 grammatical relations in the Stanford dependency parser, we keep the dependents that have the following grammatical relations with their governors: nsubj, obj, iobj, dobj, acomp, det, neg, nsubjpass, pobj, predet, prep, prt, vmod, nmod, cc. Generalization: The last step for preparing training data is to extract entities from captions and replace them with the slot types we defined in Section 3.1. We apply Stanford CoreNLP name tagger (Manning et al., 2014) to the captions to extract entity mentions of the following types: Person, Location, Organization, and Miscellaneous. Next, we use an English Entity Linking algorithm (Pan et al., 2015) to link the entity mentions to DBpedia and retrieve their fine-grained types.5 We choose the higher level type if there are multiple fine-grained types for a name. For example, the entity types of Manchester United, Eric Bailly, and Jesse Lingard are SoccerTeam, Athlete, and Athlete, respectively. For entity mentions that cannot be linked to DBpedia, we use their coarse-grained entity types, which are the outp"
D18-1435,E12-1076,0,0.0295396,"es generated by our model. Examples (C) in Figure 6 is about a film festival, but the model generates a template about protest, which is not related to the image. One potential improvement is to incorporate the information from associated tags, such as the number of tags and the named entity types related to the tags, as features during template caption generation to make generated templates dynamically change according to the context. 6 Related Work The goal of image captioning is to automatically generate a natural language sentence given an image. (Kulkarni et al., 2013; Yang et al., 2011; Mitchell et al., 2012) perform object recognition in images and fill hand-made templates with the recognized objects. (Kuznetsova et al., 2012, 2014) retrieve similar images, parse associated captions into phrases, and compose them into new sentences. Due to the use of static, handmade templates, these approaches are unable to generate a variety of sentence realizations, which can result in poorly generated sentences and requires one to manually create more templates to extend the generation. Our approach overcomes this by dynamically generating the output. More recent work utilizes neural networks and applies an e"
D18-1435,N15-1119,1,0.896083,"Stanford dependency parser, we keep the dependents that have the following grammatical relations with their governors: nsubj, obj, iobj, dobj, acomp, det, neg, nsubjpass, pobj, predet, prep, prt, vmod, nmod, cc. Generalization: The last step for preparing training data is to extract entities from captions and replace them with the slot types we defined in Section 3.1. We apply Stanford CoreNLP name tagger (Manning et al., 2014) to the captions to extract entity mentions of the following types: Person, Location, Organization, and Miscellaneous. Next, we use an English Entity Linking algorithm (Pan et al., 2015) to link the entity mentions to DBpedia and retrieve their fine-grained types.5 We choose the higher level type if there are multiple fine-grained types for a name. For example, the entity types of Manchester United, Eric Bailly, and Jesse Lingard are SoccerTeam, Athlete, and Athlete, respectively. For entity mentions that cannot be linked to DBpedia, we use their coarse-grained entity types, which are the outputs of name tagger. Finally, we replace the entities in the compressed captions with their corresponding slots: Generalized Template: <Athlete> pours champagne over <Athlete>. 3.3 Genera"
D18-1435,P02-1040,0,0.102062,"8 , which is an image-centric, representative social media platform. We use the same keywords as for template generator training to retrieve multi-modal data with Creative Commons license9 , for social and sports events. We choose the images that already have news-style descriptions from users and manually confirm they are well-aligned. In total, we collect 2,594 images for evaluation. For each image, we also obtain the tags (30,148 totally) and meta-data, such as EXIF and geotag data, when they are available. Evaluation Metrics We use three standard image captioning evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), REOUGE (Lin, 2004) and CIDEr (Vedantam et al., 2015), to evaluate the quality of both the generated templates and generated captions. BLEU is a metric based on correlations at the sentence level. METEOR is a metric with recall weighted higher than precision and it takes into account stemming as well as synonym matching. ROUGE is proposed for evaluation of summarization and relies 7 4018 8 9 https://www.flickr.com/ https://creativecommons.org/licenses/ highly on recall. CIDEr metric downweights the n-grams common in all image captions, which are similar"
D18-1435,W10-0721,0,0.0480132,"itician, and so on. Therefore, 4 We use the sixth level entity types in Yago ontology (Wordnet types only). a template caption like ‘Athlete celebrates after scoring.’ can be generated by the language model through leveraging image features, where the slot Athlete means a sports player (e.g., Cristiano Ronaldo, Lionel Messi). 3.2 Acquisition of Image-Template Caption Pairs High quality training data is crucial to train a template caption generator. However, the image-caption datasets used in previous work, such as Microsoft Common Objects in Context (MS COCO) (Lin et al., 2014) and Flickr30K (Rashtchian et al., 2010), are not suitable for this task because they are designed for non-specific caption generation and do not contain detailed, specific information such as named entities. Further, manual creation of captions is expensive. In this work, we utilize news image-caption pairs, which are well written and can be easily collected. We use the example in Figure 2 to describe our procedure to convert image-caption to imagetemplate caption pairs: preprocessing, compression, generalization. Figure 2: Procedure of Converting News Captions into Templates. Preprocessing: We first apply the following pre-process"
D18-1435,K17-1044,0,0.0162481,"ated images from the Flickr database, which have the same tags as the input image. Next, we apply EDL algorithms to the image titles to extract entities and link them to external knowledge bases to retrieve their finegrained entity types. Finally, for each slot generated by the template generator, we choose to fill the slot with the appropriate candidate based on entity type and frequency (Section 4). 3 EXIF data contains meta-data tags of photos such as date, time, and camera settings. 4014 3 Template Caption Generation Language models (LM) are widely used to generate text (Wen et al., 2015; Tran and Nguyen, 2017) and play a crucial role in most of the existing image captioning approaches (Vinyals et al., 2015; Xu et al., 2015). These models, learned from large-scale corpora, are able to predict a probability distribution over a vocabulary. However, LM struggle to generate specific entities, which occur sparsely, if at all, within training corpora. Moreover, the desired entity-aware captions may contain information not directly present in the image alone. Unless the LM is trained or conditioned on data specific to the emergent situation of interest, the LM alone cannot generate a caption that incorpora"
D18-1435,D15-1081,1,0.937576,"obabilities of words/slots, yt , based on the word/slot generated at last time step, yt−1 , as well as the hidden state, st . Figure 3: LSTM language generator. 4 Template Caption Entity Population With the generated template captions, our next step is to fill in the slots with the appropriate specific entities to make the caption complete and entity-aware. In this section, we expand our method to extract candidate entities from contextual information (i.e., images in Flickr with the same tags). Once we extract candidate entities, we apply the Quantified Collective Validation (QCV) algorithm (Wang et al., 2015), which constructs a number of candidate graphs and performs collective validation on those candidate graphs to choose the appropriate entities for the slots in the template caption. 4016 co-occur (0.19) Alison Berner Miles co-occur (0.03) Norfolk Modernising Junior doctors cur co-oc (0.32) Colney British co-occ ur (0.53 ) -o c cu West 1) Organization 1) 8) .6 co (0 0.0 ur r( cc ur (0.6 16 r( 0. -o cu co -oc Location Tories co ) Person co-occ England Norfolk and Norwich University Hospital Royal Building Figure 4: Knowledge graph construction. 4.1 Candidate Entity Retrieval Users on social med"
D18-1435,D15-1199,0,0.0673908,"Missing"
D18-1435,D11-1041,0,\N,Missing
D18-1435,W10-0707,0,\N,Missing
D18-1435,D14-1198,1,\N,Missing
D18-2017,H05-1091,0,0.131313,"atively short time, they usually mention the items with the exact words from the list they are given. Thus, we take the nouns and noun phrases as chunks and if any word matches with the nouns in the given list, it is recognized as the item in the given list. So far, we have the opinion words and potential targets annotated in the conversation, and we want to pair them up and find the target of the ranking. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). Based on the observation that people tend to mention items and their related ranks close to each other, we pair the item with the rank found in its shortest dependency path. where αij (calculated from the model) can tell us how much the state transition of person i is influenced by the given neighbor j. This observed IM is characterized by (Φ, A), where Φ is the state transition probability matrix, and A is the influence strength vector. At any time t, we calculate the pairwise transition probabilj ity matrix P (Sti |St−1 ) by counting, and determine αij using the constrained gradient ascent"
D18-2017,P05-1045,0,0.0134764,"takes as input transcribed meeting speech, sentence by sentence, and outputs realtime rankings (opinion words) of the items after each participant expresses her/his thoughts. 3.1 Opinion Word Identification In the context of the Lunar Survival Task discussion, we observed that participants express their opinions of item rankings in multiple ways, including 1. Explicitly mentioning an item with its rankIn the first scenario of a participant proposing an item ranking, we use the Stanford CoreNLP (Manning et al., 2014) name tagger to extract the NUMBERs and ORDINALs mentioned in the discussion (Finkel et al., 2005). We also eliminated 98 the items, and E denotes the edges between these vertices. ui denotes the ith vertex or participants’ cumulative informative score in U . vj is the jth vertex or item ranking in V . In the Lunar Survival scenario, we observed that the information given in the conversation is very helpful in getting the right result and reaching consensus. To reflect this observation, we have total number of sentences so far for each speaker in the conversation as an informativeness indicator. The edges of the bipartite graph carry weights wij , representing the relationship between vert"
D18-2017,W06-0301,0,0.057875,"d target pairing as described in Section 3 for the 5 meetings. The extraction precision and accuracy compared to human annotated ground truth is summarized in Table 1. The precision is defined as the fraction of correct ranks among all ranks retrieved from the conversation, and recall is the fraction of correct ranks that have been retrieved over all the ranks supposed to be retrieved as in ground truth. 5.2 Figure 3: Example of arguments with stronger influence. and named entities mentioned by different discussants. As for opinion extraction, various methods were used in different contexts. (Kim and Hovy, 2006) collected opinion-bearing words and classified them into 3 classes. (Ortigosa et al., 2014) also studied opinion from 3 classes. Since in our case, the opinion on an item is restricted to a ranking of 1–15, we used name tagging results to identify the ordinals and numbers mentioned in the conversation. Meeting Dynamics Analysis We see that groups have very distinct opinions on the 15 items before each meeting, and that they all achieved consensus at the final stage of the meeting. From the playback of the meeting assistant videos, we have a very clear view of the unfolding speech content that"
D18-2017,P14-5010,0,0.00504541,"., “Matches are less important than signal flares because they don’t work on the moon.”) Our system takes as input transcribed meeting speech, sentence by sentence, and outputs realtime rankings (opinion words) of the items after each participant expresses her/his thoughts. 3.1 Opinion Word Identification In the context of the Lunar Survival Task discussion, we observed that participants express their opinions of item rankings in multiple ways, including 1. Explicitly mentioning an item with its rankIn the first scenario of a participant proposing an item ranking, we use the Stanford CoreNLP (Manning et al., 2014) name tagger to extract the NUMBERs and ORDINALs mentioned in the discussion (Finkel et al., 2005). We also eliminated 98 the items, and E denotes the edges between these vertices. ui denotes the ith vertex or participants’ cumulative informative score in U . vj is the jth vertex or item ranking in V . In the Lunar Survival scenario, we observed that the information given in the conversation is very helpful in getting the right result and reaching consensus. To reflect this observation, we have total number of sentences so far for each speaker in the conversation as an informativeness indicato"
D18-2017,P12-1042,0,0.0195878,"between vertices ui and vj , i.e., ui ’s current ranking of vj . Thus, we can represent all the edge weights of the graph as a |U |× |V |matrix W = [wij ]. With the item-rank pair extracted, P we dynamically update W , and calw culate vj as ii ij . numbers beyond 15 and numbers that are parts of pronouns such “this one”. Additionally, in this specific discussion, people use “last” or “least” to imply they are ranking the item at 15 and we also implemented this rule. As for the second scenario, people typically express agreement/disagreement with the person who talked immediately before them (Abu-Jbara et al., 2012). For agreement, we assume the current speaker accepts the previous speaker’s stated opinion, which means we pass the weights captured for the previous person to the current speaker if we find the expression of agreement in the current sentence. We found that expressions of disagreement are not useful since people typically express their own opinion following their disagreement. We currently do not deal with the third scenario of relative rankings, because no definitive ranking can be extracted from such statements. 3.2 4.2 Influence Model We implemented an influence model (IM) (Basu et al., 2"
D19-1025,C18-1057,1,0.811724,"ifficult for people to accurately annotate those entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Associat"
D19-1025,D18-1021,1,0.843847,"ifficult for people to accurately annotate those entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Associat"
D19-1025,P17-1149,1,0.905559,"Missing"
D19-1025,P19-1140,1,0.887209,"Missing"
D19-1025,Q16-1026,0,0.0350068,"mation, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2"
D19-1025,R11-1017,0,0.030182,"languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without feature engineering. It can thus be adapted for both low-resource languages and domains, while no previous work deals with them at the same time. • We consider name tagging from two perspec262 O B-ORG I-ORG … O … O B"
D19-1025,D14-1162,0,0.0817182,"Missing"
D19-1025,W03-0426,0,0.243404,"Missing"
D19-1025,D18-1230,0,0.367798,"tives of sequence labeling and classification, to efficiently take the best advantage of both high-quality and noisy WL data. derived WL data does not contain complete annotations, thus can not be directly used for training. Ni et al. (2017) select the sentences with highest confidence, and assume missing labels as O (i.e., non-entity), but it will introduce a bias to recognize mentions as non-entity. Another line of work is to replace CRFs with Partial-CRFs (T¨ackstr¨om et al., 2013), which assign unlabeled words with all possible labels and maximize the total probability (Yang et al., 2018; Shang et al., 2018). However, they still rely on seed annotations or domain dictionaries for high-quality training. Massive Noisy Data WL corpora are usually generated with massive noisy data including missing labels, incorrect boundaries and types. Previous work filtered out WL sentences by statistical methods (Ni et al., 2017) or the output of a trainable classifier (Yang et al., 2018). However, abandoning training data may exacerbate the issue of inadequate annotation. Therefore, maximizing the potential of massive noisy data as well as highquality part, yet being efficient, is challenging. To address these i"
D19-1025,Q13-1001,0,0.132177,"Missing"
D19-1025,K16-1022,0,0.0256046,"vy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without f"
D19-1025,N16-1030,0,0.0729178,"damental task of extracting entity information, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et"
D19-1025,P17-1004,1,0.804855,"hose entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tives of se"
D19-1025,D18-1034,0,0.17044,"I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical to obtain, since it is difficult for"
D19-1025,P18-1074,1,0.911104,"t> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical to obtain, since i"
D19-1025,P17-1114,0,0.0456129,"Missing"
D19-1025,P16-1101,0,0.396346,"acting entity information, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et"
D19-1025,D17-1269,0,0.130772,"1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data"
D19-1025,C18-1183,0,0.328348,"tional Linguistics tives of sequence labeling and classification, to efficiently take the best advantage of both high-quality and noisy WL data. derived WL data does not contain complete annotations, thus can not be directly used for training. Ni et al. (2017) select the sentences with highest confidence, and assume missing labels as O (i.e., non-entity), but it will introduce a bias to recognize mentions as non-entity. Another line of work is to replace CRFs with Partial-CRFs (T¨ackstr¨om et al., 2013), which assign unlabeled words with all possible labels and maximize the total probability (Yang et al., 2018; Shang et al., 2018). However, they still rely on seed annotations or domain dictionaries for high-quality training. Massive Noisy Data WL corpora are usually generated with massive noisy data including missing labels, incorrect boundaries and types. Previous work filtered out WL sentences by statistical methods (Ni et al., 2017) or the output of a trainable classifier (Yang et al., 2018). However, abandoning training data may exacerbate the issue of inadequate annotation. Therefore, maximizing the potential of massive noisy data as well as highquality part, yet being efficient, is challengin"
D19-1025,P17-1135,0,0.153128,"E8QVifZpt4Zpw1+5v31Hjqu03o7+deEbEKN8T+pZtl/lena1EY4sTUwKmm1DC6uiB3yUxX9M3tL1UpckiJ03hAcUE4MMpZn22jkaZ23VvPxN9Mpmb1PshzM7zrW9KA3Z/jnAftWtV1qu75UaV+mo+6iD3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of we"
D19-1025,U08-1016,0,0.0450237,"gging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without feature engineering. It can thus be adapted for both low-resource languages and domains, while no previous work deals with them at the same time. • We consider name tagging from two perspec262"
D19-1025,P17-1178,1,0.902871,"XwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical"
D19-1025,N16-1029,1,0.847236,"Fs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model"
D19-1025,P16-2025,0,0.167213,"3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words"
D19-1030,D14-1164,0,0.0220708,"f manually annotated mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al.,"
D19-1030,D12-1092,0,0.0655205,"Missing"
D19-1030,L18-1245,0,0.0189856,"so are reported predominantly in the low-resource language data sources available to that community. For example, though English language news will occasionally discuss the Person Aung San Suu Kyi, the vast majority of Physical-Located relations and Meeting events involving her are only reported locally in Burmese news, and thus, without Burmese relation and event extraction, a knowledge graph of this person will lack this available information. Unfortunately, publiclyavailable gold-standard annotations for relation and event extraction exist for only a few languages (Doddington et al., 2004; Getman et al., 2018), and Burmese is not among them. Compared to other IE tasks such as name tagging, the annotations for Relation and Event Extraction are also more costly to obtain, because they are structured and require a rich label space. Recent research (Lin et al., 2017) has found that relational facts are typically expressed by identifiable patterns within languages and has shown Introduction Advanced Information Extraction (IE) tasks entail predicting structures, such as relations between entities, and events involving entities. Given a pair of entity mentions, Relation Extraction aims to identify the re"
D19-1030,P14-1038,1,0.937832,"ces by limiting the number of negative samples to be no more than the number of positive samples for each document. For data preprocessing, we apply the Stanford CoreNLP toolkit (Manning et al., 2014) for Chinese word segmentation and English tokenization, and the API provided by UDPipe (Straka and s m2 yij log(σ(U r · [hm1 i ; hij ; hj ])) i=1 j=1 (1) where U r is a weight matrix. 316 Straková, 2017) for Arabic tokenization. We use UDPipe1 for POS tagging and dependency parsing for all three languages. We follow the following criteria in previous work (Ji and Grishman, 2008; Li et al., 2013; Li and Ji, 2014) for evaluation: performance by applying models trained with various combinations of training and test data from these three languages, as shown in Tables 3 and 4. We can see that the results are promising. For both tasks, the models trained from English are best, followed by Chinese, and then Arabic. We find that extraction task performance degrades as the accuracy of language-dependent tools (for sentence segmentation, POS tagging, dependency parsing) degrades. • A relation mention is considered correct if its relation type is correct, and the head offsets of the two related entity mention a"
D19-1030,C16-1114,0,0.359758,"al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervise"
D19-1030,P13-1008,1,0.874134,"e training instances by limiting the number of negative samples to be no more than the number of positive samples for each document. For data preprocessing, we apply the Stanford CoreNLP toolkit (Manning et al., 2014) for Chinese word segmentation and English tokenization, and the API provided by UDPipe (Straka and s m2 yij log(σ(U r · [hm1 i ; hij ; hj ])) i=1 j=1 (1) where U r is a weight matrix. 316 Straková, 2017) for Arabic tokenization. We use UDPipe1 for POS tagging and dependency parsing for all three languages. We follow the following criteria in previous work (Ji and Grishman, 2008; Li et al., 2013; Li and Ji, 2014) for evaluation: performance by applying models trained with various combinations of training and test data from these three languages, as shown in Tables 3 and 4. We can see that the results are promising. For both tasks, the models trained from English are best, followed by Chinese, and then Arabic. We find that extraction task performance degrades as the accuracy of language-dependent tools (for sentence segmentation, POS tagging, dependency parsing) degrades. • A relation mention is considered correct if its relation type is correct, and the head offsets of the two relate"
D19-1030,P18-1074,1,0.816541,"2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Even"
D19-1030,P09-1113,0,0.169793,"Missing"
D19-1030,P16-1105,0,0.0446401,"ing techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et"
D19-1030,N19-1112,0,0.0293072,"raction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several individual monolingual NLP tasks, including relation extraction (Zhang et al., 2018b), event d"
D19-1030,N19-1392,0,0.0323235,"mantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several individual monolingual NLP tasks, including relation extraction (Zhang et al., 2018b), event detection (Nguyen and Grishm"
D19-1030,D18-1156,0,0.172682,"Missing"
D19-1030,P18-2106,0,0.0942397,"Missing"
D19-1030,N15-1028,0,0.0215415,"symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on multilingual common space construction that makes use of linear mappings (Mikolov et al., 2013; Rothe et al., 2016; Zhang et al., 2016; Lazaridou et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016; Faruqui and Dyer, 2014; Lu et al., 2015) to transfer surface features across languages, our major innovation is to convert the text data into structured representations derived from universal dependency parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-lingual structure transfer learning framework to project source language (SL) training data and target language (TL) test data into the common semantic space, so that we can train a rela"
D19-1030,N16-1034,0,0.107021,"nd thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew"
D19-1030,D18-1517,0,0.0143756,"nguage (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored"
D19-1030,P14-5010,0,0.00571376,"Missing"
D19-1030,P15-2060,0,0.0927352,"e-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name"
D19-1030,D17-1159,0,0.194902,"cy parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-lingual structure transfer learning framework to project source language (SL) training data and target language (TL) test data into the common semantic space, so that we can train a relation or event extractor from SL annotations and apply the resulting extractor to TL texts. We adopt graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017) to encode graph structures over the input data, applying graph convolution operations to generate entity and word representations in a latent space. In contrast to other encoders such a Tree-LSTM (Tai et al., 2015), GCN can cover more complete contextual information from dependency parses because, for each word, it captures all parse tree neighbors of the word, rather than just the child nodes of the word. Using this shared encoder, we treat the two tasks of relation extraction and event argument role labeling as mappings from the latent space to relation type and to event type and argument r"
D19-1030,W15-1506,0,0.0597738,"e-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name"
D19-1030,P17-1178,1,0.854212,"re seen in packed emergency rooms) Figure 2: Multilingual common semantic space and cross-lingual structure transfer. that the consistency in patterns observed across languages can be used to improve relation extraction. Inspired by their results, we exploit languageuniversal features relevant to relation and event argument identification and classification, by way of both symbolic and distributional representations. For example, language-universal POS tagging and universal dependency parsing is available for 76 languages (Nivre et al., 2018), entity extraction is available for 282 languages (Pan et al., 2017), and multi-lingual word embeddings are available for 44 languages (Bojanowski et al., 2017; Joulin et al., 2018). As shown in Figure 2, even for distinct pairs of entity mentions (colored pink and blue, in both English and Russian), the structures share similar language-universal symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on mul"
D19-1030,E17-1077,0,0.0555006,"Missing"
D19-1030,D12-1042,0,0.0476729,"y extracted by Stanford CoreNLP instead of manually annotated mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al."
D19-1030,P11-2052,0,0.0755243,"Missing"
D19-1030,P15-1150,0,0.12426,"Missing"
D19-1030,prazak-konopik-2017-cross,0,0.0648181,"nhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several in"
D19-1030,D18-1248,0,0.0916018,"ise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques h"
D19-1030,P14-1055,0,0.115821,"l., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has"
D19-1030,C18-1099,0,0.356373,"ise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques h"
D19-1030,P18-1046,0,0.0122437,"tions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine le"
D19-1030,N15-1104,0,0.0713422,"Missing"
D19-1030,E17-1110,0,0.0234348,"m extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of sup"
D19-1030,N16-1033,0,0.150323,"cluded in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingu"
D19-1030,N16-1091,0,0.0130887,"Joulin et al., 2018). As shown in Figure 2, even for distinct pairs of entity mentions (colored pink and blue, in both English and Russian), the structures share similar language-universal symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on multilingual common space construction that makes use of linear mappings (Mikolov et al., 2013; Rothe et al., 2016; Zhang et al., 2016; Lazaridou et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016; Faruqui and Dyer, 2014; Lu et al., 2015) to transfer surface features across languages, our major innovation is to convert the text data into structured representations derived from universal dependency parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-"
D19-1030,D15-1203,0,0.025466,"mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work"
D19-1030,C14-1220,0,0.0417218,"Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et a"
D19-1030,K17-3009,0,0.116769,"Missing"
D19-1030,N16-1156,0,0.0604827,"Missing"
D19-1030,D18-1244,0,0.136645,"versal, we first convert each tree node into a vector which is a concatenation of three language-universal representations at wordlevel: multilingual word embedding, POS embedding (Nivre et al., 2016), entity-type embedding, and dependency relation embedding. More details are reported in Section 3.2. Model Overview 2.3 GCN Encoder Structural information is important for relation extraction and event argument role labeling, thus we aim to generate contextualized word representations by leveraging neighbors in dependency trees for each node. Our GCN encoder is based on the monolingual design by Zhang et al. (2018b). The graphical sentence representation obtained from dependency parsing of a sentence with N tokens is converted into an N × N adjacency matrix A, with added self-connections at each node to help capture information about the current node itself, as in Kipf and Welling (2017). Here, Ai,j = 1 denotes the presence of a directed edge from node i to node j in the dependency tree. Initially, each node contains distributional information about the ith word, including word embedding xw i , embeddings for symbolic information including its POS tag xpi , dependency relation xdi and entity type xei ."
D19-1030,C18-1037,0,0.0763441,"have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignme"
D19-1641,P18-1009,0,0.323495,"rs in that aspect”, the mention “Rogers” should be labeled as athlete in addition to person according to the context (e.g., game, Huskies). These fine-grained entity types are proven to be effective in supporting a wide range of downstream applications such as relation extraction (Yao et al., 2010), question answering (Lin et al., 2012), and coreference resolution (Recasens et al., 2013). 1 Code for this paper is available at: https://github. com/limteng-rpi/fet. Fine-grained entity typing is usually formulated as a multi-label classification problem. Previous approaches (Ling and Weld, 2012; Choi et al., 2018; Xin et al., 2018) typically address it with binary relevance that decomposes the problem into isolated binary classification subproblems and independently predicts each type. However, this method is commonly criticized for its label independence assumption, which is not valid for finegrained entity typing. For example, if the model is confident at predicting the type artist, it should promote its parent type person but discourage organization and its descendant types. In order to capture inter-dependencies between types, we propose a hybrid model that incorporates latent type representation"
D19-1641,D12-1082,0,0.077549,"Missing"
D19-1641,N18-1202,0,0.434006,"latent type features obtained through Principle Label Space Transformation (Tai and Lin, 2012) and reconstruct the sparse and high-dimensional type vector from this latent representation. Another major challenge in fine-grained entity typing is to differentiate similar types, such as director and actor, which requires the model to capture slightly different nuances in texts. Previous neural models (Shimaoka et al., 2016; Xin et al., 2018; Choi et al., 2018; Xu and Barbosa, 2018) generally extract features from pre-trained word embeddings. Instead, we adopt contextualized word representations (Peters et al., 2018), which can capture context-aware word semantics and better represent out-of-vocabulary words. We further propose a two-step attention mechanism to actively extract the most relevant information from the sentence. Particularly, we calculate the attention for context words in a mention-aware manner, allowing the model to focus on different parts of the sentence for different mentions. For example, in the following sentence, “In 2005 two fed6197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1641,N13-1071,0,0.0848339,"Missing"
D19-1641,D16-1144,1,0.915683,"Missing"
D19-1641,W16-1313,0,0.337463,"etween types, we propose a hybrid model that incorporates latent type representation in addition to binary relevance. Specifically, the model learns to predict a low-dimensional vector that encodes latent type features obtained through Principle Label Space Transformation (Tai and Lin, 2012) and reconstruct the sparse and high-dimensional type vector from this latent representation. Another major challenge in fine-grained entity typing is to differentiate similar types, such as director and actor, which requires the model to capture slightly different nuances in texts. Previous neural models (Shimaoka et al., 2016; Xin et al., 2018; Choi et al., 2018; Xu and Barbosa, 2018) generally extract features from pre-trained word embeddings. Instead, we adopt contextualized word representations (Peters et al., 2018), which can capture context-aware word semantics and better represent out-of-vocabulary words. We further propose a two-step attention mechanism to actively extract the most relevant information from the sentence. Particularly, we calculate the attention for context words in a mention-aware manner, allowing the model to focus on different parts of the sentence for different mentions. For example, in"
D19-1641,E17-1119,0,0.688452,", unlike previous neural models that generally use fixed word embeddings, we employ contextualized word representations (ELMo, Peters et al. 2018) that can capture word semantics in different contexts. Furthermore, because ELMo takes as input characters instead of words, it can better represent out-ofvocabulary words that are prevalent in entity mentions by leveraging sub-word information. Given a sentence of S words, the encoder generates a sequence of word vectors {r 1 , ..., r S }, where r i ∈ Rdr is the representation of the i-th word. 2.2 Mention Representation Previous attentive models (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xin et al., 2018; Choi et al., 2018) only apply attention mechanisms to the context. However, some words in an entity mention may provide more useful information for typing, such as “Department” in Figure 1. To allow the model to focus on more informative words, we represent a mention m consisting of M words as a weighted sum of its contextualized word representations with an attention mechanism (Bahdanau et al., 2015) as P m m= M i ai r i , where the attention score am i is computed as exp (em i ) m am = Softmax(e ) = , P i i M m k exp (ek ) m&gt; em tanh (W m r i ), i =v"
D19-1641,N18-1002,0,0.782594,"tent type representation in addition to binary relevance. Specifically, the model learns to predict a low-dimensional vector that encodes latent type features obtained through Principle Label Space Transformation (Tai and Lin, 2012) and reconstruct the sparse and high-dimensional type vector from this latent representation. Another major challenge in fine-grained entity typing is to differentiate similar types, such as director and actor, which requires the model to capture slightly different nuances in texts. Previous neural models (Shimaoka et al., 2016; Xin et al., 2018; Choi et al., 2018; Xu and Barbosa, 2018) generally extract features from pre-trained word embeddings. Instead, we adopt contextualized word representations (Peters et al., 2018), which can capture context-aware word semantics and better represent out-of-vocabulary words. We further propose a two-step attention mechanism to actively extract the most relevant information from the sentence. Particularly, we calculate the attention for context words in a mention-aware manner, allowing the model to focus on different parts of the sentence for different mentions. For example, in the following sentence, “In 2005 two fed6197 Proceedings of"
D19-1641,D10-1099,0,0.0218565,"in macro averaged F-score and micro averaged Fscore respectively. 1 1 Introduction Fine-grained entity typing aims to assign one or more types to each entity mention given a certain context. For example, in the following sentence, “If Rogers is in the game, the Huskies will be much better equipped to match the Cougars in that aspect”, the mention “Rogers” should be labeled as athlete in addition to person according to the context (e.g., game, Huskies). These fine-grained entity types are proven to be effective in supporting a wide range of downstream applications such as relation extraction (Yao et al., 2010), question answering (Lin et al., 2012), and coreference resolution (Recasens et al., 2013). 1 Code for this paper is available at: https://github. com/limteng-rpi/fet. Fine-grained entity typing is usually formulated as a multi-label classification problem. Previous approaches (Ling and Weld, 2012; Choi et al., 2018; Xin et al., 2018) typically address it with binary relevance that decomposes the problem into isolated binary classification subproblems and independently predicts each type. However, this method is commonly criticized for its label independence assumption, which is not valid for"
D19-1641,P15-2048,0,0.24898,"Missing"
D19-1641,C12-2133,0,0.699568,"propose a hybrid type classification model consisting of two classifiers as Figure 1 shows. We first learn a matrix W b ∈ Rdt ×2dr to predict type scores by ˜ b = W b (m ⊕ c), y where y˜ib is the score for the i-th type and dt is the number of types. However, this method independently predicts each type and does not consider their inter-dependencies. To tackle this issue, we introduce an additional classifier inspired by Principle Label Space Transformation (Tai and Lin, 2012). Under the hypercube sparsity assumption that the number of training examples is much smaller than 2dt , Tai and Lin (2012) project high-dimensional type vectors into a lowdimensional space to find underlying type correlations behind the first order co-occurrence through Singular Value Decomposition (SVD) Y ≈ Y˜ = U ΣL&gt; , where U ∈ Rdt ×dl , Σ ∈ Rdl ×dl , L ∈ RN ×dl , and dl  dt . This low-dimensional space is similar to the hidden concept space in Latent Semantic Analysis (Deerwester et al., 1990). The i-th row of L is the latent representation of the i-th type vector. After that, we learn to predict the latent type representation from the feature vector using 3 Experiments 3.1 Data Sets In our experiments, we e"
D19-5804,P04-3031,0,0.221819,"l language exams, respectively. Experiments and Discussions Datasets in Section 2.1). We first fine-tune BERTLARGE for five epochs on RACE to get the pre-fine-tuned model and then further fine-tune the model for eight epochs on the target QA datasets in scientific domains. We show the accuracy of the prefine-tuned model on RACE in Table 4. We use the noun phrase chunker in spaCy2 to extract concept mentions. For information retrieval, we use the version 7.4.0 of Lucene (McCandless et al., 2010) and set the maximum number of the retrieved sentences K to 50. We use the stop word list from NLTK (Bird and Loper, 2004). In addition, we design two slightly different settings for information retrieval. In setting 1, the original reference corpus of each dataset is independent. Formally, for each dataset x ∈ D, we perform information retrieval based on the corresponding original reference corpus of x and/or the external corpus generated based on problems in x, where D = {ARC-Easy, ARC-Challenge, OpenBookQA}. In setting 2, all original reference corpora are integrated to further leverage external in-domain knowledge. Formally, for each dataset x ∈ D, we conduct information retrieval based on the given reference"
D19-5804,N19-1423,0,0.365628,"through magnetism” and “iron is always magnetic”}, as well as general world knowledge extracted from an external source such as {“a belt buckle is often made of iron” and “iron is metal”} are required. Thus, these QA tasks provide suitable testbeds for evaluating external knowledge exploitation and intergration. Previous subject-area QA methods (e.g., (Khot et al., 2017; Zhang et al., 2018; Zhong et al., 2018)) explore many ways of exploiting structured knowledge. Recently, we have seen that the framework of fine-tuning a pre-trained language model (e.g., GPT (Radford et al., 2018) and BERT (Devlin et al., 2019)) outperforms previous state-of* Equal contribution. This work was conducted when the two authors were at Tencent AI Lab, Bellevue, WA. 1 Ground truth facts are usually not provided in this kind of question answering tasks. 1 Introduction 27 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 27–37 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics sides, our promising results emphasize the importance of external unstructured knowledge for subject-area QA. We expect there is still much scope for further improvements by exploitin"
D19-5804,P17-1147,0,0.0411808,"activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained language model such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) on QA tasks (Radford et al., 2018; Devlin et al., 2019; Hu et al., 2019; Yang"
D19-5804,K17-1010,0,0.0528847,"et al. (2014)) are trained using pre-defined classes in general domain such as P ERSON, L OCATION, and O RGA NIZATION . However, in ARC and OpenBookQA, the vast majority of mentions are from scientific domains (e.g., “rotation”, “revolution”, “magnet”, and “iron”). Therefore, we simply consider all noun phrases as candidate concept mentions, which are extracted by a noun phrase chunker. For example, in the sample problem in Table 2, we extract concept mentions such as “Mercury”. Then each concept mention is disambiguated and linked to its corresponding concept (page) in Most previous methods (Khashabi et al., 2017; Musa et al., 2018; Ni et al., 2019; Yadav et al., 2019) perform information retrieval on the reference corpus to retrieve relevant sentences to form reference documents. In contrast, we retrieve relevant sentences from the combination of an opendomain resource and the original reference corpus to generate a reference document for each (question, answer option) pair. We still keep up to top K sentences for each reference document (Section 2.1). See the framework overview in Figure 1. 29 2.3 Utilization of External Knowledge from In-Domain Data Since there are a relatively small number of trai"
D19-5804,D18-1260,0,0.475044,"nell University, Ithaca, NY, USA 3 Tencent AI Lab, Bellevue, WA, USA obtained from sources outside of the text (McNamara et al., 2004; Salmer´on et al., 2006). It is perhaps not surprising then, that machine readers also require knowledge external to the text itself to perform well on question answering (QA) tasks. We focus on multiple-choice QA tasks in subject areas such as science, in which facts from the given reference corpus (e.g., a textbook) need to be combined with broadly applicable external knowledge to select the correct answer from the available options (Clark et al., 2016, 2018; Mihaylov et al., 2018). For convenience, we call these subject-area QA tasks. Abstract We focus on multiple-choice question answering (QA) tasks in subject areas such as science, where we require both broad background knowledge and the facts from the given subject-area reference corpus. In this work, we explore simple yet effective methods for exploiting two sources of external knowledge for subject-area QA. The first enriches the original subject-area reference corpus with relevant text snippets extracted from an open-domain resource (i.e., Wikipedia) that cover potentially ambiguous concepts in the question and a"
D19-5804,I17-1097,0,0.0238765,"leading to large forest fires”, instead of the real cause “humanity” supported by “the problem now is with anthropogenic climate change—that is, climate change caused by human activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a"
D19-5804,D18-1055,0,0.0207706,"Missing"
D19-5804,Q19-1026,0,0.0212927,"change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained language model such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) on QA tasks (Radford et al., 2018; Devlin et al., 2019; Hu et al., 2019; Yang et al., 2019). As shown in Table 5, the basel"
D19-5804,N19-1030,0,0.587879,"sks. 1 Introduction 27 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 27–37 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics sides, our promising results emphasize the importance of external unstructured knowledge for subject-area QA. We expect there is still much scope for further improvements by exploiting more sources of external knowledge, and we hope the present empirical study can serve as a new starting point for researchers to identify the remaining challenges in this area. the-art methods (Mihaylov et al., 2018; Ni et al., 2019). However, it is still not clear how to incorporate different sources of external knowledge, especially unstructured knowledge, into this powerful framework to further improve subject-area QA. We investigate two sources of external knowledge (i.e., open-domain and in-domain), which have proven effective for other types of QA tasks, by incorporating them into a pre-trained language model during the fine-tuning stage. First, we identify concepts in question and answer options and link these potentially ambiguous concepts to an open-domain resource that provides unstructured background informatio"
D19-5804,N15-1119,1,0.804061,"tion retrieval; MRC: machine reading comprehension). Q, O, q, oi , di , and n denote the set of all questions, the set of all answer options, a question, one of the answer options associated with question q, the document (formed by retrieved sentences) associated with the (q, oi ) pair, and the number of answer options of q, respectively. Wikipedia. For example, the ambiguous concept mention “Mercury” in Table 2 should be linked to the concept Mercury (planet) rather than Mercury (element) in Wikipedia. For concept disambiguation and linking, we simply adopt an existing unsupervised approach (Pan et al., 2015) that first selects high quality sets of concept collaborators to feed a simple similarity measure (i.e., Jaccard) to link concept mentions. Question: Mercury, the planet nearest to the Sun, has extreme surface temperatures, ranging from 465◦ C in sunlight to −180◦ C in darkness. Why is there such a large range of temperatures on Mercury? A. The planet is too small to hold heat. B. The planet is heated on only one side. C. The planet reflects heat from its dark side. D. The planet lacks an atmosphere to hold heat. X Table 2: A sample problem from the ARC-Challenge dataset (Clark et al., 2018)"
D19-5804,D17-1082,0,0.346799,"sentences using the non-stop words in q and oi as the query and then concatenate the retrieved sentences to form di (Sun et al., 2019). The final prediction for each question is obtained by a linear plus softmax layer over the output of the final hidden state of the first token in each input sequence. By default, we employ the following two-step fine-tuning approach unless explicitly specified. Following previous work (Sun et al., 2019) based on GPT (Radford et al., 2018), we first finetune BERT (Devlin et al., 2019) on a large-scale multiple-choice machine reading comprehension dataset RACE (Lai et al., 2017) collected from English-as-a-foreign-language exams, which provides a ground truth reference document instead of a reference corpus for each question. Then, we further fine-tune the model on the target multiplechoice science QA datasets. For convenience, we call the model obtained after the first fine-tuning phase as a pre-fine-tuned model. We conduct experiments on three challenging multiple-choice science QA tasks where existing methods stubbornly continue to exhibit performance gaps in comparison with humans: ARC-Easy, ARC-Challenge (Clark et al., 2016, 2018), and OpenBookQA (Mihaylov et al"
D19-5804,D18-1053,0,0.0549991,"Missing"
D19-5804,D16-1264,0,0.190821,"Missing"
D19-5804,D15-1236,0,0.106165,"d science QA datasets. As shown in Figure 2, we see that the performance drops dramatically without using pre-fine-tuning on the RACE dataset. 4 4.1 Utilization of External Knowledge for Subject-Area QA Previous studies have explored many ways to leverage structured knowledge to solve questions in subject areas such as science exams. Many researchers investigate how to directly or indirectly use automatically constructed knowledge bases/graphs from reference corpora (Khot et al., 2017; Kwon et al., 2018; Khashabi et al., 2018; Zhang et al., 2018) or existing external general knowledge graphs (Li and Clark, 2015; Sachan et al., 2016; Wang et al., 2018a,c; Zhong et al., 2018; Musa et al., 2018) such as ConceptNet (Speer et al., 2017). However, for subject-area QA, unstructured knowledge is seldom considered in previous studies, and it is still not clear the usefulness of this kind of knowledge. As far as we know, for subject-area QA tasks, this is the first attempt to impart sources of external unstructured knowledge into one state-of-theart pre-trained language model, and we are among the first to investigate the effectiveness of the exRelated Work Subject-Area QA Tasks and Methods As there is not a"
D19-5804,P16-2076,0,0.0294061,"s. As shown in Figure 2, we see that the performance drops dramatically without using pre-fine-tuning on the RACE dataset. 4 4.1 Utilization of External Knowledge for Subject-Area QA Previous studies have explored many ways to leverage structured knowledge to solve questions in subject areas such as science exams. Many researchers investigate how to directly or indirectly use automatically constructed knowledge bases/graphs from reference corpora (Khot et al., 2017; Kwon et al., 2018; Khashabi et al., 2018; Zhang et al., 2018) or existing external general knowledge graphs (Li and Clark, 2015; Sachan et al., 2016; Wang et al., 2018a,c; Zhong et al., 2018; Musa et al., 2018) such as ConceptNet (Speer et al., 2017). However, for subject-area QA, unstructured knowledge is seldom considered in previous studies, and it is still not clear the usefulness of this kind of knowledge. As far as we know, for subject-area QA tasks, this is the first attempt to impart sources of external unstructured knowledge into one state-of-theart pre-trained language model, and we are among the first to investigate the effectiveness of the exRelated Work Subject-Area QA Tasks and Methods As there is not a clear distinction bet"
D19-5804,P18-1161,0,0.0703199,"Missing"
D19-5804,P14-5010,0,0.00309586,"can serve as a reliable piece of evidence to infer the correct answer option D for the question in Table 2. Just as human readers activate their background knowledge related to the text materials (Kendeou and Van Den Broek, 2007), we link concepts identified in questions and answer options to an opendomain resource (i.e., Wikipedia) and provide machine readers with unstructured background information relevant to these concepts, used to enrich the original reference corpus. Concept Identification and Linking: We first extract concept mentions from texts. Most mention extraction systems (e.g., Manning et al. (2014)) are trained using pre-defined classes in general domain such as P ERSON, L OCATION, and O RGA NIZATION . However, in ARC and OpenBookQA, the vast majority of mentions are from scientific domains (e.g., “rotation”, “revolution”, “magnet”, and “iron”). Therefore, we simply consider all noun phrases as candidate concept mentions, which are extracted by a noun phrase chunker. For example, in the sample problem in Table 2, we extract concept mentions such as “Mercury”. Then each concept mention is disambiguated and linked to its corresponding concept (page) in Most previous methods (Khashabi et a"
D19-5804,S18-1120,0,0.138342,"ence corpora are integrated to further leverage external in-domain knowledge. Formally, for each dataset x ∈ D, we conduct information retrieval based on the given reference corpus of D and/or the external corpus generated based on problems in D instead of x.3 . In our experiment, we use RACE (Lai et al., 2017) — the largest existing multiple-choice machine reading comprehension dataset collected from real and practical language exams — in the pre-finetuning stage. Questions in RACE focus on evaluating linguistic knowledge acquisition of participants and are commonly used in previous methods (Wang et al., 2018a; Sun et al., 2019). We evaluate the performance of our methods on three multiple-choice science QA datasets: ARC-Easy, ARC-Challenge, and OpenBookQA. ARC-Challenge and ARC-easy originate from the same set of exam problems collected from multiple sources. ARC-Challenge contains questions answered incorrectly by both a retrieval-based method and a word co-occurrence method, and the remaining questions form ARC-Easy. Questions in OpenBookQA are crowdsourced by turkers and then carefully filtered and modified by experts. See the statistics of these datasets in Table 3. Note that for OpenBookQA,"
D19-5804,W17-4413,0,0.028917,"fires”, instead of the real cause “humanity” supported by “the problem now is with anthropogenic climate change—that is, climate change caused by human activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained languag"
D19-5804,N19-1274,0,0.223526,"to this powerful framework to further improve subject-area QA. We investigate two sources of external knowledge (i.e., open-domain and in-domain), which have proven effective for other types of QA tasks, by incorporating them into a pre-trained language model during the fine-tuning stage. First, we identify concepts in question and answer options and link these potentially ambiguous concepts to an open-domain resource that provides unstructured background information relevant to the concepts and used to enrich the original reference corpus (Section 2.2). In comparison to previous work (e.g., (Yadav et al., 2019)), we perform information retrieval based on the enriched corpus instead of the original one to form a document for answering a question. Second, we increase the amount of training data by appending additional in-domain subject-area QA datasets (Section 2.3). 2 Method In this section, we first introduce our BERT-based QA baseline (Section 2.1). Then, we present how we incorporate external open-domain (Section 2.2) and in-domain (Section 2.3) sources of knowledge into the baseline. 2.1 Baseline Framework Given a question q, an answer option oi , and a reference document di , we concatenate them"
D19-5804,N19-4013,0,0.0363013,"Missing"
D19-5804,P06-4018,0,\N,Missing
D19-5804,P16-1223,0,\N,Missing
D19-5804,P17-2049,0,\N,Missing
D19-5804,N18-1202,0,\N,Missing
D19-5804,S18-1119,0,\N,Missing
D19-5804,D18-1453,0,\N,Missing
D19-5804,N19-1270,1,\N,Missing
D19-6107,P16-2058,0,0.0183015,"and Italy.) 5 Related Work Cross-lingual Word Embedding Learning. Mikolov et al. (2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of min"
D19-6107,W12-5604,0,0.0326989,"(2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translati"
D19-6107,D07-1074,0,0.0882441,"ntion to Entity Probability: |A∗,m |, where A∗,m is a set of anchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear togeth"
D19-6107,P17-1042,0,0.0148444,"LEU score. Classical Chinese - Modern Chinese * ⾄⼆战之时，南斯拉夫屡败，终为德意志、义⼤利所分。 * 在⼆次世界⼤战期间，南斯拉夫多次战败，分别被德国、 意⼤利占领。 (During the World War II, Yugoslavia was defeated several times and was occupied by Germany and Italy.) 5 Related Work Cross-lingual Word Embedding Learning. Mikolov et al. (2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not p"
D19-6107,E14-1049,0,0.0966405,"Missing"
D19-6107,P18-1073,0,0.0156503,"wledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts"
D19-6107,W16-2506,0,0.0173554,"ition, considering each pair of Wikipedia articles connected by an inter-language link as comparable documents, we use this multilingual common space to represent sentences and extract many parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor lin"
D19-6107,W04-3208,0,0.0345301,"parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-li"
D19-6107,D18-1021,0,0.0360999,"Missing"
D19-6107,P19-1118,0,0.0197823,"ined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017"
D19-6107,D11-1072,0,0.0617505,"ere A∗,m is a set of anchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear together and have similar contexts in text. Using CL"
D19-6107,D13-1184,0,0.0273206,"r text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear together and have similar contexts in text. Using CLEW embedding Z, the coherence score can be e"
D19-6107,C12-1089,0,0.0349723,"ment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts including (Conneau et al., 2017) do not explicitly handle entity representations. Moreover, we perform comprehensive extrinsic evaluations based on down-stream NLP ap"
D19-6107,P15-1027,0,0.0565409,"Missing"
D19-6107,P11-1138,0,0.0513717,"nchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as a knowledge graph and models coherence based on the overlapped neighbors of two entities in the knowledge graph. These approaches heavily rely on explicit connections among entities in the knowledge graph and thus cannot capture the coherence between two entities that are implicitly connected. For example, two entities en/Mosquito and en/Cockroach only have very few overlapped neighbors in the knowledge graph, but they usually appear together and have similar contexts in text. Using CLEW embedding Z, the co"
D19-6107,C16-1030,0,0.0342359,"Missing"
D19-6107,Q15-1023,0,0.0216008,"= 5, 10, 50. Following this work, we set K = 10. 3 Unsupervised Cross-lingual Entity Linking Downstream Applications We apply CLEW to enhance two important downstream tasks: Cross-lingual Entity Linking and Parallel Sentence Mining. Ftxt (e) = cos(vm , ze ) = 59 vm · z e kvm k kze k Feature Description Fprior (e) e,∗ Entity Prior: |A∗,∗ |, where Ae,∗ is a set of anchor links that link to entity e and A∗,∗ is all anchor links in the KB |Ae,m | Mention to Entity Probability: |A∗,m |, where A∗,m is a set of anchor links with anchor text m and Ae,m is a subset that links to entity e. Entity Type (Ling et al., 2015): P p(e|m) , where e 7→ t indicates that t is one of e’s e7→t p(e|m) entity types. Conditional probability p(e|m) can be estimated by Fprob (e|m). Fprob (e|m) Ftype (e|m, t) |A | Table 1: Mention disambiguation features. Coherence is driven by the assumption that if multiple mentions appear together within a context window, their referent entities are more likely to be strongly connected to each other in the KB. Previous work (Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013; Ceccarelli et al., 2013; Ling et al., 2015) considers the KB as"
D19-6107,P16-1100,0,0.020695,"-lingual Word Embedding Learning. Mikolov et al. (2013b) first notice that word embedding spaces have similar geometric arrangements across languages. They use this property to learn a linear mapping between two spaces. After that, several methods attempt to improve the mapping (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017). The measures used to compute similarity between a foreign word and an English word often include distributed monolingual representations on character-level (Costa-jussà and Fonollosa, 2016; Luong and Manning, 2016), subwordlevel (Anwarus Salam et al., 2012; Rei et al., Table 4: Examples of mined parallel sentences from Wikipedia. A portion of alignments are highlighted using the same colors. We randomly select 100 mined parallel sentence pairs for each of 3 language pairs, and ask linguistic experts to judge the quality of these sentence pairs (perfect, partial, or not parallel). The results are shown in Table 5. We can see that the quality of mined parallel sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extr"
D19-6107,P18-2037,0,0.0256022,"and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual w"
D19-6107,W15-1521,0,0.0248712,"ons which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts including (Conneau et al., 2017) do not explicitly handle entity representations. Moreover, we perform comprehensive extrinsic evaluations based on down-stream NLP applications including"
D19-6107,P16-1162,0,0.0101754,"ctionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large s"
D19-6107,W17-2617,0,0.0388491,"Missing"
D19-6107,P17-2062,0,0.0219451,"el sentence is promising and the quality of word and entity alignment is decent. Furthermore, we evaluate the quality of mined parallel sentences extrinsically using a neural machine translation (NMT) model. We use the 5 https://github.com/tensorflow/ tensor2tensor 62 Parallel Sentence Mining. Automatic mining parallel sentences from comparable documents is an important and useful task to improve Statistical Machine Translation. Early efforts mainly exploited bilingual word dictionaries for bootstrapping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that"
D19-6107,N16-1072,0,0.0734668,"ntences and extract many parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor links in Wikipedia to learn crosslingual joint entity and word embedding, so that entity mentions across multiple languages are disambiguated and grounded into one un"
D19-6107,D17-1270,0,0.0491752,"Missing"
D19-6107,D14-1167,0,0.0273262,"ace to represent sentences and extract many parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor links in Wikipedia to learn crosslingual joint entity and word embedding, so that entity mentions across multiple languages are disambiguated and"
D19-6107,N15-1104,0,0.0600246,"Missing"
D19-6107,K16-1025,0,0.0200526,"any parallel sentence pairs. The novel contributions of this paper are: Traditional approaches to derive training data from Wikipedia usually replace each anchor link with its anchor text, for example, “apple is a technology company.”. These methods have two limitations: (1) Information loss: For example, the anchor text “apple” itself does not convey information such as the entity is a company; (2) Ambiguity (Faruqui et al., 2016): For example, the fruit sense and the company sense of “apple” mistakenly share one surface form. Similar to previous work (Wang et al., 2014; Tsai and Roth, 2016; Yamada et al., 2016), we replace each anchor link with its corresponding entity title, and thus treat each entity title as a unique word. For example, “en/Apple_Inc. is a technology company.”. Using this kind of data mix of entity titles and contextual words, we can learn joint embedding of entities and words. pear juice apple microsoft computer company ibm steve macintosh jobs • We develop a novel approach based on rich anchor links in Wikipedia to learn crosslingual joint entity and word embedding, so that entity mentions across multiple languages are disambiguated and grounded into one unified common space. wo"
D19-6107,D17-1150,0,0.0181583,"pping (Fung and Cheung, 2004). Recent approaches are mainly based on bilingual word embeddings (Marie and Fujita, 2017) and sentence embeddings (Schwenk, 2018) to detect sentence pairs or continuous parallel segments (Hangya and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data"
D19-6107,P17-1179,0,0.0157094,"and Fraser, 2019). To the best of our knowledge, this is the first work to incorporate joint entity and word embedding into parallel sentence mining. As a result the sentence pairs we include reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1."
D19-6107,D13-1141,0,0.0414272,"ude reliable alignment between entity mentions which are often out-of-vocabulary and ambiguous and thus receive poor alignment quality from previous methods. 2016; Sennrich et al., 2016; Yang et al., 2017), and bi-lingual word embedding (Madhyastha and España-Bonet, 2017). Recent attempts have shown that it is possible to derive cross-lingual word embedding from unaligned corpora in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). Another strategy for cross-lingual word embedding learning is to combine monolingual and cross-lingual training objectives (Zou et al., 2013; Klementiev et al., 2012; Luong et al., 2015; Ammar et al., 2016; Vuli´c et al., 2017). Compared to our direct mapping approach, these methods generally require large size of parallel data. Our work is largely inspired from (Conneau et al., 2017). However, our work focuses on better representing entities, which are fundamentally different from common words or phrases in many aspects as described in Section 1. Previous multilingual word embedding efforts including (Conneau et al., 2017) do not explicitly handle entity representations. Moreover, we perform comprehensive extrinsic evaluations ba"
D19-6131,D16-1153,0,0.0134648,"talog 278 should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. Clark, 2003) develop a feature-based model using a maximum entropy tagger to achieve good results in English, Dutch and German. Because we do not assume access to capitalization which does not exist in many languages, many of their most valuable features are not suitable for our setting. (Bharadwaj et al., 2016) demonstrates crosslingual transfer for name tagging using phonologically grounded word representations. In particular, the authors demonstrate 0-shot transfer for their name tagging system between Uzbek and Turkish. While this approach requires monolingual word embeddings in the target language and benefits greatly from capitalization information, our method makes no such assumptions. (Ji et al., 2008) used a phonetically based method to match English person names in Mandarin audio segments. This method uses an English-to-pinyin transliteration model and then applies fuzzy matching to the tra"
D19-6131,W19-2804,1,0.869701,"ions. Finding names in documents is a critical part of extracting structured information from unstructured natural language documents. Therefore, it is an essential component for applications including Information Retrieval, Question Answering and Knowledge Base Population. Typical name finding methods rely on supervised learning and re1 https://translate.google.com/ 275 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 275–280 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 by (Blissett and Ji, 2019) by using a convolutional neural network (CNN) for our encoder rather than a recurrent model. We use a CNN in this case rather than an RNN because we find that CNNs can be trained faster, require fewer parameters, and provide similar overall performance. We apply our encoder network to character embeddings trained jointly with the rest of the encoder. We then use max pooling to derive a fixed length vector from the encoder filter values. similarity across all of the world’s languages, not just those in the training set. We encourage the model to learn more general similarity features across la"
D19-6131,W03-0424,0,0.266616,"Missing"
D19-6131,P17-1178,1,0.817367,"et includes annotations for the following types of entities: person, location, and geopolitical entities. We exclude organizations since the names of organizations are commonly translated based on meaning rather than transliterated. We use the top 30 most common names in the dataset as queries to simulate a user who only 2 4 Related Work The problem of name tagging in low-resource languages has had real attention within the last few years. For example, (Zhang et al., 2016) use a variety of non-traditional linguistic resources in order to train a name tagger for use in low-resource languages. (Pan et al., 2017) and (Tsai et al., 2016) both rely on Wikipedia to provide data for training name tagging models for all Wikipedia languages. Much work has also been pursued for systems that rely on very limited silver-standard training data annotated from the target language by nonspeakers (e.g., (Ji et al., 2017)). Our method differs from the above in that we do not require our target language to be present in Wikipedia or any other additional resources. Cross-linguistic name tagging systems have also been pursued. For example, (Curran and LDC2017E57 and LDC2017E58 in the LDC Catalog 278 should not be inter"
D19-6131,N16-1029,1,0.805191,"uages in order to show that our model transfers well even without training data in languages closely related to the target language. Our dataset includes annotations for the following types of entities: person, location, and geopolitical entities. We exclude organizations since the names of organizations are commonly translated based on meaning rather than transliterated. We use the top 30 most common names in the dataset as queries to simulate a user who only 2 4 Related Work The problem of name tagging in low-resource languages has had real attention within the last few years. For example, (Zhang et al., 2016) use a variety of non-traditional linguistic resources in order to train a name tagger for use in low-resource languages. (Pan et al., 2017) and (Tsai et al., 2016) both rely on Wikipedia to provide data for training name tagging models for all Wikipedia languages. Much work has also been pursued for systems that rely on very limited silver-standard training data annotated from the target language by nonspeakers (e.g., (Ji et al., 2017)). Our method differs from the above in that we do not require our target language to be present in Wikipedia or any other additional resources. Cross-linguisti"
D19-6204,P18-2108,0,0.0218652,"aihani and Laachfoubi, 2017). To tackle this problem, recent studies apply Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to automatically learn feature representations with input words encoded as pre-trained word embeddings (Zhao et al., 2016; Liu et al., 2016; Quan et al., 2016; Zhang et al., 2017; Zhou et al., 2018a; Sun et al., 2019). Learning representations of graphs are widely studied and several graph neural networks have been applied in the biomedical domain. Lim et al. (2018) proposed recursive neural network based model with a subtree containment feature. Asada et al. (2018) encoded drug pairs with CNNs and used external knowledge base to encode their molecular pairs with two graph neural networks. Here we directly apply syntax-aware GCNs on biomedical text to extract drug-drug interaction. Results and Analysis The experiment results are reported from a 2-layer GCN which achieves the best performance and shown in Table 1. Our model significantly outperforms all previous methods at the significance level of 0.05. To analyze the contributions and effects of the various components in our model, we also perform ablation tests. The ablated GCN model outperforms the LS"
D19-6204,D17-1159,0,0.0791783,"Missing"
D19-6204,D14-1082,0,0.0867919,"Missing"
D19-6204,N19-1423,0,0.0273502,"iron ← cobalt ← interactions → absorption → retention} can effectively help on the classification of the relation between these two mentions hcobalt, ironi. In order to capture indicative information from wide contexts, we adopt the graph convolutional networks (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017) to obtain the syntactic information by encoding the dependency structure over the input sentence with graph convolution operations. To compensate the loss of local context information in GCN, we incorporate the contextualized word representation pre-trained by the BERT model (Devlin et al., 2019) in large-scale biomedical corpora containing over 200K abstracts from PubMed and over 270K full texts from PMC (Lee et al., 2019) . Introduction Recently relation extraction in biomedical literature has attracted increasing interests from medical language processing research community as an important stage for downstream tasks such as question answering (Hristovski et al., 2015) and decision making (Agosti et al., 2019). Biomedical relation extraction aims to identify and classify relations between two entity mentions into pre-defined types based on contexts. In this paper we aim to extract d"
D19-6204,P19-1132,0,0.0616566,"Missing"
D19-6204,1983.tc-1.13,0,0.238442,"Missing"
D19-6204,D18-1244,0,0.0150484,"tion task by forcing the loss implicitly focus on ambiguous examples. To recap, our contributions are twofold: First, we adopt the syntax-aware graph convolutional networks incorporating contextualized representation. Second, we further design an auxiliary task to solve the data imbalance problem, which achieves the state-of-the-art micro F-score on the DDIExtraction 2013 shared task. 2 (l) hi = σ( e = A+I with A is the adjacent matrix of where A tokens in dependency tree, I is the identity matrix. W (l) is a linear transformation, b(l) is a bias term, and σ is a nonlinear function. Following Zhang et al. (2018), di is the degree of the token i in dependency tree with an additional self-loop. We notice that some token representations are more informative by gathering information from syntactically related neighbors through GCN. For example, the representation of the token interactions from a 2-layer GCN operating on its two edges apart neighbors provides inductive information for predicting a mechanism relation. Thus, we adopt attentive pooling (Zhou et al., 2016) to achieve the optimal pooling: Focal Loss Output Layer Pooling Layer Relation Classificaton Attentive Pooling eij W (l) hl−1 /di + b(l) )"
D19-6204,P16-2034,0,0.0384573,"ependency tree, I is the identity matrix. W (l) is a linear transformation, b(l) is a bias term, and σ is a nonlinear function. Following Zhang et al. (2018), di is the degree of the token i in dependency tree with an additional self-loop. We notice that some token representations are more informative by gathering information from syntactically related neighbors through GCN. For example, the representation of the token interactions from a 2-layer GCN operating on its two edges apart neighbors provides inductive information for predicting a mechanism relation. Thus, we adopt attentive pooling (Zhou et al., 2016) to achieve the optimal pooling: Focal Loss Output Layer Pooling Layer Relation Classificaton Attentive Pooling eij W (l) hl−1 /di + b(l) ) A j j=1 Methods Relation Identification n X Max Pooling α = sof tmax(wT tanh(h)) GCN Layer Contextual Embedding Layer via BioBERT hattentive = hαT where w is a trained parameter to assign weights based on the importance of each token representation. We obtain the final representation by concatenating the sentence from attentive pooling and the mention representations from max pooling. We finally obtain the prediction of relation type by feeding the final r"
H05-1003,M95-1005,0,0.421288,"2003 training corpora. We trained the relation tagger on 328 ACE 2004 texts. We used 126 newswire texts from the ACE 2004 data to train the English second-stage model, and 65 newswire texts from the ACE 2004 evaluation set as a test set for the English system. Chinese For Chinese, the baseline reference resolver was trained on 767 texts from ACE 2003 and ACE 2004 training data. Both the baseline relation tagger and the rescoring model were trained on 646 texts from ACE 2004 training data. We used 100 ACE texts for a final blind test. 6.2 Experiments We used the MUC coreference scoring metric (Vilain et al 1995) to evaluate3 our systems. To establish an upper limit for the possible improvement offered by our models, we first did experiments using perfect (hand-tagged) mentions and perfect relations as inputs. The algorithms for 3 In our scoring, we use the ACE keys and only score mentions which appear in both the key and system response. This therefore includes only mentions identified as being in the ACE semantic categories by both the key and the system response. Thus these scores cannot be directly compared against coreference scores involving all noun phrases. (Ng 2005) applies another variation"
H05-1003,A97-1029,0,0.0346084,"tric between two examples based on: 1 whether the heads of the mentions match 1 whether the ACE types of the heads of the mentions match (for example, both are people or both are organizations) 1 whether the intervening words match To tag a test example, we find the k nearest training examples, use the distance to weight each neighbor, and then select the most heavily weighted class in the weighted neighbor set. Name tagger and noun phrase chunker Our baseline name tagger consists of a HMM tagger augmented with a set of post-processing rules. The HMM tagger generally follows the Nymble model (Bikel et al. 1997), but with a larger number of states (12 for Chinese, 30 for English) to handle name prefixes and suffixes, and, for Chinese, transliterated foreign names separately. For Chinese it operates on the output of a word segmenter from Tsinghua University. Our nominal mention tagger (noun phrase chunker) is a maximum entropy tagger trained on treebanks from the University of Pennsylvania. 5.2 Rescoring stage To incorporate information from the relation tagger into the final coreference decision, we split the maxent classification into two stages. The first stage simply applies the baseline maxent mo"
H05-1003,C88-1021,0,0.101855,"upenn.edu/Projects/ACE/ 17 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 17–24, Vancouver, October 2005. 2005 Association for Computational Linguistics Section 6 presents the results of experiments on both English and Chinese test data. Section 7 presents our conclusions and directions for future work. 2 Prior Work Much of the earlier work in anaphora resolution (from the 1970’s and 1980’s, in particular) relied heavily on deep semantic analysis and inference procedures (Charniak 1972; Wilensky 1983; Carbonell and Brown 1988; Hobbs et al. 1993). Using these methods, researchers were able to give accounts of some difficult examples, often by encoding quite elaborate world knowledge. Capturing sufficient knowledge to provide adequate coverage of even a limited but realistic domain was very difficult. Applying these reference resolution methods to a broad domain would require a large scale knowledge-engineering effort. The focus for the last decade has been primarily on broad coverage systems using relatively shallow knowledge, and in particular on corpus-trained statistical models. Some of these systems attempt to"
H05-1003,W98-1119,0,0.0920301,"hods, researchers were able to give accounts of some difficult examples, often by encoding quite elaborate world knowledge. Capturing sufficient knowledge to provide adequate coverage of even a limited but realistic domain was very difficult. Applying these reference resolution methods to a broad domain would require a large scale knowledge-engineering effort. The focus for the last decade has been primarily on broad coverage systems using relatively shallow knowledge, and in particular on corpus-trained statistical models. Some of these systems attempt to apply shallow semantic information. (Ge et al. 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. (Tetreault and Allen 2004) use a semantic parser to add semantic constraints to the syntactic and agreement constraints in their Left-Right Centering algorithm. (Soon et al. 2001) use WordNet to test the semantic compatibility of individual noun phrase pairs. In general these approaches do not explore the possibility of exploiting the global semantic context provided by the document as a whole. Recently Bean and Rilof"
H05-1003,mitkov-2000-towards,0,0.0285571,"ng, text summarization and a number of other natural language processing tasks. Most reference resolution systems use representations built out of the lexical and syntactic attributes of the noun phrases (or “mentions”) for which reference is to be established. These attributes may involve string matching, agreement, syntactic distance, and positional information, and they tend to rely primarily on the immediate context of the noun phrases (with the possible exception of sentence-spanning distance measures such as Hobbs distance). Though gains have been made with such methods (Tetreault 2001; Mitkov 2000; Soon et al. 2001; Ng and Cardie 2002), there are clearly cases where this sort of local information will not be sufficient to resolve coreference correctly. Coreference is by definition a semantic relationship: two noun phrases corefer if they both refer to the same real-world entity. We should therefore expect a successful coreference system to exploit world knowledge, inference, and other forms of semantic information in order to resolve hard cases. If, for example, two nouns refer to people who work for two different organizations, we want our system to infer that these noun phrases canno"
H05-1003,N04-1038,0,\N,Missing
H05-1003,J01-4003,0,\N,Missing
H05-1003,P02-1014,0,\N,Missing
H05-1003,J01-4004,0,\N,Missing
I17-1037,N13-1006,0,0.0596371,"Missing"
I17-1037,P15-1017,0,0.0455657,"Missing"
I17-1037,Q16-1026,0,0.103444,"Missing"
I17-1037,2005.mtsummit-posters.10,0,0.00943266,", title, nationalities, topical keywords, organization suffixes, temporal words, locations and people, and stop words which are unlikely to be names. Elicitation Corpus. An elicitation corpus is a controlled corpus translated by a bilingual consultant in order to produce high quality word aligned sentence pairs. During the elicitation process, the user will translate a subset of these sentences that is dynamically determined to be sufficient for learning the desired grammar rules. We extracted word and phrase translation pairs from the Elicitation corpus developed by CMU (Probst et al., 2001; Alvarez et al., 2005) 11 for the DARPA LORELEI which contains pairs of sentences in a low-resource language and English. 3.5 Encoding Linguistic Features We merged the linguistic resources collected above into three types of features: (1) name gazetteers; (2) list of suffixes and contextual words (e.g., titles) that indicate names; and (3) list of words that indicate non-names (e.g., time expressions). Ultimately we obtained 30 explicit linguistic feature categories. Table 5 shows the statistics of the encoded features. For each token wi in a sentence, we check whether wi , its previous token wi−1 and its next tok"
I17-1037,P03-2031,0,0.0451114,"Missing"
I17-1037,C10-3010,0,0.0268383,"Missing"
I17-1037,W13-2322,1,0.738035,"2015) for linking: (1) Popularity: we prefer popular entities in the KB; (2) Coherence: we link a pair of a foreign name and its English translation simultaneously and favor their candidate entities that are also strongly connected in the KB through a direct cross-lingual page link, a common neighbor, or sharing similar properties. After linking, we assign an entity type to each pair based on their properties in the KB (e.g., an entity with a birthdate and a death-date is likely to be a person). The typing component is a Maximum Entropy model learned from the Abstract Meaning Representation (Banarescu et al., 2013) corpus that includes both entity type and Wikipedia link for each entity mention, using KB properties as features. 3.4 Language Hausa Turkish Uzbek 9 PER 1,174 2,819 1,771 LOC 5,123 7,271 5,331 ORG 199 262 103 Title Non-Name Suffix 42 231 178 391 411 271 21 181 209 Table 5: Name Related List Statistics (# of entries). related words and phrases. For each language, we first extracted 2, 000 to 3, 000 parallel sentence/phrase pairs. Then we ran GIZA++ over these pairs and combined structure rules from WALS to obtain word translation pairs. We also extracted translations of the following English"
I17-1037,P16-2011,1,0.90027,"Missing"
I17-1037,P09-1113,0,0.155476,"Missing"
I17-1037,kamholz-etal-2014-panlex,0,0.0231862,"Missing"
I17-1037,P15-2060,0,0.0610169,"Missing"
I17-1037,P12-1073,0,0.122701,"Missing"
I17-1037,W15-1506,0,0.0387008,"Missing"
I17-1037,N16-1030,0,0.10763,"Missing"
I17-1037,N16-1034,0,0.0528109,"Missing"
I17-1037,U08-1016,0,0.0884468,"Missing"
I17-1037,C16-1095,0,0.0204274,"eature design. Our work argues that data-driven implicit knowledge like word embeddings cannot cover all linguistic phenomena in low-resource settings. We propose to embrace the readily available universal resources for many languages, and proved this process of making them actionable is not costly and does not require a system developer to “know” the language. Many more non-traditional linguistic resources remain to explore in the future, including Lexvo (de Melo, 2015), Multilingual Entity Taxonomy (de Melo and Weikum, 2010), EZGlot, URIEL knowledge Some recent studies (Zhang et al., 2016a; Littell et al., 2016a; Tsai et al., 2016; Pan et al., 2017) under the DARPA LORELEI program focused on name tagging for low-resource languages. Most noise tolerant supervised learning algorithms (Bylander, 1994; Dredze et al., 2008; Crammer et al., 2009; Kalapanidas et al., 2003; Scott et al., 2013) have been applied for improving image classifi1 cation (Mnih and Hinton, 2012; Natarajan et al., 2013; Sukhbaatar et al., 2014; Xiao et al., 2015). Coupling our idea with these algorithms is also likely to yield further improvement. 369 base (Littell et al., 2016b), travel phrase books and yellow phone books. We will"
I17-1037,J03-1002,0,0.00957413,"Missing"
I17-1037,C16-1123,0,0.0386226,"Missing"
I17-1037,P15-2047,1,0.903517,"Missing"
I17-1037,P15-2034,0,0.0143569,"stical NLP research, because they were not specifically designed for NLP purpose at the first place and they are often far from complete. Thus they are not immediately actionable - converted into features, rules or patterns for a target NLP application. In this paper we design various methods to convert them into machine readable features for a new DNN architecture. Very little work has used non-traditional resources mentioned in this paper for practical downstream NLP applications. Limited work only used them for resource building (e.g., (Sarma et al., 2012)) or studying word order typology (Ostling, 2015). To the best of our knowledge, our work is the first to encode them as actionable knowledge for IE. We aim to answer the following research questions: How to effectively acquire linguistic knowledge from non-traditional resources, and represent them for computational models? How much further gain can be obtained in addition to traditional resources? 2 2.1 Languages Hausa Turkish Uzbek # of Documents # of Names # of Sentences Train 137 128 127 Test 100 100 100 Train 3,414 2,341 3,577 Test 1,320 2,173 3,137 Train Test 3,156 1,130 1,973 2,119 3,588 3,037 Table 1: Data Statistics. in the entire s"
I17-1037,P14-5010,0,0.00635514,"Missing"
I17-1037,N15-1119,1,0.847033,"titude, longitude, feature class, feature code, country code, administrative code, population, elevation and time zone. JRC Names. Finally we include the JRC Names (Steinberger et al., 20011), a large list of person and organization names (about 205,000 entries) in over 20 different scripts. Some entries include additional information such as frequency, title and date ranges. Grounding to KB and Typing. For names that we are able to acquire English translations, we further ground (“wikify”) them to an external knowledge base (KB, DBpedia in our work) if they are linkable. We use two measures (Pan et al., 2015) for linking: (1) Popularity: we prefer popular entities in the KB; (2) Coherence: we link a pair of a foreign name and its English translation simultaneously and favor their candidate entities that are also strongly connected in the KB through a direct cross-lingual page link, a common neighbor, or sharing similar properties. After linking, we assign an entity type to each pair based on their properties in the KB (e.g., an entity with a birthdate and a death-date is likely to be a person). The typing component is a Maximum Entropy model learned from the Abstract Meaning Representation (Banare"
I17-1037,P17-1178,1,0.840946,"riven implicit knowledge like word embeddings cannot cover all linguistic phenomena in low-resource settings. We propose to embrace the readily available universal resources for many languages, and proved this process of making them actionable is not costly and does not require a system developer to “know” the language. Many more non-traditional linguistic resources remain to explore in the future, including Lexvo (de Melo, 2015), Multilingual Entity Taxonomy (de Melo and Weikum, 2010), EZGlot, URIEL knowledge Some recent studies (Zhang et al., 2016a; Littell et al., 2016a; Tsai et al., 2016; Pan et al., 2017) under the DARPA LORELEI program focused on name tagging for low-resource languages. Most noise tolerant supervised learning algorithms (Bylander, 1994; Dredze et al., 2008; Crammer et al., 2009; Kalapanidas et al., 2003; Scott et al., 2013) have been applied for improving image classifi1 cation (Mnih and Hinton, 2012; Natarajan et al., 2013; Sukhbaatar et al., 2014; Xiao et al., 2015). Coupling our idea with these algorithms is also likely to yield further improvement. 369 base (Littell et al., 2016b), travel phrase books and yellow phone books. We will also investigate whether these linguist"
I17-1037,K16-1022,0,0.0417752,"argues that data-driven implicit knowledge like word embeddings cannot cover all linguistic phenomena in low-resource settings. We propose to embrace the readily available universal resources for many languages, and proved this process of making them actionable is not costly and does not require a system developer to “know” the language. Many more non-traditional linguistic resources remain to explore in the future, including Lexvo (de Melo, 2015), Multilingual Entity Taxonomy (de Melo and Weikum, 2010), EZGlot, URIEL knowledge Some recent studies (Zhang et al., 2016a; Littell et al., 2016a; Tsai et al., 2016; Pan et al., 2017) under the DARPA LORELEI program focused on name tagging for low-resource languages. Most noise tolerant supervised learning algorithms (Bylander, 1994; Dredze et al., 2008; Crammer et al., 2009; Kalapanidas et al., 2003; Scott et al., 2013) have been applied for improving image classifi1 cation (Mnih and Hinton, 2012; Natarajan et al., 2013; Sukhbaatar et al., 2014; Xiao et al., 2015). Coupling our idea with these algorithms is also likely to yield further improvement. 369 base (Littell et al., 2016b), travel phrase books and yellow phone books. We will also investigate whe"
I17-1037,P13-1106,0,0.2057,"Missing"
I17-1037,2001.mtsummit-road.7,0,0.034043,"ase, location affixes, title, nationalities, topical keywords, organization suffixes, temporal words, locations and people, and stop words which are unlikely to be names. Elicitation Corpus. An elicitation corpus is a controlled corpus translated by a bilingual consultant in order to produce high quality word aligned sentence pairs. During the elicitation process, the user will translate a subset of these sentences that is dynamically determined to be sufficient for learning the desired grammar rules. We extracted word and phrase translation pairs from the Elicitation corpus developed by CMU (Probst et al., 2001; Alvarez et al., 2005) 11 for the DARPA LORELEI which contains pairs of sentences in a low-resource language and English. 3.5 Encoding Linguistic Features We merged the linguistic resources collected above into three types of features: (1) name gazetteers; (2) list of suffixes and contextual words (e.g., titles) that indicate names; and (3) list of words that indicate non-names (e.g., time expressions). Ultimately we obtained 30 explicit linguistic feature categories. Table 5 shows the statistics of the encoded features. For each token wi in a sentence, we check whether wi , its previous toke"
I17-1037,Q14-1005,0,0.0590155,"Missing"
I17-1037,C12-2095,0,0.0450664,"Missing"
I17-1037,C16-1080,0,0.055237,"Missing"
I17-1037,U09-1004,0,0.0672211,"Missing"
I17-1037,D16-1007,0,0.0239341,"Missing"
I17-1037,W12-5113,0,0.0137604,"rces have been largely ignored by the mainstream statistical NLP research, because they were not specifically designed for NLP purpose at the first place and they are often far from complete. Thus they are not immediately actionable - converted into features, rules or patterns for a target NLP application. In this paper we design various methods to convert them into machine readable features for a new DNN architecture. Very little work has used non-traditional resources mentioned in this paper for practical downstream NLP applications. Limited work only used them for resource building (e.g., (Sarma et al., 2012)) or studying word order typology (Ostling, 2015). To the best of our knowledge, our work is the first to encode them as actionable knowledge for IE. We aim to answer the following research questions: How to effectively acquire linguistic knowledge from non-traditional resources, and represent them for computational models? How much further gain can be obtained in addition to traditional resources? 2 2.1 Languages Hausa Turkish Uzbek # of Documents # of Names # of Sentences Train 137 128 127 Test 100 100 100 Train 3,414 2,341 3,577 Test 1,320 2,173 3,137 Train Test 3,156 1,130 1,973 2,119 3,58"
I17-1037,C14-1220,0,0.0708708,"Missing"
I17-1037,N16-1029,1,0.884103,"challenge, especially because we don’t have enough resources and tools to perform a deep understanding of the contexts. For example, when a journal name “New England” appears in Hausa texts, all of its mentions are mistakenly labeled as location instead of organization, because the dominant type label of “New England” is location in all of our resources. Uzbek 64.1 67.4 67.2 68.4 Table 6: Feature Integration Methods Comparison. We compare the following models: a baseline model that uses only character and word embedding features, a model adding traditional linguistic features as described in (Zhang et al., 2016a), and a model further adding non-traditional linguistic features using the third integration method. Figure 4 presents the results. Clearly models trained with linguistic features substantially outperform the baseline models on all noise levels for all languages. As the noise level increases, the performance of the baseline model drops drastically while the model trained with linguistic features successfully curbs the downward trend and forms a relatively flat curve at last. Adding non-traditional linguistic features provides further gains in almost all settings. Notably for Turkish, adding"
I17-1037,E17-1119,0,0.026404,"e 5 Related Work The major novel contribution of this paper is to systematically explore many non-traditional linguistic resources which have been largely neglected by the mainstream NLP community. Some previous efforts used WALS to study the typological relations across languages (Rama and Prasanth, 2012; O’Horan et al., 2016; Yamauchi and Murawaki, 2016) but very little work used it for practical NLP applications. Most DNN methods solely relied on character embeddings and word embeddings as features for name tagging (e.g., (Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016)). (Shimaoka et al., 2017) used hand-crafted features to improve the performance of DNN on fine-grained entity typing. (Chiu and Nichols, 2016) attempted to incorporate gazetteers as ex368 Hausa 72.2 65 64 60 65.7 60 53.8 56 45.8 49 0 9.5 19.9 32.8 66 55 50 39.5 35 59 64.1 57.8 55 51 48 43.3 44 0 12.7 19.9 27.1 39.9 40 0 8.4 Noise Level Noise Level Embedding Features 63 60.0 65.9 40 44.7 Uzbek 68.4 70 45 53 45 Turkish 74.3 70 68 F-score F-score 71 75 F-score 75 Embedding+Traditional Linguistic Features 18.7 30 40.06 Noise Level Embedding+Traditional+Non-traditional Linguistic Features Figure 4: Name Tagging Performance"
I17-1037,C16-1045,1,0.922682,"challenge, especially because we don’t have enough resources and tools to perform a deep understanding of the contexts. For example, when a journal name “New England” appears in Hausa texts, all of its mentions are mistakenly labeled as location instead of organization, because the dominant type label of “New England” is location in all of our resources. Uzbek 64.1 67.4 67.2 68.4 Table 6: Feature Integration Methods Comparison. We compare the following models: a baseline model that uses only character and word embedding features, a model adding traditional linguistic features as described in (Zhang et al., 2016a), and a model further adding non-traditional linguistic features using the third integration method. Figure 4 presents the results. Clearly models trained with linguistic features substantially outperform the baseline models on all noise levels for all languages. As the noise level increases, the performance of the baseline model drops drastically while the model trained with linguistic features successfully curbs the downward trend and forms a relatively flat curve at last. Adding non-traditional linguistic features provides further gains in almost all settings. Notably for Turkish, adding"
I17-1086,D16-1236,0,0.023933,"Missing"
I17-1086,P15-1034,0,0.627631,"Natural Language Processing, pages 854–864, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Framework overview. spond to the same relation type. Besides, overlyspecific or implicit relation phrases are incapable of providing adequate information for downstream applications. For example, the relation between “Patricia” and “Gary Cooper” cannot be clearly expressed by a set of words in the following sentence E2. Therefore, previous studies heavily rely on resources such as patterns (Soderland et al., 2013), training data (Weston et al., 2013), or distantly-labeled corpora (Angeli et al., 2015b) to map open RE triples to a known relation schema. E2 “Patricia later described her relation with Gary Cooper as one of the most beautiful things that ever happed to her in her life.” Compared with a small number of predefined relation types such as those defined in Automatic Content Extraction (ACE) 1 , the relation schema in a large-scale Knowledge Base (KB) such as DBpedia (Auer et al., 2007) covers a much wider range of informative relations along with their type signatures. Considering the open-domain nature shared by open RE and a large-scale KB, we propose an unsupervised grounding m"
I17-1086,P08-1004,0,0.279049,"Missing"
I17-1086,D16-1006,0,0.142422,"cally, we assume that a relational triple is important if there is a relatively short random walk-based distance between two relatively important arguments, measured against the entire dependency tree of a given sentence. For each argument pair, we apply an effective randomwalk based method to assign weights to context words in the sentence (Section 2). How to assign a meaningful relation type name to a relational triple is also a primary challenge for open RE. Previous methods use relevant context words in the associated sentence as relation phrases (type names) (Del Corro and Gemulla, 2013; Bhutani et al., 2016). However, there is still no generally accepted guideline for relation phrase extraction. Multiple relation phrases can correPrevious open Relation Extraction (open RE) approaches mainly rely on linguistic patterns and constraints to extract important relational triples from large-scale corpora. However, they lack of abilities to cover diverse relation expressions or measure the relative importance of candidate triples within a sentence. It is also challenging to name the relation type of a relational triple merely based on context words, which could limit the usefulness of open RE in downstre"
I17-1086,W12-4503,0,0.0207327,"Missing"
I17-1086,Q15-1038,0,0.0283352,"Missing"
I17-1086,P14-5010,0,0.00333432,"emind that the order of arguments in the candidate triple has been introduced in Section 2.2. 4 4.1 Slot Types founder keyPeople education workInstitution birthDate org:founded by org:top members employees per:schools attended per:employee or member of per:date of birth Table 3: Example Mappings from DBpedia relations to slot types. Experiments Knowledge Base and Word Embeddings We ignore all the slot types which require nominal phrases as fillers (e.g., per:cause of death) and slot types per/org:alternate names which depend on cross-document coreference resolution. We apply Stanford CoreNLP (Manning et al., 2014) for English part-of-speech tagging, name tagging, time expression extraction, dependency parsing, and coreference resolution. We use the official Slot Filling evaluation scoring metrics: Precision (P), Recall (R), and F-measure (F1 ). As shown in Table 4, our method outperforms the KBP2013 SF submission from the University of Washington (Soderland et al., 2013) which applies Open IE V4.0, which is an extension of SRL-based IE (Christensen et al., 2011) and noun We use the April 2016 dump of DBpedia as our KB which contains 2, 060 relation types and 30, 024, 093 relation triples in total. We u"
I17-1086,D12-1048,0,0.360457,"Missing"
I17-1086,N13-1008,0,0.423465,"Automatic Content Extraction (ACE) 1 , the relation schema in a large-scale Knowledge Base (KB) such as DBpedia (Auer et al., 2007) covers a much wider range of informative relations along with their type signatures. Considering the open-domain nature shared by open RE and a large-scale KB, we propose an unsupervised grounding method to name the relation type between two arguments as either a KB relation or NONE, by leveraging KB triples and weighted context information associated with each argument pair based on pre-trained word embeddings (Section 3). Compared with previous methods (e.g., (Riedel et al., 2013; Weston et al., 2013)), we regard intra-sentence context words as intermediate results for the subsequent grounding process, and we do not require any aligned training corpora or relation phrases for KB triples. The proposed framework is illustrated in Figure 1. To the best of our knowledge, this is the first open RE method which exploits the global structure of a dependency tree to extract salient relational triples. This is also the first unsupervised relation grounding method to name relation 1 types for open RE based on KB triples and intrasentence context information. Experiments on the"
I17-1086,N15-1118,0,0.0592061,"Missing"
I17-1086,D12-1094,0,0.128741,"raints. 5 5.1 Related Work Open Information Extraction Lexical or syntactic features and patterns have been widely used to extract relational triples (Suchanek et al., 2009; Poon and Domingos, 2009; Wu and Weld, 2010; Nakashole et al., 2011; Fader et al., 2011; Nakashole et al., 2012; Mausam et al., 2012; Bovi et al., 2015; Angeli et al., 2015b; Grycner and Weikum, 2016). Our work explores the global structure of a dependency tree to identify salient triples within a sentence. Some open IE approaches have the capability to extract relations between concepts or phrases (Kok and Domingos, 2008; Min et al., 2012; Del Corro and Gemulla, 2013). Currently we focus on relations between two entities. Given the SF schema, Soderland et al. (2013) manually design rules to map relational triples to slot types within hours. Researchers also use distantly labeled corpora to compute the PMI2 value between open IE and SF relation pairs (Angeli et al., 2015b). Instead, we propose a novel grounding approach which facilitates building a mapping table between KB relations and slot types. We do not compare with RE methods specifically designed for SF (Sun et al., 2011; Li et al., 2012; Angeli et al., 2015a) since thes"
I17-1086,W11-0148,0,0.0137174,"we ground each relational triple to a KB relation or assign NONE (Section 3.2). (4) Table 2: Importance score of each entity in E1. 2.4 I(i) × I(j) c2ij Combination and Filtering 3.1 Given the average commute time cij between nodes vi and vj (Section 2.2) and their relative importance scores I(i) and I(j) in G (Section 2.3), we will discuss how to combine them and generate the final score which can be used to measure the relation strength between two nodes. Intuitively, there exists a strong relation when there is a shorter distance between two relatively important nodes. Previous approaches (Spagnola and Lagoze, 2011; Guo et al., 2011) consider the distance between two nodes and the influence of each node Context Word Selection and Weighting In this section, we introduce how to extract informative context words and their associated weights given an argument pair (vi , vj ) in a sentence based on the average commute time matrix C introduced in Section 2.2. Previous work (Yu and Ji, 2016) regards this problem as finding important nodes in G relative to given arguments. However, they need to run the algorithm repeatedly to analyze the same graph for each argument pair. Here we discuss an efficient method to"
I17-1086,P09-1113,0,0.200962,"urs. Researchers also use distantly labeled corpora to compute the PMI2 value between open IE and SF relation pairs (Angeli et al., 2015b). Instead, we propose a novel grounding approach which facilitates building a mapping table between KB relations and slot types. We do not compare with RE methods specifically designed for SF (Sun et al., 2011; Li et al., 2012; Angeli et al., 2015a) since these methods actively search for candidate fillers of the given queries Relation Grounding Besides textual features, large-scale knowledge bases are widely used for distant supervised relation extraction (Mintz et al., 2009; Riedel et al., 2010) to deal with the challenges caused by insufficient training data. Weston et al. (2013) combine two relation representations trained from KB triples and context words independently for relation extraction. Recent studies such as (Toutanova et al., 2015) train relation representations of KB and textual relations jointly. Another kind of representations combining matrix factorization (Riedel et al., 2013) with first-order logic information is learned by Rockt¨aschel et al. (2015). Compared with these previous efforts, our unsupervised grounding method does not need the alig"
I17-1086,D16-1252,0,0.0438428,"submission from the University of Washington (Soderland et al., 2013) which applies Open IE V4.0, which is an extension of SRL-based IE (Christensen et al., 2011) and noun We use the April 2016 dump of DBpedia as our KB which contains 2, 060 relation types and 30, 024, 093 relation triples in total. We use the 300-dimensional GloVe vectors (Pennington et al., 2014) pretrained on 6 billion tokens from the English Gigaword Fifth Edition and a 2014 Wikipedia dump. 4.2 DBpedia Relations Evaluation based on Slot Filling There are several benchmarks developed for open RE (e.g., (Fader et al., 2011; Stanovsky and Dagan, 2016)). However, we mainly focus on relations between entities and therefore we cannot directly compare with state-of-the-art open RE methods on those datasets. To evaluate the effectiveness of our approach, we choose the TACKBP SF (McNamee and Dang, 2009; Ji et al., 3 The resource is publicly available for research purposes at: http://nlp.cs.rpi.edu/data/dbpedia2slot.zip. 859 Method P R F1 UW Official (Soderland et al., 2013) UMass Official (Singh et al., 2013) 69.9 10.6 12.2 19.5 20.8 13.7 [1] KB Tuples Our Approach [2] Relation Names [1]+[2] Joint 17.3 24.3 26.2 21.1 30.9 32.4 19.0 27.2 28.9 Tan"
I17-1086,D12-1104,0,0.14558,"Missing"
I17-1086,W16-1307,0,0.127424,"Missing"
I17-1086,D15-1174,0,0.0504093,"do not compare with RE methods specifically designed for SF (Sun et al., 2011; Li et al., 2012; Angeli et al., 2015a) since these methods actively search for candidate fillers of the given queries Relation Grounding Besides textual features, large-scale knowledge bases are widely used for distant supervised relation extraction (Mintz et al., 2009; Riedel et al., 2010) to deal with the challenges caused by insufficient training data. Weston et al. (2013) combine two relation representations trained from KB triples and context words independently for relation extraction. Recent studies such as (Toutanova et al., 2015) train relation representations of KB and textual relations jointly. Another kind of representations combining matrix factorization (Riedel et al., 2013) with first-order logic information is learned by Rockt¨aschel et al. (2015). Compared with these previous efforts, our unsupervised grounding method does not need the aligned training corpus or relation mentions for KB tuples. Wijaya and Mitchell (2016) introduce an approach to map words to KB relations based on web text, but they only focus on verb phrases. 5.3 Node Importance Computation Graph-based algorithms such as PageRank (Page et al.,"
I17-1086,D14-1162,0,0.0853666,"ame tagging, time expression extraction, dependency parsing, and coreference resolution. We use the official Slot Filling evaluation scoring metrics: Precision (P), Recall (R), and F-measure (F1 ). As shown in Table 4, our method outperforms the KBP2013 SF submission from the University of Washington (Soderland et al., 2013) which applies Open IE V4.0, which is an extension of SRL-based IE (Christensen et al., 2011) and noun We use the April 2016 dump of DBpedia as our KB which contains 2, 060 relation types and 30, 024, 093 relation triples in total. We use the 300-dimensional GloVe vectors (Pennington et al., 2014) pretrained on 6 billion tokens from the English Gigaword Fifth Edition and a 2014 Wikipedia dump. 4.2 DBpedia Relations Evaluation based on Slot Filling There are several benchmarks developed for open RE (e.g., (Fader et al., 2011; Stanovsky and Dagan, 2016)). However, we mainly focus on relations between entities and therefore we cannot directly compare with state-of-the-art open RE methods on those datasets. To evaluate the effectiveness of our approach, we choose the TACKBP SF (McNamee and Dang, 2009; Ji et al., 3 The resource is publicly available for research purposes at: http://nlp.cs.r"
I17-1086,D13-1136,0,0.21317,"gs of the The 8th International Joint Conference on Natural Language Processing, pages 854–864, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Framework overview. spond to the same relation type. Besides, overlyspecific or implicit relation phrases are incapable of providing adequate information for downstream applications. For example, the relation between “Patricia” and “Gary Cooper” cannot be clearly expressed by a set of words in the following sentence E2. Therefore, previous studies heavily rely on resources such as patterns (Soderland et al., 2013), training data (Weston et al., 2013), or distantly-labeled corpora (Angeli et al., 2015b) to map open RE triples to a known relation schema. E2 “Patricia later described her relation with Gary Cooper as one of the most beautiful things that ever happed to her in her life.” Compared with a small number of predefined relation types such as those defined in Automatic Content Extraction (ACE) 1 , the relation schema in a large-scale Knowledge Base (KB) such as DBpedia (Auer et al., 2007) covers a much wider range of informative relations along with their type signatures. Considering the open-domain nature shared by open RE and a lar"
I17-1086,P10-1013,0,0.32592,"Missing"
I17-1086,N13-1107,0,0.264019,"Missing"
I17-1086,P16-1005,1,0.908787,"score which can be used to measure the relation strength between two nodes. Intuitively, there exists a strong relation when there is a shorter distance between two relatively important nodes. Previous approaches (Spagnola and Lagoze, 2011; Guo et al., 2011) consider the distance between two nodes and the influence of each node Context Word Selection and Weighting In this section, we introduce how to extract informative context words and their associated weights given an argument pair (vi , vj ) in a sentence based on the average commute time matrix C introduced in Section 2.2. Previous work (Yu and Ji, 2016) regards this problem as finding important nodes in G relative to given arguments. However, they need to run the algorithm repeatedly to analyze the same graph for each argument pair. Here we discuss an efficient method to extract weighted context words. 857 separate a DBpedia relation type name politicalGroups into {political, groups}. Similarly, we average the vectors of all the words in a relation type name as its embedding el ∈ Rk . Incorporating both implicit semantics from KB tuples and explicit semantics from KB relation names, we represent the relation embedding of each KB relation l a"
I17-1086,W04-3252,0,\N,Missing
I17-1086,D09-1001,0,\N,Missing
I17-1086,D11-1142,0,\N,Missing
ji-etal-2010-annotating,N04-1043,0,\N,Missing
ji-etal-2010-annotating,J93-2004,0,\N,Missing
ji-etal-2010-annotating,P07-1034,0,\N,Missing
ji-etal-2010-annotating,W06-1615,0,\N,Missing
ji-etal-2010-annotating,J05-1004,0,\N,Missing
ji-etal-2010-annotating,N03-4011,0,\N,Missing
ji-etal-2010-annotating,P09-1116,0,\N,Missing
ji-etal-2010-annotating,R09-1032,1,\N,Missing
K18-1009,D14-1159,0,0.0164085,"nces: corpus-level supporting sentences. Figure 5: Comparison of name tagging results between the baseline and our methods. There have been efforts in other areas of information extraction to exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compa"
K18-1009,D08-1073,0,0.0445657,"ia Eagles 37-10 in a showdown of playoff contenders. * D-lvl sentences: document-level supporting sentences. * C-lvl sentences: corpus-level supporting sentences. Figure 5: Comparison of name tagging results between the baseline and our methods. There have been efforts in other areas of information extraction to exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from ext"
K18-1009,I17-2016,0,0.0174878,"al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, our work incorporates both document-level and corpus-level conmorphological analysis can be applied in this case to help fetch supporting sentences. 4 Related Work Name tagging methods based on sequence labeling have been extensively studied rec"
K18-1009,D12-1062,0,0.0135061,"ent-level supporting sentences. * C-lvl sentences: corpus-level supporting sentences. Figure 5: Comparison of name tagging results between the baseline and our methods. There have been efforts in other areas of information extraction to exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the d"
K18-1009,N04-1001,0,0.0348341,"oduce our baseline model. Then, we enhance this baseline model by adding documentlevel and corpus-level contextual information to the prediction process via our document-level and corpus-level attention mechanisms, respectively. We propose to utilize local, document-level, and corpus-level contextual information to improve name tagging. Generally, we follow the one sense per discourse hypothesis introduced by Yarowsky (2003). Some previous name tagging efforts apply this hypothesis to conduct majority voting for multiple mentions with the same name string in a discourse through a cache model (Florian et al., 2004) or post-processing (Hermjakob et al., 2017). However, these rule-based methods require manual tuning of thresholds. Moreover, it’s challenging to explicitly define the scope of discourse. We propose a new neural network framework with global attention to tackle these challenges. Specifically, for each token in a query sentence, we propose to retrieve sentences that contain the same token from the document-level and corpuslevel contexts (e.g., document-level and corpuslevel supporting evidence for “Zywiec” in Figure 1). To utilize this additional information, we propose a model that, first, pr"
K18-1009,D15-1104,0,0.128719,"Missing"
K18-1009,P16-1101,0,0.219305,"outside (O) of a name mention. The tagged names are then classified into predefined entity types. In this paper, we only use the person (PER), organization (ORG), location (LOC), and miscellaneous (MISC) types, which are the predefined types in CoNLL-02 and CoNLL-03 name tagging dataset (Tjong Kim Sang and De Meulder, 2003). Our baseline model has two parts: 1) Encoding the sequence of tokens by incorporating the preceding and following contexts using a bi-directional long short-term memory (BiLSTM) (Graves et al., 2013), so each token is assigned a local contextual embedding. Here, following Ma and Hovy (2016a), we use the concatenation of pre-trained word embeddings and character-level word representations composed by a convolutional neural network (CNN) as input 87 sq and sr in Figure 2) and select the local contextual representations of wij from these sentences as ˜ ij = {h ˜1ij , h ˜2ij , ...} (e.g., h ˜qj supporting evidence, h ˜ ˜rk in Figure 2), where hij and hij are oband h tained with the same Bi-LSTM. Since each representation in the supporting evidence is not equally valuable to the final prediction, we apply an attention mechanism to weight the contextual representations of the support"
K18-1009,P16-1025,1,0.856507,"s beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, our work incorporates both document-level and corpus-level conmorphological analysis can be applied in this case to help fetch supporting sentences. 4 Related Work Name tagging metho"
K18-1009,D18-1023,1,0.729114,"ng dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, our work incorporates both document-level and corpus-level conmorphological analysis can be applied in this case to help fetch supporting sentences. 4 Related Work Name tagging methods based on sequence labeling have been extensively studied recently. Huang et al. (2015) and Lample et"
K18-1009,D12-1080,0,0.0345425,"Missing"
K18-1009,P08-1030,1,0.728498,"of playoff contenders. * D-lvl sentences: document-level supporting sentences. * C-lvl sentences: corpus-level supporting sentences. Figure 5: Comparison of name tagging results between the baseline and our methods. There have been efforts in other areas of information extraction to exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sourc"
K18-1009,P17-1092,0,0.0234636,"des der Gr¨ unen” (Federal Executive of the Greens) indicates “Gr¨ unen” to be a company name. 3.5 By investigating the remaining errors, most of the named entity type inconsistency errors are eliminated, however, a few new errors are introduced due to the model propagating labels from negative instances to positive ones. Figure 5 presents a negative example, where our model, being influenced by the prediction “[B-ORG Indianapolis]” in the supporting sentence, incorrectly predicts “Indianapolis” as ORG in the query sentence. A potential solution is to apply sentence classification (Kim, 2014; Ji and Smith, 2017) to the documents, divide the document into finegrained clusters of sentences, and select supporting sentences within the same cluster. In morphologically rich languages, words may have many variants. When retrieving supporting evidence, our exact query word match criterion misses potentially useful supporting sentences that contain variants of the word. Normalization and Table 3: Performance of our methods versus the baseline and state-of-the-art models. We also compare our approach with a simple rule-based propagation method, where we use token-level majority voting to make labels consistent"
K18-1009,M98-1021,0,0.0690568,"[B-ORG Indianapolis] 1996-12-06 Our model The injury-plagued [B-ORG Indianapolis] [I-ORG Colts] lost another quarterback on Thursday but D-lvl sentence last year's AFC finalists rallied together to shoot down the Philadelphia Eagles 37-10 in a showdown of playoff contenders. * D-lvl sentences: document-level supporting sentences. * C-lvl sentences: corpus-level supporting sentences. Figure 5: Comparison of name tagging results between the baseline and our methods. There have been efforts in other areas of information extraction to exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell"
K18-1009,D14-1181,0,0.00453721,"ndesvorstandes der Gr¨ unen” (Federal Executive of the Greens) indicates “Gr¨ unen” to be a company name. 3.5 By investigating the remaining errors, most of the named entity type inconsistency errors are eliminated, however, a few new errors are introduced due to the model propagating labels from negative instances to positive ones. Figure 5 presents a negative example, where our model, being influenced by the prediction “[B-ORG Indianapolis]” in the supporting sentence, incorrectly predicts “Indianapolis” as ORG in the query sentence. A potential solution is to apply sentence classification (Kim, 2014; Ji and Smith, 2017) to the documents, divide the document into finegrained clusters of sentences, and select supporting sentences within the same cluster. In morphologically rich languages, words may have many variants. When retrieving supporting evidence, our exact query word match criterion misses potentially useful supporting sentences that contain variants of the word. Normalization and Table 3: Performance of our methods versus the baseline and state-of-the-art models. We also compare our approach with a simple rule-based propagation method, where we use token-level majority voting to m"
K18-1009,D16-1261,0,0.014561,"erns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, our work incorporates both document-level and corpus-level conmorphological analysis can be applied in this case to help fetch supporting sentences. 4 Related Work Name tagging methods based on sequence labeling have been extensively studied recently. Huang et al. (2015) and Lample et al. (2016) proposed a neural architecture consisting of a bi-directional long short-term memory network (Bi-LSTM) encoder and a conditional rando"
K18-1009,N16-1030,0,0.828889,"ij to obtain its final representation. This representation is fed to another BiLSTM to further encode the supporting evidence and local contextual features into an unified representation, which is given as input to an affine-CRF layer for label prediction. Table 2: Hyper-parameters. 3.2 3 Experiments 3.1 For word representations, we use 100-dimensional pre-trained word embeddings and 25-dimensional randomly initialized character embeddings. We train word embeddings using the word2vec package.5 English embeddings are trained on the English Giga-word version 4, which is the same corpus used in (Lample et al., 2016). Dutch, Spanish, and German embeddings are trained on corresponding Wikipedia articles (2017-12-20 dumps). Word embeddings are fine-tuned during training. Table 2 shows our hyper-parameters. For each model with an attention, since the BiLSTM encoder must encode the local, documentlevel, and/or corpus-level contexts, we pre-train a Bi-LSTM CRF model for 50 epochs, add our document-level attention and/or corpus-level attention, and then fine-tune the augmented model. Additionally, Reimers and Gurevych (2017) report that neural models produce different results even with same hyper-parameters due"
K18-1009,P17-1161,0,0.0466816,"tention mechanism to weight the contextual representations of the supporting evidence:   ˜kij + be , ekij = v&gt; tanh Wh hij + Wh˜ h   k αij = Softmax ekij , to the Bi-LSTM. 2) Using a Conditional Random Fields (CRFs) output layer to render predictions for each token, which can efficiently capture dependencies among name tags (e.g., “I-LOC” cannot follow “B-ORG”). The Bi-LSTM CRF network is a strong baseline due to its remarkable capability of modeling contextual information and label dependencies. Many recent efforts combine the Bi-LSTM CRF network with language modeling (Liu et al., 2017; Peters et al., 2017, 2018) to boost the name tagging performance. However, they still suffer from the limited contexts within individual sequences. To overcome this limitation, we introduce two attention mechanisms to incorporate document-level and corpus-level supporting evidence. 2.2 where hij is the local contextual representation of ˜kij is the k-th supportword j in sentence si and h ing contextual representation. Wh , Wh˜ and be are learned parameters. We compute the weighted average of the supporting representations by X k ˜k ˜ ij = H αij hij , Document-level Attention Many entity mentions are tagged as mu"
K18-1009,N18-1202,0,0.154714,"Missing"
K18-1009,P13-1008,1,0.834271,"o exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, our work incorporates both document-level and corpus-level conmorphological analysis can be applied in this case to help fetch supporting sentences. 4 Related Wor"
K18-1009,P10-1081,0,0.0313591,"* D-lvl sentences: document-level supporting sentences. * C-lvl sentences: corpus-level supporting sentences. Figure 5: Comparison of name tagging results between the baseline and our methods. There have been efforts in other areas of information extraction to exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents t"
K18-1009,I17-2017,0,0.0393046,"Missing"
K18-1009,W03-0419,0,0.473718,"Missing"
K18-1009,P16-1201,0,0.0160694,"sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, our work incorporates both document-level and corpus-level conmorphological analysis can be applied in this case to help fetch supporting sentences. 4 Related Work Name tagging methods based on sequen"
K18-1009,N16-1033,0,0.0120056,"pporting sentences. Figure 5: Comparison of name tagging results between the baseline and our methods. There have been efforts in other areas of information extraction to exploit features beyond individual sequences. Early attempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, ou"
K18-1009,I17-1037,1,0.774485,"Missing"
K18-1009,N16-1029,1,0.796723,"ttempts (Mikheev et al., 1998; Mikheev, 2000) on MUC-7 name tagging dataset used document centered approaches. A number of approaches explored document-level features (e.g., temporal and co-occurrence patterns) for event extraction (Chambers and Jurafsky, 2008; Ji and Grishman, 2008; Liao and Grishman, 2010; Do et al., 2012; McClosky and Manning, 2012; Berant et al., 2014; Yang and Mitchell, 2016). Other approaches leveraged features from external resources (e.g., Wiktionary or FrameNet) for low resource name tagging and event extraction (Li et al., 2013; Huang et al., 2016; Liu et al., 2016; Zhang et al., 2016; Cotterell and Duh, 2017; Zhang et al., 2017; Huang et al., 2018). Yaghoobzadeh and Sch¨utze (2016) aggregated corpus-level contextual information of each entity to predict its type and Narasimhan et al. (2016) incorporated contexts from external information sources (e.g., the documents that contain the desired information) to resolve ambiguities. Compared with these studies, our work incorporates both document-level and corpus-level conmorphological analysis can be applied in this case to help fetch supporting sentences. 4 Related Work Name tagging methods based on sequence labeling have bee"
L18-1700,P03-1036,0,0.100019,"Missing"
L18-1700,W15-0812,0,0.0244584,"ﯔ ﺷﻴﺎﯞﭘﯩﯔDeng Xiaoping) is often misspelled as  ﺩﯦﯔ ﺷﻴﺎﯞﭘﯩﯔeven including its Wikipedia title; the Wikipedia title of “( ﺟﯘﯕﮕﻮChina”) is also misspelled as ( ﺋﯜﺭﯛﻣﭽﻰ ;ﺟﻮﯕﮕﻮUrumqi) is often misspelled as ﺋﯜﺭﯛﻣﭽﻰ. Up to date there are no effective Uyghur spelling correction techniques available yet. 3..9 Name Definition and Annotation Challenges In the past two decades, many efforts have been made at defining the name tagging task, including Message Understanding Conference (MUC) (Grishman and Sundheim, 1996), Automatic Content Extraction (ACE) 6 , and Entity, Relation and Event (ERE) (Song et al., 2015). However, there are many open issues which may cause confusions for both human annotators and systems. In particular, for low-resource languages like Uyghur, it’s also challenging to train native speakers to follow a long annotation guideline 4424 6 http://www.itl.nist.gov/iad/mig/tests/ace/ Table 5: Long Nested Organizations Nested Organization [ORG [ ﻣﯘﺳﯘﻟﻤﺎﻧﭽﻪ ﻳﯩﻤﻪﻛﻠﯩﻚ ﺳﺎﻧﺎﺋﯩﺘﻰ ﮔﯘﺭﯗﮬﻰ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰPER [ ]ﺋﺎﺭﻣﺎﻥGPE ]]ﺷﯩﻨﺠﺎﯓ [ORG  ﺗﯧﺨﻨﯩﻜﺎ ﺗﻪﺭﻩﻗﻘﯩﻴﺎﺕ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰ- [ ﺑﯩﺌﻮ ﭘﻪﻥPER]ﺋﺎﺑﯩﺪﻩ [GPE ] ]ﺷﯩﻨﺠﺎﯓ [ORG [ORG [ ﺋﯧﻜﻮﻟﻮﮔﯩﻴﻪﺳﻰ ﯞﻩ ﺟﯘﻏﺮﺍﭘﯩﻴﻪ ﺗﻪﺗﻘﯩﻘﺎﺕ ﻣﻪﺭﻛﯩﺰﯨﻨﯩﯔGPE [ ]]ﺷﯩﻨﺠﺎﯓOR"
L18-1700,C96-1079,0,0.684694,"n Uyghur texts in both formal and informal genres are misspelled. For example, the common person name ( ﺩﯨﯔ ﺷﻴﺎﯞﭘﯩﯔDeng Xiaoping) is often misspelled as  ﺩﯦﯔ ﺷﻴﺎﯞﭘﯩﯔeven including its Wikipedia title; the Wikipedia title of “( ﺟﯘﯕﮕﻮChina”) is also misspelled as ( ﺋﯜﺭﯛﻣﭽﻰ ;ﺟﻮﯕﮕﻮUrumqi) is often misspelled as ﺋﯜﺭﯛﻣﭽﻰ. Up to date there are no effective Uyghur spelling correction techniques available yet. 3..9 Name Definition and Annotation Challenges In the past two decades, many efforts have been made at defining the name tagging task, including Message Understanding Conference (MUC) (Grishman and Sundheim, 1996), Automatic Content Extraction (ACE) 6 , and Entity, Relation and Event (ERE) (Song et al., 2015). However, there are many open issues which may cause confusions for both human annotators and systems. In particular, for low-resource languages like Uyghur, it’s also challenging to train native speakers to follow a long annotation guideline 4424 6 http://www.itl.nist.gov/iad/mig/tests/ace/ Table 5: Long Nested Organizations Nested Organization [ORG [ ﻣﯘﺳﯘﻟﻤﺎﻧﭽﻪ ﻳﯩﻤﻪﻛﻠﯩﻚ ﺳﺎﻧﺎﺋﯩﺘﻰ ﮔﯘﺭﯗﮬﻰ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰPER [ ]ﺋﺎﺭﻣﺎﻥGPE ]]ﺷﯩﻨﺠﺎﯓ [ORG  ﺗﯧﺨﻨﯩﻜﺎ ﺗﻪﺭﻩﻗﻘﯩﻴﺎﺕ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰ- [ ﺑﯩﺌﻮ ﭘﻪﻥPER]ﺋﺎﺑﯩﺪ"
L18-1700,K16-1022,0,0.0248593,"rtment, the power supply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing language-independent features and machine learni"
L18-1700,N16-1030,0,0.065419,"alysis because of two reasons: (1) it achieves top performance at NIST LoreHLT2016 Evaluation 3 so it represents state-of-the-art; (2) unlike most previous work, this system has already exploited extensive language-specific features. We briefly describe the system as follows. 2..1 Learning Model This system considers name tagging as a sequence labeling problem, to tag each token in a sentence as the Beginning (B), Inside (I) or Outside (O) of a name mention with a certain type (Person (PER), Organization (ORG), Geo-political Entity (GPE), and Location (LOC)). Following a framework similar to (Lample et al., 2016). The architecture consists of Bi-directional Long Short-Term Memory and Conditional Random Fields (CRFs) network. After processing through the Bi-LSTM networks, each token in a sentence sequence obtains a feature embedding that captures left and 4421 3 https://www.nist.gov/itl/iad/mig/lorehlt16-evaluations right context information, which is then fed into the CRF networks. • If a name includes a URL link, remove the URL. • Label places that don’t have governing organizations as Location(LOC), including all continents, ﺋﻮﺗﺘﯘﺭﺍ ﺷﻪﺭﻕ (Latin: ottura sheriq, English: Middle East), etc. • Label p"
L18-1700,C16-1095,0,0.0238989,"police, the fire department, the power supply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing language-independent features"
L18-1700,N16-1029,1,0.843536,"alth department, the police, the fire department, the power supply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing languag"
L18-1700,N15-1119,1,0.794018,"n in China”) should be tagged as one single organization mention. • Boundary extension: if a name doesn’t include any suffix and its right contextual word is a name designator, then extend the name boundary to include the designator. • Cross-genre propagation: when the types of the same name mention are conflicting between formal genres and informal genres, propagate the types from formal genres to informal genres. • Poster names: Extract all poster names from the original thread structures, and identify all mentions in the posts, posters, and Twitter user names. Apply English entity linking (Pan et al., 2015) to each string after ‘@’ or ‘#’, and if it’s linkable and its type can be inferred based on KB properties, then assign the type; otherwise tag it as PER. 2..2 Pre-processing The system starts with segmenting a document into tokens based on 50 punctuations pulled from Uyghur grammar books ((translated by Anne Lee), 2003; Zakir, 2007; Engesæth et al., 2009). Since Uyghur is a morphologically rich language, a set of name related suffixes is also extracted from grammar books, Wikitionary 4 and WALS 5 , for stemming and feature encoding. 2..3 Features Typical implicit linguistic features including"
L18-1700,P17-1178,1,0.844656,"upply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing language-independent features and machine learning methods which we"
li-etal-2012-linguistic,W11-2213,0,\N,Missing
li-etal-2012-linguistic,mcnamee-etal-2010-evaluation,1,\N,Missing
li-etal-2012-linguistic,W06-0206,1,\N,Missing
li-etal-2012-linguistic,simpson-etal-2010-wikipedia,1,\N,Missing
li-etal-2014-comparison,W04-3230,0,\N,Missing
li-etal-2014-comparison,W06-0116,0,\N,Missing
li-etal-2014-comparison,W03-1718,0,\N,Missing
li-etal-2014-comparison,W03-0428,0,\N,Missing
li-etal-2014-comparison,W08-0336,0,\N,Missing
li-etal-2014-comparison,J05-4005,0,\N,Missing
li-etal-2014-comparison,W10-4126,0,\N,Missing
li-etal-2014-comparison,W06-0123,0,\N,Missing
li-etal-2014-comparison,W06-0129,0,\N,Missing
li-etal-2014-comparison,W06-0115,0,\N,Missing
li-etal-2014-comparison,I08-4013,0,\N,Missing
li-etal-2014-comparison,I08-4010,0,\N,Missing
li-etal-2014-comparison,I08-4017,0,\N,Missing
li-etal-2014-comparison,I05-3027,0,\N,Missing
li-etal-2014-comparison,I05-3017,0,\N,Missing
li-etal-2014-comparison,W04-1118,0,\N,Missing
li-etal-2014-comparison,W04-1109,0,\N,Missing
li-etal-2014-comparison,W06-0132,0,\N,Missing
li-etal-2014-comparison,W06-0110,0,\N,Missing
li-etal-2014-comparison,W06-0122,0,\N,Missing
lin-etal-2010-new,N04-1043,0,\N,Missing
lin-etal-2010-new,sekine-dalwani-2010-ngram,1,\N,Missing
lin-etal-2010-new,C08-3010,1,\N,Missing
lin-etal-2010-new,1999.tc-1.8,0,\N,Missing
lin-etal-2010-new,J93-2004,0,\N,Missing
lin-etal-2010-new,S07-1044,0,\N,Missing
lin-etal-2010-new,N07-2005,1,\N,Missing
lin-etal-2010-new,J92-4003,0,\N,Missing
lin-etal-2010-new,J03-3005,0,\N,Missing
lin-etal-2010-new,P08-1068,0,\N,Missing
lin-etal-2010-new,P03-1059,0,\N,Missing
lin-etal-2010-new,A00-1031,0,\N,Missing
lin-etal-2010-new,P01-1005,0,\N,Missing
lin-etal-2010-new,W05-0603,0,\N,Missing
lin-etal-2010-new,P09-1116,1,\N,Missing
lin-etal-2010-new,Y09-1024,1,\N,Missing
lin-etal-2010-new,U08-1008,0,\N,Missing
lin-etal-2010-new,I05-2018,0,\N,Missing
lin-etal-2010-new,W04-3205,0,\N,Missing
lin-etal-2010-new,J03-3001,0,\N,Missing
lin-etal-2010-new,U07-1008,0,\N,Missing
N09-2053,W06-0901,0,0.383024,"abeling. 2 Trigger Labeling 1 Introduction In this paper we address the event extraction task defined in Automatic Content Extraction (ACE) 1 program. The ACE program defines the following terminology for event extraction task: z Trigger: the word that most clearly expresses an event’s occurrence z Argument: an entity, or a temporal expression or a value that plays a certain role in the event instance z Event mention: a phrase or sentence with a distinguished trigger and participant arguments Some English event extraction systems based on supervised learning have been reported by researchers (Ahn, 2006; Ji and Grishman, 2008). In this paper we developed a modularized Chinese event extraction system. We nicely handled the language specific issue in trigger labeling and explored effective lexical, syntactic and semantic features that were applied in trigger labeling and argument labeling. Tan et al. (2008) addressed the 1 http://www.nist.gov/speech/tests/ace/ We split trigger labeling into two steps: 1) trigger identification: to recognize the event trigger 2) trigger classification: to assign an event type for the trigger. The two strategies we will discuss in trigger labeling (word-based an"
N09-2053,P08-1030,1,0.798227,"Trigger Labeling 1 Introduction In this paper we address the event extraction task defined in Automatic Content Extraction (ACE) 1 program. The ACE program defines the following terminology for event extraction task: z Trigger: the word that most clearly expresses an event’s occurrence z Argument: an entity, or a temporal expression or a value that plays a certain role in the event instance z Event mention: a phrase or sentence with a distinguished trigger and participant arguments Some English event extraction systems based on supervised learning have been reported by researchers (Ahn, 2006; Ji and Grishman, 2008). In this paper we developed a modularized Chinese event extraction system. We nicely handled the language specific issue in trigger labeling and explored effective lexical, syntactic and semantic features that were applied in trigger labeling and argument labeling. Tan et al. (2008) addressed the 1 http://www.nist.gov/speech/tests/ace/ We split trigger labeling into two steps: 1) trigger identification: to recognize the event trigger 2) trigger classification: to assign an event type for the trigger. The two strategies we will discuss in trigger labeling (word-based and character-based) only"
N09-2053,W03-1026,0,0.0294395,"labeling into two steps: 1) trigger identification: to recognize the event trigger 2) trigger classification: to assign an event type for the trigger. The two strategies we will discuss in trigger labeling (word-based and character-based) only differ in the first step. 2.1 A Language-Specific Issue Chinese, and some other languages, e.g., Japanese do not have delimiters between words. Thus, segmentation is usually an indispensible step for further processing, e.g., Part-of-Speech tagging, parsing, etc. However, the segmentation may cause a problem in some tasks, e.g., name entity recognition (Jing et al., 2003) and event trigger identification. For a specific example, “击毙” (shoot and kill) is segmented as a Chinese word. However, there are two triggers in the word, one is “ 击 ”(shoot) with the event type of Attack, and the other is “毙”(kill) with the event type of Die. The trigger may also cross two or more words, e.g., the trigger is “ 公 开 信 ” (public letter) which crosses two words, “公开” (public) and “信”(letter). In the ACE Chinese corpus, 2902 triggers exactly one-to-one match their corresponding words, 209 Proceedings of NAACL HLT 2009: Short Papers, pages 209–212, c Boulder, Colorado, June 2009"
N09-5001,P08-2045,0,0.0222501,"graphical codes in a map (Figure 2). The users can drag the timeline and map to browse the events. In addition, the aggregated event arguments are indexed and allow fast centroid searching. Each argument is also labeled by its global confidence, language sources, and linked to its context sentences and other event chains it is involved. We omit these details in these screenshots. http://projects.ldc.upenn.edu/TDT5/ 3 Figure 1. Temporal Person Tracking Figure 2. Spatial Person Tracking 5 Related Work Recently there has been heightened interest in discovering temporal event chains. For example, Bethard and Martin (2008) applied supervised learning to classify temporal and causal relations simultaneously. Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. In this paper we import these ideas into IE while take into account some major differences. Following the original idea of centering (Grosz et al., 1995) and the approach of centering events involving protagonists (Chambers and Jurafsky, 2008), we introduce a new concept of ‘centroid arguments’ to represent those entities which are involved in all kinds of salient events frequently. We operate cross-document instead o"
N09-5001,P08-1090,0,0.0258337,"the aggregated event arguments are indexed and allow fast centroid searching. Each argument is also labeled by its global confidence, language sources, and linked to its context sentences and other event chains it is involved. We omit these details in these screenshots. http://projects.ldc.upenn.edu/TDT5/ 3 Figure 1. Temporal Person Tracking Figure 2. Spatial Person Tracking 5 Related Work Recently there has been heightened interest in discovering temporal event chains. For example, Bethard and Martin (2008) applied supervised learning to classify temporal and causal relations simultaneously. Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. In this paper we import these ideas into IE while take into account some major differences. Following the original idea of centering (Grosz et al., 1995) and the approach of centering events involving protagonists (Chambers and Jurafsky, 2008), we introduce a new concept of ‘centroid arguments’ to represent those entities which are involved in all kinds of salient events frequently. We operate cross-document instead of within-document, which requires us to resolve more conflicts and ambiguities. In addition, we study the temporal"
N09-5001,J95-2003,0,0.00946381,"e resolution, operate a sentence at a time. The result are large databases containing many unconnected, unranked, redundant (and some erroneous) facts. McNamara (2001) proved that a high-coherence text has fewer conceptual gaps and thus requires fewer inferences and less prior knowledge, rendering the text easier to understand. In our task text coherence is the extent to which the relationships between events in a text can be made explicit. We noted that linking all events in temporal and spatial directions for the entire corpus was not feasible because of the large number of event arguments. Grosz et al. (1995) claimed that certain entities are more central than others and that this property imposed constraints on discourse coherence. Therefore we have developed a system which can extract globally salient and novel arguments as centroid arguments, and link all events involving each centroid argument on a time line and on a geographical map. Beyond extracting isolated facts from individual sentences, we provide coherent event chains so that the users can save time in connecting relevant events and conducting reasoning, such as tracking a person’s movement activities and an organization’s personnel ch"
N09-5001,P08-1030,1,0.817436,"stem and the detailed approaches to extract event chains. Section 3 then presents the experimental results compared to traditional IE. Section 4 demonstrates the system output. Section 5 compares our approach with related work and Section 6 then concludes the paper and sketches our future work. 2 System Overview In this section we will present the overall procedure of our system. 1 Proceedings of NAACL HLT 2009: Demonstrations, pages 1–4, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2.1 • Within-document IE We first apply a state-of-the-art English IE system (Ji and Grishman, 2008) to extract events from each single document. The IE system includes entity extraction, time expression extraction and normalization, relation extraction and event extraction. Entities include persons, locations, organizations, facilities, vehicles and weapons; Events include the 33 distinct event types defined in Automatic Content Extraction (ACE05)1. The event extraction system combines pattern matching with statistical models. For every event instance in the ACE training corpus, patterns are constructed based on the sequences of constituent heads separating the trigger and arguments. In add"
N10-1036,N03-4004,0,0.0252912,"ents as ‘centroid entities’; and then for each centroid entity, link and order the events centered around it on a time line and associate them to a geographical map. The event chains are presented in a user-friendly graphical interface (Ji and Chen, 2009). Both systems link the events back to their context documents. 3 3.1 Evaluation Methods Study Execution Our measurement challenge is to assess how IE techniques affect users’ abilities to perform realworld tasks. We followed the summary writing task described in the Integrated Feasibility Experiment of the DARPA TIDES program (Colbath and Kubala, 2003) and the daily task conducted by intelligence analysts (Bodnar, 2003). Each task in our evaluation is based on writing a summary of ACE-type events involving a specific centroid entity, using one of three levels of support: • Level (I): Read the news articles, with assistance of keyword based sentence search; • Level (II): (I) + with assistance from singledocument IE results; • Level (III): (I) + with assistance from crossdocument IE results. The summary writing task for each entity using any level should be finished in 10 minutes. The users can choose to trust the IE results to create new sen"
N10-1036,N09-5001,1,0.838892,"form. The cross-document IE system can identify important person entities which are frequently in1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 285 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285–288, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics volved in events as ‘centroid entities’; and then for each centroid entity, link and order the events centered around it on a time line and associate them to a geographical map. The event chains are presented in a user-friendly graphical interface (Ji and Chen, 2009). Both systems link the events back to their context documents. 3 3.1 Evaluation Methods Study Execution Our measurement challenge is to assess how IE techniques affect users’ abilities to perform realworld tasks. We followed the summary writing task described in the Integrated Feasibility Experiment of the DARPA TIDES program (Colbath and Kubala, 2003) and the daily task conducted by intelligence analysts (Bodnar, 2003). Each task in our evaluation is based on writing a summary of ACE-type events involving a specific centroid entity, using one of three levels of support: • Level (I): Read th"
N10-1036,R09-1032,1,\N,Missing
N15-1119,C12-1028,1,0.775921,"Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguyen et al., 2012; Huang et al., 2014), socially related (Cassidy et al., 2012; Huang et al., 3 The mapping from AMR entity types to these three main types is at: amr.isi.edu/lib/ne-type-sc.txt 1131 2014), dependent (Ling et al., 2014), or a combination of these through meta-paths (Huang et al., 2014). These measures can collect more precise collaborators but suffer from low coverage of predefined information templates and the unsatisfying quality of state-of-the-art coreference resolution, relation a"
N15-1119,D11-1071,1,0.898714,"coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 201"
N15-1119,D13-1184,0,0.069909,"; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguyen et al., 2012; Huang et al., 2014), socially related (Cassidy et al., 2012; Huang et al., 3 The mapping from AMR entity types to these three main types is at: amr.isi.edu/lib/ne-type-sc.txt 1131 2014), dependent (Ling et al., 2014), or a combination of these through meta-paths (Huang et al., 2014). These measures can collect more precise collaborators but suffer from low coverage of predefined information templates and the unsatisfying quality of state-of-the-art coreference resolution, relation and event extraction. In this paper, we demonstrate that AMR is an appropria"
N15-1119,P14-1134,0,0.0471633,"Missing"
N15-1119,I11-1113,0,0.0155229,"entity targets - one target entity for each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al.,"
N15-1119,P11-1095,0,0.00935942,"ne target entity for each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relat"
N15-1119,N06-2015,0,0.0411608,"ailed tests and the deployment prospects are uncertain.”, AMR labels “Yuri dolgoruky” as a product instead of a person. We manually mapped AMR entity types to equivalent DBpedia types to inform type matching restrictions 5 . However, to make our context comparison algorithm less dependent on the quality of this mapping, and on automatic AMR name type assignment, we add a mention’s type to its collaborators 6 . In future work we plan to investigate the effects of different type matching techniques, varying degrees of strictness. 3.2 Semantic Roles AMR defines core roles based on the OntoNotes (Hovy et al., 2006) semantic role layer. Each predicate is associated with a sense and frame description. If a target entity mention m and a context entity mention n are both playing core roles for the same predicate, we consider n as a collaborator of m. Consider the following post: “Did Palin apologize to Giffords? He needs to conduct a beer summit between Palin and NBC.”. We add “Giffords” and “NBC” as collaborators of “Palin”, because they play core roles in both the “apologize-01” and “meet-03” events. AMR defines new core semantic roles which did not exist in PropBank (Palmer et al., 2005), NomBank (Meyers"
N15-1119,P14-1036,1,0.792998,"z, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguyen et al., 2012; Huang et al., 2014), socially related (Cassidy et al., 2012; Huang et al., 3 The mapping from AMR entity types to these three main types is at: amr.isi.edu/lib/ne-type-sc.txt 1131 2014), dependent (Ling et al., 2014), or a combination of these through meta-paths (Huang et al., 2014). These measures can collect more precise collaborators but suffer from low coverage of predefined information templates and the unsatisfying quality of state-of-the-art coreference resolution, relation and event extraction. In this paper, we demonstrate that AMR is an appropriate and elegant way to acquire, select, represent and orga"
N15-1119,R09-1032,1,0.796787,"nce, to the correct target entity Indonesian Agency for Meteorology, Climatology and Geophysics, whose headquarters are listed as Jakarta in the KB: “It keeps on shaking. Jakarta BMKG spokesman Mujuhidin said”. Here, “Jakarta” is added as a collaborator of “BMKG” since AMR labels it as the location of the organization, which facilitates the correct link because in DBpedia Jakarta is listed as its headquarter. Authors often assume that readers will infer implicit temporal information about events. In fact, half of the events extracted by information extraction (IE) systems lack time arguments (Ji et al., 2009). Therefore if an AMR parse includes no time information, we use the document creation time as an additional collaborator for mention in question. For example, knowing the document creation time “2005-06-05” can help us link “Hsiung Feng” in the following sentence “The BBC reported that Taiwan has successfully test fired the Hsiung Feng, its first cruise missile.” to Hsiung Feng IIE, which was deployed in 2005. Similarly, we include document creation location as a global collaborator. 3.4 Coreference For linking purposes, we treat a coreferential chain of mentions as a single “mention”. In doi"
N15-1119,D11-1011,0,0.010915,"ow preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set (Cheng and Roth, 2013), coreferential (Nguy"
N15-1119,W04-2705,0,0.0509936,"2006) semantic role layer. Each predicate is associated with a sense and frame description. If a target entity mention m and a context entity mention n are both playing core roles for the same predicate, we consider n as a collaborator of m. Consider the following post: “Did Palin apologize to Giffords? He needs to conduct a beer summit between Palin and NBC.”. We add “Giffords” and “NBC” as collaborators of “Palin”, because they play core roles in both the “apologize-01” and “meet-03” events. AMR defines new core semantic roles which did not exist in PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), or Ontonotes (Hovy et al., 2006). Intuitively, the following special roles should provide discriminative collaborators: • The ARG2 role of the have-org-role-91 frame indicates the title held by an entity (ARG0), such as President and Governor, within a particular organization (ARG1). • ARG2 and ARG3 of have-rel-role-91 are used to describe two related entities of the same type, such as family members. AMR defines a rich set of general semantic relations through non-core semantic roles. We choose the following subset of non-core roles to provide collaborators for entity mentions: domain, mod,"
N15-1119,J05-1004,0,0.0263293,"on the OntoNotes (Hovy et al., 2006) semantic role layer. Each predicate is associated with a sense and frame description. If a target entity mention m and a context entity mention n are both playing core roles for the same predicate, we consider n as a collaborator of m. Consider the following post: “Did Palin apologize to Giffords? He needs to conduct a beer summit between Palin and NBC.”. We add “Giffords” and “NBC” as collaborators of “Palin”, because they play core roles in both the “apologize-01” and “meet-03” events. AMR defines new core semantic roles which did not exist in PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), or Ontonotes (Hovy et al., 2006). Intuitively, the following special roles should provide discriminative collaborators: • The ARG2 role of the have-org-role-91 frame indicates the title held by an entity (ARG0), such as President and Governor, within a particular organization (ARG1). • ARG2 and ARG3 of have-rel-role-91 are used to describe two related entities of the same type, such as family members. AMR defines a rich set of general semantic relations through non-core semantic roles. We choose the following subset of non-core roles to provide collaborators fo"
N15-1119,D09-1025,0,0.0156305,"tions are linked simultaneously by choosing an “optimal” or maximally “coherent” set of named entity targets - one target entity for each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosi"
N15-1119,P11-1138,0,0.0329246,"r each mention in the coherent set. We show preliminary results suggesting that AMR is effective for the partitioning of all mentions in a document into coherent sets for collective linking. We evaluate our approach using both human and automatic AMR annotation, limiting target named entity types to person (PER), organization (ORG), and geo-political entities (GPE) 3 . 2 Related Work In most recent collective inference methods for EL (e.g., (Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Dalton and Dietz, 2013)), the target entity mention’s “collaborators” may simply include all mentions which co-occur in the same discourse (sentence, paragraph or document) (Ratinov et al., 2011; Nguyen et al., 2012). But this approach usually introduces many irrelevant mentions, and it’s very difficult to automatically determine the scope of discourse. In contrast, some recent work exploited more restricted measures by only choosing those mentions which are topically related (Cassidy et al., 2012; Xu et al., 2012), bear a relation from a fixed set ("
N15-1119,W13-2322,1,\N,Missing
N15-1126,P11-1051,0,0.0155573,"Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time. Acknowledgement This work was supported by the U.S. DARPA Award No. FA8750-13-2-0045 in the Deep Exploration a"
N15-1126,N12-1009,0,0.0328479,"Missing"
N15-1126,W14-2907,0,0.0253086,"Missing"
N15-1126,P14-5010,0,0.00539561,"types is regarded as the right boundary. The rule-based scoping result of the walk-through example is as follows: Paul Francis Conrad and his twin [<brother>, James, were] [<born> in Cedar Rapids, Iowa, on June 27, 1924,] [<sons> of Robert H. Conrad and Florence Lawler Conrad.] 2.2.2 Supervised Classification Alternatively we regard scope identification as a classification task. For each detected trigger, scope identification is performed as a binary classification of each token in the sentence as to whether it is within or outside of a trigger’s scope. We apply the Stanford CoreNLP toolkit (Manning et al., 2014) to annotate part-of-speech tags and names in each document. We design the following features to train a classifier. • Position: The feature takes value 1 if the word appears before the trigger, and 0 otherwise. • Distance: The distance (in words) between the word and the trigger. • POS: POS tags of the word and the trigger. • Name Entity: The name entity type of the word. • Interrupt: The feature takes value 1 if there is a verb or a trigger with other fact type between the trigger and the word, and 0 otherwise. Verbs and triggers with other fact types can effectively change the current topic"
N15-1126,W09-1304,0,0.0189502,"upon external knowledge bases and it is timeconsuming to manually write or edit patterns (Sun et al., 2011; Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate a trigger list for a “surprise” new fact type within limited time. Ackn"
N15-1126,W08-0606,0,0.0120013,"ly expensive: Distant Supervision (Surdeanu et al., 2010) relies upon external knowledge bases and it is timeconsuming to manually write or edit patterns (Sun et al., 2011; Li et al., 2012). The main impact of our trigger scoping strategy is to narrow down the text span of searching for facts, from sentence-level 3 A poss−1 B means there is a possession modifier relation (poss) between B and A. 1207 to fragment-level. We only focus on analyzing the content which is likely to contain an answer. Our trigger scoping method is also partially inspired from the negation scope detection work (e.g., (Szarvas et al., 2008; Elkin et al., 2005; Chapman et al., 2001; Morante and Daelemans, 2009; Agarwal and Yu, 2010)) and reference scope identification in citing sentences (Abu-Jbara and Radev, 2011; AbuJbara and Radev, 2012). 5 Conclusions and Future Work In this paper we explore the role of triggers and their scopes in biographical fact extraction. We implement the trigger scoping strategy using two simple but effective methods. Experiments demonstrate that our approach outperforms state-of-the-art without any syntactic analysis and external knowledge bases. In the future, we will aim to explore how to generate"
N16-1029,N03-1002,0,0.00885673,"2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech"
N16-1029,W13-2322,1,0.0318382,", and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotations, to automatically map 9, 514 entity types in DBPedia to three main entity types of interest: Person (PER), Location (LOC) and Organization (ORG). Then we adopt a language-independent crosslingual entity linking system (Wang et al., 2015) 4 5 to link each IL name mention to English DBPedia. This linker is based on an unsupervised quantified collective inference approach. It constructs knowledge networks from the IL source documents based on entity mention co-occurrence, and knowledge networks from KB. Each IL name is matched with"
N16-1029,P00-1011,0,0.050254,"ng (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in"
N16-1029,E03-1038,0,0.0920325,"et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which"
N16-1029,N13-1006,0,0.0156684,"We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Cat"
N16-1029,C02-1025,0,0.0358446,"typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such"
N16-1029,D10-1098,0,0.00551045,"s “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 199"
N16-1029,W99-0613,0,0.0761662,"ey that aims to acquire a wide-range of IL-specific knowledge from native speakers in an efficient way. The survey categorizes questions and organizes them into a tree structure, so that the order of questions is chosen based on the answers of previous questions. The survey answers are then automatically translated into rules, patterns or gazetteers in the tagger. Some example questions are shown in Table 2. 3.3 Mono-lingual Expectation Mining We use a bootstrapping method to acquire IL patterns from unlabeled mono-lingual IL documents. Following the same idea in (Agichtein and Gravano, 2000; Collins and Singer, 1999), we first use names identified by high-confident rules as seeds, and generalize patterns from the contexts of these seeds. Then we evaluate the patterns and apply high-quality ones to find more names as new seeds. This process is repeated iteratively 2 . We define a pattern as a triple ⟨lef t, name, right⟩, where name is a name, left and right3 are context vectors with weighted terms (the weight is computed based on each token’s tf-idf score). For example, from a Hausa sentence “gwamnatin kasar Sin ta samar wa kasashen yammacin Afirka ... (the Government of China has given ... products to the"
N16-1029,P98-1045,0,0.125064,"er et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in a new emergent setting where we need to process a surprise I"
N16-1029,demiros-etal-2000-named,0,0.0375117,"stic phenomena. Typing Accuracy* Overall F-score 84.1 93.6 86.2 92.8 72.0 69.1 82.3 40.7 51.6 33.8 65.1 35.0 43.6 47.1 * typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2"
N16-1029,P05-1045,0,0.0179112,"preposition suffixes as many as you can (e.g. “’da” in “Ankara’da yaşıyorum (I live in Ankara)” is a preposition suffix which means “in”). Translation 1. Please translate the following English words and phrases: - organization suffix: agency, group, council, party, school, hospital, company, office, ... - time expression: January, ..., December; Monday, ..., Sunday; ... Table 2: Survey Question Examples Besides the static knowledge like patterns, we can also dynamically acquire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Be"
N16-1029,W06-2005,0,0.0333424,"9; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in a new emergent setting where we need to process a surprise IL within very short time using very few resources. The TIDES 2003 Surprise Language Hindi Named En"
N16-1029,P05-1051,1,0.726173,"ncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as lan"
N16-1029,Y09-1024,1,0.804456,"pervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish"
N16-1029,P12-1073,0,0.0266001,"for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney a"
N16-1029,li-etal-2014-comparison,1,0.597073,"Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rul"
N16-1029,W98-1002,0,0.0349112,"al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge"
N16-1029,W03-0430,0,0.0347683,"ntified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al."
N16-1029,E99-1001,0,0.0394154,"Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (H"
N16-1029,N03-2025,0,0.0601139,"usa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Ita"
N16-1029,J03-1002,0,0.00415466,"uary, ..., December; Monday, ..., Sunday; ... Table 2: Survey Question Examples Besides the static knowledge like patterns, we can also dynamically acquire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company,"
N16-1029,W04-2905,0,0.0351028,"uire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotation"
N16-1029,E12-2015,0,0.00508774,"omputed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Gr"
N16-1029,D08-1112,0,0.0103785,"abeled pool ϕ(·) ← query strategy, B ← query batch size M ← maximum number of tokens while Length(L)&lt; M do θ = train(L); for b ∈ {1, 2, ..., B} do x∗b = arg maxx∈U ϕ(x) L = L ∪ {x∗b , label(x∗b )} U = U − x∗b end for end while T ∑ M ∑ Name 4,713 1,619 6,119 4120 4,954 2,694 3,745 Unique Name 2,820 950 3,375 2,871 3,314 1,323 2,337 IL Dev. Docs 12,495 13,652 1,616 4,597 10,000 10,000 427 IL-English Docs 169 645 145 166 191 484 252 Table 3: Data Statistics Jing et al. (2004) proposed an entropy measure for active learning for image retrieval task. We compared it with other measures proposed by (Settles and Craven, 2008) and found that sequence entropy (SE) is most effective for our name tagging task. We use ϕSE to represent how informative a sentence is: ϕSE (x) = − Language IL Test Docs Bengali 100 Hausa 100 Tagalog 100 Tamil 100 Thai 100 Turkish 100 Yoruba 100 Pθ (yt = m)logPθ (yt = m) each expectation-driven rule based on its precision score on a small development set of ten documents. Then we apply these rules in the priority order of their confidence values. When the results of two taggers are conflicting on either mention boundary or type, if the applied rule has high confidence we will trust its outpu"
N16-1029,P13-1106,0,0.256647,"ame identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al"
N16-1029,D15-1081,1,0.782214,"earning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotations, to automatically map 9, 514 entity types in DBPedia to three main entity types of interest: Person (PER), Location (LOC) and Organization (ORG). Then we adopt a language-independent crosslingual entity linking system (Wang et al., 2015) 4 5 to link each IL name mention to English DBPedia. This linker is based on an unsupervised quantified collective inference approach. It constructs knowledge networks from the IL source documents based on entity mention co-occurrence, and knowledge networks from KB. Each IL name is matched with candidate entities in English KB using name translation pairs derived from inter-lingual KB links in Wikipedia and DBPedia. We also apply the wordfor-word translation tables constructed from parallel data as described in Section 3.4 to translate some uncommon names. Then it performs semantic compariso"
N16-1029,D09-1158,0,0.00820629,"low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Gree"
N16-1029,P02-1060,0,0.0248791,"1 35.0 43.6 47.1 * typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEng"
N16-1029,C98-1045,0,\N,Missing
N16-1137,P13-1024,1,0.842141,"on of events which includes event participants, event phrase, calendar date, and event type. The news event extractor (Li et al., 2013) is a joint framework based on structured prediction which extracts triggers and arguments simultaneously while incorporating diverse lexical, syntactic, semantic and global features. It takes raw documents as input, distinguish events from non-events by classifying event triggers and identifying and classifying argument roles. 3.2 Knowledge Enrichment Approach To produce the latent vector representations for the whole dataset, we follow the same procedure in (Guo et al., 2013): represent the dataset in a matrix X, where each cell stores the TF-IDF values of words. Word vectors P and tweet vectors Q are optimized by minimizing the following objective function: XX i 2 Wij (P·,i · Q·,j − Xij ) + λ||P ||22 + λ||Q||22 j Q·,j1 · Q·,j2 +δ · ( − 1)2 |Q·,j1 ||Q·,j2 |  1, if Xij 6= 0 Wi,j = wm , if Xij = 0 (1) where λ is a regularization term, Q·,j1 and Q·,j2 are linked pairs connected by text-to-text relations, |Q·,j |denotes the length of vector Q·,j and the coefficient δ denotes the importance of the text-to-text links. we follow the same optimization procedure as (Steck"
N16-1137,P08-1030,1,0.776996,"tter et al., 2012). Identifying and extracting events in social media is more challenging than traditional event extraction due to two major reasons: (1). Lack of Context: compared with traditional genres (e.g., new articles), social media context is usually short and incomplete (e.g., each tweet has a length limitation of 140 characters). Lacking of context, a single tweet itself usually cannot provide a complete picture of the corresponding events. For example, for the tweet “Pray for Mali - the situation is coming to light, and it isn’t pretty.”, an event extraction/discovery system (e.g. (Ji and Grishman, 2008)) fails to discover that it is about the same war event in Mali as mentioned in the news article “State military forces on Friday retook a key town in northern Mali after intense fighting that included help from French military forces, a defense ministry spokesman said.” (2). Informal Nature: social media messages are written in an informal style, which causes the poor performance of event extractors designed for formal genres. For example, the tweet “#AaronSwartz, Dead @ 26, #CarmenOrtiz “pushed him to exhaustion” don’t let her get away with this! #scandal.” includes an “Die” event with Aaron"
N16-1137,P13-1008,1,0.831524,"are the extracted event tuples for the tweet and the news article respectively and e1 is the final cross-genre event extraction output after merging. To evaluate the performance of an event extractor, the precision, recall and f-measure of the extracted event phrases and event arguments will be measured using the following criteria: an event phrase is correctly labeled if it matches a reference trigger; an argument is correctly labeled if it matches a reference argument. 3 3.1 Approach Baseline Event Extraction Systems We use two state-of-the-art event extraction systems (Ritter et al., 2012; Li et al., 2013) to extract events from tweets and news articles respectively. The tweet event extractor TwiCal-Event (Ritter et al., 2012) is able to extract open-domain significant 1159 events from Twitter. It is a supervised system that identifies event phrases and event participants with tailored part-of-speech tagging and shallow parsing for tweets. In addition, it is also able to discover event categories and classify extracted events based on latent variable models. It takes tweets as input and outputs a four-tuple representation of events which includes event participants, event phrase, calendar date,"
N16-1137,P11-1040,0,\N,Missing
N16-3015,P15-1056,1,0.400174,"ructured data in different modalities (e.g., texts, images and videos) is posted online for ready viewing. Complex event extraction and recommendation is critical for many information distillation tasks, including tracking current events, providing alerts, and predicting possible changes, as related to topics of ongoing concern. State-of-the-art Information Extraction (IE) technologies focus on extracting events from a single data modality and ignore cross-media fusion. More importantly, users are presented with extracted events in a passive way (e.g., in a temporally ordered event chronicle (Ge et al., 2015)). Such technologies do not leverage user behavior to identify the event 1 The system demo is available at: http://nlp.cs. rpi.edu/multimedia/event/navigation_dark. html properties of interest to them in selecting new scenarios for presentation. In this paper we present a novel event extraction and recommendation system that incorporates advances in extracting events across multiple sources with data in diverse modalities and so yields a more comprehensive understanding of collective events, their importance, and their inter-connections. The novel capabilities of our system include: • Event Ex"
N16-3015,P12-1038,0,0.0307608,"of sentences and apply spectral clustering to find several clustering centers (i.e., representative sentences including the most important phrases) as the summary. The user is also provided two options to show the original documents and the document containing the summary. 4.2 Visual Information Extraction For each event, we retrieve the most representative video/image online using the key-phrases such as date and entities as queries. Videos and images are often more impressive and efficient at conveying information. We first apply a pretrained convolutional neural network (CNN) architecture (Kuznetsova et al., 2012) to extract visual concepts from each video key frame based on the EventNet concept library (Ye et al., 2015). For example, the extracted visual concepts “crowd on street, riot, demonstration or protest, people marching” appear when the user’s mouse is over the video of the primary event (Figure 3). Then we adopt the approach described in (Li et al., 2015) which applies CNN and association rule mining technique to generate visual patterns and extract semantically meaningful relations between visual and textual information to name the patterns. Figure 3: Recommendation Interface. 5 6 Conclusion"
N16-3015,D14-1198,1,0.888744,"Missing"
N18-5009,P16-1213,0,0.0310293,". In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy or sanitation, (6) evacuation, (7) shelter, (8) search and rescue, (9) civil unrest or widespread crime, and (10) infrastructure, as defined in the"
N18-5009,W13-2322,1,0.753485,"ifier} /entity linking amr /localize/{identifier} Description Retrieve the current server status, including supported languages, language identifiers, and the state (offline, online, or pending) of each model. Retrieve the current status of a given language. Main entry of the EDL system. Take input in either plain text or *.ltf format, tag names that are PER, ORG or LOC/GPE, and link them to Wikipedia. Transliterate a name to Latin script. Query based entity linking. Link each mention to KBs. English entity linking for Abstract Meaning Representation (AMR) style input (Pan et al., 2015). AMR (Banarescu et al., 2013) is a structured semantic representation scheme. The rich semantic knowledge in AMR boosts linking performance. Localize a LOC/GPE name based on GeoNames database. Table 1: RUN APIs description. APIs /status /status/{identifier} /train/{identifier} Description An alias of /status Query the current status of a model being trained. Train a new name tagging model for a language. A model id is automatically generated and returned based on model name, and time stamp. Table 2: TRAIN APIs description. Figure 1: Cross-lingual Entity Extraction and Linking Interface Figure 2: Cross-lingual Entity Extra"
N18-5009,K16-1022,0,0.0371881,"e test interface, where a user can select one of the 282 languages, enter a text or select an example document, and run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence,"
N18-5009,Q16-1026,0,0.0419127,"Missing"
N18-5009,N16-1029,1,0.928423,"on Sciences Institute {jonmay,knight}@isi.edu Abstract tity mention with a certain type. Our model is based on a bi-directional long short-term memory (LSTM) networks with a Conditional Random Fields (CRFs) layer (Chiu and Nichols, 2016). It is challenging to perform entity extraction across a massive variety of languages because most languages don’t have sufficient data to train a machine learning model. To tackle the low-resource challenge, we developed creative methods of deriving noisy training data from Wikipedia (Pan et al., 2017), exploiting non-traditional languageuniversal resources (Zhang et al., 2016) and crosslingual transfer learning (Cheung et al., 2017). We demonstrate E LISA -E DL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets1 , resources and system training and testing APIs2 publicly available for research purpose. 1 Introduction Our cross-lingual entity extraction, linking and localization system is capable of extracting named entities from unstructured text in any of 282 Wikipedia languag"
N18-5009,W16-2701,1,0.830496,"ages, and the TRAIN section provides a re-training function for users who want to train their own customized name tagging models using their own datasets. We also published our training and test data sets, as well as resources related to at morphology analysis and name translation at: https://elisa-ie.github.io/wikiann. Table 1 and Table 2 present the detailed functionality and usages of the APIs of these two sections. Besides the core components as described in Section 2 and Section 3, we also provide the APIs of additional components, including a re-trainable name transliteration component (Lin et al., 2016) and a universal name and word translation component based on word alignment derived from crossTable 3: Name Tagging Performance on Low-Resource Languages GeoNames and Wikipedia as their salience scores. Then we construct knowledge networks from source language texts, where each node represents a entity mention, and each link represents a sentence-level co-occurrence relation. If two mentions cooccur in the same sentence, we prefer their entity candidates in the GeoNames to share an administrative code and type, or be geographically close in the world, as measured in terms of latitude and long"
N18-5009,C16-1095,0,0.0142781,"here a user can select one of the 282 languages, enter a text or select an example document, and run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy"
N18-5009,I11-1029,0,0.0340278,"d run the system. Figure 2 shows an output example. In addition to the entity extraction and linking results, we also display the top 5 images for each entity retrieved from Google Image Search11 . In this way even when a user cannot read a document in a lowresource language, s/he will obtain a high-level summary of entities involved in the document. 6 Figure 4: Different Map Styles 7 Heatmap Visualization Related Work Some recent work has also focused on lowresource name tagging (Tsai et al., 2016; Littell et al., 2016; Zhang et al., 2016; Yang et al., 2017) and cross-lingual entity linking (McNamee et al., 2011; Spitkovsky and Chang, 2011; Sil and Florian, 2016), but the system demonstrated in this paper is the first publicly available end-to-end system to perform both tasks and all of the 282 Wikipedia languages. Using disaster monitoring as a use case, we detect the following ten topics from the input multilingual data based on translating 117 English disaster keywords via PanLex12 : (1) water supply, (2) food supply, (3) medical assistance, (4) terrorism or other extreme violence, (5) utilities, energy or sanitation, (6) evacuation, (7) shelter, (8) search and rescue, (9) civil unrest or widespre"
N18-5009,N15-1119,1,0.949944,"efforts reported from incident regions in low-resource languages. In the rest of the paper, we will present a comprehensive overview of the system components (Section 2 and Section 3), APIs (Section 4), interface3 (Section 5), and visualization4 (Section 6). 2 3 Entity Linking and Localization After we extract entity mentions, we link GPE and LOC mentions to GeoNames5 , and PER and ORG mentions to Wikipedia6 . We adopt the name translation approach described in (Pan et al., 2017) to translate each tagged entity mention into English, then we apply an unsupervised collective inference approach (Pan et al., 2015) to link each translated mention to the target KB. Figure 2 shows an example output of a Hausa document. The extracted entity mentions “Stephane Dujarric” and “birnin Bentiu” are linked to their corresponding entries in Wikipedia and GeoNames respectively. Compared to traditional entity linking, the unique challenge of linking to GeoNames is that it is very scarce, without rich linked structures or text descriptions. Only 500k out of 4.7 million entities in Wikipedia are linked to GeoNames. Therefore, we associate mentions with entities in the KBs in a collective manner, based on salience, sim"
N18-5009,P17-1178,1,0.947324,"er Polytechnic Institute {zhangb8,liny9,panx2,lud2,jih}@rpi.edu 2 Information Sciences Institute {jonmay,knight}@isi.edu Abstract tity mention with a certain type. Our model is based on a bi-directional long short-term memory (LSTM) networks with a Conditional Random Fields (CRFs) layer (Chiu and Nichols, 2016). It is challenging to perform entity extraction across a massive variety of languages because most languages don’t have sufficient data to train a machine learning model. To tackle the low-resource challenge, we developed creative methods of deriving noisy training data from Wikipedia (Pan et al., 2017), exploiting non-traditional languageuniversal resources (Zhang et al., 2016) and crosslingual transfer learning (Cheung et al., 2017). We demonstrate E LISA -E DL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets1 , resources and system training and testing APIs2 publicly available for research purpose. 1 Introduction Our cross-lingual entity extraction, linking and localization system is capable of ext"
N19-1145,W06-0901,0,0.0269479,". In contrast, by incorporating KB concept embeddings, especially 1427 the information from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural N"
N19-1145,W11-1828,0,0.208157,"Missing"
N19-1145,W13-2003,0,0.0422846,"Missing"
N19-1145,W18-2311,0,0.291907,"Missing"
N19-1145,W13-2014,0,0.0391315,"Missing"
N19-1145,P17-1038,0,0.0125929,"nt extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016"
N19-1145,P15-1017,0,0.0283369,"re design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for"
N19-1145,D18-1158,0,0.0117435,"ation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of t"
N19-1145,W09-1407,0,0.0102761,"develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs and RNNs with features derived from the biomedical text. Lim et al."
N19-1145,D17-1070,0,0.0143517,"litate the explicit pattern learning for argument role labeling, for example, the gene expression event pattern (Theme: Protein, Trigger: transduced) is more popular than (Theme: Tax, Trigger: transduced) in Figure 1. The gene ontology function can provide implicit clues to determine the trigger type as aforementioned in Section 1. As shown in Figure 1, we assign a word embedding which pretrained on PubMed and PMC texts (Moen and Ananiadou, 2013) to represent each entity type. For each gene ontology function which is usually a long phrase, we use a stateof-the-art sentence embedding approach (Conneau et al., 2017) to automatically learn a vector representation. We then concatenate these two types of KB property representations as the final KB concept embedding. 1423 2.3 Event Trigger Extraction After obtaining the KB concept embeddings, we further incorporate them into the Tree-LSTM to leverage the domain-specific knowledge. Given a sentence, for example the sentence shown in Figure 3, we first perform the dependency parsing with the Stanford dependency parser (Chen and Manning) and obtain a dependency tree structure. For each node j in the tree structure, C(j) is the set of children nodes of node j an"
N19-1145,E12-1029,0,0.011886,"mation from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), ze"
N19-1145,N16-1056,0,0.0219326,"global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs and RNNs with features derived from the biomedical text. Lim et al. (2018) implements a binary Tree-LSTM architecture for biomedical relation extraction. Compared with these methods, our approach only requires pretrained distributed word representations as input features and incorporates meaningful KB information into a Tree-LSTM. 5 Conclusions and Future Work In this paper, we show the effectiveness of using a KB-driven tree-structured LSTM for event extraction in biomedical domai"
N19-1145,P08-1030,1,0.652212,"KB concept embeddings, especially 1427 the information from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), gener"
N19-1145,W11-1827,0,0.0217041,"n-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs and RNNs with features derived from the biomedical text. Lim et al. (2018) implements a binary Tr"
N19-1145,W09-1401,0,0.110226,"the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be"
N19-1145,W11-1802,0,0.646021,"are manually annotated and given as part of the input. We evaluate our results on the test set using the official online tool provided by the Genia task organizers.1 Following previous studies (Bj¨orne and Salakoski, 2011; Venugopal et al., 2014; Rao et al., 2017; Bj¨orne and Salakoski, 2018), we report scores obtained by the approximate span (allowing trigger spans to differ from gold spans by single words). As we only focus on matching core arguments, we use recursive matching criterion for evaluation which not requires matching of additional arguments for events referred from other events (Kim et al., 2011). We use the word embedding pretrained on PubMed and PMC texts (Moen and Ananiadou, 1425 1 http://bionlp-st.dbcls.jp/GE/2011/eval-test/ System 2013) for word and type embeddings. The hyperparameters are tuned on the development set and listed in Table 2. Word representations are updated during training with an initial learning rate of 0.1. Parameter Word embedding size Type embedding size Sentence embedding size Tree-LSTM hidden size Batch size Epoch size Dropout rate Learning rate Initial embedding learning rate Optimizer KB-driven Tree-LSTM Value 200 200 4096 100 25 30 0.5 0.05 0.1 AdaGrad T"
N19-1145,P13-1008,1,0.861127,"heme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the"
N19-1145,D14-1198,1,0.870375,"ask in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information ex"
N19-1145,R11-1002,0,0.0142687,"especially 1427 the information from the function description positive regulation of transcription, DNA-templated for Tax, our approach successfully promotes the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial network"
N19-1145,P17-1164,0,0.0122005,"nd category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global fe"
N19-1145,D18-1156,0,0.14784,"me the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extr"
N19-1145,W16-6308,0,0.425834,".24 82.31 87.50 87.28 80.28 85.95 53.16 53.61 57.90 52.39 55.73 67.01 83.41 48.72 53.54 64.56 78.75 43.05 53.81 62.18 F1 80.28 75.39 60.87 84.36 68.47 78.73 44.10 43.52 48.26 49.02 47.72 58.65 76.83 40.62 45.64 56.53 73.03 40.65 44.30 54.46 Table 3: Precision (Prec), recall (Rec) and F-score (F1) results achieved by the KB-driven Tree-LSTM model on the test set of BioNLP Genia 2011, evaluated on approximate span and recursive criteria. System TEES(Bj¨orne and Salakoski, 2011) FAUST(Riedel and McCallum, 2011) EventMine-CR(Miwa et al., 2012) BioMLN(Venugopal et al., 2014) Stacked Generalization(Majumder et al., 2016) CNN(Bj¨orne and Salakoski, 2018) Rec Prec F1 49.56 57.65 53.30 49.41 64.75 56.04 53.35 63.48 57.98 53.42 63.61 58.07 48.96 66.46 56.38 49.94 69.45 58.07 Table 4: State-of-the-art system results evaluated on BioNLP Genia 2011 test dataset with approximate span and recursive criteria. ing event extraction is more challenging since it usually has multiple arguments. For example, Figure 4 shows two sentences which are chosen from the output of the development data set. There are two Binding event mentions in the first sentence: E1 (Trigger: interacting, Type: Binding, Theme: RUNX1, Theme2: p3000)"
N19-1145,P16-1105,0,0.0197849,"g (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) u"
N19-1145,D16-1085,0,0.0273283,"Missing"
N19-1145,N10-1123,0,0.218195,"es the probability of E1 being predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also rela"
N19-1145,W17-2315,0,0.547653,"Missing"
N19-1145,W09-1406,0,0.0272412,"ng predicted as the Theme of E2. 4 Related Work As a crucial task in information extraction, event extraction has gained a lot of interest. In general news domain, previous work on event extraction can be divided into two main categories. The first is feature-based methods which mainly focus on feature design, leveraging local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies wh"
N19-1145,W11-1807,0,0.0792817,"opts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint inference. Rao et al. (2017) uses the Abstract Meaning Representations (AMR) to extract events based on the assumption that an event structure can be derived from an AMR subgraph. Recently, some representationbased models (Jagannatha and Yu, 2016; Rao et al., 2017; Bj¨orne and Salakoski, 2018) have been proposed while most of them adopt the widely used CNNs"
N19-1145,P15-1150,0,0.0463187,"−2 fj−2 fj−2 xj ij cj ˜ cj oj hj xj ij cj ˜ fj−1 fj−1 hj−1 oj cj cj−1 hj−1 μ j−1 ˜ j−1 A. a Tree-LSTM unit gj cj−1 KB concept of x hj μj ˜ KB concept of x j B. a KB-driven Tree-LSTM unit Figure 2: (A): a Tree-LSTM unit. (B): a KB-driven Tree-LSTM unit. The yellow circles with µ e notations denote external KB concept embeddings. We first introduce the Tree-LSTM framework, and then describe the construction of KB concept embedding for each entity. Finally we incorporate the KB concept embedding into a Tree-LSTM and apply it for event trigger and argument extraction. 2.1 Tree-LSTM The Tree-LSTM (Tai et al., 2015) is a variation of LSTM (Hochreiter and Schmidhuber, 1997) to a tree-structured network topology. It shows improvement in representing sentence semantic meaning compared to sequential LSTM such as Bidirectional LSTM (BiLSTM) (Graves et al., 2013). The main difference between sequential LSTM and Tree-LSTM is, at each time step, the former calculates its hidden state from the input at the current time step and the hidden state from previous step, while Tree-LSTM computes its hidden state from the input token and the hidden states of all its children nodes from the tree structure. A Tree-LSTM red"
N19-1145,D14-1090,0,0.604672,"al., 2013). The Genia task defines 9 fine-grained event types as shown in Table 1. Note that a Binding event may take more than one protein as its Theme arguments. A Regulation event may take one protein or event as its Theme argument and also optionally take one Experimental Setup We apply our KB-driven Tree-LSTM model on Genia 2011 data set. The entities in Genia data set are manually annotated and given as part of the input. We evaluate our results on the test set using the official online tool provided by the Genia task organizers.1 Following previous studies (Bj¨orne and Salakoski, 2011; Venugopal et al., 2014; Rao et al., 2017; Bj¨orne and Salakoski, 2018), we report scores obtained by the approximate span (allowing trigger spans to differ from gold spans by single words). As we only focus on matching core arguments, we use recursive matching criterion for evaluation which not requires matching of additional arguments for events referred from other events (Kim et al., 2011). We use the word embedding pretrained on PubMed and PMC texts (Moen and Ananiadou, 1425 1 http://bionlp-st.dbcls.jp/GE/2011/eval-test/ System 2013) for word and type embeddings. The hyperparameters are tuned on the development"
N19-1145,D15-1206,0,0.0349407,"zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for information extraction, especially for relation classification (Xu et al., 2015; Miwa and Bansal, 2016) and shows promising benefits. Biomedical event extraction task part of the BioNLP Shared Task series (Kim et al., 2009, 2011; N´edellec et al., 2013). Previous studies mainly explore local and global features with SVM model (Miwa et al., 2010, 2012; Bj¨orne and Salakoski, 2013; Majumder et al., 2016). Riedel and McCallum (2011) develop a joint model with dual decomposition. Cohen et al. (2009), Kilicoglu and Bergler (2011) and Bui et al. (2013) develop rule-based methods and achieve high precision. Venugopal et al. (2014) leverage Markov logic networks for joint infere"
N19-1145,W13-2001,0,0.256189,"Missing"
N19-1145,N16-1034,0,0.0384089,"ures (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s more, shortest dependency path is broadly explored for informati"
N19-1145,P15-2060,0,0.0467616,"ng local features (Grishman et al., 2005; Ahn, 2006) and global features (Ji and Grishman, 2008; Liao and Grishman, 2011; Huang and Riloff, 2012) to improve the performance. Some studies proposed joint models to overcome the error propagation problem (Poon and Vanderwende, 2010; Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Li et al., 2014). The second category includes distributional representation based methods which have been applied into event extraction extensively. Most of these approaches are based on the standard Convolutional Neural Networks (CNNs) (Chen et al., 2015; Nguyen and Grishman, 2015, 2016), Recurrent Neural Networks (RNNs) (Nguyen et al., 2016), generative adversarial networks (Hong et al., 2018), zero-shot learning (Huang et al., 2017) and advanced attention mechanisms (Liu et al., 2018b; Chen et al., 2018). Our work is also related to the studies which leverage the external knowledge base for information extraction. Liu et al. (2017) takes advantage of external resources, such as FrameNet, to label events while Chen et al. (2017) adopts distance supervision to augment the training data. Liu et al. (2018a) develops an attention-based model for event extraction. What’s m"
N19-1252,D18-1549,0,0.0207217,"mote locales still remains. As an example, when international aid organizations respond to new disasters, they are often unable to deploy technology to understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019, pages 2428–2439 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods either do not work in practice on true lowresource cases or unrealistically assume the availability of some amount of supervision. Consider syntactic parsing as a prime example. Past editions of the CoNLL Shared Task on Multilingual Parsing (Zeman et a"
N19-1252,N10-1083,0,0.0864509,"Missing"
N19-1252,Q17-1010,0,0.0929895,"Missing"
N19-1252,J92-4003,0,0.758212,"laer Polytechnic Institute ♥ Information Sciences Institute, University of Southern California ronald.cardenas@matfyz.cz liny9@rpi.edu jih@rpi.edu jonmay@isi.edu Abstract Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a ‘ciphertext’ and seeks a POS tag-tocluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to stateof-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagg"
N19-1252,Q16-1026,0,0.0475058,"Missing"
N19-1252,D10-1056,0,0.0303461,"d, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganchev et al., 2009); others, by using logistic distributions instead of multinomial ones (Berg-Kirkpatrick et al., 2010; Stratos et al., 2016). However, these models still need to ground or map hidden states to actual POS tags to evaluate, and they inevitably resort to many-to-one or one-to-one accuracy scoring. Some previous work has been cautious in pointing out this ill-defined setting (Ravi and Knight, 2009; Christodoulopoulos et al., 2010), and we argue its inappropriateness for scenarios in which the test set is extremely small or even when no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machi"
N19-1252,E03-1009,0,0.156984,"oy 102 features obtained from WALS9 related to word order and morphosyntactic alignment, further reduced to 50 dimensions using PCA. However, none these criteria correlates significantly to tagging accuracy, as we elaborate in Section 5.1. We instead try a combined approach. The likelihood of cluster ID replacement, Pθ (ˆ ci |pj ), ∀ˆ ci ∈ C, ∀pj in the tagset, is replaced by P ci |psj ) s∈S,s6=t Pθ (ˆ Pavg (ˆ ci |pj ) ∼ |S |− 1 https://github.com/percyliang/ brown-cluster 7 Optimized and implemented by M¨uller and Schuetze (2015). Available at http://cistern.cis.lmu.de/ marlin/ 8 As noted by Clark (2003) and Johnson (2007), in the limit, keeping each type (or, in the case of A - HMM , TOKEN in its own cluster will result in the maximum possible manyto-one (polysemic types prevent perfect accuracy when type clustering). where Pall (p) is a language model trained over the concatenation of POS sequences of all parent languages in S. We call this approach CIPHER - AVG. 4 Downstream Tasks 4.1 Name Tagging We experiment with the LSTM-CNN model proposed by Chiu and Nichols (2016), one of the 2431 9 https://wals.info/ Seq. Tagger BROWN MARLIN A - HMM E - KMEANS en 81.37 81.53 77.12 63.01 de 81.28 81."
N19-1252,P11-1061,0,0.208466,"o deploy technology to understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019, pages 2428–2439 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods either do not work in practice on true lowresource cases or unrealistically assume the availability of some amount of supervision. Consider syntactic parsing as a prime example. Past editions of the CoNLL Shared Task on Multilingual Parsing (Zeman et al., 2017, 2018) featured a category of target languages for which either little or no training data was provided. However, even"
N19-1252,P15-1081,0,0.022531,"no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al"
N19-1252,D14-1096,0,0.116435,"understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019, pages 2428–2439 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics methods either do not work in practice on true lowresource cases or unrealistically assume the availability of some amount of supervision. Consider syntactic parsing as a prime example. Past editions of the CoNLL Shared Task on Multilingual Parsing (Zeman et al., 2017, 2018) featured a category of target languages for which either little or no training data was provided. However, even in the ‘no-resource’"
N19-1252,P15-1033,0,0.0569815,"Missing"
N19-1252,P17-2093,0,0.0203432,"different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganchev et al., 2009); others, by using logistic distributions instead of multinomial ones (Berg-Kirkpatrick et al., 2010; Stratos et al., 2016"
N19-1252,P08-1085,0,0.030201,"a without any path constraint (a cluster ID could be mapped to any POS tag). In this sense, our approach applies EM to simplify the task (e.g. when using Brown clustering (Brown et al., 1992)), followed by another EM run to optimize cipher table parameters. Under this lens, the methods closest to our approach are those which attempt to reduce or constrain the parameter search space prior to running EM. For instance, Ravi and Knight (2009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora"
N19-1252,P07-1094,0,0.121402,"der “zeroresource” conditions. In order to function, however, the model requires users to provide gold POS tags and word mappings from these languages into a common semantic space, using approaches that require parallel data (Guo et al., 2015). Indeed, the compulsion to use POS tag-labeled data in zero-resource circumstances extends to the vast, varied lines of research in unsupervised POS tagging itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the inferred clusters or states to actual POS tags. Making matters worse, evaluation is generally offered in terms of the ‘many-to-one’ or ‘one-toone’ analyses Johnson (2007). These metrics use a reference corpus to determine the optimal mapping of clusters to tags. While this evaluation approach is intuitively sensible for measuring cluster purity, to actually use such an output, an en"
N19-1252,P15-1119,0,0.166435,"her little or no training data was provided. However, even in the ‘no-resource’ scenario that most closely matches our use case, gold part-of-speech (POS) tags for test data were provided for the participants to use. Prior to these shared tasks, Ammar et al. (2016) proposed a variant of their main model, MALOPA, that was meant to produce reasonable parses for languages under “zeroresource” conditions. In order to function, however, the model requires users to provide gold POS tags and word mappings from these languages into a common semantic space, using approaches that require parallel data (Guo et al., 2015). Indeed, the compulsion to use POS tag-labeled data in zero-resource circumstances extends to the vast, varied lines of research in unsupervised POS tagging itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the"
N19-1252,N06-1041,0,0.0670558,"009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganche"
N19-1252,C08-1042,0,0.0436749,"Missing"
N19-1252,P18-4003,1,0.811244,"been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al. (2016) has semiautomatically built inflectional paradigms for hundreds of languages. 7 Conclusion We present a POS tag grounding strategy based on decipherment that does not require human-labeled data to map states or clusters to actual POS tags and thus can be used in real-world situations requiring grounded POS tags. The decipherment model considers state or word cluster IDs of a CL as a cipher tex"
N19-1252,D07-1031,0,0.362308,"itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the inferred clusters or states to actual POS tags. Making matters worse, evaluation is generally offered in terms of the ‘many-to-one’ or ‘one-toone’ analyses Johnson (2007). These metrics use a reference corpus to determine the optimal mapping of clusters to tags. While this evaluation approach is intuitively sensible for measuring cluster purity, to actually use such an output, an entire annotated training corpus is required.1 It is not enough to simply rely on ungrounded clusters in real-world systems; grounded labels offer a sort of universal API between other resources such as rule-based modules that operate on certain word types or between resources built from other annotated high-resource language data. Since POS tag and parallel data resources for new lan"
N19-1252,L16-1498,0,0.0232341,"al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al. (2016) has semiautomatically built inflectional paradigms for hundreds of languages. 7 Conclusion We present a POS tag grounding strategy based on decipherment that does not require human-labeled data to map states or clusters to actual POS tags and thus can be used in real-world situations requiring grounded POS tags. The decipherment model considers state or word cluster IDs of a CL as a cipher text to be deciphered back to a POS sequence. The model operates on top of Brown cluster IDs and requires a POS language model trained on annotated corpora of one or more PLs. Experimental results over a la"
N19-1252,W11-2164,0,0.0217165,"e use an unsupervised clustering method (Section 3.2) then reduce and ground the clusters using a decipherment approach informed by POS tag sequence data from many languages (Section 3.3). Introduction While cellular, satellite, and hardware advances have ensured that sophisticated NLP technology can reach all corners of the earth, the language barrier upon reaching remote locales still remains. As an example, when international aid organizations respond to new disasters, they are often unable to deploy technology to understand local reports detailing specific events (Munro and Manning, 2012; Lewis et al., 2011). An inability to communicate with partner governments or civilian populations in a timely manner leads to preventable casualties. The lack of adequate labeled training data has been the major obstacle to expanding NLP’s outreach more multilingually. Developments in unsupervised techniques that require only monolingual corpora (Lample et al., 2018a; Artetxe et al., 2018) and the ability to leverage labeled resources in other languages have been proposed to address this issue (Das and Petrov, 2011; Duong et al., 2014; Ammar et al., 2016). Unfortunately, these 2428 Proceedings of NAACL-HLT 2019,"
N19-1252,D12-1127,0,0.0474251,"Missing"
N19-1252,P13-2017,0,0.0530892,"Missing"
N19-1252,J94-2001,0,0.765507,"pt to reduce or constrain the parameter search space prior to running EM. For instance, Ravi and Knight (2009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the f"
N19-1252,N15-1055,0,0.0677038,"Missing"
N19-1252,P17-1178,1,0.84514,"hine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to hundreds of languages by leveraging Wikipedia. Kirov et al. (2016) has semiautomatically built inflectional paradigms for hundreds of languages. 7 Conclusion We present a POS tag grounding strategy based on decipherment that does not require human-labeled data to map states or clusters to actual POS tags and thus can be used in real-world situations requiring grounded POS tags. The decipherment model considers state or word cluster IDs of a CL as a cipher text to be deciphered back to a POS sequence. The model operates on top of Brown cluster IDs"
N19-1252,D17-1266,0,0.0275221,"ess for scenarios in which the test set is extremely small or even when no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to Unicode. Pan et al. (2017) extends name tagging and linking capability to"
N19-1252,D15-1039,0,0.0209949,"(fr), Italian (it), Spanish (es), Japanese (ja), Czech (cs), Russian (ru), Arabic (ar), and Farsi (fa). For Swahili (sw), we use the Helsinki Corpus of Swahili 2.0.4 Overall in these experiments we cover 11 languages and 4 language families. In our dependency parsing experiments, we use the Universal Treebank v2.0 (McDonald et al., 2013) for en, de, fr, es, it, Portuguese (pt), and Swedish (sv). This set of treebanks is chosen instead of UD in order to obtain results comparable to those of previous work on simulated zeroresource parsing scenarios (Ammar et al., 2016; Zhang and Barzilay, 2015; Rasooli and Collins, 2015). In our name tagging experiments, we use monolingual texts for Sinhalese (si) and Kinyarwanda (rw) provided by DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) Program during the 2018 Low Resource Human Languages Technologies (LoReHLT) evaluation. 3.2 While unrealistic for POS tagger performance purposes, many-to-one is a good choice for determining cluster ‘purity’ and provides a reasonable grounding upper bound. As the calculation of many-to-one does require labeled data, we constrain the use of these labels for development and will evaluate extrinsically using languages for"
N19-1252,P09-1057,0,0.134614,"agging pipeline can be interpreted as first reducing the vocabulary size to a fixed number of clusters, and then finding a cluster– POS tag mapping table that best explains the data without any path constraint (a cluster ID could be mapped to any POS tag). In this sense, our approach applies EM to simplify the task (e.g. when using Brown clustering (Brown et al., 1992)), followed by another EM run to optimize cipher table parameters. Under this lens, the methods closest to our approach are those which attempt to reduce or constrain the parameter search space prior to running EM. For instance, Ravi and Knight (2009) explicitly search for the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Hagh"
N19-1252,P11-1002,0,0.0987625,"odel. If we assume a deterministic pipelined clustering of words and a tag labeling model that does not depend on words, then for chosen cˆ, this becomes X argmax Pθ (p|c, w)Pθ (c|w) p c∈C |w| = argmax Pθ (p|ˆ c) p = argmax Pθ (ˆ c|p)Pθ (p) (1) p We call this model the cipher grounder. As presented it requires an estimate for Pθ (p) for the CL, which requires POS training data. Under the zero-resource scenario, we instead approximate Pθ (p) by the tag distribution of a PL. Then, the cipher table Pθ (ˆ c|p) can be trained using a noisy-channel, expectation-maximization (EM)based approach as in Ravi and Knight (2011). 3 POS Tagger construction We approach the search for optimal components in the two-step pipeline outlined in Section 2 in a cascaded manner. First, an optimal word clustering is determined by means of the many-to-one evaluation method. This method is explained well by Johnson (2007): “ ...deterministically map each hidden state to the POS tag it co-occurs most frequently with, and return the proportion of the resulting POS tags that are the same as the POS tags of the goldstandard corpus.” which we do not have any training data; see Section 5.2. Secondly, we search for the best approach to g"
N19-1252,P05-1044,0,0.108207,"the smallest model that explains the data using Integer Programming, and then use EM to set parameter values. In a different approach, Goldberg et al. (2008) obtain competitive performance with a classic HMM model by initializing the emission probability distribution with a mixture of language-specific, linguistically constrained distributions. However, both of these approaches are framed around the task of unsupervised POS disambiguation with a full dictionary (Merialdo, 1994). Previous work relaxes the full dictionary constraint by leveraging monolingual lexicons (Haghighi and Klein, 2006; Smith and Eisner, 2005; Merialdo, 1994; Ravi and Knight, 2009), multilingual tagged dictionaries (Li et al., 2012; Fang and Cohn, 2017), and parallel corpora (Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011). In addition, previous work includes sequence models that do not rely on any resource besides raw text during training, namely unsupervised POS induction models. These models are based, with few exceptions, on extensions to the standard HMM; most, in the form of appropriate priors over the HMM multinomial parameters (Goldwater and Griffiths, 2007; Johnson, 2007; Ganchev et al., 2009); others,"
N19-1252,P10-1107,0,0.0331855,"hristodoulopoulos et al., 2010), and we argue its inappropriateness for scenarios in which the test set is extremely small or even when no annotated reference corpus exists. Therefore, the problem of grounding the sequence of states or cluster IDs to POS tags without using any linguistic resource remains unsolved. We formulate this task as a decipherment problem. Decipherment aims to find a substitution table between alphabets or tokens of an encrypted code and a known language without the need of parallel corpora. The task has been successfully applied in alphabet mapping for lost languages (Snyder et al., 2010), and machine translation at the character (Pourdamghani and Knight, 2017) and token level (Ravi and Knight, 2011; Dou et al., 2015). For the task of POS tag grounding, the sequence of states or cluster IDs is modeled as an encrypted code to be deciphered back to a POS sequence. Furthermore, we tackle the problem from a ‘universal’ perspective by allowing the cipher learn from POS sequences from a varied pool of languages. Other recent work has declared a ‘radically universal’ mantra to language inclusivity. Hermjakob et al. (2018) presents a Romanizer that covers all writing systems known to"
N19-1252,K17-3009,0,0.0657579,"Missing"
N19-1252,Q16-1018,0,0.181264,"the model requires users to provide gold POS tags and word mappings from these languages into a common semantic space, using approaches that require parallel data (Guo et al., 2015). Indeed, the compulsion to use POS tag-labeled data in zero-resource circumstances extends to the vast, varied lines of research in unsupervised POS tagging itself! Every approach explored so far ultimately requires POS-annotated resources for the language being studied in order to produce a final, grounded output. Even the most conservative strategies (Goldwater and Griffiths, 2007; BergKirkpatrick et al., 2010; Stratos et al., 2016) that do not require any supervised signal during training still ultimately produce only ungrounded clusters, and require a reference annotated corpus to map the inferred clusters or states to actual POS tags. Making matters worse, evaluation is generally offered in terms of the ‘many-to-one’ or ‘one-toone’ analyses Johnson (2007). These metrics use a reference corpus to determine the optimal mapping of clusters to tags. While this evaluation approach is intuitively sensible for measuring cluster purity, to actually use such an output, an entire annotated training corpus is required.1 It is no"
N19-1252,Q13-1001,0,0.0574881,"Missing"
N19-1252,D15-1213,0,0.0201838,"(en), German (de), French (fr), Italian (it), Spanish (es), Japanese (ja), Czech (cs), Russian (ru), Arabic (ar), and Farsi (fa). For Swahili (sw), we use the Helsinki Corpus of Swahili 2.0.4 Overall in these experiments we cover 11 languages and 4 language families. In our dependency parsing experiments, we use the Universal Treebank v2.0 (McDonald et al., 2013) for en, de, fr, es, it, Portuguese (pt), and Swedish (sv). This set of treebanks is chosen instead of UD in order to obtain results comparable to those of previous work on simulated zeroresource parsing scenarios (Ammar et al., 2016; Zhang and Barzilay, 2015; Rasooli and Collins, 2015). In our name tagging experiments, we use monolingual texts for Sinhalese (si) and Kinyarwanda (rw) provided by DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) Program during the 2018 Low Resource Human Languages Technologies (LoReHLT) evaluation. 3.2 While unrealistic for POS tagger performance purposes, many-to-one is a good choice for determining cluster ‘purity’ and provides a reasonable grounding upper bound. As the calculation of many-to-one does require labeled data, we constrain the use of these labels for development and will evaluate extrin"
N19-1383,W16-1614,0,0.0301494,"16) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) f"
N19-1383,D18-1017,0,0.0222598,"age-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With word-level adversarial training, it can automatically project the source language i"
N19-1383,D18-1024,0,0.103202,"features from both t and s are further fed into a context encoder to better capture and refine contextual information and a conditional random field (CRF) (Lafferty et al., 2001) based name tagger. Next we show the details of each component in our architecture. 2.2 Word-level Adversarial Transfer To better leverage the resources from the source language, our first step is to construct a shared se3824 mantic space where the words from the source and target languages are semantically aligned. Without requiring any bilingual gazetteers, recent efforts (Zhang et al., 2017b; Conneau et al., 2017; Chen and Cardie, 2018) explore unsupervised approaches to learn cross-lingual word embeddings and achieve comparable performance to supervised methods. Following these studies, we perform word-level adversarial training to automatically align word representations from s and t. Formally, assume we are given pretrained monolingual word embeddings t t t N ×d t Vt = {v1 , v2 , ..., vN } ∈ R for t, and Vs = {vs1 , vs2 , ..., vsM } ∈ RM ×ds for s, where vti and vsj are the vector representations of words wit and wis from t and s, N and M denote the vocabulary sizes, dt and ds denote the embedding dimensionality of t and"
N19-1383,R11-1017,0,0.214416,"Missing"
N19-1383,W18-6125,0,0.156228,"et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word e"
N19-1383,K16-1018,0,0.0292479,"language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not on"
N19-1383,P17-2093,0,0.0225366,"g can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017;"
N19-1383,2005.mtsummit-papers.11,0,0.102348,"Missing"
N19-1383,I17-2072,0,0.0448683,"gual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With word-level adversarial training, it can automatically project the source language into a shared semantic space with the target language without requiring any comparable data or bilingual gazetteers. Moreover, considering the"
N19-1383,N16-1155,0,0.268781,"rce languages, the addition of English to the limited target language training data yields a considerably noisy corpus. However, by forcing the sequence encoder to extract language-agnostic features, our approach still achieves better performance than the monolingual baseline for most languages. All of these experiments demonstrate that our approach is more effective in leveraging annotations from other languages to improve target language name tagging. 3.5 Cross-lingual Transfer for High Resource Languages Language Dutch Spanish Model Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) F-score 81.74 85.19 85.71 82.84 85.87 86.43 86.87 Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) 85.75 85.77 85.02 82.95 85.92 85.84 86.41 Table 6: Comparison on cross-lingual transfer for Dutch and Spanish with various baselines: monolingual baseline (Lample et al. (2016)), multitask baselines (Yang et al. (2017) and Lin et al. (2018)), language universal representation baselines (Gillick et al. (2016), Word-Adv1 , Word-Adv2 ). We finally investigate the results when both the sour"
N19-1383,D18-1023,1,0.852801,"e first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversa"
N19-1383,N16-1030,0,0.138096,"glish is very different from these low-resource languages, the addition of English to the limited target language training data yields a considerably noisy corpus. However, by forcing the sequence encoder to extract language-agnostic features, our approach still achieves better performance than the monolingual baseline for most languages. All of these experiments demonstrate that our approach is more effective in leveraging annotations from other languages to improve target language name tagging. 3.5 Cross-lingual Transfer for High Resource Languages Language Dutch Spanish Model Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) F-score 81.74 85.19 85.71 82.84 85.87 86.43 86.87 Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) 85.75 85.77 85.02 82.95 85.92 85.84 86.41 Table 6: Comparison on cross-lingual transfer for Dutch and Spanish with various baselines: monolingual baseline (Lample et al. (2016)), multitask baselines (Yang et al. (2017) and Lin et al. (2018)), language universal representation baselines (Gillick et al. (2016), Word-Adv1 , Word-Ad"
N19-1383,P18-1074,1,0.845725,"artment, Rensselaer Polytechnic Institute {huangl7,jih}@rpi.edu 2 Information Sciences Institute, University of Southern California jonmay@isi.edu Abstract and Hagiwara, 2015), cross-lingual word embedding (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018), or cross-lingual Wikification (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), but these resources are still only available for dozens of languages. Recent efforts on multi-task learning model each language as one single task while all the tasks share the same encoding layer (Yang et al., 2016, 2017; Lin et al., 2018). These methods can transfer knowledge via the shared encoder without using bilingual resources. However, different languages usually have different underlying sequence structures, as shown in Figure 1. Without an explicit constraint, the encoder is not guaranteed to extract language-independent sequential features. Moreover, when the size of annotated resources is not balanced, the encoder is likely to be biased toward the resource-dominant language. We focus on improving name tagging for lowresource languages using annotations from related languages. Previous studies either directly project"
N19-1383,P16-1101,0,0.0759447,"statistics of these data sets. For fair comparison, we use the same pretrained word embeddings of English, Dutch and Spanish as Lin et al. (2018), while for each lowresource language we train their word embeddings using the documents from their LDC packages with FastText.6 Table 3 lists the key hyperparameters we used in our experiments. 3.2 Baselines We compare our methods with three categories of baseline methods:7 • Monolingual Name Tagging Using monolingual annotations only, the current state-of-theart name tagging model is the Bi-LSTM-CRF network (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016).8 • Multi-task Learning Lin et al. (2018) apply multi-task learning to boost name tagging performance by introducing additional annotations from source languages using a weight sharing context encoder across multiple languages. • Language Universal Representations We apply word adversarial transfer only to project the source language into the same semantic space as the target language, then train the name tagger on the annotations of source and target languages. Word-Adv1 refers to the approach which is directly trained on the combination of the anno4 The annotations are from: am (LDC2016E87)"
N19-1383,D17-1302,0,0.106268,"Missing"
N19-1383,P12-1073,0,0.0708481,"hiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding bet"
N19-1383,P17-1135,0,0.53374,"andom fields (Bi-LSTM-CRF) (Lample et al., 2016; Huang et al., 2015; Ma and 3823 Proceedings of NAACL-HLT 2019, pages 3823–3833 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Target Language BPER ... ... Sequence Feature Encoder Convolutional Neural Networks Word Discriminator IPER Sequence Discriminator Context Encoder O CRF Name Tagger ... O BGPE Linear Projection ... Source Language Figure 2: Architecture overview. Hovy, 2016), integrated with multi-level adversarial transfer: (1) word level adversarial transfer, similar to Conneau et al. (2017), applying a projection function on the source language and a discriminator to distinguish each word of the target language from that of the source language, resulting in a bilingual shared semantic space; (2) sentence-level adversarial transfer, where a discriminator is trained to distinguish each sentence of the target language from that of the source language,2 and a sequence encoder is applied to each sentence of both languages to prevent the discriminator from correctly predicting the source of each sentence, yielding language-agnostic sequential features. These features can better facili"
N19-1383,P17-1178,1,0.875125,"uistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensiv"
N19-1383,P17-1161,0,0.0332845,"0 0 0 0.2 0.4 0.6 0.8 1 Sample Rate of Source Training Data Figure 4: The impact of the size of annotations from source and target languages on Amharic name tagging. 4 Related Work Name tagging methods based on sequence labeling have been widely studied in recent years. Huang et al. (2015) and Lample et al. (2016) propose an effective Bi-LSTM-CRF architecture; the BiLSTM encodes previous and following contexts, and the CRF is used for tag prediction. Other studies incorporate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et a"
N19-1383,N18-1202,0,0.0481708,"any comparable data or bilingual gazetteers. Moreover, considering the different underlying sequential structures among various languages, we further design a sentence-level adversarial transfer to encourage the sequence encoder to extract language-agnostic features. The experiments show that our approach achieves the state-of-the-art on both CoNLL data sets and 10 low-resource languages. In the future, we will further explore selecting the feature-consistent annotations from the source language and add to the target language, and explore unsupervised pretrained cross-lingual language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Lample and Conneau, 2019) for cross-lingual low resource name tagging. Acknowledgments This research is based upon work supported in part by U.S. DARPA LORELEI Program # HR001115-C-0115, and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116, and ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied"
N19-1383,D18-1125,1,0.833352,"udies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With word-level adversarial training, it can automatically project the source language into a shared semantic space with the target language without requiring any comparable data or bilingual gazetteers. Moreover, considering the different underlying sequential structures"
N19-1383,W02-2024,0,0.510491,"Missing"
N19-1383,W03-0419,0,0.373808,"Missing"
N19-1383,K16-1022,0,0.0721972,"glish is very different from these low-resource languages, the addition of English to the limited target language training data yields a considerably noisy corpus. However, by forcing the sequence encoder to extract language-agnostic features, our approach still achieves better performance than the monolingual baseline for most languages. All of these experiments demonstrate that our approach is more effective in leveraging annotations from other languages to improve target language name tagging. 3.5 Cross-lingual Transfer for High Resource Languages Language Dutch Spanish Model Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) F-score 81.74 85.19 85.71 82.84 85.87 86.43 86.87 Lample et al. (2016) Yang et al. (2017) Lin et al. (2018) Gillick et al. (2016) Word-Adv1 Word-Adv2 Our Model (Bi-LSTM) 85.75 85.77 85.02 82.95 85.92 85.84 86.41 Table 6: Comparison on cross-lingual transfer for Dutch and Spanish with various baselines: monolingual baseline (Lample et al. (2016)), multitask baselines (Yang et al. (2017) and Lin et al. (2018)), language universal representation baselines (Gillick et al. (2016), Word-Adv1 , Word-Ad"
N19-1383,I17-2065,0,0.240629,"andom fields (Bi-LSTM-CRF) (Lample et al., 2016; Huang et al., 2015; Ma and 3823 Proceedings of NAACL-HLT 2019, pages 3823–3833 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Target Language BPER ... ... Sequence Feature Encoder Convolutional Neural Networks Word Discriminator IPER Sequence Discriminator Context Encoder O CRF Name Tagger ... O BGPE Linear Projection ... Source Language Figure 2: Architecture overview. Hovy, 2016), integrated with multi-level adversarial transfer: (1) word level adversarial transfer, similar to Conneau et al. (2017), applying a projection function on the source language and a discriminator to distinguish each word of the target language from that of the source language, resulting in a bilingual shared semantic space; (2) sentence-level adversarial transfer, where a discriminator is trained to distinguish each sentence of the target language from that of the source language,2 and a sequence encoder is applied to each sentence of both languages to prevent the discriminator from correctly predicting the source of each sentence, yielding language-agnostic sequential features. These features can better facili"
N19-1383,P13-1106,0,0.0297461,"global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Comp"
N19-1383,N15-1104,0,0.0139277,"gs t t t N ×d t Vt = {v1 , v2 , ..., vN } ∈ R for t, and Vs = {vs1 , vs2 , ..., vsM } ∈ RM ×ds for s, where vti and vsj are the vector representations of words wit and wis from t and s, N and M denote the vocabulary sizes, dt and ds denote the embedding dimensionality of t and s respectively. We then apply a mapping function f to project s into the same semantic space as t: e s = f (Vs ) = Vs U V (1) where U ∈ Rds ×dt is the transformation matrix. e s ∈ RM ×dt are the projected word embeddings V for s, and Θf = {θf } denotes the set of parameters to be optimized for f . Similar to Xing et al. (2015), Conneau et al. (2017), and Chen and Cardie (2018), we constrain the transformation matrix U to be orthogonal with singular value decomposition (SVD) to reduce the parameter search space: e s V⊤ U = AB⊤ , with AΣB⊤ = SVD(V s) (2) To automatically optimize the mapping function f without using extra bilingual signals, we introduce a multi-layer perceptron D as a word discriminator, which takes word embeddings of t and projected word embeddings of s as input features and outputs a single scalar. D(wi∗ ) represents the probability of wi∗ coming from t. The word discriminator is trained by minimiz"
N19-1383,H01-1035,0,0.148427,"ate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang"
N19-1383,I17-1037,1,0.902957,"Missing"
N19-1383,N16-1029,1,0.848295,"s from source and target languages on Amharic name tagging. 4 Related Work Name tagging methods based on sequence labeling have been widely studied in recent years. Huang et al. (2015) and Lample et al. (2016) propose an effective Bi-LSTM-CRF architecture; the BiLSTM encodes previous and following contexts, and the CRF is used for tag prediction. Other studies incorporate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipe"
N19-1383,K18-1009,1,0.889982,"Missing"
N19-1383,C16-1045,1,0.862131,"s from source and target languages on Amharic name tagging. 4 Related Work Name tagging methods based on sequence labeling have been widely studied in recent years. Huang et al. (2015) and Lample et al. (2016) propose an effective Bi-LSTM-CRF architecture; the BiLSTM encodes previous and following contexts, and the CRF is used for tag prediction. Other studies incorporate a character-level CNN (Ma and Hovy, 2016), global contexts (Zhang et al., 2018), or language models (Liu et al., 2018; Peters et al., 2017, 2018; Devlin et al., 2018) to improve name tagging. In addition, several approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipe"
N19-1383,P17-1179,0,0.068792,"Missing"
N19-1383,D17-1207,0,0.138743,"ame tagger The language-agnostic sequential features from both t and s are further fed into a context encoder to better capture and refine contextual information and a conditional random field (CRF) (Lafferty et al., 2001) based name tagger. Next we show the details of each component in our architecture. 2.2 Word-level Adversarial Transfer To better leverage the resources from the source language, our first step is to construct a shared se3824 mantic space where the words from the source and target languages are semantically aligned. Without requiring any bilingual gazetteers, recent efforts (Zhang et al., 2017b; Conneau et al., 2017; Chen and Cardie, 2018) explore unsupervised approaches to learn cross-lingual word embeddings and achieve comparable performance to supervised methods. Following these studies, we perform word-level adversarial training to automatically align word representations from s and t. Formally, assume we are given pretrained monolingual word embeddings t t t N ×d t Vt = {v1 , v2 , ..., vN } ∈ R for t, and Vs = {vs1 , vs2 , ..., vsM } ∈ RM ×ds for s, where vti and vsj are the vector representations of words wit and wis from t and s, N and M denote the vocabulary sizes, dt and d"
N19-1383,P15-2064,0,0.0685169,"eral approaches (Zhang et al., 2016a, 2017a; AlBadrashiny et al., 2017) attempt to incorporate hand-crafted linguistic features into a Bi-LSTMCRF to improve low-resource name tagging performance. Recent attempts on cross-lingual transfer for name tagging can be divided into two categories: the first projects annotations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-"
N19-1383,D18-1034,0,0.0714901,"otations from a source language to a target language via parallel corpora (Yarowsky et al., 2001; Wang and Manning, 2013; Wang et al., 2013; Zhang et al., 2016b; Fang and Cohn, 2016; Ehrmann et al., 2011; Enghoff et al., 2018; Ni et al., 2017), a bilingual gazetteer (Feng et al., 2017; Zirikly and Hagiwara, 2015), Wikipedia anchor links (Kim et al., 2012; Nothman et al., 2013; Tsai et al., 2016; Pan et al., 2017), and language universal representations, including Unicode bytes (Gillick et al., 2016) and cross-lingual word embeddings (Fang and Cohn, 2017; Wang et al., 2017; Huang et al., 2018; Xie et al., 2018). The second is based on multitask learning via a weight sharing encoder (Yang et al., 2016, 2017; Lin et al., 2018). Compared to these studies, our approach not only automatically learns cross-lingual word embeddings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to au"
N19-1383,C18-1037,0,0.022651,"dings without requiring any parallel resources, but also carefully extracts language-agnostic sequential features, yielding better performance. Adversarial training has also been extensively studied and applied for cross-lingual and crossdomain transfer. Several studies (Barone, 2016; Zhang et al., 2017c,b; Conneau et al., 2017; Chen and Cardie, 2018) explore adversarial training to automatically induce bilingual and multilingual word representations without using any parallel corpora or bilingual gazetteers. Adversarial training is also applied to extract languageagnostic (Chen et al., 2016; Zou et al., 2018; Wang and Pan, 2018; Kim et al., 2017a; Muis et al., 3830 2018; Cao et al., 2018) and domain-agnostic features (Kim et al., 2017b; Ganin et al., 2016; Tzeng et al., 2017; Chen et al., 2017; Li et al., 2017; Fu et al., 2017; Bousmalis et al., 2016; Shi et al., 2018) for cross-lingual and cross-domain adaptation. Compared with these methods, our approach combines both word-level and sentence-level adversarial training. 5 Conclusions and Future Work We design a new neural architecture which integrates multi-level adversarial transfer into a BiLSTM-CRF to improve low-resource name tagging. With w"
N19-4019,P14-5010,0,0.00631396,"Missing"
N19-4019,P17-1178,1,0.823224,", Movement, Business, Conflict, Contact, Manufacture, Personnel, Justice, Transaction, Government, Inspection, Existence Entity Table 1: Main types of knowledge elements Multilingual Knowledge Extraction not adopt the alternative approach of translating the source documents into English and then applying English knowledge extraction system due to the low-quality of state-of-the-art machine translation and word alignment for these two languages. The overall architecture of our multilingual knowledge extraction system is illustrated in Figure 1. The system performs entity discovery and linking (Pan et al., 2017; Lin et al., 2018), time expression extraction and normalization (Manning et al., 2014), relation extraction (Shi et al., 2018), event extraction (Zhang et al., 2017, 2019), and event coreference (Zhang et al., 2015). The system supports the extraction of 7 entity types, 23 relations, and 47 event types, as defined in the DARPA AIDA ontology.4 Table 1 shows the main types. For Russian and Ukrainian text input, we did 4 https://www.darpa.mil/program/ active-interpretation-of-disparate-alternatives 111 Once within-document knowledge elements for each language are extracted, the system performs"
N19-4019,I17-2072,0,0.0222216,"Missing"
N19-4019,D18-1125,1,0.8884,"Missing"
N19-4019,I17-1037,1,0.88047,"Missing"
N19-4019,D15-1020,1,0.85196,"the alternative approach of translating the source documents into English and then applying English knowledge extraction system due to the low-quality of state-of-the-art machine translation and word alignment for these two languages. The overall architecture of our multilingual knowledge extraction system is illustrated in Figure 1. The system performs entity discovery and linking (Pan et al., 2017; Lin et al., 2018), time expression extraction and normalization (Manning et al., 2014), relation extraction (Shi et al., 2018), event extraction (Zhang et al., 2017, 2019), and event coreference (Zhang et al., 2015). The system supports the extraction of 7 entity types, 23 relations, and 47 event types, as defined in the DARPA AIDA ontology.4 Table 1 shows the main types. For Russian and Ukrainian text input, we did 4 https://www.darpa.mil/program/ active-interpretation-of-disparate-alternatives 111 Once within-document knowledge elements for each language are extracted, the system performs cross-lingual entity linking to Wikipedia, crossdocument entity clustering for unlinkable mentions, and cross-document event coreference resolution for cross-lingual information fusion. Further details of each compone"
P05-1051,A97-1029,0,0.537895,"ities). There were 7 types of relations, with 23 subtypes. Examples of these relations are “the CEO of Microsoft” (an employ-exec relation), “Fred’s wife” (a family relation), and “a military base in Germany” (a located relation). In this paper we look at the problem of identifying name mentions in Chinese text and classifying them as persons, organizations, or GPEs. Because Chinese has neither capitalization nor overt word boundaries, it poses particular problems for name identification. Prior Work A wide variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same toke"
P05-1051,C02-1025,0,0.0421356,"variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token. Recently, in (Ji and Grishman, 2004) we proposed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upp"
P05-1051,N04-4010,0,0.0727642,"e rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upper bound on recall. Traditional statistical name tagging methods have generated a single name hypothesis. BBN proposed the N-Best algorithm for speech recognition in (Chow and Schwartz, 1989). Since then NBest methods have been widely used by other researchers (Collins, 2002; Zhai et al., 2004). In this paper, we tried to combine the advantages of the prior work, and incorporate broader knowledge into a more general re-ranking model. 412 Task and Terminology Our experiments were conducted in the context of the ACE Information Extraction evaluations, and we will use the terminology of these evaluations: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) name mention: a reference by name to an entity nominal mention: a reference by a common noun or noun phrase to an entity relation: one of a"
P05-1051,H89-2027,0,\N,Missing
P05-1051,W04-0705,1,\N,Missing
P05-1051,P02-1062,0,\N,Missing
P06-2055,A97-1029,0,0.199668,"best of the N hypotheses1, using different models: English MonoCase (EN-Mono, without capitalization), English Mixed Case (EN-Mix, with capitalization), Chinese without the usable character restriction (CH-NoRes) and Chinese with the usable character restriction (CH-WithRes). Baseline Name Tagger We apply a multi-lingual (English / Chinese) bigram HMM tagger to identify four named entity types: Person, Organization, GPE (‘geopolitical entities’ – locations which are also political units, such as countries, counties, and cities) and Location. The HMM tagger generally follows the Nymble model (Bikel et al, 1997), and uses best-first search to generate N-Best hypotheses for each input sentence. In mixed-case English texts, most proper names are capitalized. So capitalization provides a crucial clue for name boundaries. In contrast, a Chinese sentence is composed of a string of characters without any word boundaries or capitalization. Even after word segmentation there are still no obvious clues for the name boundaries. However, we can apply the following coarse “usable-character” restrictions to reduce the search space. Standard Chinese family names are generally single characters drawn from a set of"
P06-2055,P05-1051,1,0.935459,"human annotators. 1 Introduction High-performance named entity (NE) tagging is crucial in many natural language processing tasks, such as information extraction and machine translation. In &apos;traditional&apos; pipelined system architectures, NE tagging is one of the first steps in the pipeline. NE errors adversely affect subsequent stages, and error rates are often compounded by later stages. However, (Roth and Yi 2002, 2004) and our recent work have focused on incorporating richer linguistic analysis, using the “feedback” from later stages to improve name taggers. We expanded our last year’s model (Ji and Grishman, 2005) that used the results of coreference analysis and relation extraction, by adding ‘feedback’ from more information extraction components – name structure parsing, cross-document coreference, and event extraction – to incrementally rerank the multiple hypotheses from a baseline name tagger. While together these components produced a further improvement on last year’s model, our goal in this paper is to look behind the overall performance figures in order to understand how these varied components contribute to the improvement, and compare the remaining system errors with the human annotator’s pe"
P06-2055,W06-3607,1,0.87638,"since these names are not tagged in the key -- the automatic scorer treats them as spurious names. Sentence Document Boundary Boundary Name Local Related Event Coreferring Candidate Context Mention trigger&arg Mentions Mutual Inferences between Information Extraction Stages Name tagging is typically one of the first stages 2 in an information extraction pipeline. Specifically, we will consider a system which was developed for the ACE (Automatic Content Extraction) task 3 and includes the following stages: name structure parsing, coreference, semantic relation extraction and event extraction (Ji et al., 2006). All these stages are performed after name tagging since they take names as input “objects”. However, the inferences from these subsequent stages can also provide valuable constraints to identify and classify names. Each of these stages connects the name candidate to other linguistic elements in the sentence, document, or corpus, as shown in Figure 3. The ACE task description can be found at http://www.itl.nist.gov/iad/894.01/tests/ace/ and the ACE guidelines at http://www.ldc.upenn.edu/Projects/ACE/ 4 Rather than offer the most fluent translation, we have provided one that more closely corre"
P06-2055,W04-2401,0,0.0574269,"Missing"
P06-2055,C02-1151,0,0.230639,"Missing"
P06-2055,N04-4010,0,0.0599371,"Missing"
P06-2055,J05-4005,0,\N,Missing
P08-1030,W06-0901,0,0.110363,"“Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve"
P08-1030,N07-1042,0,0.0638666,"ency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use informa260 tion retrieval techniques to obtain wider background knowledge from related documents. 8 Conclusion and Future Work One of the initial goals for IE was to create a database of relations and events from the entire input corpus, and allow further log"
P08-1030,D07-1075,0,0.0405993,"single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use informa260 tion retrieval techniques to obtain wider background knowledge from related documents. 8 Conclusion and Future Work One of the initial goals for IE was to create a database of relations and events from the entire input corpus, and allow further logical reasoning on the database. The artificial constraint that extraction should be done independently for each document was introduced in part to simplify the task and its"
P08-1030,H05-1008,0,0.0494685,"ic annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use informa260 tion retrieval techniques to obtain wider background knowledge from related do"
P08-1030,P95-1026,0,0.150088,"fact, compared to a statistical tagger trained on the corpus after expert adjudication, a human annotator tends to make more mistakes in trigger classification. For example it’s hard to decide whether “named” represents a “Personnel_Nominate” or “Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific infere"
P09-1048,P98-1013,0,0.0125282,"Missing"
P09-1048,W05-0620,0,0.0201316,"Missing"
P09-1048,P01-1017,0,0.0402229,"Missing"
P09-1048,W03-1006,0,0.020803,"Missing"
P09-1048,erk-pado-2006-shalmaneser,0,0.021266,"Missing"
P09-1048,J02-3001,0,0.10105,"Missing"
P09-1048,P02-1031,0,0.0346015,"Missing"
P09-1048,P07-1098,0,0.019224,"Missing"
P09-1048,N07-1051,0,0.0240934,"Missing"
P09-1048,C04-1127,1,0.822126,"Missing"
P09-1048,N04-1032,0,0.0335295,"Missing"
P09-1048,W05-0623,0,0.0539303,"Missing"
P09-1048,W04-3212,0,0.049316,"Missing"
P09-1048,W01-1511,1,0.848003,"Missing"
P09-1048,D07-1077,0,0.0941353,"Missing"
P09-1048,N10-1005,1,\N,Missing
P09-1048,C98-1013,0,\N,Missing
P09-2093,W06-3913,0,0.0609652,"Missing"
P09-2093,P07-2044,0,0.0136606,"to ACE event extraction, we exclude generic, negative, and hypothetical events. event mention: a phrase or sentence within which an event is described. event argument: an entity involved in an event with some specific role. event time: an exact date normalized from time expressions and a role to indicate that an event occurs before/after/within the date. Introduction Event time argument detection is important to many NLP applications such as textual inference (Baral et al., 2005), multi-document text summarization (e.g. Barzilay e al., 2002), temporal event linking (e.g. Bethard et al., 2007; Chambers et al., 2007; Ji and Chen, 2009) and template based question answering (Ahn et al., 2006). It’s a challenging task in particular because about half of the event instances don’t include explicit time arguments. Various methods have been exploited to identify or infer the implicit time arguments (e.g. Filatova and Hovy, 2001; Mani et al., 2003; Lapata and Lascarides, 2006; Eidelman, 2008). Most of the prior work focused on the sentence level by clustering sentences into topics and ordering sentences on a time line. However, many sentences in news articles include multiple events with different time argument"
P09-2093,P08-3003,0,0.0711009,"gument detection is important to many NLP applications such as textual inference (Baral et al., 2005), multi-document text summarization (e.g. Barzilay e al., 2002), temporal event linking (e.g. Bethard et al., 2007; Chambers et al., 2007; Ji and Chen, 2009) and template based question answering (Ahn et al., 2006). It’s a challenging task in particular because about half of the event instances don’t include explicit time arguments. Various methods have been exploited to identify or infer the implicit time arguments (e.g. Filatova and Hovy, 2001; Mani et al., 2003; Lapata and Lascarides, 2006; Eidelman, 2008). Most of the prior work focused on the sentence level by clustering sentences into topics and ordering sentences on a time line. However, many sentences in news articles include multiple events with different time arguments. And it was not clear how the errors of topic clustering techniques affected the inference scheme. Therefore it will be valuable to design inference methods for more fine-grained events. In addition, in the previous approaches the linguistic evidences such as verb tense were mainly applied for inferring the exact dates of implicit time expressions. In this paper we are int"
P09-2093,W01-1313,0,0.0104589,"an event occurs before/after/within the date. Introduction Event time argument detection is important to many NLP applications such as textual inference (Baral et al., 2005), multi-document text summarization (e.g. Barzilay e al., 2002), temporal event linking (e.g. Bethard et al., 2007; Chambers et al., 2007; Ji and Chen, 2009) and template based question answering (Ahn et al., 2006). It’s a challenging task in particular because about half of the event instances don’t include explicit time arguments. Various methods have been exploited to identify or infer the implicit time arguments (e.g. Filatova and Hovy, 2001; Mani et al., 2003; Lapata and Lascarides, 2006; Eidelman, 2008). Most of the prior work focused on the sentence level by clustering sentences into topics and ordering sentences on a time line. However, many sentences in news articles include multiple events with different time arguments. And it was not clear how the errors of topic clustering techniques affected the inference scheme. Therefore it will be valuable to design inference methods for more fine-grained events. In addition, in the previous approaches the linguistic evidences such as verb tense were mainly applied for inferring the e"
P09-2093,N09-5001,1,0.78803,", we exclude generic, negative, and hypothetical events. event mention: a phrase or sentence within which an event is described. event argument: an entity involved in an event with some specific role. event time: an exact date normalized from time expressions and a role to indicate that an event occurs before/after/within the date. Introduction Event time argument detection is important to many NLP applications such as textual inference (Baral et al., 2005), multi-document text summarization (e.g. Barzilay e al., 2002), temporal event linking (e.g. Bethard et al., 2007; Chambers et al., 2007; Ji and Chen, 2009) and template based question answering (Ahn et al., 2006). It’s a challenging task in particular because about half of the event instances don’t include explicit time arguments. Various methods have been exploited to identify or infer the implicit time arguments (e.g. Filatova and Hovy, 2001; Mani et al., 2003; Lapata and Lascarides, 2006; Eidelman, 2008). Most of the prior work focused on the sentence level by clustering sentences into topics and ordering sentences on a time line. However, many sentences in news articles include multiple events with different time arguments. And it was not cl"
P09-2093,P06-1095,0,0.0889437,"e. Introduction Event time argument detection is important to many NLP applications such as textual inference (Baral et al., 2005), multi-document text summarization (e.g. Barzilay e al., 2002), temporal event linking (e.g. Bethard et al., 2007; Chambers et al., 2007; Ji and Chen, 2009) and template based question answering (Ahn et al., 2006). It’s a challenging task in particular because about half of the event instances don’t include explicit time arguments. Various methods have been exploited to identify or infer the implicit time arguments (e.g. Filatova and Hovy, 2001; Mani et al., 2003; Lapata and Lascarides, 2006; Eidelman, 2008). Most of the prior work focused on the sentence level by clustering sentences into topics and ordering sentences on a time line. However, many sentences in news articles include multiple events with different time arguments. And it was not clear how the errors of topic clustering techniques affected the inference scheme. Therefore it will be valuable to design inference methods for more fine-grained events. In addition, in the previous approaches the linguistic evidences such as verb tense were mainly applied for inferring the exact dates of implicit time expressions. In this"
P09-2093,N03-2019,0,0.0728785,"fter/within the date. Introduction Event time argument detection is important to many NLP applications such as textual inference (Baral et al., 2005), multi-document text summarization (e.g. Barzilay e al., 2002), temporal event linking (e.g. Bethard et al., 2007; Chambers et al., 2007; Ji and Chen, 2009) and template based question answering (Ahn et al., 2006). It’s a challenging task in particular because about half of the event instances don’t include explicit time arguments. Various methods have been exploited to identify or infer the implicit time arguments (e.g. Filatova and Hovy, 2001; Mani et al., 2003; Lapata and Lascarides, 2006; Eidelman, 2008). Most of the prior work focused on the sentence level by clustering sentences into topics and ordering sentences on a time line. However, many sentences in news articles include multiple events with different time arguments. And it was not clear how the errors of topic clustering techniques affected the inference scheme. Therefore it will be valuable to design inference methods for more fine-grained events. In addition, in the previous approaches the linguistic evidences such as verb tense were mainly applied for inferring the exact dates of impli"
P11-1115,S07-1012,0,0.00903138,"Missing"
P11-1115,D08-1029,0,0.0318388,"Missing"
P11-1115,C10-1032,0,0.358436,"Missing"
P11-1115,mcnamee-etal-2010-evaluation,0,0.0214604,"Missing"
P11-1115,I08-2112,0,0.00582762,"y the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly in conflict, coming from these documents. Furthermore, we may want to use the extracted information to augment an existing data base. This requires the ability to link individuals mentioned in a document, and information about these individuals, to entries in the data base. On the other hand, traditional Question Answering (QA) evaluations made limited efforts at disambiguating entities in queries (e.g. Pizzato et al., 2006), and limited use of relation/event extraction in answer search (e.g. McNamee et al., 2008). The Knowledge Base Population (KBP) shared task, conducted as part of the NIST Text Analysis Conference, aims to address and evaluate these capabilities, and bridge the IE and QA communities to promote research in discovering facts about entities and expanding a knowledge base with these facts. KBP is done through two separate subtasks, Entity Linking and Slot Filling; in 2010, 23 teams submitted results for one or both sub-tasks. A variety of approaches have been proposed to address both tasks with considerable success; nevertheless, there are many aspects of the task that remain unclear. W"
P11-1115,U06-1013,0,0.0182869,"s scattered among the documents of a large collection. This requires the ability to identify the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly in conflict, coming from these documents. Furthermore, we may want to use the extracted information to augment an existing data base. This requires the ability to link individuals mentioned in a document, and information about these individuals, to entries in the data base. On the other hand, traditional Question Answering (QA) evaluations made limited efforts at disambiguating entities in queries (e.g. Pizzato et al., 2006), and limited use of relation/event extraction in answer search (e.g. McNamee et al., 2008). The Knowledge Base Population (KBP) shared task, conducted as part of the NIST Text Analysis Conference, aims to address and evaluate these capabilities, and bridge the IE and QA communities to promote research in discovering facts about entities and expanding a knowledge base with these facts. KBP is done through two separate subtasks, Entity Linking and Slot Filling; in 2010, 23 teams submitted results for one or both sub-tasks. A variety of approaches have been proposed to address both tasks with co"
P11-1115,P06-1135,0,0.0384318,"Missing"
P11-1115,D10-1033,0,\N,Missing
P13-1008,W06-0901,0,0.87674,"he application of early-update in this work. To further investigate the difference between early-update and standardupdate, we tested the performance of both strategies, which is summarized in Table 5. As we can see the performance of standard-update is generally worse than early-update. When the beam size is increased (b = 4), the gap becomes smaller as the ratio of invalid updates is reduced. 4.6 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. Overall performance Table 6 shows the overall performance on the blind test set. In addition to our baseline, we compare against the sentence-level system reported in Hong et al. (2011), which, to the best of our knowledge, 80 Methods Sentence-level in Hong et al. (2011) Staged MaxEnt classifiers Joint w/ local features Joint w/ local + global features Cross-entity"
P13-1008,J92-4003,0,0.194657,"a simple example of global features:   1 if yg(i) = Attack and f101 (x, i, k, y) = y has only one “Attacker”   0 otherwise Category Type Lexical Trigger Syntactic Entity Information Basic Argument Syntactic Feature Description 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun 10. unigrams/bigrams normalized by entity types 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subtype and entity role if"
P13-1008,P10-1081,0,0.844465,"look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier deci73 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lingu"
P13-1008,N09-2053,1,0.817299,"this work. To further investigate the difference between early-update and standardupdate, we tested the performance of both strategies, which is summarized in Table 5. As we can see the performance of standard-update is generally worse than early-update. When the beam size is increased (b = 4), the gap becomes smaller as the ratio of invalid updates is reduced. 4.6 5 Related Work Most recent studies about ACE event extraction rely on staged pipeline which consists of separate local classifiers for trigger labeling and argument labeling (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012a; Chen and Ng, 2012). To the best of our knowledge, our work is the first attempt to jointly model these two ACE event subtasks. Overall performance Table 6 shows the overall performance on the blind test set. In addition to our baseline, we compare against the sentence-level system reported in Hong et al. (2011), which, to the best of our knowledge, 80 Methods Sentence-level in Hong et al. (2011) Staged MaxEnt classifiers Joint w/ local features Joint w/ local + global features Cross-entity in Hong et al. (2011)† Trigger Identifica"
P13-1008,P11-1163,0,0.323068,"Missing"
P13-1008,C12-1033,0,0.474159,"Missing"
P13-1008,N04-1043,0,0.0143919,"global features:   1 if yg(i) = Attack and f101 (x, i, k, y) = y has only one “Attacker”   0 otherwise Category Type Lexical Trigger Syntactic Entity Information Basic Argument Syntactic Feature Description 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun 10. unigrams/bigrams normalized by entity types 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subtype and entity role if it is a geo-political"
P13-1008,P04-1015,0,0.530784,"tel. Die Attack Figure 1: Event mentions of example (1). There are two event mentions that share three arguments, namely the Die event mention triggered by “died”, and the Attack event mention triggered by “fired”. arbitrary global features over multiple local predictions. However, different from easier tasks such as part-of-speech tagging or noun phrase chunking where efficient dynamic programming decoding is feasible, here exact joint inference is intractable. Therefore we employ beam search in decoding, and train the model using the early-update perceptron variant tailored for beam search (Collins and Roark, 2004; Huang et al., 2012). We make the following contributions: • Event mention: an occurrence of an event with a particular type and subtype. • Event trigger: the word most clearly expresses the event mention. • Event argument: an entity mention, temporal expression or value (e.g. Job-Title) that serves as a participant or attribute with a specific role in an event mention. • Event mention: an instance that includes one event trigger and some arguments that appear within the same sentence. 1. Different from traditional pipeline approach, we present a novel framework for sentencelevel event extrac"
P13-1008,D09-1016,0,0.471219,"Missing"
P13-1008,W02-1001,0,0.671658,", we can propagate the Victim argument of the Die event to the Target argument of the Attack event. As another example, knowing that an Attack event usually only has one Attacker argument, we could penalize assignments in which one trigger has more than one Attacker. Such global features cannot be easily exploited by a local classifier. Therefore, we take a fresh look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common d"
P13-1008,D11-1001,0,0.0273916,"Missing"
P13-1008,de-marneffe-etal-2006-generating,0,0.0254867,"Missing"
P13-1008,W11-1807,0,0.015807,"Missing"
P13-1008,W09-1406,0,0.0844449,"Missing"
P13-1008,P11-1113,0,0.809091,"formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier deci73 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Place Target"
P13-1008,N12-1015,1,0.772036,"Missing"
P13-1008,P11-1053,0,0.0118276,"1 if yg(i) = Attack and f101 (x, i, k, y) = y has only one “Attacker”   0 otherwise Category Type Lexical Trigger Syntactic Entity Information Basic Argument Syntactic Feature Description 1. unigrams/bigrams of the current and context words within the window of size 2 2. unigrams/bigrams of part-of-speech tags of the current and context words within the window of size 2 3. lemma and synonyms of the current token 4. base form of the current token extracted from Nomlex (Macleod et al., 1998) 5. Brown clusters that are learned from ACE English corpus (Brown et al., 1992; Miller et al., 2004; Sun et al., 2011). We used the clusters with prefixes of length 13, 16 and 20 for each token. 6. dependent and governor words of the current token 7. dependency types associated the current token 8. whether the current token is a modifier of job title 9. whether the current token is a non-referential pronoun 10. unigrams/bigrams normalized by entity types 11. dependency features normalized by entity types 12. nearest entity type and string in the sentence/clause 1. context words of the entity mention 2. trigger word and subtype 3. entity type, subtype and entity role if it is a geo-political entity mention 4."
P13-1008,P08-1030,1,0.505369,"efore, we take a fresh look at this problem and formulate it, for the first time, as a structured learning problem. We propose a novel joint event extraction algorithm to predict the triggers and arguments simultaneously, and use the structured perceptron (Collins, 2002) to train the joint model. This way we can capture the dependencies between triggers and argument as well as explore Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Most state-of-the-art approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) use sequential pipelines as building blocks, which break down the whole task into separate subtasks, such as trigger identification/classification and argument identification/classification. As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream classifiers. The downstream components, however, cannot impact earlier deci73 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82, c Sofia, Bulgaria, August 4-9 2013. 2013 Associatio"
P13-1008,D12-1092,0,0.173965,"Missing"
P13-1024,S12-1051,1,0.377412,"Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics believe many NLP tasks will benefit from this task. In fact, in the topic modeling research, previous work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent"
P13-1024,P11-1040,0,0.0373227,"vated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task. 1 Pray for Mali... A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al., 2011), and thus fails to shed light on the users focus/interests. To enable the NLP tools to better understand Twitter feeds, we propose the task of linking a tweet to a news article that is relevant to the tweet, thereby augmenting the context of the tweet. For example, we want to supplement the implicit context of the above tweet with a news article such as the following entitled: State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system. To create a gold standard dataset, we download tweets spanning over 18 days, each with a url"
P13-1024,C10-1034,0,0.0180773,"Missing"
P13-1024,P12-2028,1,0.697599,"ious work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) by a large margin. We employ it as a strong baseline in this task as it exploits and effectively models the missing words in a tw"
P13-1024,P12-1091,1,0.822339,"ious work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) by a large margin. We employ it as a strong baseline in this task as it exploits and effectively models the missing words in a tw"
P13-1024,S12-1086,1,0.625346,"ious work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) by a large margin. We employ it as a strong baseline in this task as it exploits and effectively models the missing words in a tw"
P13-1024,N13-1089,1,0.859083,"Missing"
P13-1024,P04-1035,0,0.00450094,"Missing"
P13-1024,P05-1015,0,0.0718262,"Missing"
P13-1024,D09-1026,0,0.0355149,"on cannot be efficiently exploited. Guo and Diab (2012b; 2012a; 2013) show the superiority of the latent space approach in the WTMF model achieving state-of-the-art performance on two datasets. However, all of them only reply on text-to-word information. In this paper, we focus on modeling inter-text relations induced by Twitter/news features. We extend the WTMF model and adapt it into tweets modeling, achieving significantly better results. Modeling Tweets in a Latent Space: Ramage et al. (2010) also use hashtags to improve the latent representation of tweets in a LDA framework, Labeled-LDA (Ramage et al., 2009), treating each hashtag as a label. Similar to the experiments presented in this paper, the result of using LabeledLDA alone is worse than the IR model, due to the sparseness in the induced LDA latent vector. Jin et al. (2011) apply an LDA based model on clustering by incorporating url referred documents. The semantics of long documents are transferred to the topic distribution of tweets. News recommendation: A news recommendation system aims to recommend news articles to a user based on the features (e.g., key words, tags, category) in the documents that the user likes (hence these documents"
P13-1024,P08-1030,1,0.806816,"le model that models the inter short text correlations (text-to-text information). This is motivated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task. 1 Pray for Mali... A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al., 2011), and thus fails to shed light on the users focus/interests. To enable the NLP tools to better understand Twitter feeds, we propose the task of linking a tweet to a news article that is relevant to the tweet, thereby augmenting the context of the tweet. For example, we want to supplement the implicit context of the above tweet with a news article such as the following entitled: State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system."
P13-1024,P11-1016,0,0.0154431,"State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system. To create a gold standard dataset, we download tweets spanning over 18 days, each with a url linking to a news article of CNN or NYTIMES, as well as all the news of CNN and NYTIMES published during the period. The goal is to predict the url referred news article based on the text in each tweet.1 We Introduction Recently there has been an increasing interest in language understanding of Twitter messages. Researchers (Speriosui et al., 2011; Brody and Diakopoulos, 2011; Jiang et al., 2011) were in1 The data and code is publicly available at www.cs. 239 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 239–249, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics believe many NLP tasks will benefit from this task. In fact, in the topic modeling research, previous work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our data"
P13-1024,W11-2207,0,0.0239471,"with a news article such as the following entitled: State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system. To create a gold standard dataset, we download tweets spanning over 18 days, each with a url linking to a news article of CNN or NYTIMES, as well as all the news of CNN and NYTIMES published during the period. The goal is to predict the url referred news article based on the text in each tweet.1 We Introduction Recently there has been an increasing interest in language understanding of Twitter messages. Researchers (Speriosui et al., 2011; Brody and Diakopoulos, 2011; Jiang et al., 2011) were in1 The data and code is publicly available at www.cs. 239 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 239–249, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics believe many NLP tasks will benefit from this task. In fact, in the topic modeling research, previous work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of"
P13-1024,Y12-1013,1,0.865303,"Missing"
P13-1024,P11-1015,0,0.108427,"Missing"
P13-1024,N03-1033,0,0.0334268,"uring automatic NER accumulates much faster given the large number of named entities in news data. Therefore we only extract temporal relations for news articles. 2 Note that there are some false positive named entities detected such as apple. We plan to address removing noisy named entities and hashtags in future work 242 5 WTMF on Graphs treated as a document), sense definitions of Wiktionary and Wordnet (Fellbaum, 1998). The tweets and news articles are also included in the corpus, generating 441,258 short texts and 5,149,122 words. The data is tokenized, POS-tagged by Stanford POS tagger (Toutanova et al., 2003), and lemmatized by WordNet::QueryData.pm. The value of each word in matrix X is its TF-IDF value in the short text. Baselines: We present 4 baselines: 1. Information Retrieval model [IR], which simply treats a tweet as a document, and performs traditional surface word matching. 2. LDA-θ with Gibbs Sampling as inference method. We use the inferred topic distribution θ as a latent vector to represent the tweet/news. 3. LDA-wvec. The problem with LDA-θ is the inferred topic distribution latent vector is very sparse with only a few non-zero values, resulting in many tweet/news pairs receiving a h"
P13-1024,P12-1054,0,0.0172256,"imilar than those are not chronologically close (Wang and McCallum, 2006). However, we cannot simply assume any two tweets are similar only based on the timestamp. Therefore, for a tweet we link it to the k most similar tweets whose published time is within 24 hours of the target tweet’s timestamp. We use the similarity score returned by WTMF model to measure the similarity of two tweets. We experimented with other features such as authorship. We note that it was not a helpful feature. While authorship information helps in the task of news/tweets recommendation for a user (Corso et al., 2005; Yan et al., 2012), the authorship information is too general for this task where we target on “recommending” a news article for a tweet. where λ is a regularization term. 4 Creating Text-to-text Relations via Twitter/News Features WTMF exploits the text-to-word information in a very nuanced way, while the dependency between texts is ignored. In this Section, we introduce how to create text-to-text relations. 4.1 Temporal Relations Hashtags and Named Entities Hashtags highlight the topics in tweets, e.g., The #flu season has started. We believe two tweets sharing the same hashtag should be related, hence we pla"
P13-1024,P11-1037,0,\N,Missing
P13-1024,D11-1052,0,\N,Missing
P13-1024,C12-1076,1,\N,Missing
P13-1059,P02-1051,0,0.0444271,"Missing"
P13-1059,W05-0808,0,0.0474066,"Missing"
P13-1059,W03-2201,0,0.104823,"Missing"
P13-1059,W12-4204,0,0.0372498,"Missing"
P13-1059,2007.tmi-papers.6,0,0.0334329,"Missing"
P13-1059,D07-1007,0,0.0684312,"Missing"
P13-1059,C12-1083,0,0.0297948,"Missing"
P13-1059,P05-1033,0,0.0224596,"nment and grammar extraction (Section 3.1). 2. Tightly integrate name tagging and translation into MT decoding via name-aware grammar (Section 3.2). 3. Optimize name translation and context translation simultaneously and conduct name translation driven decoding with language model (LM) based selection (Section 3.2). 4. Propose a new MT evaluation metric which can discriminate names and non-informative words (Section 4). 2 Baseline MT As our baseline, we apply a high-performing Chinese-English MT system (Zheng, 2008; Zheng et al., 2009) based on hierarchical phrase-based translation framework (Chiang, 2005). It is based on a weighted synchronous context-free grammar (SCFG). All SCFG rules are associated with a set of features that are used to compute derivation probabilities. The features include: 3 • Relative frequency in two directions P (γ|α) and P (α|γ), estimating the likelihoods of one side of the rule r: X →&lt; γ, α > translating into the other side, where γ and α are strings of terminals and non-terminals in the source side and target side. Non-terminals in γ and α are in one-to-one correspondence. • Lexical weights in two directions: Pw (γ|α) and Pw (α|γ), estimating likelihoods of words"
P13-1059,D07-1025,0,0.0344239,"Missing"
P13-1059,J98-4003,0,0.158543,"Missing"
P13-1059,P08-1115,0,0.0920713,"Missing"
P13-1059,N03-1017,0,0.00567209,"SCFG rules are associated with a set of features that are used to compute derivation probabilities. The features include: 3 • Relative frequency in two directions P (γ|α) and P (α|γ), estimating the likelihoods of one side of the rule r: X →&lt; γ, α > translating into the other side, where γ and α are strings of terminals and non-terminals in the source side and target side. Non-terminals in γ and α are in one-to-one correspondence. • Lexical weights in two directions: Pw (γ|α) and Pw (α|γ), estimating likelihoods of words in one side of the rule r: X →&lt; γ, α > translating into the other side (Koehn et al., 2003). • Phrase penalty: a penalty exp(1) for a rule with no non-terminal being used in derivation. • Rule penalty: a penalty exp(1) for a rule with at least one non-terminal being used in derivation. • Glue rule penalty: a penalty exp(1) if a glue rule used in derivation. • Translation length: number of words in translation output. Name-aware MT We tightly integrate name processing into the above baseline to construct a NAMT model. Figure 1 depicts the general procedure. 3.1 Training This basic training process of NAMT requires us to apply a bilingual name tagger to annotate parallel training corp"
P13-1059,W04-3248,0,0.34534,"Missing"
P13-1059,Y04-1019,0,0.0313731,"Missing"
P13-1059,P06-1060,0,0.0490941,"Missing"
P13-1059,P98-1069,0,0.260538,"Missing"
P13-1059,W12-3129,0,0.0345205,"Missing"
P13-1059,P08-1045,0,0.22479,"Translation (BOLT) program and National Institute of Standards and Technology (NIST) MT evaluations. The training corpus includes 1,686,458 sentence pairs. The joint name tagger extracted 1,890,335 name pairs (295,087 Persons, 1,269,056 Geopolitical entities and 326,192 Organizations). Four LMs, denoted LM1, LM2, LM3, and LM4, were trained from different English corpora. LM1 is a 7-gram LM trained on the tarOverall Performance Besides the new name-aware MT metric, we also adopt two traditional metrics, TER to evaluate the overall translation performance and Named Entity Weak Accuracy (NEWA) (Hermjakob et al., 2008) to evaluate the name translation performance. TER measures the amount of edits required to change a system output into one of the reference translations. Specifically: TER = # of edits average # of reference words (10) Possible edits include insertion, substitution deletion and shifts of words. The NEWA metric is defined as follows. Using a manually assembled name variant table, we also support the matching of name variants (e.g., “World Health Organization” and “WHO”). NEWA = 608 Count # of correctly translated names Count # of names in references (11) Corpus Genre Sentence # BOLT 1 BOLT 2 B"
P13-1059,Y06-1018,0,0.0269992,"Missing"
P13-1059,N04-1036,0,0.0899779,"Missing"
P13-1059,P09-2084,0,0.0697914,"Missing"
P13-1059,P06-2055,1,0.868806,"Missing"
P13-1059,I11-1029,0,0.0217614,"etically, semantically, or a mixture of both (Ji et al., 2009). • The artificial settings of assigning low weights to information translation (compared to overall word translation) in some largescale government evaluations have discouraged MT developers to spend time and explore resources to tackle this problem. Introduction A shrinking fraction of the world’s Web pages are written in English, therefore the ability to access pages across a range of languages is becoming increasingly important. This need can be addressed in part by cross-lingual information access tasks such as entity linking (McNamee et al., 2011; Cassidy et al., 2012), event extraction (Hakkani-Tur et al., 2007), slot filling (Snover et al., 2011) and question answering (Parton et al., 2009; Parton and McKeown, 2010). A key bottleneck of highquality cross-lingual information access lies in the performance of Machine Translation (MT). Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information, especially names. We propose a novel Name-aware MT (NAMT) approach which can tightly integrate name processing into the"
P13-1059,W11-1010,0,0.0496334,"Missing"
P13-1059,C10-2104,0,0.0351684,"Missing"
P13-1059,W09-3107,1,0.880182,"Missing"
P13-1059,J03-1002,0,0.00729466,"Missing"
P13-1059,2005.iwslt-1.20,0,0.0655874,"Missing"
P13-1059,P03-1021,0,0.0119875,"614, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics corpora (details described in section 5.1), with the LM weights optimized on a development set and determined by minimum error rate training (MERT), to estimate the probability of a word given the preceding words. All four LMs were trained using modified Kneser-Ney smoothing algorithm (Chen and Goodman, 1996) and converted into Bloom filter LMs (Talbot and Brants, 2008) supporting memory map. The scaling factors for all features are optimized by minimum error rate training algorithm to maximize BLEU score (Och, 2003). Given an input sentence in the source language, translation into the target language is cast as a search problem, where the goal is to find the highest-probability derivation that generates the source-side sentence, using the rules in our SCFG. The source-side derivation corresponds to a synchronous targetside derivation and the terminal yield of this targetside derivation is the output of the system. We employ our CKY-style chart decoder, named SRInterp, to solve the search problem. names in parallel corpora, updating word segmentation, word alignment and grammar extraction (Section 3.1). 2"
P13-1059,P02-1040,0,0.0876484,"single reference translation. The total count of each candidate n-gram is clipped at sentence level by its maximum reference count. Then we add up the weights of clipped n-grams and divide them by the total weight of all n-grams. Based on BLEU score, we design a name-aware BLEU metric as follows. Depending on whether a token t is contained in a name in reference translation, we assign a weight weightt to t as follows: weightt = ( 1 − e−tf (t,d)·idf (t,D) , if t never appears in names (4) 1 + PZE , if t occurs in name(s) Name-aware MT Evaluation Traditional MT evaluation metrics such as BLEU (Papineni et al., 2002) and Translation Edit Rate (TER) (Snover et al., 2006) assign the same weights to all tokens equally. For example, incorrect translations of “the” and “Bush” will receive the same penalty. However, for crosslingual information processing applications, we should acknowledge that certain informationally critical words are more important than other common words. In order to properly evaluate the translation quality of NAMT methods, we propose to modify the BLEU metric so that they can dynamically assign more weights to names during evaluation. BLEU considers the correspondence between a system tr"
P13-1059,C10-2109,0,0.0179213,"anslation) in some largescale government evaluations have discouraged MT developers to spend time and explore resources to tackle this problem. Introduction A shrinking fraction of the world’s Web pages are written in English, therefore the ability to access pages across a range of languages is becoming increasingly important. This need can be addressed in part by cross-lingual information access tasks such as entity linking (McNamee et al., 2011; Cassidy et al., 2012), event extraction (Hakkani-Tur et al., 2007), slot filling (Snover et al., 2011) and question answering (Parton et al., 2009; Parton and McKeown, 2010). A key bottleneck of highquality cross-lingual information access lies in the performance of Machine Translation (MT). Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information, especially names. We propose a novel Name-aware MT (NAMT) approach which can tightly integrate name processing into the training and decoding processes of an end-to-end MT pipeline, and a new name-aware metric to evaluate MT which can assign different weights to different tokens according to t"
P13-1059,D08-1063,0,0.0287756,"Missing"
P13-1059,2012.eamt-1.34,0,0.0317489,"Missing"
P13-1059,P99-1067,0,0.152375,"Missing"
P13-1059,C04-1089,0,0.0617821,"Missing"
P13-1059,2006.amta-papers.25,0,0.0260188,"andidate n-gram is clipped at sentence level by its maximum reference count. Then we add up the weights of clipped n-grams and divide them by the total weight of all n-grams. Based on BLEU score, we design a name-aware BLEU metric as follows. Depending on whether a token t is contained in a name in reference translation, we assign a weight weightt to t as follows: weightt = ( 1 − e−tf (t,d)·idf (t,D) , if t never appears in names (4) 1 + PZE , if t occurs in name(s) Name-aware MT Evaluation Traditional MT evaluation metrics such as BLEU (Papineni et al., 2002) and Translation Edit Rate (TER) (Snover et al., 2006) assign the same weights to all tokens equally. For example, incorrect translations of “the” and “Bush” will receive the same penalty. However, for crosslingual information processing applications, we should acknowledge that certain informationally critical words are more important than other common words. In order to properly evaluate the translation quality of NAMT methods, we propose to modify the BLEU metric so that they can dynamically assign more weights to names during evaluation. BLEU considers the correspondence between a system translation and a human translation: X  N BLEU = BP ·"
P13-1059,W11-1215,1,0.840711,"ow weights to information translation (compared to overall word translation) in some largescale government evaluations have discouraged MT developers to spend time and explore resources to tackle this problem. Introduction A shrinking fraction of the world’s Web pages are written in English, therefore the ability to access pages across a range of languages is becoming increasingly important. This need can be addressed in part by cross-lingual information access tasks such as entity linking (McNamee et al., 2011; Cassidy et al., 2012), event extraction (Hakkani-Tur et al., 2007), slot filling (Snover et al., 2011) and question answering (Parton et al., 2009; Parton and McKeown, 2010). A key bottleneck of highquality cross-lingual information access lies in the performance of Machine Translation (MT). Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information, especially names. We propose a novel Name-aware MT (NAMT) approach which can tightly integrate name processing into the training and decoding processes of an end-to-end MT pipeline, and a new name-aware metric to evaluate M"
P13-1059,P08-1058,0,0.0137059,"lingual name tagging into MT training by coordinating tagged 604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604–614, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics corpora (details described in section 5.1), with the LM weights optimized on a development set and determined by minimum error rate training (MERT), to estimate the probability of a word given the preceding words. All four LMs were trained using modified Kneser-Ney smoothing algorithm (Chen and Goodman, 1996) and converted into Bloom filter LMs (Talbot and Brants, 2008) supporting memory map. The scaling factors for all features are optimized by minimum error rate training algorithm to maximize BLEU score (Och, 2003). Given an input sentence in the source language, translation into the target language is cast as a search problem, where the goal is to find the highest-probability derivation that generates the source-side sentence, using the rules in our SCFG. The source-side derivation corresponds to a synchronous targetside derivation and the terminal yield of this targetside derivation is the output of the system. We employ our CKY-style chart decoder, name"
P13-1059,E09-1091,0,0.0475212,"Missing"
P13-1059,N09-2004,0,0.0638368,"Missing"
P13-1059,C10-1081,0,\N,Missing
P13-1059,C98-1066,0,\N,Missing
P13-1059,P09-1048,1,\N,Missing
P13-1107,W08-0336,0,0.0614196,"Missing"
P13-1107,P98-1069,0,0.0611667,"m well on morph resolution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit information morphs from th"
P13-1107,P08-1030,1,0.799097,"than one type of links. In order to construct the information networks for morphs, we apply the Standford Chinese word 1085 segmenter with Chinese Penn Treebank segmentation standard (Chang et al., 2008) and Stanford part-of-speech tagger (Toutanova et al., 2003) to process each sentence in the comparable data sets. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al., 2003) to extract named entities, noun phrases and events. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Unfortunately the stateof-the-art techniques for these tasks still perform poorly on social media in terms of both accuracy and coverage of important information, these sophisticated semantic links all produced negative impact on the target ranking performance. Therefore we limited the types of vertices into: Morph (M), Entity(E), which includes target candidates, Event (EV), and Non-Entity Noun Phrases (NP); and used co-occurrence as the edge type. We extract entities, events, and non-entity noun phrases that occur in more than one tweet as neighbors. And for two ve"
P13-1107,W09-3107,1,0.850868,"not perform well on morph resolution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit inform"
P13-1107,P10-1142,0,0.0237712,"candidate of m if and only if, for each tmi ∈ Tm (i = 1, 2, ..., Zm ), there exist a j ∈ {1, 2, ..., Ze } such that tmi − tej ≤ δ, where δ is a threshold value (in this paper we set the threshold to 7 days, which is optimized from a development set). For comparison we also attempted topic modeling approach to detect target candidates, as shown in section 5.3. 4 Target Candidate Ranking 4.2.1 Surface Features We first extract surface features between the morph and the candidate based on measuring orthographic similarity measures which were commonly used in entity coreference resolution (e.g. (Ng, 2010; Hsiung et al., 2005)). The measures we use include “string edit distance”, “normalized string edit distance” (Wagner and Fischer, 1974) and “longest common subsequence” (Hirschberg, 1977). Semantic Features Motivations Fortunately, although a morph and its target may have very different orthographic forms, they tend to be embedded in similar semantic contexts which involve similar topics and events. Figure 2 presents some example messages under censorship (Weibo) and not under censorship (Twitter and Chinese Daily). We can see that they include similar topics, events (e.g., “fell from power”"
P13-1107,P99-1067,0,0.0682673,"lution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit information morphs from the data under"
P13-1107,C04-1089,0,0.0294334,"his paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving implicit information morphs from the data under active censorship."
P13-1107,N03-1033,0,0.0200878,"(e) ∈ R. If two links belong to the same relation type, then they share the same starting object type as well as the same ending object type. An information network is homogeneous if and only if there is only one type for both objects and links, and an information network is heterogeneous when the objects are from multiple distinct types or there exist more than one type of links. In order to construct the information networks for morphs, we apply the Standford Chinese word 1085 segmenter with Chinese Penn Treebank segmentation standard (Chang et al., 2008) and Stanford part-of-speech tagger (Toutanova et al., 2003) to process each sentence in the comparable data sets. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al., 2003) to extract named entities, noun phrases and events. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Unfortunately the stateof-the-art techniques for these tasks still perform poorly on social media in terms of both accuracy and coverage of important information, these sophisticated semantic links all produced nega"
P13-1107,E09-1091,0,0.0220577,"tection methods did not perform well on morph resolution. In this paper we exploit cross-genre information and social correlation to measure semantic similarity. (Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking. Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a). Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes. 7 Conclusion and Future Work To the best of our knowledge, this is the first work of resolving impli"
P13-1107,W03-1730,0,0.0181433,"eneous if and only if there is only one type for both objects and links, and an information network is heterogeneous when the objects are from multiple distinct types or there exist more than one type of links. In order to construct the information networks for morphs, we apply the Standford Chinese word 1085 segmenter with Chinese Penn Treebank segmentation standard (Chang et al., 2008) and Stanford part-of-speech tagger (Toutanova et al., 2003) to process each sentence in the comparable data sets. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al., 2003) to extract named entities, noun phrases and events. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Unfortunately the stateof-the-art techniques for these tasks still perform poorly on social media in terms of both accuracy and coverage of important information, these sophisticated semantic links all produced negative impact on the target ranking performance. Therefore we limited the types of vertices into: Morph (M), Entity(E), which includes target candidates, Event (EV), and N"
P13-1107,C98-1066,0,\N,Missing
P13-1107,C12-1076,1,\N,Missing
P14-1036,P11-1095,0,0.124343,"iu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-coll"
P14-1036,E06-1002,0,0.353752,"ion m as an n-gram from a specific tweet. Each concept has a set of textual representation fields (Meij et al., 2012), including title (the title of the article), sentence (the first sentence of the article), paragraph (the first paragraph of the article), content (the entire content of the article), and anchor (the set of all anchor texts with incoming links to the article). Wikipedia Lexicon Construction We first construct an offline lexicon with each entry as hm, {c1 , ..., ck }i, where {c1 , ..., ck } is the set of possible referent concepts for the mention m. Following the previous work (Bunescu, 2006; Cucerzan, 2007; Hachey et al., 2013), we extract the possible mentions for a given concept c using the following resources: the title of c; the aliases appearing in the introduction and infoboxes of c (e.g., The Evergreen State is an alias of Washington state); the titles of pages redirecting to c (e.g., State of Washington is a redirecting page of Washington (state)); the titles of the disambiguaIn order to address these unique challenges for wikification for the short tweets, we employ graph-based semi-supervised learning algorithms (Zhu et al., 2003; Smola and Kondor, 2003; Blum et al., 2"
P14-1036,C12-1028,1,0.900153,"ept mention detection, (ii) concept mention disambiguation. Wikification is a particularly useful task for short messages such as tweets because it allows a reader to easily grasp the related topics and enriched information from the KB. From a systemto-system perspective, wikification has demonstrated its usefulness in a variety of applications, including coreference resolution (Ratinov and Roth, 2012) and classification (Vitale et al., 2012). Sufficient labeled data is crucial for supervised models. However, manual wikification annotation for short documents is challenging and timeconsuming (Cassidy et al., 2012). The challenges are: (i) unlinkability, a valid concept may not exist in the KB. (ii) ambiguity, it is impossible to determine the correct concept due to the dearth of information within a single tweet or multiple correct answer. For instance, it would be difficult to determine the correct referent concept for “Gators” in t1 in Figure 1. Linking “UCONN” in t3 to University of Connecticut may also be acceptable since Connecticut Huskies is the athletic team of the university. (iii) prominence, it is challenging to select a set of linkable mentions that are important and relevant. It is not tri"
P14-1036,D11-1071,1,0.28354,"ollective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified framework based on meta paths to explore richer relevant context, (iii) Joint identification and linking of mentions under semi-supervised setting."
P14-1036,P06-1017,0,0.0608106,"cea and Csomai, 2007; Milne and Witten, 2008b; Milne and Witten, 2008a; Kulkarni et al., 2009; He et al., 2011; Ratinov et al., 2011; Cassidy et al., 2012; Cheng and Roth, 2013), to the linking of a cluster of corefer0 .5 0 F 1 6.4 Parameter Analysis 4 0 0 L a b e le d T w e e t S iz e Figure 5: The effect of Labeled Tweet Size. 387 ple tweets. This work is also related to graph-based semisupervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been successfully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011"
P14-1036,D13-1184,0,0.386453,"one is an abbreviation of the other, and at least one meta path exists between mi and mj . Then we define the weight matrix representing the coreferential relation as:  if mi and mj are coreferential,  1.0 and ci = cj Wijcoref =  0 Otherwise 4.4 log max(|Ci |, |Cj |) − log |Ci ∩ Cj | , log(|C|) − log min(|Ci |, |Cj |) Semantic Relatedness Ensuring topical coherence (Principle 3) has been beneficial for wikification on formal texts (e.g., News) by linking a set of semantically-related mentions to a set of semantically-related concepts simultaneously (Han et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). However, the shortness of a single tweet means that it may not provide enough topical clues. Therefore, it is important to extend this evidence to capture semantic relatedness information from multiple tweets. We define the semantic relatedness score between two mentions as SR(mi , mj ) = 1.0 if at least one meta path exists between mi and mj , otherwise SR(mi , mj ) = 0. In order to compute the semantic relatedness of two concepts ci and cj , we adopt the approach proposed by (Milne and 384 The Combined Relational Graph gators, Florida Gators men&apos;s basketball 0.404 bucks, Milwaukee Bucks to"
P14-1036,D13-1041,0,0.0355207,"that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context si"
P14-1036,D07-1074,0,0.950267,"ram from a specific tweet. Each concept has a set of textual representation fields (Meij et al., 2012), including title (the title of the article), sentence (the first sentence of the article), paragraph (the first paragraph of the article), content (the entire content of the article), and anchor (the set of all anchor texts with incoming links to the article). Wikipedia Lexicon Construction We first construct an offline lexicon with each entry as hm, {c1 , ..., ck }i, where {c1 , ..., ck } is the set of possible referent concepts for the mention m. Following the previous work (Bunescu, 2006; Cucerzan, 2007; Hachey et al., 2013), we extract the possible mentions for a given concept c using the following resources: the title of c; the aliases appearing in the introduction and infoboxes of c (e.g., The Evergreen State is an alias of Washington state); the titles of pages redirecting to c (e.g., State of Washington is a redirecting page of Washington (state)); the titles of the disambiguaIn order to address these unique challenges for wikification for the short tweets, we employ graph-based semi-supervised learning algorithms (Zhu et al., 2003; Smola and Kondor, 2003; Blum et al., 2004; Zhou et al."
P14-1036,P13-1107,1,0.761714,"nalysis 4 0 0 L a b e le d T w e e t S iz e Figure 5: The effect of Labeled Tweet Size. 387 ple tweets. This work is also related to graph-based semisupervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been successfully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately a"
P14-1036,D11-1074,0,0.0166395,"We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior populari"
P14-1036,D11-1011,0,0.0273495,"sually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified framework based on meta paths to explore richer relevant context, (iii) Joint identification and linking of mentions under semi-supervised setting. 8 Conclusions We have"
P14-1036,I11-1113,0,0.0531393,"ting a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified framework based on meta paths to explore richer relevant context, (ii"
P14-1036,N13-1122,0,0.450775,"identification and disambiguation. We first introduce the following three principles that our approach relies on. Principle 1 (Local compatibility): Two pairs of hm, ci with strong local compatibility tend to 4.1 Local Compatibility We first compute local compatibility (Principle 1) by considering a set of novel local features to cap382 ture the importance and relevance of a mention m to a tweet t, as well as the correctness of its linkage to a concept c. We have designed a number of features which are similar to those commonly used in wikification and entity linking work (Meij et al., 2012; Guo et al., 2013; Mihalcea and Csomai, 2007). Mention Features We define the following features based on information from mentions. vectors vc and vt , and the average tf-idf value of common items in vc and vt , where vc and vt are the top 100 tf-idf word vectors in c and t. Local Compatibility Computation For each node vi = hmi , ci i, we collect its local features as a feature vector Fi = hf1 , f2 , ..., fd i. To avoid features with large numerical values that dominate other features, the value of each feature is re-scaled using feature standardization approach. The cosine similarity is then adopted to comp"
P14-1036,P13-1128,0,0.290658,"ated to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Wit"
P14-1036,C10-1145,0,0.0139728,"ully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant m"
P14-1036,I11-1063,0,0.0178731,"Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013). A significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective ap"
P14-1036,P05-1049,0,0.0816585,"single text (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Milne and Witten, 2008a; Kulkarni et al., 2009; He et al., 2011; Ratinov et al., 2011; Cassidy et al., 2012; Cheng and Roth, 2013), to the linking of a cluster of corefer0 .5 0 F 1 6.4 Parameter Analysis 4 0 0 L a b e le d T w e e t S iz e Figure 5: The effect of Labeled Tweet Size. 387 ple tweets. This work is also related to graph-based semisupervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been successfully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links. ent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 20"
P14-1036,D09-1025,0,0.0644804,"formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation wit"
P14-1036,D12-1113,0,0.0641886,"Missing"
P14-1036,P11-1138,0,0.90289,"e same surface form or one is an abbreviation of the other, and at least one meta path exists between mi and mj . Then we define the weight matrix representing the coreferential relation as:  if mi and mj are coreferential,  1.0 and ci = cj Wijcoref =  0 Otherwise 4.4 log max(|Ci |, |Cj |) − log |Ci ∩ Cj | , log(|C|) − log min(|Ci |, |Cj |) Semantic Relatedness Ensuring topical coherence (Principle 3) has been beneficial for wikification on formal texts (e.g., News) by linking a set of semantically-related mentions to a set of semantically-related concepts simultaneously (Han et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). However, the shortness of a single tweet means that it may not provide enough topical clues. Therefore, it is important to extend this evidence to capture semantic relatedness information from multiple tweets. We define the semantic relatedness score between two mentions as SR(mi , mj ) = 1.0 if at least one meta path exists between mi and mj , otherwise SR(mi , mj ) = 0. In order to compute the semantic relatedness of two concepts ci and cj , we adopt the approach proposed by (Milne and 384 The Combined Relational Graph gators, Florida Gators men&apos;s basketball 0.404 bu"
P14-1038,H05-1091,0,0.719454,"Missing"
P14-1038,P13-1008,1,0.90657,"intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event triggers and arguments. By contrast, we aim to address a more challenging task: identifying mention boundaries and types together with relations, which raises the issue that assignments for the same sentence with different mention boundaries are difficult to synThe goal of end-to-end entity mention and relation extraction is to discover relational structures of entity mentions from unstructured texts. This problem has been artificially broken down into several components such as entity mention boundary identification, ent"
P14-1038,C10-1018,0,0.0684085,"al of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or ⊥ (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: PHYS AllanU-PER from? NewB-GPE YorkL-GPE Stock Exchange • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment it by head words of each phrase. • Four syntactico - semantic patterns described in (Chan and Roth, 2010). • We replicated each lexical feature by replacing each word with its Brown cluster. 3 3.1 AllanU-PER from? NewB-ORG YorkI-ORG Stock Exchange PHYS The model would bias towards the incorrect assignment “New/B-GPE York/L-GPE ” since it can have more informative features as a complete mention (e.g., a binary feature indicating if the entire mention appears in a GPE gazetter). Furthermore, the predictions of the two PHYS relations cannot be synchronized since “New/B-FAC York/I-FAC ” is not yet a complete mention. To tackle these problems, we employ the idea of semi-Markov chain (Sarawagi and Cohe"
P14-1038,de-marneffe-etal-2006-generating,0,0.0310421,"Missing"
P14-1038,P11-1056,0,0.767583,"Missing"
P14-1038,P04-1015,0,0.0200409,"ew framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a number of new and effective global features as soft constraints to capture the interdependency among entity mentions and relations. Experiments on Automatic Content Extraction (ACE)1 corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system. 1 Introduction Following the above intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event triggers and arguments. B"
P14-1038,W12-4304,0,0.012429,"Missing"
P14-1038,W02-1001,0,0.0313524,"dopted to the new framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a number of new and effective global features as soft constraints to capture the interdependency among entity mentions and relations. Experiments on Automatic Content Extraction (ACE)1 corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system. 1 Introduction Following the above intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event"
P14-1038,P13-1147,0,0.0289063,"Missing"
P14-1038,P04-1054,0,0.24174,"Missing"
P14-1038,D10-1034,0,0.0634295,"Missing"
P14-1038,N04-1001,0,0.0243881,"Missing"
P14-1038,C08-1088,0,0.18175,"Missing"
P14-1038,P06-1060,0,0.471353,"ous research on relation extraction assumed that entity mentions were given In this work we aim to address the problem of end-to-end entity mention and relation extraction from raw texts. 2.2 In order to develop a baseline system representing state-of-the-art pipelined approaches, we trained a linear-chain Conditional Random Fields model (Lafferty et al., 2001) for entity mention extraction and a Maximum Entropy model for relation extraction. Entity Mention Extraction Model We re-cast the problem of entity mention extraction as a sequential token tagging task as in the state-of-theart system (Florian et al., 2006). We applied the BILOU scheme, where each tag means a token is the Beginning, Inside, Last, Outside, and Unit of an entity mention, respectively. Most of our features are similar to the work of (Florian et al., Experimental results show that the proposed framework achieves better performance than pipelined approaches, and global features provide further significant gains. 2 Background 2.1 Task Definition The entity mention extraction and relation extraction tasks we are addressing are those of the Automatic Content Extraction (ACE) program2 . ACE defined 7 main entity types including Person (P"
P14-1038,W09-1119,0,0.135379,"hose of the Automatic Content Extraction (ACE) program2 . ACE defined 7 main entity types including Person (PER), Organization (ORG), Geographical Entities (GPE), Location (LOC), 2 Baseline System 3 Throughout this paper we refer to relation mention as relation since we do not consider relation mention coreference. http://www.nist.gov/speech/tests/ace 403 ferent assignments. Assignments for the same sentence can have different numbers of entity mentions and relation arcs. The entity mention extraction task is often re-cast as a token-level sequential labeling problem with BIO or BILOU scheme (Ratinov and Roth, 2009; Florian et al., 2006). A naive solution to our task is to adopt this strategy by treating each token as a state. However, different assignments for the same sentence can have various mention boundaries. It is unfair to compare the model scores of a partial mention and a complete mention. It is also difficult to synchronize the search process of relations. For example, consider the two hypotheses ending at “York” for the same sentence: 2004; Florian et al., 2006) except that we do not have their gazetteers and outputs from other mention detection systems as features. Our additional features a"
P14-1038,D10-1033,0,0.0470919,"Missing"
P14-1038,P09-2092,0,0.020892,"Missing"
P14-1038,P10-1110,0,0.00786358,"a strong pipelined baseline, which attains better performance than the best-reported end-to-end system. 1 Introduction Following the above intuitions, we introduce a joint framework based on structured perceptron (Collins, 2002; Collins and Roark, 2004) with beam-search to extract entity mentions and relations simultaneously. With the benefit of inexact search, we are also able to use arbitrary global features with low cost. The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. Our task differs from dependency parsing (such as (Huang and Sagae, 2010)) in that relation structures are more flexible, where each node can have arbitrary relation arcs. Our previous work (Li et al., 2013) used perceptron model with token-based tagging to jointly extract event triggers and arguments. By contrast, we aim to address a more challenging task: identifying mention boundaries and types together with relations, which raises the issue that assignments for the same sentence with different mention boundaries are difficult to synThe goal of end-to-end entity mention and relation extraction is to discover relational structures of entity mentions from unstruct"
P14-1038,W04-2401,0,0.804884,"e maker} still employs 1,400 . GPE PER |{z |{z } ORG PER (a) Interactions between Two Tasks |Somalia {z } , |Haiti {z } and |Kosovo {z }. GPE GPE GPE P S HY PH YS conj and GPE GPE conj and (b) Example of Global Feature Figure 1: End-to-End Entity Mention and Relation Extraction. chronize during search. To tackle this problem, we adopt a segment-based decoding algorithm derived from (Sarawagi and Cohen, 2004; Zhang and Clark, 2008) based on the idea of semi-Markov chain (a.k.a, multiple-beam search algorithm). Most previous attempts on joint inference of entity mentions and relations (such as (Roth and Yih, 2004; Roth and Yih, 2007)) assumed that entity mention boundaries were given, and the classifiers of mentions and relations are separately learned. As a key difference, we incrementally extract entity mentions together with relations using a single model. The main contributions of this paper are as follows: 1. This is the first work to incrementally predict entity mentions and relations using a single joint model (Section 3). 2. Predicting mention boundaries in the joint framework raises the challenge of synchronizing different assignments in the same beam. We solve this problem by detecting entit"
P14-1038,N12-1015,0,0.0184155,"Missing"
P14-1038,P05-1051,1,0.852443,"Missing"
P14-1038,N07-1015,0,0.119628,"Missing"
P14-1038,P04-3022,0,0.00993922,"tputs from other mention detection systems as features. Our additional features are as follows: • Governor word of the current token based on dependency parsing (Marneffe et al., 2006). • Prefix of each word in Brown clusters learned from TDT5 corpus (Sun et al., 2011). Relation Extraction Model Given a sentence with entity mention annotations, the goal of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or ⊥ (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: PHYS AllanU-PER from? NewB-GPE YorkL-GPE Stock Exchange • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment it by head words of each phrase. • Four syntactico - semantic patterns described in (Chan and Roth, 2010). • We replicated each lexical feature by replacing each word with its Brown cluster. 3 3.1 AllanU-PER from? NewB-ORG YorkI-ORG Stock Exchange PHYS The model would bias towards the incorrect assignment “New/B-GPE York/L-GPE ” since it can have"
P14-1038,W10-2924,0,0.459107,"E’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) 7 usually studied separately. Most relation extraction work assumed that entity mention boundaries and/or types were given. Chan and Roth (2011) reported the best results using predicted entity mentions. Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint distribution over numerous variables, therefore it"
P14-1038,P11-1053,0,0.0322295,"have various mention boundaries. It is unfair to compare the model scores of a partial mention and a complete mention. It is also difficult to synchronize the search process of relations. For example, consider the two hypotheses ending at “York” for the same sentence: 2004; Florian et al., 2006) except that we do not have their gazetteers and outputs from other mention detection systems as features. Our additional features are as follows: • Governor word of the current token based on dependency parsing (Marneffe et al., 2006). • Prefix of each word in Brown clusters learned from TDT5 corpus (Sun et al., 2011). Relation Extraction Model Given a sentence with entity mention annotations, the goal of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or ⊥ (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: PHYS AllanU-PER from? NewB-GPE YorkL-GPE Stock Exchange • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment i"
P14-1038,P13-1161,0,0.0664698,"8 36.1 45.3 Table 3: 5-fold cross-validation on ACE’04 corpus. Bolded scores indicate highly statistical significant improvement as measured by paired t-test (p &lt; 0.01) 7 usually studied separately. Most relation extraction work assumed that entity mention boundaries and/or types were given. Chan and Roth (2011) reported the best results using predicted entity mentions. Some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking (Ji and Grishman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint d"
P14-1038,C10-2160,0,0.687424,"shman, 2005), Integer Linear Programming (ILP) (Roth and Yih, 2004; Roth and Yih, 2007; Yang and Cardie, 2013), and Card-pyramid Parsing (Kate and Mooney, 2010). All these work noted the advantage of exploiting crosscomponent interactions and richer knowledge. However, they relied on models separately learned for each subtask. As a key difference, our approach jointly extracts entity mentions and relations using a single model, in which arbitrary soft constraints can be easily incorporated. Some other work applied probabilistic graphical models for joint extraction (e.g., (Singh et al., 2013; Yu and Lam, 2010)). By contrast, our work employs an efficient joint search algorithm without modeling joint distribution over numerous variables, therefore it is more flexible and computationally simpler. In addition, (Singh et al., 2013) used goldstandard mention boundaries. Our previous work (Li et al., 2013) used structured perceptron with token-based decoder to jointly predict event triggers and arguments based on the assumption that entity mentions and other argument candidates are given as part of the input. In this paper, we solve a more challenging problem: take raw texts as input and identify the bou"
P14-1038,P08-1101,0,0.0386039,"ational Linguistics, pages 402–412, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics PHYS EMP-ORG PER EMP-ORG ... |{z} US forces |{z } in The tire maker} still employs 1,400 . GPE PER |{z |{z } ORG PER (a) Interactions between Two Tasks |Somalia {z } , |Haiti {z } and |Kosovo {z }. GPE GPE GPE P S HY PH YS conj and GPE GPE conj and (b) Example of Global Feature Figure 1: End-to-End Entity Mention and Relation Extraction. chronize during search. To tackle this problem, we adopt a segment-based decoding algorithm derived from (Sarawagi and Cohen, 2004; Zhang and Clark, 2008) based on the idea of semi-Markov chain (a.k.a, multiple-beam search algorithm). Most previous attempts on joint inference of entity mentions and relations (such as (Roth and Yih, 2004; Roth and Yih, 2007)) assumed that entity mention boundaries were given, and the classifiers of mentions and relations are separately learned. As a key difference, we incrementally extract entity mentions together with relations using a single model. The main contributions of this paper are as follows: 1. This is the first work to incrementally predict entity mentions and relations using a single joint model (Se"
P14-1038,P05-1052,0,0.0608084,"Missing"
P14-1038,P05-1053,0,0.273636,"their gazetteers and outputs from other mention detection systems as features. Our additional features are as follows: • Governor word of the current token based on dependency parsing (Marneffe et al., 2006). • Prefix of each word in Brown clusters learned from TDT5 corpus (Sun et al., 2011). Relation Extraction Model Given a sentence with entity mention annotations, the goal of baseline relation extraction is to classify each mention pair into one of the pre-defined relation types with direction or ⊥ (non-relation). Most of our relation extraction features are based on the previous work of (Zhou et al., 2005) and (Kambhatla, 2004). We designed the following additional features: PHYS AllanU-PER from? NewB-GPE YorkL-GPE Stock Exchange • The label sequence of phrases covering the two mentions. For example, for the sentence in Figure 1a, the sequence is “NP VP NP”. We also augment it by head words of each phrase. • Four syntactico - semantic patterns described in (Chan and Roth, 2010). • We replicated each lexical feature by replacing each word with its Brown cluster. 3 3.1 AllanU-PER from? NewB-ORG YorkI-ORG Stock Exchange PHYS The model would bias towards the incorrect assignment “New/B-GPE York/L-G"
P14-1038,D07-1076,0,0.0981623,"Missing"
P14-1038,D08-1063,0,0.0107465,"Missing"
P14-2046,I08-8003,0,0.0195626,"and Viterbi alignments, such as: g ge 5.2 r r ae n uan uan d e m a d e r r ae n uan d de m m ah a dh d er e Second, we extract phoneme phrase pairs consistent with these alignments. We use no phrasesize limit, but we do not cross word boundaries. From the example above, we pull out phrase pairs like: g→g e g r→g e r ... r→r r ae n → r uan ... FSTs A, C, and D are unweighted, and remain so throughout this paper. 5.1 r Here, “ae n” should be decoded as “uan” when preceded by “r”. Following phrase-based methods in statistical machine translation (Koehn et al., 2003) and machine transliteration (Finch and Sumita, 2008), we model substitution of longer sequences. First, we obtain Viterbi alignments using the phoneme-based model, e.g.: to one or two Pinyin-split tokens, and it also allows two English sounds to map to one Pinyin-split token. Finally, FST C converts Pinyin-split into Pinyin, and FST D chooses Chinglish characters. We also experiment with an additional wFST E that translates English words directly into Chinglish. 5 e We add these phrase pairs to FST B, and call this the phoneme-phrase-based model. 5.3 Word-based model We now turn to WFST E, which short-cuts directly from English words to Pinyin."
P14-2046,N13-1072,0,0.0310329,"he system’s output should be both unambiguously pronounceable by the speaker and readily understood by the listener. Our goal is to build an application that covers many language pairs and directions. The current paper describes a single system that lets a Chinese person speak English. We take a statistical modeling approach to this problem, as is done in two lines of research that are most related. The first is machine transliteration (Knight and Graehl, 1998), in which names and technical terms are translated across languages with different sound systems. The other is respelling generation (Hauer and Kondrak, 2013), where an English speaker is given a phonetic hint about how to pronounce a rare or foreign word to another English speaker. By contrast, we aim English: Leave me alone. French: Laissez-moi tranquille. Franglish: Less-ay mwah trahn-KEEL. The user ignores the French and goes straight to the Franglish. If the Franglish is well designed, an English speaker can pronounce it and be understood by a French listener. Figure 1 shows a sample entry from another book—an English phrasebook for Chinese speakers. If a Chinese speaker wants to say “非 常 感 谢 你 这 顿 美 餐”, she need only read off the Chinglish “三"
P14-2046,N03-1017,0,0.0104914,"EM algorithm to learn FST B parameters (Table 3) and Viterbi alignments, such as: g ge 5.2 r r ae n uan uan d e m a d e r r ae n uan d de m m ah a dh d er e Second, we extract phoneme phrase pairs consistent with these alignments. We use no phrasesize limit, but we do not cross word boundaries. From the example above, we pull out phrase pairs like: g→g e g r→g e r ... r→r r ae n → r uan ... FSTs A, C, and D are unweighted, and remain so throughout this paper. 5.1 r Here, “ae n” should be decoded as “uan” when preceded by “r”. Following phrase-based methods in statistical machine translation (Koehn et al., 2003) and machine transliteration (Finch and Sumita, 2008), we model substitution of longer sequences. First, we obtain Viterbi alignments using the phoneme-based model, e.g.: to one or two Pinyin-split tokens, and it also allows two English sounds to map to one Pinyin-split token. Finally, FST C converts Pinyin-split into Pinyin, and FST D chooses Chinglish characters. We also experiment with an additional wFST E that translates English words directly into Chinglish. 5 e We add these phrase pairs to FST B, and call this the phoneme-phrase-based model. 5.3 Word-based model We now turn to WFST E, wh"
P14-2046,mcenery-xiao-2004-lancaster,0,0.0431329,"pular, because while consonant clusters like English “st” are impossible to reproduce exactly, the particular vowels in “si” and “te” are fortunately very weak. Evaluation Our system’s input is Chinese. The output is a string of Chinese characters that approximate English sounds, which we call Chinglish. We build several candidate Chinese-to-Chinglish systems and evaluate them as follows: Frequency Rank 1 2 3 4 5 • We compute the normalized edit distance between the system’s output and a humangenerated Chinglish reference. Chinglish si te de yi fu Table 2: Top 5 frequent syllables in Chinese (McEnery and Xiao, 2004) and Chinglish • A Chinese speaker pronounces the system’s output out loud, and an English listener takes dictation. We measure the normalized edit distance against an English reference. We find that multiple occurrences of an English word type are generally associated with the same Chinglish sequence. Also, Chinglish characters do not generally span multiple English words. It is reasonable for “can I” to be rendered as “kan nai”, with “nai” spanning both English words, but this is rare. • We automate the previous evaluation by replace the two humans with: (1) a Chinese speech synthesizer, and"
P14-2046,J98-4003,1,\N,Missing
P14-2081,P05-1077,0,0.0193555,"up of over one order of magnitude in query time. 1 Introduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). 1 http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics codes and hq ; the smaller the Hamming distance, the higher the data item is ranked. 2013), all of which endeavor to make the learned hash codes preserve or reveal some intrinsic structure, such as local neighborhood structure, lowd"
P14-2081,D10-1122,0,0.0208141,"conventional Information Retrieval (IR) method in terms of retrieving semantically similar documents, and meanwhile achieves a speedup of over one order of magnitude in query time. 1 Introduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). 1 http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics codes and hq ; the smaller the Hamming distance, the higher the data item is ranked. 2013), all"
P14-2081,W11-2210,0,0.021484,"troduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). 1 http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics codes and hq ; the smaller the Hamming distance, the higher the data item is ranked. 2013), all of which endeavor to make the learned hash codes preserve or reveal some intrinsic structure, such as local neighborhood structure, lowdimensional manifolds, or the closest hypercube"
P14-2081,W11-2125,0,0.0183922,"rieving semantically similar documents, and meanwhile achieves a speedup of over one order of magnitude in query time. 1 Introduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). 1 http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics codes and hq ; the smaller the Hamming distance, the higher the data item is ranked. 2013), all of which endeavor to make the learned hash codes preserve or revea"
P14-2081,N10-1021,0,0.0230151,"ching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). 1 http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics codes and hq ; the smaller the Hamming distance, the higher the data item is ranked. 2013), all of which endeavor to make the learned hash codes preserve or reveal some intrinsic structure, such as local neighborhood structure, lowdimensional manifolds, or the closest hypercube, underlying the training data. Despite achieving data-dependen"
P14-2115,P13-2041,0,0.0216675,"sformable, we substitute the part of e with t to form a new morph. For example, we can substitute the characters of “比尔 盖茨 (Bill Gates) [Bi Er Gai Ci]” with “鼻耳 (Nose and ear) [Bi Er]” and “盖子 (Lid) [Gai Zi]” to form new morph “鼻耳 盖子 (Nose and ear Lid) [Bi Er Gai Zi]”. We rank the candidates based on the following two criteria: (1) If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by replacing the last name “伯德 (Bird)” with its Chinese translation “鸟儿 (bird)”. 2.6 M5: Semantic Interpretation For e"
P14-2115,P13-2044,0,0.0208414,") are mutually transformable. If a part of pinyin(e) and pinyin(t) are identical or their initials are transformable, we substitute the part of e with t to form a new morph. For example, we can substitute the characters of “比尔 盖茨 (Bill Gates) [Bi Er Gai Ci]” with “鼻耳 (Nose and ear) [Bi Er]” and “盖子 (Lid) [Gai Zi]” to form new morph “鼻耳 盖子 (Nose and ear Lid) [Bi Er Gai Zi]”. We rank the candidates based on the following two criteria: (1) If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by rep"
P14-2115,P13-1107,1,0.575788,"ng morphs. One main purpose of encoding morphs is to disseminate them widely so they can become part of the new Internet language. Therefore morphs should be interesting, fun, intuitive and easy to remember. (5) Morphs rapidly evolve over time, as some morphs are discovered and blocked by censorship and newly created morphs emerge. We propose a brand new and challenging research problem - can we automatically encode morphs for any given entity to help users communicate in an appropriate and fun way? Introduction One of the most innovative linguistic forms in social media is Information Morph (Huang et al., 2013). Morph is a special case of alias to hide the original objects (e.g., sensitive entities and events) for different purposes, including avoiding censorship (Bamman et al., 2012; Chen et al., 2013), expressing strong sentiment, emotion or sarcasm, and making descriptions more vivid. Morphs are widely used in Chinese social media. Here is an example morphs: “由于瓜爹的事情，方便面与 天线摊牌. (Because of Gua Dad’s issue, Instant Noodles faces down with Antenna.)”, where • “瓜爹 (Gua Dad)” refers to “薄熙来 (Bo Xilai)” because it shares one character “瓜 (Gua)” with “薄瓜瓜 (Bo Guagua)” who is the son of “薄熙 来 (Bo Xilai)"
P14-2115,P13-1072,0,0.0337907,"Missing"
P14-2115,P11-1115,1,0.782334,"taring at sea and listening to surf)” as a present when he visited China. In the films Ma Jingtao starred, he always used exaggerated roaring to express various emotions. The morph derives from Ma Yingjiu’s political position on cross-strait relations. Table 2: Morph Examples Categorized based on Human Generation Methods based on their semantic contexts. For example, this approach generates a morph “太祖 (the First Emperor)” for “毛泽东 (Mao Zedong)” who is the first chairman of P. R. China and “高祖 (the Second Emperor )” for “邓小平 (Deng Xiaoping )” who succeeded Mao. 2.8 al., 2010; Ji et al., 2011; Ji and Grishman, 2011) which include 3 million news and web documents, and DARPA BOLT program’s discussion forum corpora with 300k threads. Given an entity e, we compute the semantic relationship between e and each word from these corpora. We then rank the words by: (1) cosine similarity, (2) the same criteria as in section 2.6. Finally we append the top ranking word to the entity’s last name to obtain a new morph. Using this method, we are able to generate many vivid morphs such as “姚 奇才 (Yao Wizard)” for “姚 明 (Yao Ming)”. M7: Characteristics Modeling Finally, we propose a novel approach to automatically generate"
P14-2115,I13-1015,0,0.0216269,"Missing"
P14-2115,W06-2808,0,0.0198146,"Missing"
P14-2115,I05-3013,0,0.0556604,"Missing"
P14-2115,P06-1125,0,0.0446686,"Missing"
P14-2115,N04-1033,0,0.0106403,"If the morph includes more negative words (based on a gazetteer including 11,729 negative words derived from HowNet (Dong and Dong, 1999), it’s more humorous (Valitutti et al., 2013). (2) If the morph includes rarer terms with low frequency, it is more interesting (Petrovic and Matthews, 2013). 2.3 2.5 M4: Translation and Transliteration Given an entity e, we search its English translation EN(e) based on 94,015 name translation pairs (Ji et al., 2009). Then, if any name component in EN(e) is a common English word, we search for its Chinese translation based on a 94,966 word translation pairs (Zens and Ney, 2004), and use the Chinese translation to replace the corresponding characters in e. For example, we create a morph “拉里 鸟儿 (Larry bird)” for “拉里 伯德 (Larry Bird)” by replacing the last name “伯德 (Bird)” with its Chinese translation “鸟儿 (bird)”. 2.6 M5: Semantic Interpretation For each character ck in the first name of a given entity name e, we search its semantic interpretation sentence from the Xinhua Chinese character dictionary including 20,894 entries 3 . If a word in the sentence contains ck , we append the word with the last name of e to form a new morph. Similarly to M1, we prefer positive, ne"
P14-2115,D08-1108,0,0.0445781,"Missing"
P14-2115,J98-4003,1,\N,Missing
P14-6004,C10-1032,0,\N,Missing
P14-6004,E06-1002,0,\N,Missing
P14-6004,C10-1150,0,\N,Missing
P14-6004,D12-1010,0,\N,Missing
P14-6004,W11-2213,0,\N,Missing
P14-6004,C10-1145,0,\N,Missing
P14-6004,D11-1074,0,\N,Missing
P14-6004,mcnamee-etal-2010-evaluation,0,\N,Missing
P14-6004,D09-1025,0,\N,Missing
P14-6004,D12-1011,0,\N,Missing
P14-6004,D12-1082,0,\N,Missing
P14-6004,P11-1138,1,\N,Missing
P14-6004,P12-1086,0,\N,Missing
P14-6004,P13-2006,0,\N,Missing
P14-6004,P11-1095,0,\N,Missing
P14-6004,D13-1041,0,\N,Missing
P14-6004,I11-1029,0,\N,Missing
P14-6004,C12-1028,1,\N,Missing
P14-6004,D13-1184,1,\N,Missing
P14-6004,P13-1128,0,\N,Missing
P14-6004,D07-1074,0,\N,Missing
P14-6004,N10-1072,0,\N,Missing
P14-6004,W12-1014,0,\N,Missing
P14-6004,D11-1011,0,\N,Missing
P14-6004,P13-1107,1,\N,Missing
P14-6004,D11-1071,1,\N,Missing
P14-6004,I11-1113,0,\N,Missing
P14-6004,D12-1113,1,\N,Missing
P15-1056,W06-0901,0,0.0467242,"relevant event chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is re"
P15-1056,S13-2002,0,0.0329879,"Missing"
P15-1056,P08-1030,1,0.800799,"vent chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most"
P15-1056,R09-1032,1,0.842039,"er from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the per"
P15-1056,D08-1073,0,0.0878802,"Missing"
P15-1056,Q14-1022,0,0.0297756,"Missing"
P15-1056,S13-2012,0,0.0607878,"Missing"
P15-1056,P13-2099,1,0.719355,"n aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the performance of our approach in labeling event time. For disaster, sports and war, the accurac"
P15-1056,D12-1092,0,0.0143416,"times called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymou"
P15-1056,N09-2053,1,0.822412,"on work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focu"
P15-1056,P13-1008,1,0.798212,"2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for their thought-provoki"
P15-1056,C12-1033,0,0.0200147,"c detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for the"
P15-1056,P10-1081,0,0.0232743,"re some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeA"
P15-1056,D12-1062,0,0.0241202,"blems. Disaster event chronicles suffer from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time"
P15-1056,D13-1001,1,0.844429,"in sports chronicles but it is not a good entry in comprehensive chronicles. Compared with comprehensive event chronicles, events in other chronicles tend to describe more details. For example, a sports chronicle may regard each match in the World Cup as an event while comprehensive chronicles consider the World Cup as one event, which requires us to adapt event granularity for different chronicles. Also, we evaluate the time of event entries in these five event chronicles because event’s happening time is not always equal to the timestamp of the document creation time (UzZaman et al., 2012; Ge et al., 2013). We collect existing manually edited 2010 chronicles on the web and use their event time as gold standard. We define a metric to evaluate if the event entry’s time in our chronicle is accurate: P diff = e∈E∩E∗ |(te − t∗e )|/|E ∩ E∗ | not show significant improvement. A possible reason is that a comprehensive event chronicle does not care the topical relevance of a event. In other words, its ranking problem is simpler so that the learning-to-rank does not improve the basic ranking criterion much. Moreover, we analyze the incorrect entries in the chronicles generated by our approaches. In gener"
P15-1056,P14-5010,0,0.00567471,"ifically, we collected disaster, sports, war, politics and comprehensive chronicles during 2009 from mapreport7 , infoplease and Wikipedia8 . To generate chronicles during 2010, we use 2009-2010 APW and Xinhua news in English Gigaword (Graff et al., 2003) and remove documents whose titles and first paragraphs do not include any burst words. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which is a 2-state finite automaton model and widely used to detect bursts. In total, there are 140,557 documents in the corpus. Preprocessing: We remove stopwords and use Stanford CoreNLP (Manning et al., 2014) to do lemmatization. Parameter setting: For TaHBM, we empirically set α = 0.05, βz = 0.005, βe = 0.0001, γs = 0.05, γx = 0.5, ε = 0.01, the number of topics K = 50, and the number of events E = 5000. We run Gibbs sampler for 2000 iterations with burn-in period of 500 for inference. For event ranking, we set regularization parameter of SVMRank c = 0.1. Chronicle display: We use a heuristic way to generate the description of each event. Since the first paragraph of a news article is usually a good summary of the article and the earliest document in a cluster usually explicitly describes the eve"
P15-1056,P11-1113,0,0.0152544,"ent detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We t"
P15-1056,P05-3021,0,0.114846,"Missing"
P15-1056,P12-2009,0,0.252735,"to the event chronicle) in Section 3.3 are labeled as high rank priority while those without positive documents are labeled as low priority. 4.2 |De | σe : Features We use the following features to train the ranking model, all of which can be provided by TaHBM. • P (s = 1|e): the probability that an event e is topically relevant to the reference chronicle. • P (e|z): the probability reflects an event’s impact given its topic. • σe : the parameter of an event e’s Gaussian distribution. It determines the ‘bandwidth’ 7 8 580 http://www.mapreport.com http://en.wikipedia.org/wiki/2009 5.2 schema (Zhao et al., 2012) to detect events, which is the state-of-the-art event detection method for general domains. Evaluation Methods and Baselines Since there is no existing evaluation metric for the new task, we design a method for evaluation. Although there are manually edited event chronicles on the web, which may serve as references for evaluation, they are often incomplete. For example, the 2010 politics event chronicle on Wikipedia has only two event entries. Hence, we first pool all event entries of existing chronicles on the web and chronicles generated by approaches evaluated in this paper and then have 3"
P15-1057,W08-0336,0,0.0275193,"riginal meanings. 2 Problem Formulation Following the recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers t"
P15-1057,P06-1017,0,0.020688,"Missing"
P15-1057,P13-2006,0,0.0430093,"Missing"
P15-1057,W13-0908,0,0.0277964,"d malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detect"
P15-1057,P13-1107,1,0.928563,"ecoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practical morph decoder, as depicted in Figure 1, consists of two problems: (1) Morph Extraction: given a corpus, extract morph mentions; and (2). Morph Resolution: For each morph mention, figure out the entity that it refers to. In this paper, we aim to solve the fundamental research problem of end-to-end morph deco"
P15-1057,P14-1036,1,0.900239,"Missing"
P15-1057,D14-1067,0,0.0149744,"(e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. A"
P15-1057,W13-0906,0,0.0253293,"avior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some comm"
P15-1057,P14-1038,1,0.800559,"2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and ca"
P15-1057,D14-1071,0,0.0150722,"s), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in secti"
P15-1057,P95-1026,0,0.257659,"s in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genre"
P15-1057,P14-2105,0,0.0142968,"nd formal entities (e.g., named entities), while morphs tend to be informal and convey implicit information. Morph mention detection is also related to malware detection (e.g., (Firdausi et al., 2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to t"
P15-1057,N07-1025,0,0.0387675,"ntext. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic categories and can include either abstractive or concrete information. Some common features for detecting metaphors (e.g. (Tsvetkov, 593 cial to keep the genres consistent bet"
P15-1057,W03-1730,0,0.00977908,"4), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. For each morph mention, we discover a list of target candid"
P15-1057,P14-2115,1,0.48139,"{zhangb8,huangh9,panx2,jih,yener}@rpi.edu, 2 lisujian@pku.edu.cn, 3 cyl@microsoft.com 4 hanj@illinois.edu, 5 zhenwen@us.ibm.com, 6 yzsun@ccs.neu.edu, 7 hanj@illinois.edu Abstract a morph “Su-tooth” was created to refer to the Uruguay striker “Luis Suarez” for his habit of biting other players. Automatically decoding humangenerated morphs in text is critical for downstream deep language understanding tasks such as entity linking and event argument extraction. However, even for human, it is difficult to decode many morphs without certain historical, cultural, or political background knowledge (Zhang et al., 2014). For example, “The Hutt” can be used to refer to a fictional alien entity in the Star Wars universe (“The Hutt stayed and established himself as ruler of Nam Chorios”), or the governor of New Jersey, Chris Christie (“The Hutt announced a bid for a seat in the New Jersey General Assembly”). Huang et al. (2013) did a pioneering pilot study on morph resolution, but their approach assumed the entity morphs were already extracted and used a large amount of labeled data. In fact, they resolved morphs on corpus-level instead of mention-level and thus their approach was context-independent. A practic"
P15-1057,P05-1049,0,0.0177967,"Missing"
P15-1057,D08-1063,0,0.0286454,"2010; Chandola et al., 2009; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wide"
P15-1057,W12-4304,0,0.0148793,"; Firdausi et al., 2010; Christodorescu and Jha, 2003)) which discovers abnormal behavior in code and malicious software. In contrast our task tackles anomaly texts in semantic context. Deep learning-based approaches have been demonstrated to be effective in disambiguation related tasks such as WSD (Bordes et al., 2012), entity linking (He et al., 2013) and question linking (Yih et al., 2014; Bordes et al., 2014; Yang et al., 2014). In this paper we proved that it’s cruFigure 4: Resolution Acc@K for Perfect Morph Mentions NLP tasks: entity mention extraction (e.g., (Zitouni and Florian, 2008; Ohta et al., 2012; Li and Ji, 2014)), metaphor detection (e.g., (Wang et al., 2006; Tsvetkov, 2013; Heintz et al., 2013)), word sense disambiguation (WSD) (e.g., (Yarowsky, 1995; Mihalcea, 2007; Navigli, 2009)), and entity linking (EL) (e.g., (Mihalcea and Csomai, 2007; Ji et al., 2010; Ji et al., 2011; Ji et al., 2014). However, none of these previous techniques can be applied directly to tackle this problem. As mentioned in section 3.1, entity morphs are fundamentally different from regular entity mentions. Our task is also different from metaphor detection because morphs cover a much wider range of semantic"
P15-1057,N03-1033,0,0.00839243,"e recent work on morphs (Huang et al., 2013; Zhang et al., 2014), we use Chinese Weibo tweets for experiments. Our goal is to develop an end-to-end system that automatically extract morph mentions and resolve them to their target entities. Given a corpus of tweets D = {d1 , d2 , ..., d|D |}, we define a candidate morph mi as a unique term tj in T , where T = {t1 , t2 , ..., t|T |} is the set of unique terms in D. To extract T , we first apply several well-developed Natural Language Processing tools, including Stanford Chinese word segmenter (Chang et al., 2008), Stanford part-ofspeech tagger (Toutanova et al., 2003) and Chinese lexical analyzer ICTCLAS (Zhang et al., 2003), to process the tweets and identify noun phrases. Then we define a morph mention mpi of mi as the p-th occurrence of mi in a specific document dj . Note that a mention with the same surface form as mi but referring to its original entity is not considered as a morph mention. For instance, the “平西 王 (Conquer West King)” in d1 and d3 in Figure 1 are morph mentions since they refer to the modern politician “薄熙来 (Bo Xilai)”, while the one in d4 is not a morph mention since it refers to the original entity, who was king “吴三桂 (Wu Sangui)”. F"
P15-1083,I13-1039,0,0.0127684,"more by reading the comments from others and acquiring more background knowledge (Bandura, 1971). Our contribution is that we extend the DAP method and combine it with the signed network partition in order to cluster the hidden group members. We also develop a novel cluster ensemble approach in order to analyze the dynamic network. Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task"
P15-1083,P12-2034,0,0.553475,"al., 2004). Therefore, it is more challenging to identity a deceiver in an interactive process of deception. Most deception detection research addressed individual deceivers, but deceivers often act in pairs or larger groups (Vrij et al., 2010). The interacIntroduction Deception generally entails messages and information intentionally transmitted to create a false conclusion (Buller et al., 1994). Deception detection is an important task for a wide range of applications including law enforcement, intelligence gathering, and financial fraud. Most of the previous work (e.g., (Ott et al., 2011; Feng et al., 2012)) focused on content analysis of a single document in isolation (e.g., a product review). The promoters of a product may post fake complimentary reviews, while their competitors may hire people to write fake negative reviews (Ott et al., 2011). 1 The data set is publicly available for research purposes at: http://nlp.cs.rpi.edu/data/killer.zip 857 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 857–866, c Beijing, China, July 26-31, 2015. 2015 Association for Computational L"
P15-1083,P12-1042,0,0.161502,"k with discussants telling lies and truths. 3.1 Clustering based on Attitude Profile We use a vector containing numerical values to represent each player’s attitude toward identified targets in each round. The values correspond to the polarity scores in a player’s attitude tuple list. For example, the polarity score of player 16’s attitude toward target 11 is −1 as shown in Figure 2. 2 Each player has a game ID, assigned by the online game system based on when he entered the game room. 3 e.g., http://www.3j3f.com/how/ 859 We call this vector as the discussant attitude profile (DAP) following (Abu-Jbara et al., 2012a). Suppose there are n players who participate in a single game. Since a player’s identity is not exposed to the public after his death4 , people can still analyze the identity of a “dead” player. Therefore, the number of possibly mentioned targets in each round equals to n. Given all the statements from m surviving players in a single round, each player’s DAP has n + 1 dimensions including his vote and thus we can have a m × (n + 1) attitude matrix A where Aij represents the attitude polarity of i toward j we got from Section 2. Ai(n+1) represents i’s vote. In a certain round, given a set of"
P15-1083,D12-1006,0,0.0282946,"ginal DAPC method, for each opinion target, there are 3 dimensions in the feature vector, corresponding to (1) the number of positive expressions, (2) negative expressions toward the target from the online posts and (3) the number of times the discussant mentioned the target. For our experiment, we only keep one dimension representing the discussant’s attitude (positive, negative, neutral) toward the target since a discussant attitude remains the same in his statement within a single round. 2. Network: We also implemented the signed network partition method for subgroup detection proposed by (Hassan et al., 2012). To determine the number of subgroups t, we set an upper limit of t = 3 in order to minimize the optimization function. Evaluation Metrics purity(Ω, C) = P (i, j) × log2 P (i, j) We compare our approach with two state-of-theart subgroup detection methods and human performance as follows: Figure 5: Screenshot of the online killer game interface. 4.2 n where P (i, j) is the probability of finding an element from the category i in the cluster j, nj is the number of items in cluster j and n is the total number of items in the distribution. The entropy decreases as the quality of clustering improv"
P15-1083,N12-3009,0,0.118534,"k with discussants telling lies and truths. 3.1 Clustering based on Attitude Profile We use a vector containing numerical values to represent each player’s attitude toward identified targets in each round. The values correspond to the polarity scores in a player’s attitude tuple list. For example, the polarity score of player 16’s attitude toward target 11 is −1 as shown in Figure 2. 2 Each player has a game ID, assigned by the online game system based on when he entered the game room. 3 e.g., http://www.3j3f.com/how/ 859 We call this vector as the discussant attitude profile (DAP) following (Abu-Jbara et al., 2012a). Suppose there are n players who participate in a single game. Since a player’s identity is not exposed to the public after his death4 , people can still analyze the identity of a “dead” player. Therefore, the number of possibly mentioned targets in each round equals to n. Given all the statements from m surviving players in a single round, each player’s DAP has n + 1 dimensions including his vote and thus we can have a m × (n + 1) attitude matrix A where Aij represents the attitude polarity of i toward j we got from Section 2. Ai(n+1) represents i’s vote. In a certain round, given a set of"
P15-1083,W06-0301,0,0.0897013,"Missing"
P15-1083,P13-2144,0,0.0223457,"the comments from others and acquiring more background knowledge (Bandura, 1971). Our contribution is that we extend the DAP method and combine it with the signed network partition in order to cluster the hidden group members. We also develop a novel cluster ensemble approach in order to analyze the dynamic network. Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task"
P15-1083,P13-2039,0,0.018035,"ide his own stance. Moreover, their work did not take into account that a person’s attitude or stance will change as he learns more by reading the comments from others and acquiring more background knowledge (Bandura, 1971). Our contribution is that we extend the DAP method and combine it with the signed network partition in order to cluster the hidden group members. We also develop a novel cluster ensemble approach in order to analyze the dynamic network. Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated comm"
P15-1083,D13-1199,0,0.0176604,"ide his own stance. Moreover, their work did not take into account that a person’s attitude or stance will change as he learns more by reading the comments from others and acquiring more background knowledge (Bandura, 1971). Our contribution is that we extend the DAP method and combine it with the signed network partition in order to cluster the hidden group members. We also develop a novel cluster ensemble approach in order to analyze the dynamic network. Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated comm"
P15-1083,P11-1032,0,0.318342,"Missing"
P15-1083,D14-1155,0,0.0120732,"e hidden group members. We also develop a novel cluster ensemble approach in order to analyze the dynamic network. Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task scenario and obtained many interesting findings (e.g., deceivers send fewer messages than truth-tellers). Our work is most related to the work of Chittaranjan and Hung (2010) on detecting deceptive roles"
P15-1083,J11-1002,0,0.0776781,"Missing"
P15-1083,N13-1041,0,0.0223237,"Missing"
P15-1083,P09-1026,0,0.0816919,"Missing"
P15-1083,C12-2131,0,0.229548,"r contribution is that we extend the DAP method and combine it with the signed network partition in order to cluster the hidden group members. We also develop a novel cluster ensemble approach in order to analyze the dynamic network. Deception Detection Most of the previous computational work for deception detection used supervised/semisupervised classification methods (Li et al., 2013b). Besides lexical and syntactical features (Ott et al., 2011; Feng et al., 2012; Yancheva and Rudzicz, 2013), Feng and Hirst (2013) proposed using profile compatibility to distinguish fake and genuine reviews. Xu and Zhao (2012) used deep linguistic features such as text genre to detect deceptive opinion spams. Banerjee et al. (2014) used extended linguistic signals such as keystroke patterns. Li et al. (2013a) used topic models to detect the difference between deceptive and truthful topic-word distribution. Researchers have began to realize the importance of analyzing computer-mediated communication in deception detection. Zhou and Sung (2008) conducted an empirical study on deception cues using the killer game as a task scenario and obtained many interesting findings (e.g., deceivers send fewer messages than truth-"
P15-1083,P13-1093,0,0.330708,"971). Therefore, a person’s behavior concerning deception or truth-telling can change constantly, while he learns from others’ statements during conversations. 2. Global. People may form groups for purpose of deception. Research in social psychology has shown that an individual’s object-related behavior may be affected by the attitudes of other people due to group dynamics (Friedkin, 2010). Recent studies typically have been conducted over “static” written or oral deceptive statements. There is no obligatory requirement for communication between the author and the readers of these statements (Yancheva and Rudzicz, 2013). As a result, a victim of deception tends to trust the story mainly based on the statement he reads (Ott et al., 2011). However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. A deceiver may make a statement, which is partially true in order to deceive or mislead victims and adjust his deceptive strategies based on the reactions of victims (Zhou et al., 2004). Therefore, it is more challenging to identity a deceiver in an interactive process of deception. Most deception detection research addressed individual deceivers, but deceivers often"
P15-1083,W03-1730,0,0.00785407,"te for 2 · · · 10, 13, 5, 2 vote for 7 · · · 9, 6 vote for 11 · · · 2 is out. Figure 2: Killer sample log (the 1st round). 2.1 Target andgame Attitude Word Identification We start by identifying targets and attitude words from Target conversations. In the Word killer game, a target is 2.1 and Attitude Identification We start by identifying targets and attitude words from conversations. In the killer game, a target is represented by his unique ID2 and game terms are regarded as attitude words. We collected 41 terms in total from the game’s website 3 and related discussion forum posts. ICTCLAS (Zhang et al., 2003) is used for word segmentation and part-of-speech (POS) tagging. There are two kinds of game terms: positive and negative. Positive terms include “citizen”, “good person”, “good person certified by the detectives” and “detective”. Negative terms include “killer”, “killer verified by the detectives” and “a killer who claimed himself/herself to be a detective”. We assign the polarity score +1, -1 to positive and negative terms respectively. when they are mentioned, the attitude polarity score1 Each is setplayer to 0.hasFor instance, given Player 16’s sa game ID, assigned by the online game tatem"
P15-2047,P03-1054,0,0.0500541,"output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 287 Model relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 SVM MV-RNN CNN FCM DT-RNN DepNN Contributions of different components baseline (Path words) +Depedency relations +Attached subtrees +Lexical features 50-d 73.8 80.3 81.2 82.7 F1 200-d 75.5 81.8 82.8 83.6 We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the"
P15-2047,N07-2032,0,0.0416795,"Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path"
P15-2047,S10-1057,0,0.346496,"ve for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indi"
P15-2047,D12-1110,0,0.0907578,"word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings."
P15-2047,Q14-1017,0,0.023951,"n learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepNN, but not as obvious as NER. Yu et al. (2014) had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly rel"
P15-2047,H05-1091,0,0.250704,"n Li1,2 Heng Ji4 Ming Zhou3 Houfeng Wang1,2 1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 2 Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China 3 Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2"
P15-2047,I08-2119,0,0.124479,"Missing"
P15-2047,C14-1220,0,0.431582,"re, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combinat"
P15-2047,P05-1053,0,\N,Missing
P15-2047,W08-1301,0,\N,Missing
P15-2047,P04-1054,0,\N,Missing
P15-2047,D14-1070,0,\N,Missing
P15-2047,P06-1104,0,\N,Missing
P15-2061,P08-1030,1,0.909629,"Introduction Event trigger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-3"
P15-2061,P13-1008,1,0.784185,"s are encoded as a small set of event-independent classification features, based on lexical matches and external resources like WordNet. Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types. Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system. We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system (Li et al., 2013). When evaluated on the ACE-2005 dataset,1 our system outperforms the fully-supervised one (Section 4), even though we don’t utilize any annotated triggers of the test events during the labeling phase, and only The task of event trigger labeling is typically addressed in the standard supervised setting: triggers for each target event type are annotated as training data, based on annotation guidelines. We propose an alternative approach, which takes the example trigger terms mentioned in the guidelines as seeds, and then applies an eventindependent similarity-based classifier for trigger labeli"
P15-2061,C10-1077,0,0.363517,"ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association"
P15-2061,P10-1081,0,0.783761,"ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association"
P15-2061,N07-4013,0,0.0609519,"Missing"
P15-2061,P09-2015,0,0.0541745,"Missing"
P15-2061,P08-1004,0,0.0706965,"Missing"
P15-2061,P07-2005,0,0.0331584,"Missing"
P15-2061,P04-1015,0,0.0269755,"ers mentioned in each event description into a seed list for the event type, which is provided as input to our trigger labeling method. Triggers from the above quoted sentences are hence included in the Meet seed list, shown in Figure 1. As mentioned in the Introduction, our method (Section 3) is based on event-independent features 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi ∈ L ∪ {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi , yi ) is calculated as w · f , where f is the binary feature vector calculated for (xi , yi ), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additiona"
P15-2061,P06-2094,0,0.0914989,"Missing"
P15-2061,N06-1039,0,0.0182505,"Missing"
P15-2061,P05-1047,0,0.0254773,"ing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009)"
P15-2061,C00-2136,0,0.127914,"Missing"
P15-2061,N13-1092,0,0.0152668,"Missing"
P15-2061,W06-0204,0,0.0207608,"rk contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and language"
P15-2061,P11-1113,0,0.0616286,"f identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association for Computational L"
P15-2061,E12-1029,0,0.0449883,"extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009). They addressed a differ"
P15-2061,N12-1015,0,0.0101718,"nt description into a seed list for the event type, which is provided as input to our trigger labeling method. Triggers from the above quoted sentences are hence included in the Meet seed list, shown in Figure 1. As mentioned in the Introduction, our method (Section 3) is based on event-independent features 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi ∈ L ∪ {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi , yi ) is calculated as w · f , where f is the binary feature vector calculated for (xi , yi ), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additionally, each feature is"
P15-2110,D14-1082,0,0.0136346,"words. Method 2.1 Local Predictor Conversation-specific features: As mentioned in Section 1, different person roles being the subject or the object of a predicate may have an effect on the tense in a conversation. We analyze the person roles of the subject and the object of the main predicate and encode them as features, which helps our model understand effects of interactions on tense. We develop a Maximum Entropy (MaxEnt) classifier (Zhang, 2004) as the local predictor. Basic features: The unigrams, bigrams and trigrams of a sentence. Dependency parsing features: We use the Stanford parser (Chen and Manning, 2014) to conduct dependency parsing3 on the target sentences and use dependency paths associated with the main predicate of a sentence as well as their dependency types as features. By using the parsing features, 2 3 2.2 Global Predictor As we discussed before, tense ambiguity in a sentence arises from the omissions of sentence components. According to the principle of efficient information transmission (Jaeger and Levy, 2006; http://nlp.cs.rpi.edu/data/chinesetense.zip We use CCProcessed dependencies. 669 Jaeger, 2010) and Gricean Maxims (Grice et al., 1975) in cooperative theory, the omitted elem"
P15-2110,I11-1125,0,0.210627,"l) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark. 1 Introduction In natural languages, tense is important to indicate the time at which an action or event takes place. In some languages such as Chinese, verbs do not have explicit morphological or grammatical forms to indicate their tense information. Therefore, automatic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd"
P15-2110,xue-zhang-2014-buy,0,0.659972,"tion is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m telling you: the flight"
P15-2110,D08-1074,0,0.0792966,"Missing"
P15-2110,W06-0107,0,0.033577,"atic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m tel"
P15-2110,D14-1204,0,0.557711,"both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m telling you: the flight is canceled.) c:你 (you"
P15-2110,W03-1730,0,0.0363715,"−λ) n−1 X i=1 p p p p p local prediction p p c p p p global prediction p p p p p p sentences Figure 1: Global tense prediction for the conversation 5 in Table 1. 3 Experiments 3.1 Data and Scoring Metric To the best of our knowledge, tense prediction in Chinese conversations has never been studied before and there is no existing benchmark for evaluation. We collected 294 conversations (including 1,857 sentences) from 25 popular Chinese movies, dramas and TV shows. Each conversation contains 2-18 sentences. We manually annotate the main predicate and its tense in each sentence. We use ICTCLAS (Zhang et al., 2003) to do word segmentation as preprocessing. Since tense prediction can be seen as a multiclass classification problem, we use accuracy as the metric to evaluate the performance. We randomly split our dataset into three sets: training set (244 conversations), development set (25 conversations) and test set (25 conversations) for evaluation. In evaluation, we ignore imperative sentences and sentences without predicates. Global tense prediction Inspired by the burst detection algorithm proposed by Kleinberg (2003), we use a 3-state automaton sequence model to globally predict tense based on the ab"
P15-5001,P13-1107,1,0.867799,"Missing"
P15-5001,P14-1036,1,0.885699,"Missing"
P15-5001,C14-1149,1,0.832877,"Missing"
P16-1005,D14-1164,0,0.0722373,"chmark. 1 E1: Ellen Griffin Dunne, from whom he was divorced in 1965, died in 1997. died Ellen Griffin Dunne 1997 Person case Year divorced nmod whom case from nsubjpass he coreference Dominick Dunne in auxpass was nmod 1965 Year case in Person |Query Figure 1: Extended dependency tree for E1. Slot filling remains a very challenging task. The two most successful state-of-the-art techniques are as follows. (1) Supervised classification. Considering any pair of query and candidate slot filler as an instance, these approaches train a classifier from manually labeled data through active learning (Angeli et al., 2014b) or noisy labeled data through distant supervision (Angeli et al., 2014a; Surdeanu et al., 2010) to predict the existence of a specific relation between them. (2) Pattern matching. These approaches extract and generalize lexical and syntactic patterns automatically or semi-automatically (Sun et al., 2011; Li et al., 2012; Yu et al., 2013; Hong et al., 2014). They usually suffer from low recall due to numerous different ways to express a certain relation type (Surdeanu and Ji, 2014). For example, none of the top-ranked patterns (Li et al., 2012) based on dependency paths in Table 1 can captur"
P16-1005,P15-2061,1,0.818432,"final average: Ellen Griffin Dunne Person |Filler Trigger Gazetteer for slot spouse { Dominick Dunne|Query, spouse, Ellen Griffin Dunne|Filler } Figure 4: Example of slot type labeling. example, Figure 4 shows how we label the relation as a spouse slot type. In fact, some trigger gazetteers have already been constructed by previous work such as (Yu et al., 2015). However, manual construction of these triggers heavily rely upon labeled training data and high-quality patterns, which would be unavailable for a new language or a new slot type. Inspired by the trigger-based event extraction work (Bronstein et al., 2015), we propose to extract trigger seeds from the slot filling annotation guideline 1 and then expand them by paraphrasing techniques. For each slot type we manually select two trigger seeds from the guideline and then use the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015) to expand these seeds. Specifically, we select top-20 lexical paraphrases based on similarity scores as our new triggers for each slot type. Some examples are shown in Table 2. Seeds Slot Types Expanded Triggers assassinate graduate sister marriage death schools siblings spouse kill, die, slay, mur"
P16-1005,C10-3004,0,0.0991769,"Missing"
P16-1005,N13-1092,0,0.0179434,"Missing"
P16-1005,P03-1056,0,0.0644617,"Missing"
P16-1005,P14-5010,0,0.00861443,"Missing"
P16-1005,N15-1126,1,0.77059,"as the credibility score of r. The reason is that the higher the relative importance score, the more likely the tuple is to be correct. In our experiments, we use the weighted arithmetic mean as follows so that higher scores can contribute more to the final average: Ellen Griffin Dunne Person |Filler Trigger Gazetteer for slot spouse { Dominick Dunne|Query, spouse, Ellen Griffin Dunne|Filler } Figure 4: Example of slot type labeling. example, Figure 4 shows how we label the relation as a spouse slot type. In fact, some trigger gazetteers have already been constructed by previous work such as (Yu et al., 2015). However, manual construction of these triggers heavily rely upon labeled training data and high-quality patterns, which would be unavailable for a new language or a new slot type. Inspired by the trigger-based event extraction work (Bronstein et al., 2015), we propose to extract trigger seeds from the slot filling annotation guideline 1 and then expand them by paraphrasing techniques. For each slot type we manually select two trigger seeds from the guideline and then use the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015) to expand these seeds. Specifically, we s"
P16-1005,P15-2070,0,0.0215996,"Missing"
P16-1005,N10-1013,0,0.0302383,"Missing"
P16-1005,P13-1106,0,0.0209598,"Missing"
P16-1006,W13-2322,1,0.579501,"ppear in the document. Figure 2: Examples of Cartoons in Chinese (left) and English (right). cept whose images appear frequently (e.g., “宝宝 (baby)” and “螃蟹 (crab)” in “Dora Exploration”, “海盗 (pirate)” in “the Garden Guardians”, and “亨利 (Henry)” in “Thomas Train”), as illustrated in Figure 2. Representation and Structured Knowledge Transfer for Entity Linking: Besides data sparsity, another challenge for low-resource language IE lies in the lack of knowledge resources. For example, there are advanced knowledge representation parsing tools available (e.g., Abstract Meaning Representation (AMR) (Banarescu et al., 2013)) and large-scale knowledge bases for English Entity Linking, but not for other languages, including some medium-resource ones such as Chinese. For example, the following documents are both about the event of Pistorius killing his girl friend Reeva: Next we will project these names extracted from HL documents directly to LL documents to identify and verify names. In addition to textual evidence, we check visual similarity to match an HL name with its equivalent in LL. And we apply face recognition techniques to verify person names by image search. This idea matches human knowledge acquisition"
P16-1006,W14-3902,0,0.0383599,"Missing"
P16-1006,W13-2233,0,0.0496064,"Missing"
P16-1006,W09-3107,1,0.886171,"Missing"
P16-1006,C10-1064,0,0.015831,"orms our baseline and the best reported results on the same test set (Ji et al., 2015). Our approach is particularly effective for rare nicknames (e.g., “C罗” (C Luo) is used to refer to Cristiano Ronaldo) or ambiguous abbreviations (e.g., “邦联” (federal) can refer to Confederate States of America, 邦联制 (Confederation) and many other entities) for which the contexts in LLs are not sufficient for making correct linking decisions due to the lack of rich knowledge rep8 Related Work Some previous cross-lingual projection methods focused on transferring data/annotation (e.g., (Pad´o and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015)), shared feature representation/model (e.g., (McDonald et al., 2011; Kozhevnikov and Titov, 2013; Kozhevnikov and Titov, 2014)), or expectation (e.g., (Wang and Manning, 2014)). Most of them relied on a large 61 Approach Baseline State-of-the-art Our Approach Our Approach w/o KB Walker PER 49.12 49.85 52.36 50.44 ORG 60.18 64.30 67.05 67.05 Overall GPE 80.97 75.38 93.33 84.41 LOC 80.68 96.59 93.18 90.91 ALL 66.57 65.87 74.92 70.32 PER 67.27 68.28 71.72 69.09 Linkable Entities ORG GPE LOC 67.61 81.05 80.68 72.24 75.46 96.59 75.32 93.43 93.18 75.32 84.50 90.91 ALL 74.7"
P16-1006,W10-2407,0,0.030237,"Missing"
P16-1006,P06-1103,0,0.10768,"Missing"
P16-1006,N15-1151,0,0.0290662,"and the best reported results on the same test set (Ji et al., 2015). Our approach is particularly effective for rare nicknames (e.g., “C罗” (C Luo) is used to refer to Cristiano Ronaldo) or ambiguous abbreviations (e.g., “邦联” (federal) can refer to Confederate States of America, 邦联制 (Confederation) and many other entities) for which the contexts in LLs are not sufficient for making correct linking decisions due to the lack of rich knowledge rep8 Related Work Some previous cross-lingual projection methods focused on transferring data/annotation (e.g., (Pad´o and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015)), shared feature representation/model (e.g., (McDonald et al., 2011; Kozhevnikov and Titov, 2013; Kozhevnikov and Titov, 2014)), or expectation (e.g., (Wang and Manning, 2014)). Most of them relied on a large 61 Approach Baseline State-of-the-art Our Approach Our Approach w/o KB Walker PER 49.12 49.85 52.36 50.44 ORG 60.18 64.30 67.05 67.05 Overall GPE 80.97 75.38 93.33 84.41 LOC 80.68 96.59 93.18 90.91 ALL 66.57 65.87 74.92 70.32 PER 67.27 68.28 71.72 69.09 Linkable Entities ORG GPE LOC 67.61 81.05 80.68 72.24 75.46 96.59 75.32 93.43 93.18 75.32 84.50 90.91 ALL 74.70 73.91 84.06 78.91 Table"
P16-1006,P13-1117,0,0.0415773,"Missing"
P16-1006,P14-2095,0,0.0558813,"Missing"
P16-1006,D15-1070,0,0.026279,". feedback to improve name classification (Sil and Yates, 2013; Heinzerling et al., 2015; Besancon et al., 2015; Sil et al., 2015). amount of parallel data to derive word alignment and translations, which are inadequate for many LLs. In contrast, we do not require any parallel data or bi-lingual lexicon. We introduce new cross-media techniques for projecting HLs to LLs, by inferring projections using domain-rich, nonparallel data automatically discovered by image search and processing. Similar image-mediated approaches have been applied to other tasks such as cross-lingual document retrieval (Funaki and Nakayama, 2015) and bilingual lexicon induction (Bergsma and Van Durme, 2011). Besides visual similarity, their method also relied on distributional similarity computed from a large amount of unlabeled data, which might not be available for some LLs. 9 Conclusions and Future Work We describe a novel multi-media approach to effectively transfer entity knowledge from highresource languages to low-resource languages. In the future we will apply visual pattern recognition and concept detection techniques to perform deep content analysis of the retrieved images, so we can do matching and inference on concept/enti"
P16-1006,D14-1198,1,0.827757,"for the walk-through example), we start by extracting its key phrases using the following three languageindependent methods: (1) TextRank (Mihalcea and Tarau, 2004), which is a graph-based ranking model to determine key phrases. (2) Topic modeling based on Latent Dirichlet allocation (LDA) model (Blei et al., 2003), which can generate a small number of key phrases representing the main topics of each document. (3) The title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at th"
P16-1006,W11-2206,1,0.898665,"Missing"
P16-1006,P99-1067,0,0.0275473,"Missing"
P16-1006,P14-5010,0,0.00878046,"Missing"
P16-1006,C04-1089,0,0.0460909,"Missing"
P16-1006,D11-1006,0,0.0346974,"Missing"
P16-1006,J05-4003,0,0.0840005,"Missing"
P16-1006,P06-1010,0,0.0572594,"Missing"
P16-1006,W10-2412,0,0.0245475,"Missing"
P16-1006,W10-2408,0,0.0741018,"Missing"
P16-1006,E09-1091,0,0.0702251,"Missing"
P16-1006,N15-1119,1,0.842821,"e title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at the hub and leaf nodes obtained from names reachable by AMR graph traversal from nh . A subset of the leaf nodes are selected as collaborators of nh . Names connected by AMR conjunction relations are grouped into sets of coherent names. For each name nh , an initial ranked list of entity candidates E = {e1 , ..., eM } is generated based on a salience measure (Medelyan and Legg, 2008). Then a Knowledge Graph g(em ) is gen"
P16-1006,voss-etal-2014-finding,0,0.0602614,"Missing"
P16-1006,Q14-1005,0,0.0529122,"Missing"
P16-1006,P15-2141,0,0.0178262,"), which can generate a small number of key phrases representing the main topics of each document. (3) The title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at the hub and leaf nodes obtained from names reachable by AMR graph traversal from nh . A subset of the leaf nodes are selected as collaborators of nh . Names connected by AMR conjunction relations are grouped into sets of coherent names. For each name nh , an initial ranked list of entity candidates E = {e1 , ..., eM }"
P16-1006,D15-1081,1,0.841841,"), which can generate a small number of key phrases representing the main topics of each document. (3) The title of the document if it’s available. 3.2 4.1 Name Tagging After we acquire HL (English in this paper) comparable documents, we apply a state-of-the-art English name tagger (Li et al., 2014) based on structured perceptron to extract names. From the output we filter out uninformative names such as news agencies. If the same name receives multiple types across documents, we use the majority one. 4.2 Entity Linking We apply a state-of-the-art Abstract Meaning Representation (AMR) parser (Wang et al., 2015a) to generate rich semantic representations. Then we apply an AMR based entity linker (Pan et al., 2015) to link all English entity mentions to the corresponding entities in the English KB. Given a name nh , this entity linker first constructs a Knowledge Graph g(nh ) with nh at the hub and leaf nodes obtained from names reachable by AMR graph traversal from nh . A subset of the leaf nodes are selected as collaborators of nh . Names connected by AMR conjunction relations are grouped into sets of coherent names. For each name nh , an initial ranked list of entity candidates E = {e1 , ..., eM }"
P16-1006,W04-3252,0,\N,Missing
P16-1025,C96-1079,0,0.793923,"ccording to predefined event types. The extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributi"
P16-1025,P03-2030,0,0.054262,"er and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, inst"
P16-1025,W13-2322,0,0.0832222,"over vectors in the embedding space. Argument representations are generated as a by-product. Trigger and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Tempora"
P16-1025,P13-1088,0,0.0611697,"Missing"
P16-1025,N07-4013,0,0.0723425,"Missing"
P16-1025,P11-1113,0,0.0548102,"r = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event type. For example, for Attack events, besides"
P16-1025,P08-1004,0,0.117222,"Missing"
P16-1025,E06-1018,0,0.0144014,"Missing"
P16-1025,N06-2015,0,0.0484569,"sed on Hypothesis 1, we learn sense-based embeddings from a large data set, using the Continuous Skip-gram model (Mikolov et al., 2013). Specifically, we first apply WSD to link each word to its sense in WordNet using a state-of-the-art tool (Zhong and Ng, 2010), and map WordNet sense output to OntoNotes senses. 4 We map each trigger candidate to its OntoNotes sense and learn a distinct embedding for each sense. We use general lexical embeddings for arguments. Candidate Trigger and Argument Identification Given a sentence, we consider all noun and verb concepts that are assigned an OntoNotes (Hovy et al., 2006) sense by WSD as candidate event triggers. Any remaining concepts that match both a verbal and a nominal lexical unit in the FrameNet corpus are considered candidate event triggers as well. This mainly helps to identify more nominal triggers like “pickpocket” and “sin”.2 For each candidate event trigger, we consider as candidate arguments all concepts for which one of a manually-selected set of semantic relations holds between it and the event trigger. For the setting in which AMR serves as our meaning representation, we selected a subset of all AMR relations that specify event arguments, as s"
P16-1025,P08-1030,1,0.847292,"KT ) A A Ccurr = spectral(A, Eg , KA ) T A Ocurr = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event"
P16-1025,E09-1013,0,0.0124643,"Missing"
P16-1025,P11-1115,1,0.769669,"E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clustering distance metric. The distributional hypothesis (Harris, 1"
P16-1025,P11-1098,0,0.270072,"Missing"
P16-1025,D13-1185,0,0.0229869,"Missing"
P16-1025,C12-1033,0,0.0174421,"Missing"
P16-1025,P13-1008,1,0.853732,"Missing"
P16-1025,P15-1017,0,0.247756,"Missing"
P16-1025,P10-1081,0,0.0298077,"ral(A, Eg , KA ) T A Ocurr = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event type. For example, for At"
P16-1025,J14-1002,0,0.0287898,"n the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-Related AMR Relations. In E1, for example, “killed”, “injured” and “fighting” ar"
P16-1025,C10-2087,0,0.399724,"Missing"
P16-1025,N06-1039,0,0.0802104,"Missing"
P16-1025,de-marneffe-etal-2006-generating,0,0.102939,"Missing"
P16-1025,D11-1014,0,0.0551092,"Reconstruct: (X’ga,Y’l)=Z1W’mod+b’ T Z1=fmod(Wmod,Xga,Yl)=X gaWmodYl+b lose Figure 4: Partial AMR and Event Structure for E2. E(VI , VO ) = 1 ||VI − VO ||2 2 For each pair of words X and Y , the reconstruction error back-propagates from its output layer to input layer through parameters Θr = 0 0 (Wr , br , Wr , br ). Let δO be the residual error of the output layer, and δH be the error of the hidden layer: 0 O lated words for the event trigger with sense “lose1” and construct the event structure for the whole event, as shown in Figure 4. We design a Tensor based Recursive AutoEncoder (TRAE) (Socher et al., 2011) framework to utilize a tensor based composition function for each of a subset of the AMR semantic relations and compose the event structure representation based on multiple functional applications. This subset was manually selected by the authors as the set of relations that link a trigger to concepts that help to determine its type. Similarly, we selected a subset of dependency and FrameNet relations using the same criteria for experiments using those meaning representations. Figure 4 shows an instance of a TRAE applied to an event structure to generate its representation. For each semantic"
P16-1025,P11-1163,0,0.0337219,"Missing"
P16-1025,D09-1013,0,0.086859,"Missing"
P16-1025,D13-1170,0,0.00276702,"Missing"
P16-1025,W03-0419,0,0.104049,"he extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clu"
P16-1025,P11-1148,0,0.0424776,"Missing"
P16-1025,P15-2060,0,0.0603427,"Missing"
P16-1025,P15-2141,0,0.0160239,"ach cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-"
P16-1025,P15-1019,0,0.0988925,"Missing"
P16-1025,Q15-1005,0,0.0118071,"ach cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-"
P16-1025,X93-1013,0,0.717501,"a large amount of data labeled according to predefined event types. The extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster repre"
P16-1025,W12-3022,0,0.0652492,"Missing"
P16-1025,J05-1004,0,0.0379358,"ts. For each event trigger, we apply a series of compositional functions to generate that trigger’s event structure representation. Each function is specific to a semantic relation, and operates over vectors in the embedding space. Argument representations are generated as a by-product. Trigger and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning repres"
P16-1025,P12-1059,0,0.056824,"Missing"
P16-1025,S07-1096,0,0.0753062,"Missing"
P16-1025,P10-4014,0,0.0661832,"e 2: Event-Related AMR Relations. In E1, for example, “killed”, “injured” and “fighting” are identified as candidate triggers, and three concept sets are identified as candidate arguments using AMR relations: “{Two Soldiers, very large missile}”, “{one, Kut}” and “{Two Soldiers, Kut}”, as shown in Figure 3. 2.3 Trigger Sense and Argument Representation Based on Hypothesis 1, we learn sense-based embeddings from a large data set, using the Continuous Skip-gram model (Mikolov et al., 2013). Specifically, we first apply WSD to link each word to its sense in WordNet using a state-of-the-art tool (Zhong and Ng, 2010), and map WordNet sense output to OntoNotes senses. 4 We map each trigger candidate to its OntoNotes sense and learn a distinct embedding for each sense. We use general lexical embeddings for arguments. Candidate Trigger and Argument Identification Given a sentence, we consider all noun and verb concepts that are assigned an OntoNotes (Hovy et al., 2006) sense by WSD as candidate event triggers. Any remaining concepts that match both a verbal and a nominal lexical unit in the FrameNet corpus are considered candidate event triggers as well. This mainly helps to identify more nominal triggers li"
P16-1025,W11-1901,0,0.0129852,"y of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clustering distance metric"
P16-1025,D11-1001,0,0.0699272,"Missing"
P16-1025,P06-2094,0,0.0223593,"Missing"
P16-1025,Q14-1017,0,\N,Missing
P16-1025,S10-1011,0,\N,Missing
P16-2011,P15-2047,1,0.12748,"as alternatives. 2.2 Output 2.4 Training In our model, the loss function is the cross-entropy error of event trigger identification and trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our mode"
P16-2011,R15-1010,0,0.0363436,"and syntactic features, thus the precision is lower than neural network based methods. (4) RNN and LSTM perform slightly worse than Bi-LSTM. An obvious reason is that RNN and LSTM only consider the preceding sequence information of the trigger, which may miss some important following clues. Considering S1 again, when extracting the trigger “releases”, both models will miss the following sequence “20 million euros to Iraq”. This may seriously hinder the performance of RNN and LSTM for event detection. Table 2: Comparison of different methods on English event detection. (5) Pattern Recognition (Miao and Grishman, 2015), using a pattern expansion technique to extract event triggers. (6) Convolutional Neural Network (Chen et al., 2015), which exploits a dynamic multi-pooling convolutional neural network for event trigger detection. 3.2 Comparison On English Table 2 shows the overall performance of all methods on the ACE2005 English corpus. We can see that our approach significantly outperforms all previous methods. The better performance of HNN can be further explained by the following reasons: (1) Compared with feature based methods, such as MaxEnt, Cross-Event, Cross-Entity, and Joint Model, neural network"
P16-2011,C12-1033,0,0.0193323,"Missing"
P16-2011,P15-1017,0,0.563445,"t trigger or not. Specifically, we first use a Bi-LSTM to encode semantics of each word with its preceding and following information. Then, we add a convolutional neural network to capture structure information from local contexts. 2.1 Bi-LSTM In this section we describe a Bidirectional LSTM model for event detection. Bi-LSTM is a type of bidirectional recurrent neural networks (RNN), which can simultaneously model word representation with its preceding and following information. Word representations can be naturally considered as features to detect triggers and their event types. As show in (Chen et al., 2015), we take all the words of the whole sentence as the input and each token is transformed by looking up word embeddings. Specifically, we use the SkipGram model to pre-train the word embeddings to represent each word (Mikolov et al., 2013; Bahdanau et al., 2014). We present the details of Bi-LSTM for event trigger extraction in Figure 2. We can see that Bi-LSTM is composed of two LSTM neural networks, a forward LSTMF to model the preced67 vector with fixed length. C3 Max-Pooling Feature Map 1 Feature Map 2 Lookup 2.3 Feature Map n ... Convolution ... At the end, we concatenate the bidirectional"
P16-2011,P15-2060,0,0.548592,"d trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our model fix some errors due to lexical ambiguity. An illustration of CNN with three convolutional filters is given in Figure 3. Let us denote a sent"
P16-2011,P11-1113,0,0.595906,"ection aims to extract event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”)"
P16-2011,P08-1030,1,0.898397,"Missing"
P16-2011,D15-1167,1,0.218601,"it will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-LSTM and C2 , C3 are the output of CNN with convolutional filters with widths of 2 and 3. use of word embeddings to induce a more general representation for trigger candidates. Recently, deep learning techniques have been widely used in modeling complex structures and proven effective for many NLP tasks, such as machine translation (Bahdanau et al., 2014), relation extraction (Zeng et al., 2014) and sentiment analysis (Tang et al., 2015a). Bi-directional long short-term memory (Bi-LSTM) model (Schuster et al., 1997) is a two-way recurrent neural network (RNN) (Mikolov et al., 2010) which can capture both the preceding and following context information of each word. Convolutional neural network (CNN) (LeCun et al., 1995) is another effective model for extracting semantic representations and capturing salient features in a flat structure (Liu et al., 2015), such as chunks. In this work, we develop a hybrid neural network incorporating two types of neural networks: Bi-LSTM and CNN, to model both sequence and chunk information f"
P16-2011,P14-1038,1,0.0783646,"e (F). Table 1 shows the detailed description of the data sets used in our experiments. We abbreviate our model as HNN (Hybrid Neural Networks). 3.1 Baseline Methods We compare our approach with the following baseline methods. (1) MaxEnt, a basesline feature-based method, which trains a Maximum Entropy classifier with some lexical and syntactic features (Ji et al., 2008). (2) Cross-Event (Liao et al., 2010), using document-level information to improve the performance of ACE event extraction. (3) Cross-Entity (Hong et al., 2011), extracting events using cross-entity inference. (4) Joint Model (Li and Ji, 2014), a joint structured perception approach, incorporating multilevel linguistic features to extract event triggers and arguments at the same time so that local predictions can be mutually improved. 68 Language English Chinese Spanish Word Embedding corpus dim NYT 300 Gigaword 300 Gigaword 300 Gradient Learning Method method parameters SGD learning rate r = 0.03 Adadelta p = 0.95, δ = 1e−6 Adadelta p = 0.95, δ = 1e−6 Corpus ACE2005 ACE2005 ERE Data Sets Train Dev 529 30 513 60 93 12 Test 40 60 12 Table 1: Hyperparameters and # of documents used in our experiments on three languages. Model MaxEnt"
P16-2011,P13-1008,1,0.945252,"act event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are t"
P16-2011,D15-1278,0,0.0128362,"Missing"
P16-2011,C14-1220,0,0.0106635,"Missing"
P16-2011,P15-1107,0,0.0217978,"s (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-L"
P16-2011,P10-1081,0,0.353297,"Missing"
P17-1149,P14-2013,0,0.037015,"Missing"
P17-1149,D15-1077,1,0.895996,"Missing"
P17-1149,D14-1110,0,0.0679262,"Missing"
P17-1149,D07-1074,0,0.119191,"Missing"
P17-1149,P05-1045,0,0.00358486,"entity ej . Example As shown in Figure 1, Independence Day (m1 ) has two mention senses s11 , s12 , and July 4th (m2 ) has one mention sense s22 . Based on the assumption in Section 1, we have s∗2 = s12 = s22 referring to entity Independence Day (US) (e2 ). 3 An Overview of Our Method Given a knowledge base KB, an annotated text corpus D0 and a set of anchors A, we aim to jointly learn word, entity and mention sense representations: w, e, s. As shown in Figure 2, our framework contains two key components: 1 Generally, the mention boundary can be obtained by using NER tools like Standford NER (Finkel et al., 2005). In this paper, we use Wikipedia anchors as annotations of Wikipedia text corpus for the concentration of our main purpose. 1624 211 cluster center is clustering the average of all the context vecIndependence (f ilm)the context323 nearest distance context vector to sense 472(3) and (4) 422 204 336 tion has andenotes embedding (sense vector) and 1, 6 1 for max model. Conclusion (WSD) use4.6 context information to language ter isthe larger than a the threshold, we create atl clus462 dings e.same Note that ssense m that mention We jointly train (2), by373 using a un 413 on Software Engineering,"
P17-1149,N16-1150,0,0.0537076,"Missing"
P17-1149,D11-1072,0,0.51378,"Missing"
P17-1149,P12-1092,0,0.0460393,"film: Independence Day (film). Second, an entity often has various aliases when mentioned in various contexts, which implies a much larger size of mention vocabulary compared with entities. For example, in Figure 1, the documents d2 and d3 describes the same entity Independence Day (US) (e2 ) with distinct mentions: independence day and July 4th. We observe tens of millions of mentions referring to 5 millions of entities in Wikipedia. To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task (Reisinger and Mooney, 2010; Huang et al., 2012; 1623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623–1633 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1149 Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). The basic idea behind it is to consider entities in KBs that can provide a meaning repository of mentions (i.e. words or phrases) in texts. That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity. Furthermore, we assume that diff"
P17-1149,D15-1200,0,0.0220958,"ct mentions: independence day and July 4th. We observe tens of millions of mentions referring to 5 millions of entities in Wikipedia. To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task (Reisinger and Mooney, 2010; Huang et al., 2012; 1623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623–1633 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1149 Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). The basic idea behind it is to consider entities in KBs that can provide a meaning repository of mentions (i.e. words or phrases) in texts. That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity. Furthermore, we assume that different mentions referring to the same entity express the same meaning and share a common mention sense embedding, which largely reduces the size of mention vocabulary to be learned. For example, the mentions Independence Day in d2 and July 4th in d3 have a common mention sense embedding during training since t"
P17-1149,D14-1113,0,0.0387774,"Day (US) (e2 ) with distinct mentions: independence day and July 4th. We observe tens of millions of mentions referring to 5 millions of entities in Wikipedia. To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task (Reisinger and Mooney, 2010; Huang et al., 2012; 1623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623–1633 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1149 Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). The basic idea behind it is to consider entities in KBs that can provide a meaning repository of mentions (i.e. words or phrases) in texts. That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity. Furthermore, we assume that different mentions referring to the same entity express the same meaning and share a common mention sense embedding, which largely reduces the size of mention vocabulary to be learned. For example, the mentions Independence Day in d2 and July 4th in d3 have a common mention sense embedding"
P17-1149,N15-1026,0,0.14064,"Missing"
P17-1149,N10-1013,0,0.0108623,": Independence Day (US) or a film: Independence Day (film). Second, an entity often has various aliases when mentioned in various contexts, which implies a much larger size of mention vocabulary compared with entities. For example, in Figure 1, the documents d2 and d3 describes the same entity Independence Day (US) (e2 ) with distinct mentions: independence day and July 4th. We observe tens of millions of mentions referring to 5 millions of entities in Wikipedia. To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task (Reisinger and Mooney, 2010; Huang et al., 2012; 1623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623–1633 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1149 Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). The basic idea behind it is to consider entities in KBs that can provide a meaning repository of mentions (i.e. words or phrases) in texts. That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity. Furthermore,"
P17-1149,C14-1016,0,0.0310231,"ntity Independence Day (US) (e2 ) with distinct mentions: independence day and July 4th. We observe tens of millions of mentions referring to 5 millions of entities in Wikipedia. To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task (Reisinger and Mooney, 2010; Huang et al., 2012; 1623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623–1633 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1149 Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). The basic idea behind it is to consider entities in KBs that can provide a meaning repository of mentions (i.e. words or phrases) in texts. That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity. Furthermore, we assume that different mentions referring to the same entity express the same meaning and share a common mention sense embedding, which largely reduces the size of mention vocabulary to be learned. For example, the mentions Independence Day in d2 and July 4th in d3 have a comm"
P17-1149,D15-1174,0,0.0378085,"Bridging Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding Yixin Cao1 , Lifu Huang2 , Heng Ji2 , Xu Chen1 , Juanzi Li1∗ Tsinghua National Laboratory for Information Science and Technology Dept. of Computer Science and Technology, Tsinghua University, China 100084 {caoyixin2011,successcx,lijuanzi2008}@gmail.com 2 Dept. of Computer Science, Rensselaer Polytechnic Institute, USA 12180 {huangl7,jih}@rpi.edu 1 Abstract Networks (DNN). These methods suffer from the problems of expensive training and great limitations on the size of word and entity vocabulary (Han et al., 2016; Toutanova et al., 2015; Wu et al., 2016). The other is to learn word and entity embeddings separately, and then align similar words and entities into a common space with the help of Wikipedia hyperlinks, so that they share similar representations (Wang et al., 2014; Yamada et al., 2016). Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel"
P17-1149,D14-1167,0,0.445264,"gy, Tsinghua University, China 100084 {caoyixin2011,successcx,lijuanzi2008}@gmail.com 2 Dept. of Computer Science, Rensselaer Polytechnic Institute, USA 12180 {huangl7,jih}@rpi.edu 1 Abstract Networks (DNN). These methods suffer from the problems of expensive training and great limitations on the size of word and entity vocabulary (Han et al., 2016; Toutanova et al., 2015; Wu et al., 2016). The other is to learn word and entity embeddings separately, and then align similar words and entities into a common space with the help of Wikipedia hyperlinks, so that they share similar representations (Wang et al., 2014; Yamada et al., 2016). Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language m"
P17-1149,D13-1136,0,0.0163966,"tative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance. 1 Independence Day (film) Text Figure 1: Examples. Jointly learning text and knowledge representations in a unified vector space greatly benefits many Natural Language Processing (NLP) tasks, such as knowledge graph completion (Han et al., 2016; Wang and Li, 2016), relation extraction (Weston et al., 2013), word sense disambiguation (Mancini et al., 2016), entity classification (Huang et al., 2017) and linking (Huang et al., 2015). Existing work can be roughly divided into two categories. One is encoding words and entities into a unified vector space using Deep Neural Corresponding author. e1 uses alien technology… m1 e2 Independence Day (US) … holds annual Independence m2 d2 Day celebrations and other Knowledge Base festivals … … bands played it during Mention d3 public events, such as July 4th m1 Independence Day celebrations. m2 July 4th Introduction ∗ Entity … action film &apos;&apos;Independence d1"
P17-1149,K16-1025,0,0.381344,"sity, China 100084 {caoyixin2011,successcx,lijuanzi2008}@gmail.com 2 Dept. of Computer Science, Rensselaer Polytechnic Institute, USA 12180 {huangl7,jih}@rpi.edu 1 Abstract Networks (DNN). These methods suffer from the problems of expensive training and great limitations on the size of word and entity vocabulary (Han et al., 2016; Toutanova et al., 2015; Wu et al., 2016). The other is to learn word and entity embeddings separately, and then align similar words and entities into a common space with the help of Wikipedia hyperlinks, so that they share similar representations (Wang et al., 2014; Yamada et al., 2016). Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to"
P17-1178,N15-1107,0,0.0299644,"s through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consistent style and format, we propose an effective me"
P17-1178,C12-2005,0,0.0398197,"Missing"
P17-1178,R13-1005,0,0.0213967,"hods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal and informal texts and obtained promising results."
P17-1178,E14-3012,0,0.0322345,"Missing"
P17-1178,P03-2031,0,0.129469,"t handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic surve"
P17-1178,W13-2322,1,0.762364,"omputational Linguistics, pages 1946–1958 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1178 tr/Çin_K en/Comm 282 languages, and link them to an English KB (Wikipedia in this work). The major challenges and our new solutions are summarized as follows. Creating “Silver-standard” through crosslingual entity transfer. The first step is to classify English Wikipedia entries into certain entity types and then propagate these labels to other languages. We exploit the English Abstract Meaning Representation (AMR) corpus (Banarescu et al., 2013) which includes both name tagging and linking annotations for fine-grained entity types to train an automatic classifier. Furthermore, we exploit each entry’s properties in DBpedia as features and thus eliminate the need of language-specific features and resources such as part-of-speech tagging as in previous work (Section 2.2). Refine annotations through self-training. The initial annotations obtained from above are too incomplete and inconsistent. Previous work used name string match to propagate labels. In contrast, we apply self-training to label other mentions without links in Wikipedia a"
P17-1178,C10-1018,0,0.0173,"advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches requir"
P17-1178,N13-1006,0,0.0254816,"d on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same d"
P17-1178,I08-1071,0,0.089043,"Missing"
P17-1178,D15-1103,0,0.0422904,"Missing"
P17-1178,W10-2415,0,0.0132221,"ired from previous work that leveraged Wikipedia markups to train name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as back"
P17-1178,C02-1130,0,0.0349275,"er standard generation: Our work was mainly inspired from previous work that leveraged Wikipedia markups to train name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language base"
P17-1178,W09-1125,0,0.0774484,"Missing"
P17-1178,C14-1111,0,0.0173537,"Missing"
P17-1178,D14-1012,0,0.0209447,"Missing"
P17-1178,U14-1006,1,0.83456,"Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai"
P17-1178,P06-2055,1,0.710096,"2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal"
P17-1178,J03-1002,0,0.0210511,"Missing"
P17-1178,D07-1073,0,0.0495625,"ions based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI prog"
P17-1178,P12-1073,0,0.103702,"rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous met"
P17-1178,N16-1030,0,0.0776175,"Missing"
P17-1178,C16-1095,0,0.0821433,"Missing"
P17-1178,R13-1053,0,0.0239555,"). Customize annotations through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consistent style and format, we"
P17-1178,P14-5010,0,0.0041107,"Missing"
P17-1178,I11-1029,0,0.137923,"6) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (McNamee et al., 2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and"
P17-1178,P09-1113,0,0.0368323,"names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 201"
P17-1178,P13-1146,0,0.00785015,"rain name taggers (Nothman et al., 2008; Dakka and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additio"
P17-1178,U08-1016,1,0.909684,"Missing"
P17-1178,N15-1119,1,0.926167,"in English KB Salience, Similarity and Coherence Comparison Linking Translated and Linked Mentions m1 t1 e1 m2 t2 e1 m3 t3 e2 m4 t4 e3 m5 t5 NIL m6 t6 NIL Figure 3: Cross-lingual Entity Linking Overview 3.2 Name Translation The cross-lingual Wikipedia title pairs, generated through crowd-sourcing, generally follow a consistent style and format in each language. From Table 2 we can see that the order of modifier and head word keeps consistent in Turkish and English titles. 8 http://www.darpa.mil/program/low-resource-languagesfor-emergent-incidents 1949 to the KB, similar to our previous work (Pan et al., 2015). The only difference is that we construct knowledge networks (KNs) g(ti ) for T based on their co-occurrence within a context window 9 instead of their AMR relations, because AMR parsing is not available for foreign languages. For each translated name mention ti , an initial list of candidate entities E(ti ) = {e1 , e2 , ..., ek } is generated based on a surface form dictionary mined from KB properties (e.g., redirects, names, aliases). If no surface form can be matched then we determine the mention as unlinkable. Then we construct KNs g(ej ) for each entity candidate ej in ti ’s entity candi"
P17-1178,N06-1025,0,0.0111175,"ms both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-"
P17-1178,P08-1001,0,0.0334855,"1 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that self-training can provide further gains when the training data contains certain amount of noise. 6 Conclusions and Future Work We developed a simple yet effective framework that can extract names from 282 languages and link them to an English KB. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal and informal texts and ob"
P17-1178,U09-1004,1,0.913527,"Missing"
P17-1178,P08-2030,0,0.0100737,"ntions (Section 2.4). Customize annotations through cross-lingual topic transfer. For the first time, we propose to customize name annotations for specific downstream applications. Again, we use a cross-lingual knowledge transfer strategy to leverage the widely available English corpora to choose entities with specific Wikipedia topic categories (Section 2.5). Derive morphology analysis from Wikipedia markups. Another unique challenge for morphologically rich languages is to segment each token into its stemming form and affixes. Previous methods relied on either high-cost supervised learning (Roth et al., 2008; Mahmoudi et al., 2013; Ahlberg et al., 2015), or low-quality unsupervised learning (Gr¨onroos et al., 2014; Ruokolainen et al., 2016). We exploit Wikipedia markups to automatically learn affixes as language-specific features (Section 2.3). Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. Based on the observation that Wikipedia titles within any language tend to follow a consisten"
P17-1178,P16-1213,0,0.0247317,"notations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (McNamee et al., 2011) extended it to 21 languages. But their methods required labeled data and name transliteration. We share the same goal as (Sil and Florian, 2016) to extend cross-lingual entity linking to all languages in Wikipedia. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques such as (Pan et al., 2015) to directly link translated English name mentions to English KB. Efforts to save annotation cost for name tagging: Some previous work including (Ji and Grishman, 2006; Richman and Schone, 2008; Althobaiti et al., 2013) exploited semi-supervised methods to save annotation cost. We observed that s"
P17-1178,K16-1022,0,0.117852,"luding name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST TAC-KBP Tri-lingual entity linking (Ji et al., 2016) focused on three languages: English, Chinese and Spanish. (Mc"
P17-1178,P13-1106,0,0.0824147,"kups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many l"
P17-1178,Q14-1005,0,0.016415,"background features for IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual"
P17-1178,J90-1003,0,0.414992,"Missing"
P17-1178,P15-2048,0,0.0158903,"and Cucerzan, 2008; Mika et al., 2008; Ringland et al., 2009; Alotaibi and Lee, 2012; Nothman et al., 2013; Althobaiti et al., 2014). Most of these previous methods manually classified many English Wikipedia entries into pre-defined entity types. In contrast, our approach doesn’t need any manual annotations or language-specific features, while generates both coarse-grained and fine-grained types. Many fine-grained entity typing approaches (Fleischman and Hovy, 2002; Giuliano, 1953 2009; Ekbal et al., 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013; Gillick et al., 2014; Yogatama et al., 2015; Del Corro et al., 2015) also created annotations based on Wikipedia anchor links. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. Previous work on Arabic name tagging (Althobaiti et al., 2014) extracted entity titles as a gazetteer for stemming, and thus it cannot handle unknown names. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Wikipedia as background features for IE: Wikipedia pages have been used as additional features to improve various Information E"
P17-1178,C12-2133,0,0.0297288,"Missing"
P17-1178,N16-1029,1,0.814483,"IE: Wikipedia pages have been used as additional features to improve various Information Extraction (IE) tasks, including name tagging (Kazama and Torisawa, 2007), coreference resolution (Paolo Ponzetto and Strube, 2006), relation extraction (Chan and Roth, 2010) and event extraction (Hogue et al., 2014). Other automatic name annotation generation methods have been proposed, including KB driven distant supervision (An et al., 2003; Mintz et al., 2009; Ren et al., 2015) and cross-lingual projection (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013; Wang and Manning, 2014; Zhang et al., 2016b). Multi-lingual name tagging: Some recent research (Zhang et al., 2016a; Littell et al., 2016; Tsai et al., 2016) under the DARPA LORELEI program focused on developing name tagging techniques for low-resource languages. These approaches require English annotations for projection (Tsai et al., 2016), some input from a native speaker, either through manual annotations (Littell et al., 2016), or a linguistic survey (Zhang et al., 2016a). Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. Multi-lingual entity linking: NIST"
P17-1178,C16-1045,1,0.911502,"Missing"
P17-2085,D15-1077,1,0.929384,"ent, Rensselaer Polytechnic Institute, Troy, NY, USA {liny9,jih}@rpi.edu 2 Microsoft Research, Beijing, China cyl@microsoft.com Abstract not well-known and usually absent in general KBs, we may be able to acquire lists of these entities from the local government as the target KB. Voice of the Customer. EL also plays an important role in mining customer opinions from data generated on social platforms and ecommerce websites, thereby helping companies better understand the needs and expectations of their customers. However, the target products are often not covered by general KBs. For example, (Cao et al., 2015) tested 32 names of General Motors car models and only found 4 in Wikipedia. Although some companies may choose to maintain a comprehensive product KB, it will be much more practical and less costly to provide only lists of product names. Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which c"
P17-2085,C12-1028,1,0.894036,"Missing"
P17-2085,D13-1184,0,0.146291,"Missing"
P17-2085,P14-1036,1,0.932167,"Missing"
P17-2085,N15-1119,1,0.928285,"Missing"
P17-2085,C10-1112,0,0.0311579,"school on a scholarship at [[Harvard University|Harvard University]]... ∗ On October 6, 2012, [[Allison Harvard|Harvard]] made an appearance in an episode of... Because enwiki:Harvard_University is in the University list, the first mention will be considered as referential, whereas the second one is non-referential. We also apply matching rules in Table 1 to obtain more non-referential mentions. After that, we extract sentences around wikilinks as a document. Experiments 4.1 Data set In our experiment, the construction of data set consists of two steps: collecting name lists from NeedleSeek2 (Shi et al., 2010) and extracting documents from Wikipedia. NeedleSeek is a project aiming to mine semantic concepts from tera-scale data (ClueWeb09) and classify them into a wide range of semantic categories. For example, “KFC” is mined as a concept in the restaurant category, along with key sentences and attributes, such as employee number and founder. To obtain target name lists, we select 7 semantic categories (see Table 2) generated by NeedleSeek as target domains, and take the top concepts in each category as target entities. We manually map each name to its pertinent Wikipedia page as a target entity (e."
P18-1074,W17-4419,0,0.0309924,"liary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semantic and syntactic knowledge from large scale raw text without additional supervision. In (Yang et al., 2017), the authors propose three transfer models for crossdomain, cross-application,"
P18-1074,P17-1001,0,0.0522416,"rch is to apply this architecture to other types of tasks, such as Event Extract and Semantic Role Labeling that involve structure prediction. We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly. 1 Sample Rate for Auxiliary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequen"
P18-1074,Q16-1026,0,0.616647,"asons: (1) They learn features from word and character embeddings and therefore require little feature engineering; (2) As the input and output of each layer in a neural network are abstracted as vectors, it is fairly straightforward to share components between neural models; (3) Character embeddings can serve as a bridge to transfer morphological and semantic information between languages with identical or similar scripts, without requiring cross-lingual dictionaries or parallel sentences. Therefore, we design our multi-task multilingual architecture based on the LSTM-CNNs model proposed in (Chiu and Nichols, 2016). The overall framework is illustrated in Figure 1. First, each word wi is represented as the combination xi of two parts, word embedding and character feature vector, which is extracted from character embeddings of the characters in wi using convolutional neural networks (CharCNN). On top of that, a bidirectional LSTM processes the sequence x = {x1 , x2 , ...} in both directions and encodes each word and its context into a fixed-size vector hi . Next, a linear layer converts hi to a score vector yi , in which each component represents the predicted score of a target tag. In order to model cor"
P18-1074,P16-1101,0,0.167981,"such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Ratinov and Roth, 2009; Passos et al., 2014) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or languages. To avoid task-specific engineering, (Collobert et al., 2011) proposed a feed-forward neural network model that only requires word embeddings trained on a large scale corpus as features. After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) were proposed and 3 We adopt the BIOES annotation scheme. Prefixes B -, I , E -, and S - represent the beginning of a mention, inside of a mention, the end of a mention and a single-token mention respectively. The O tag is assigned to a word which is not part of any mention. 800 defined as: S(x, y, z) = L ∑ On the first level, we construct the basis of the architecture by sharing character embeddings, CharCNN and bidirectional LSTM among all models. This level of parameter sharing aims to provide universal word representation and feature extractio"
P18-1074,W17-4422,0,0.0834777,"Missing"
P18-1074,D17-1158,0,0.061291,"Missing"
P18-1074,P15-1166,0,0.0727922,"Missing"
P18-1074,N16-1155,0,0.0963742,"Missing"
P18-1074,D17-1206,0,0.0369843,"t and Semantic Role Labeling that involve structure prediction. We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly. 1 Sample Rate for Auxiliary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the sec"
P18-1074,D17-1223,0,0.0239476,"ucture prediction. We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly. 1 Sample Rate for Auxiliary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semant"
P18-1074,N16-1030,0,0.284471,"kov Models (HMMs) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Ratinov and Roth, 2009; Passos et al., 2014) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or languages. To avoid task-specific engineering, (Collobert et al., 2011) proposed a feed-forward neural network model that only requires word embeddings trained on a large scale corpus as features. After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) were proposed and 3 We adopt the BIOES annotation scheme. Prefixes B -, I , E -, and S - represent the beginning of a mention, inside of a mention, the end of a mention and a single-token mention respectively. The O tag is assigned to a word which is not part of any mention. 800 defined as: S(x, y, z) = L ∑ On the first level, we construct the basis of the architecture by sharing character embeddings, CharCNN and bidirectional LSTM among all models. This level of parameter sharing aims to provide universal word representation and feature extraction capability for all"
P18-1074,W02-2024,0,0.533222,"ropout (Srivastava et al., 2014) to the output of the LSTM layer. We conduct hyper-parameter optimization by exploring the space of parameters shown in Table 2 using random search (Bergstra and Bengio, 2012). Due to time constraints, we only perform parameter sweeping on the Dutch Name Tagging task with 200 training examples. We select the set of parameters that achieves the best performance on the development set and apply it to all models. Experiments 3.1 Data Sets For Name Tagging, we use the following data sets: Dutch (NLD) and Spanish (ESP) data from the CoNLL 2002 shared task (Tjong Kim Sang, 2002), English (ENG) data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003), Russian (RUS) data from LDC2016E95 (Russian Representative Language Pack), and Chechen (CHE) data from TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus4 . We select Chechen as another target language in addition to Dutch and Spanish because it is a truly under-resourced language and its related language, Russian, also lacks NLP resources. Code NLD ESP ENG RUS CHE Train 202,931 (13,344) 207,484 (18,797) 204,567 (23,499) 66,333 (3,143) 98,355 (2,674) Dev 37,761 (2,616) 51,645 (4,351) 51,578 (5,"
P18-1074,W14-1609,0,0.0217921,"omponent represents the predicted score of a target tag. In order to model correlations between tags, a CRFs layer is added at the top to generate the best tagging path for the whole sequence. In the CRFs layer, given an input sentence x of length L and the output of the linear layer y, the score of a sequence of tags z is Model 2.1 Basic Architecture The goal of sequence labeling is to assign a categorical label to each token in a given sentence. Though traditional methods such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Ratinov and Roth, 2009; Passos et al., 2014) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or languages. To avoid task-specific engineering, (Collobert et al., 2011) proposed a feed-forward neural network model that only requires word embeddings trained on a large scale corpus as features. After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) were proposed and 3 We adopt the BIOES annotation scheme. Prefixes B -, I , E"
P18-1074,P17-1186,0,0.0262579,"her types of tasks, such as Event Extract and Semantic Role Labeling that involve structure prediction. We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly. 1 Sample Rate for Auxiliary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et"
P18-1074,W17-2612,0,0.0273745,"cture to adapt a trained model for new tasks rapidly. 1 Sample Rate for Auxiliary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semantic and syntactic knowledge from large scale raw text without additional supervision. In (Yang et al., 2017), the"
P18-1074,W09-1119,0,0.149618,"tor yi , in which each component represents the predicted score of a target tag. In order to model correlations between tags, a CRFs layer is added at the top to generate the best tagging path for the whole sequence. In the CRFs layer, given an input sentence x of length L and the output of the linear layer y, the score of a sequence of tags z is Model 2.1 Basic Architecture The goal of sequence labeling is to assign a categorical label to each token in a given sentence. Though traditional methods such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Ratinov and Roth, 2009; Passos et al., 2014) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or languages. To avoid task-specific engineering, (Collobert et al., 2011) proposed a feed-forward neural network model that only requires word embeddings trained on a large scale corpus as features. After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) were proposed and 3 We adopt the BIOES annotation scheme"
P18-1074,P17-1194,0,0.040503,"his architecture to adapt a trained model for new tasks rapidly. 1 Sample Rate for Auxiliary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semantic and syntactic knowledge from large scale raw text without additional supervision. In ("
P18-1074,P16-2038,0,0.0269461,"incremental learning into this architecture to adapt a trained model for new tasks rapidly. 1 Sample Rate for Auxiliary Task Data Figure 7: The effect of the amount of auxiliary task data on Dutch Name Tagging. 4 Related Work Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von D¨aniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018) Collobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semantic and syntactic knowledge from large scale raw text without additional superv"
P18-1185,W17-4419,0,0.0146751,"016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Compared with traditional approaches, neural networks based approaches do not require hand-crafted features and achieved state-of-the-art performance on name tagging (Ma and Hovy, 2016). However, these methods were mainly developed for newswire and paid little attention to social media. For name tagging in social media, (Ritter et al., 2011) leveraged a large amount of unlabeled data and many dictionaries into a pipeline model. (Limsopatham and Collier, 2016) adapted the BLSTM-CRF model with additional word shape information, and (Aguilar et al., 2017) utilized an effective multi-task approach. Among these methods, our model is most similar to (Lample et al., 2016), but we designed a new visual attention component and a modulation control gate. Visual Attention. Since the attention mechanism was proposed by (Bahdanau et al., 2014), it has been widely adopted to language and vision related tasks, such as Image Captioning and Visual Question Answering (VQA), by retrieving the visual features most related to text context (Zhu et al., 2016; Anderson et al., 2017; Xu and Saenko, 2016; Chen et al., 2015). (Xu et al., 2015) proposed to predict a w"
P18-1185,W15-4319,0,0.172625,"Missing"
P18-1185,C02-1025,0,0.215589,"xample (c) is about a baseball pitcher, but our model pays attention to the top right corner of the image. The visual context feature computed by our model is not related to the sentence, and results in missed tagging of ‘SBU’, which is an organization name. 5 Related Work In this section, we summarize relevant background on previous work on name tagging and visual attention. Name Tagging. In recent years, (Chiu and Nichols, 2015; Lample et al., 2016; Ma and Hovy, 2016) proposed several neural network architectures for named tagging that outperform traditional explicit features based methods (Chieu and Ng, 2002; Florian et al., 2003; Ando and Zhang, 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). They all use Bidirectional LSTM (BLSTM) to extract features from a sequence of words. For characterlevel representations, (Lample et al., 2016) proposed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy,"
P18-1185,P15-1033,0,0.0129906,"odal Input encoded text Florence and the Machine surprises ill teen with private concert B-PER I-PER I-PER I-PER CRF CRF CRF CRF CRF Visual Gate Visual Gate Visual Gate Visual Gate Modulation Gate LSTM LSTM LSTM LSTM Backward LSTM LSTM LSTM LSTM LSTM Forward LSTM LSTM CNN Attention Model LSTM word embedding char representations Florence and the Machine Visual Attention Model Figure 2: Overall Architecture of the Visual Attention Name Tagging Model. Name Tagging benefits from both of the past (left) and the future (right) contexts, thus we implement the Bidirectional LSTM (Graves et al., 2013; Dyer et al., 2015) by concatenating the left → − ← − and right context representations, ht = [ ht , ht ], for each word. Character-level Representation. Following (Lample et al., 2016), we generate the character-level representation for each word using another BLSTM. It receives character embeddings as input and generates representations combining implicit prefix, suffix and spelling information. The final word representation xi is the concatenation of word embedding ei and character-level representation ci . ci = BLST Mchar (si ) si ∈ S xi = [ei , ci ] Conditional random fields (CRFs). For name tagging, it is"
P18-1185,W03-0425,0,0.0449222,"a baseball pitcher, but our model pays attention to the top right corner of the image. The visual context feature computed by our model is not related to the sentence, and results in missed tagging of ‘SBU’, which is an organization name. 5 Related Work In this section, we summarize relevant background on previous work on name tagging and visual attention. Name Tagging. In recent years, (Chiu and Nichols, 2015; Lample et al., 2016; Ma and Hovy, 2016) proposed several neural network architectures for named tagging that outperform traditional explicit features based methods (Chieu and Ng, 2002; Florian et al., 2003; Ando and Zhang, 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). They all use Bidirectional LSTM (BLSTM) to extract features from a sequence of words. For characterlevel representations, (Lample et al., 2016) proposed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer"
P18-1185,N16-1030,0,0.55226,"modality is not the only challenge to perform name tagging on such data. The textual components of these messages are often very short, which limits context around names. Moreover, there linguistic variations, slangs, typos and colloquial language are extremely common, such as using ‘looooove’ for ‘love’, ‘LosAngeles’ for ‘Los Angeles’, and ‘#Chicago #Bull’ for ‘Chicago Bulls’. These characteristics of social media data clearly illustrate the higher difficulty of this task, if compared to traditional newswire name tagging. In this work, we modify and extend the current state-of-the-art model (Lample et al., 2016; Ma and Hovy, 2016) in name tagging to incorporate the visual information of social media posts using an Attention mechanism. Although the usually short textual components of social media posts provide limited contextual information, the accompanying images often provide rich information that can be useful for name tagging. For ex1990 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1990–1999 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics only uses image-sentence pairs as input without any"
P18-1185,W16-3920,0,0.0374922,"(Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Compared with traditional approaches, neural networks based approaches do not require hand-crafted features and achieved state-of-the-art performance on name tagging (Ma and Hovy, 2016). However, these methods were mainly developed for newswire and paid little attention to social media. For name tagging in social media, (Ritter et al., 2011) leveraged a large amount of unlabeled data and many dictionaries into a pipeline model. (Limsopatham and Collier, 2016) adapted the BLSTM-CRF model with additional word shape information, and (Aguilar et al., 2017) utilized an effective multi-task approach. Among these methods, our model is most similar to (Lample et al., 2016), but we designed a new visual attention component and a modulation control gate. Visual Attention. Since the attention mechanism was proposed by (Bahdanau et al., 2014), it has been widely adopted to language and vision related tasks, such as Image Captioning and Visual Question Answering (VQA), by retrieving the visual features most related to text context (Zhu et al., 2016; Anderson e"
P18-1185,P09-1116,0,0.0313767,"rner of the image. The visual context feature computed by our model is not related to the sentence, and results in missed tagging of ‘SBU’, which is an organization name. 5 Related Work In this section, we summarize relevant background on previous work on name tagging and visual attention. Name Tagging. In recent years, (Chiu and Nichols, 2015; Lample et al., 2016; Ma and Hovy, 2016) proposed several neural network architectures for named tagging that outperform traditional explicit features based methods (Chieu and Ng, 2002; Florian et al., 2003; Ando and Zhang, 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). They all use Bidirectional LSTM (BLSTM) to extract features from a sequence of words. For characterlevel representations, (Lample et al., 2016) proposed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Compared with traditional approaches, neu"
P18-1185,D15-1104,0,0.0387866,"eature computed by our model is not related to the sentence, and results in missed tagging of ‘SBU’, which is an organization name. 5 Related Work In this section, we summarize relevant background on previous work on name tagging and visual attention. Name Tagging. In recent years, (Chiu and Nichols, 2015; Lample et al., 2016; Ma and Hovy, 2016) proposed several neural network architectures for named tagging that outperform traditional explicit features based methods (Chieu and Ng, 2002; Florian et al., 2003; Ando and Zhang, 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). They all use Bidirectional LSTM (BLSTM) to extract features from a sequence of words. For characterlevel representations, (Lample et al., 2016) proposed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Compared with traditional approaches, neural networks based approaches do not req"
P18-1185,D15-1166,0,0.0573159,"al areas and they are extracted from the last convolutional layer of ResNet, and the dimension is 1,024x7x7 as shown in Figure 3. 7x7 is the number of regions in the image and 1,024 is the 3 the last fully connect layer outputs the probabilities over 1,000 classes of objects. 1992 dimension of the feature vector. Thus each feature vector of Vr corresponds to a 32x32 pixel region of the rescaled input image. tence and it is unidirectional: 2.3 Attention Implementation. There are many implementations of visual attention mechanism such as Multi-layer Perceptron (Bahdanau et al., 2014), Bilinear (Luong et al., 2015), dot product (Luong et al., 2015), Scaled Dot Product (Vaswani et al., 2017), and linear projection after summation (Yang et al., 2016b). Based on our experimental results, dot product implementations usually result in more concentrated attentions and linear projection after summation results in more dispersed attentions. In the context of name tagging, we choose the implementation of linear projection after summation because it is beneficial for the model to utilize as many related visual features as possible, and concentrated attentions may make the model bias. For implementation, we first"
P18-1185,P16-1101,0,0.42182,"nly challenge to perform name tagging on such data. The textual components of these messages are often very short, which limits context around names. Moreover, there linguistic variations, slangs, typos and colloquial language are extremely common, such as using ‘looooove’ for ‘love’, ‘LosAngeles’ for ‘Los Angeles’, and ‘#Chicago #Bull’ for ‘Chicago Bulls’. These characteristics of social media data clearly illustrate the higher difficulty of this task, if compared to traditional newswire name tagging. In this work, we modify and extend the current state-of-the-art model (Lample et al., 2016; Ma and Hovy, 2016) in name tagging to incorporate the visual information of social media posts using an Attention mechanism. Although the usually short textual components of social media posts provide limited contextual information, the accompanying images often provide rich information that can be useful for name tagging. For ex1990 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1990–1999 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics only uses image-sentence pairs as input without any human designed featu"
P18-1185,D16-1209,0,0.101163,"Missing"
P18-1185,N13-1039,0,0.0769862,"Missing"
P18-1185,W14-1609,0,0.0252774,"The visual context feature computed by our model is not related to the sentence, and results in missed tagging of ‘SBU’, which is an organization name. 5 Related Work In this section, we summarize relevant background on previous work on name tagging and visual attention. Name Tagging. In recent years, (Chiu and Nichols, 2015; Lample et al., 2016; Ma and Hovy, 2016) proposed several neural network architectures for named tagging that outperform traditional explicit features based methods (Chieu and Ng, 2002; Florian et al., 2003; Ando and Zhang, 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). They all use Bidirectional LSTM (BLSTM) to extract features from a sequence of words. For characterlevel representations, (Lample et al., 2016) proposed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Compared with traditional approaches, neural networks based ap"
P18-1185,D14-1162,0,0.0864876,"osed model outperforms baseline on both datasets. We believe the Twitter dataset can be an important step towards more research in multimodal name tagging and we plan to provide it as a benchmark upon request. 4 Experiment 4.1 Training Tokenization. To tokenize the sentences, we use the same rules as (Owoputi et al., 2013), except we separate the hashtag ‘#’ with the words after. Labeling Schema. We use the standard BIO schema (Sang and Veenstra, 1999), because we see little difference when we switch to BIOES schema (Ratinov and Roth, 2009). Word embeddings. We use the 100-dimensional GloVe4 (Pennington et al., 2014) embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training. Character embeddings. As in (Lample et al., 2016), we randomly initialize the character embeddings with uniform samples. Based on experimental results, the size of the character embeddings affects little, and we set it as 50. 1994 4 https://nlp.stanford.edu/projects/glove/ Snapchat Twitter Sentences Tokens Sentences Tokens Training 4,817 39,035 4,290 68,655 Development 1,032 8,334 1,432 22,872 Testing 1,033 8,110 1,459 23,051 Table 1: Sizes of the datasets in numbers of sentence and toke"
P18-1185,D11-1141,0,0.0929807,"ichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Compared with traditional approaches, neural networks based approaches do not require hand-crafted features and achieved state-of-the-art performance on name tagging (Ma and Hovy, 2016). However, these methods were mainly developed for newswire and paid little attention to social media. For name tagging in social media, (Ritter et al., 2011) leveraged a large amount of unlabeled data and many dictionaries into a pipeline model. (Limsopatham and Collier, 2016) adapted the BLSTM-CRF model with additional word shape information, and (Aguilar et al., 2017) utilized an effective multi-task approach. Among these methods, our model is most similar to (Lample et al., 2016), but we designed a new visual attention component and a modulation control gate. Visual Attention. Since the attention mechanism was proposed by (Bahdanau et al., 2014), it has been widely adopted to language and vision related tasks, such as Image Captioning and Visua"
P18-1185,E99-1023,0,0.078448,"e roles of text and image are switched. Captions are often added to complement what is being portrayed by the snap. On our experiment section we will show that our proposed model outperforms baseline on both datasets. We believe the Twitter dataset can be an important step towards more research in multimodal name tagging and we plan to provide it as a benchmark upon request. 4 Experiment 4.1 Training Tokenization. To tokenize the sentences, we use the same rules as (Owoputi et al., 2013), except we separate the hashtag ‘#’ with the words after. Labeling Schema. We use the standard BIO schema (Sang and Veenstra, 1999), because we see little difference when we switch to BIOES schema (Ratinov and Roth, 2009). Word embeddings. We use the 100-dimensional GloVe4 (Pennington et al., 2014) embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training. Character embeddings. As in (Lample et al., 2016), we randomly initialize the character embeddings with uniform samples. Based on experimental results, the size of the character embeddings affects little, and we set it as 50. 1994 4 https://nlp.stanford.edu/projects/glove/ Snapchat Twitter Sentences Tokens Sentences Tokens"
P18-1185,W09-1119,0,0.17587,"g portrayed by the snap. On our experiment section we will show that our proposed model outperforms baseline on both datasets. We believe the Twitter dataset can be an important step towards more research in multimodal name tagging and we plan to provide it as a benchmark upon request. 4 Experiment 4.1 Training Tokenization. To tokenize the sentences, we use the same rules as (Owoputi et al., 2013), except we separate the hashtag ‘#’ with the words after. Labeling Schema. We use the standard BIO schema (Sang and Veenstra, 1999), because we see little difference when we switch to BIOES schema (Ratinov and Roth, 2009). Word embeddings. We use the 100-dimensional GloVe4 (Pennington et al., 2014) embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training. Character embeddings. As in (Lample et al., 2016), we randomly initialize the character embeddings with uniform samples. Based on experimental results, the size of the character embeddings affects little, and we set it as 50. 1994 4 https://nlp.stanford.edu/projects/glove/ Snapchat Twitter Sentences Tokens Sentences Tokens Training 4,817 39,035 4,290 68,655 Development 1,032 8,334 1,432 22,872 Testing 1,033 8,1"
P18-1201,W13-2322,0,0.048347,"for Computational Linguistics (Long Papers), pages 2160–2170 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Event Mention Example: dispatching is the trigger of a Transport-Person event with four arguments: the solid lines show the event annotations for the sentence while the dotted lines show the Abstract Meaning Representation parsing output. patching is the trigger for the event mention of type Transport Person and in E2, conflict is the trigger for the event mention of type Attack. We make use of Abstract Meaning Representations (AMR) (Banarescu et al., 2013) to identify the candidate arguments and construct event mention structures as shown in Figure 2 (top). Figure 2 (bottom) also shows event type structures defined in the Automatic Content Extraction (ACE) guideline.2 We can see that a trigger and its event type name usually have some shared meaning. Furthermore, their structures also tend to be similar: a Transport Person event typically involves a Person as its patient role, while an Attack event involves a Person or Location as an Attacker. This observation matches the theory by Pustejovsky (1991): “the semantics of an event structure can be"
P18-1201,N07-4013,0,0.12327,"Missing"
P18-1201,P08-1004,0,0.122949,"Missing"
P18-1201,P15-2061,1,0.915193,"Missing"
P18-1201,P15-1017,0,0.661906,"Missing"
P18-1201,P16-2011,1,0.921764,"Missing"
P18-1201,C16-1017,0,0.0883192,"Missing"
P18-1201,P11-1163,0,0.389091,"Missing"
P18-1201,P11-1113,0,0.557271,"Missing"
P18-1201,P16-1025,1,0.911858,"Missing"
P18-1201,D09-1013,0,0.0959611,"Missing"
P18-1201,N16-1034,1,0.915997,"Missing"
P18-1201,P08-1030,1,0.902064,"Missing"
P18-1201,P15-2060,0,0.36822,"Missing"
P18-1201,P11-1115,1,0.767935,"Missing"
P18-1201,K17-1034,0,0.0861971,"Missing"
P18-1201,P13-1008,1,0.94998,"Missing"
P18-1201,P10-1081,0,0.668857,"Missing"
P18-1201,C10-2087,0,0.0604086,"Missing"
P18-1201,D16-1038,0,0.0734626,"Missing"
P18-1201,D16-1087,0,0.0557832,"Missing"
P18-1201,D11-1001,1,0.900064,"Missing"
P18-1201,P16-1201,0,0.0472642,"Missing"
P18-1201,P06-2094,0,0.0941745,"Missing"
P18-1201,P12-1088,0,0.128376,"Missing"
P18-1201,N06-1039,0,0.0621108,"Missing"
P18-1201,D13-1170,0,0.00421772,"xtraction as a classification problem, by assigning event triggers to event types from a pre-defined fixed set. These methods rely heavily on manual annotations and features specific to each event type, and thus are not easily adapted to new event types without extra annotation effort. Handling new event types may even entail starting over, without being able to re-use annotations from previous event types. To make event extraction effective as new realworld scenarios emerge, we take a look at this task from the perspective of zero-shot learning, ZSL (Frome et al., 2013; Norouzi et al., 2013; Socher et al., 2013a). ZSL, as a type of transfer learning, makes use of separate, pre-existing classifiers to build a semantic, cross-concept space that maps between their respective classes. The resulting shared semantic space then allows for building a novel “zero-shot” classifier, i,e,, requiring no (zero) additional training examples, to handle unseen cases. We observe that each event mention has a structure consisting of a candidate trigger and arguments, with corresponding predefined name labels for the event type and argument roles. We propose to enrich the semantic representations of each event mention"
P18-1201,W15-0812,0,0.0620811,"Missing"
P18-1201,N15-1040,0,0.0767342,"Missing"
P18-1201,P10-4014,0,0.0766931,"Missing"
P18-2042,D16-1126,1,0.85459,"erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedzior"
P18-2042,D10-1051,1,0.710458,"ns. Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov"
P18-2042,P16-1154,0,0.0520132,"ernational Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback N"
P18-2042,I08-1046,0,0.0121287,"ps://en.wikipedia.org/wiki/Eight-legged essay 3 https://www.goodreads.com/quotes/398754-it-isperfectly-okay-to-write-garbage–as-long-as-you 260 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 260–265 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Writing-editing Network architecture overview. Title Human written abstract LSTM LM Seq2seq with attention (Initial Draft) Writingediting Networks (Final Draft) An effective method of using Web based information for Relation Extraction (Keong and Su, 2008) We propose a method that incorporates paraphrase information from the Web to boost the performance of a supervised relation extraction system. Contextual information is extracted from the Web using a semisupervised process, and summarized by skip-bigram overlap measures over the entire extract. This allows the capture of local contextual information as well as more distant associations. We observe a statistically significant boost in relation extraction performance. This paper proposes a method for automatic extraction of salient information from an original text. Our method shows promising r"
P18-2042,D16-1032,0,0.0578948,"m learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word"
P18-2042,D16-1168,0,0.0219676,"d et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback Network (Zamir et al., 2017) by using previous generated outputs as feedback to guide subsequent generation. Moreover, our Writing-editing Network treats previous drafts as independent observations and does not propagate errors to previous draft generation stages. This property is vital for training feedback architectures for discrete data. Another similar approach is the deliberation network used for MaMichael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for an"
P18-2042,P17-1125,0,0.0246041,"obert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) app"
P18-2042,D16-1128,0,0.179894,"Missing"
P18-2042,W04-1013,0,0.0451412,"Missing"
P18-2042,D15-1166,0,0.123507,"Missing"
P18-2042,P17-1099,0,0.0575064,"ing. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback Network (Zamir et al., 2017) by using pre"
P18-2042,N16-1007,0,0.0188604,"ence on Machine Learning. Related work Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Deep neural networks are widely applied to text generation tasks such as poetry creation (Greene et al., 2010; Ghazvininejad et al., 2016; Zhang et al., 2017), recipe generation (Kiddon et al., 2016), abstractive summarization (Gu et al., 2016; Wang and Ling, 2016; See et al., 2017), and biography generation (Lebret et al., 2016; Liu et al., 2018). We introduce a new task of generating paper abstracts from the given titles. We design a Writing-editing Network which shares ideas with Curriculum Learning (Bengio et al., 2009), where training on a data point from coarse to finegrained can lead to better convergence (Krueger and Dayan, 2009). Our model is different from previous theme-rewriting (Polozov et al., 2015; Koncel-Kedziorski et al., 2016) approach which has been applied to math word problems but more similar to the Feedback Network (Zamir et al.,"
P18-2042,W14-3348,0,\N,Missing
P18-4001,P03-2031,0,0.0574051,"Missing"
P18-4001,J92-4003,0,0.232744,"ry, takes a more word-centric approach to annotation. Each sentence to be annotated is laid out in a row, each column of which shows a word augmented with a variety of information about that word. Figure 4 shows a screenshot of a portion of the Dragonfly tool being used to annotate text written in the Kannada language. The top entry in each column is the Kannada word. Next is a Romanization of the word (Hermjakob et al., 2018). The third entry is one or more dictionary translations, if available. The fourth entry is a set of dictionary translations of other words in the word’s Brown cluster. (Brown et al., 1992) While these tend to be less accurate than translations of the word, they can give a strong signal that a word falls into a particular category. For example, a Brown cluster containing translations such as “Paris,” “Rome” and “Vienna” is likely to refer to a city, even if no translation exists to indicate which city. Finally, if automated labels for the sentence have been generated, e.g., by a trained name tagger, those labels The ELISA IE annotation platform was developed at Rensselaer Polytechnic Institute.6 Figure 1 depicts ELISA’s overall architecture. Figure 2 demonstrates the main annota"
P18-4001,P18-4003,0,0.177245,"s, possible annotations for a mention along with their frequency) and making them available to the annotator is essential for large collaborative annotation efforts. word on its left, “Dr,” is similar to “Dr.” in English, and its spelling looks similar to the English “Jamac Warsame Ali.” Similarly, we can identify “Baardheere” as a location name if we know that “magaalada” in English is “town” from a common word dictionary, and its spelling is similar to the English name “Bardhere.” What about languages that are not written in Roman (Latin) script? Fortunately language universal romanization (Hermjakob et al., 2018) or transliteration3 tools are available for most living languages. For example, the following is a Tigrinya sentence and its romanized form: “ናይዚ እዋን&apos;ዚ ፕረዝደንት ዓብደልፈታሕ አል-ሲሲ ነቲ ናይ 2011 ዓ.ም.ፈ ተቃውሞ ብምንኣድ እቲ ተቃውሚ ሓዳስ ግብጺ ዘምጸአ&apos;ዩ ኢሎም።” “naayezi ’ewaane’zi perazedanete ’aabedalefataahhe ’ale-sisi nati naaye 2011 ’aa.me.fa taqaawemo bemene’aade ’eti taqaawemi hhaadaase gebetsi zametsa’a ’yulome .” An English speaker can guess that “ዓብደልፈታሕ አል-ሲሲ” is a person name because its romanized form “aabedalefataahhe ’ale-sisi” sounds similar to the English name “Abdel-Fattah el-Sissi,” and the romanized form"
P18-4001,N16-1030,0,0.0440995,"o each) to annotate 50 VOA news documents for each of the five languages listed in Table 2. Their annotations were then adjudicated through the ELISA adjudication interface. The process took about one week. For each language we used 40 documents for training and 10 documents for test in the TACKBP2017 EDL Pilot. In Table 2 we see that the languages with more annotated names (i.e., Albanian and Swahili) achieved higher performance. Table 1: How Platforms Fulfill Desiderata name taggers based on a bi-directional long shortterm memory (LSTM) network with a Conditional Random Fields (CRFs) layer (Lample et al., 2016). The lexicons loaded into the ELISA IE annotation platform were acquired from Panlex,8 Geonames9 and Wiktionary.10 Dragonfly used bilingual lexicons by (Rolston and Kirchhoff, 2016). 4.1 Albanian 1,652 41,785 96,911 4.2 Silver Standard Creation Overall Performance We compare our method with Wikipedia based silver standard annotations (Pan et al., 2017) on Oromo and Tigrinya, two low-resource languages in the LoreHLT2017 evaluation. Table 3 shows the data statistics. We can see that with the ELISA annotation platform we were able to acquire many more topically-relevant training sentences and t"
P18-4001,P17-1178,1,0.850308,"et al., 2017) that achieved state-of-the-art performance on two surprise languages (Oromo and Tigrinya) at LoreHLT20171 and ten languages at TACKBP EDL2017 (Ji et al., 2017). We discuss strengths and limitations and compare other methods of creating silver- and goldstandard annotations using native speakers. We will make our tools publicly available for research use. 1 Introduction Although researchers have been working on unsupervised and semi-supervised approaches to alleviate the demand for training data, most state-ofthe-art models for name tagging, especially neural network-based models (Pan et al., 2017; Zhang et al., 2017) still rely on a large amount of training data to achieve good performance. When applied to low-resource languages, these models suffer from data sparsity. Traditionally, native speakers of a language have been asked to annotate a corpus in that language. This approach is uneconomical for several reasons. First, for some languages “Sida uu saxaafadda u sheegay Dr Jaamac Warsame Cali oo fadhigiisu yahay magaalada Baardheere hadda waxaa shuban caloolaha la yaalla xarumaha caafimaadka 15-cunug oo lagu arkay fuuq bax joogto ah, wuxuu xusay dhakhtarku in ay wadaan dadaallo ay w"
P18-4001,K16-1022,0,0.0911211,"Missing"
P18-4001,Q14-1005,0,0.0506432,"Missing"
P18-5008,I11-1095,0,0.071247,"Missing"
P18-5008,P12-1086,0,0.0400622,"Missing"
P18-5008,P15-1026,0,0.046284,"Missing"
P18-5008,E06-1002,0,0.0205863,"ge processing and information extraction. We will try to provide a concise road-map of recent approaches, perspectives, and results, as well as point to some of our EL resources that are available to the research community. We live in a golden age of information, where we have access to vast amount of data in various forms: text, video and audio. Over the last few years, one of the key task that has been studied in support of natural language understanding and information extraction from text, is the task of Entity Linking (previously studied as Wikification). Entity Linking (henceforth, EL) (Bunescu and Pasca, 2006; Cucerzan, 2007; Ratinov et al., 2011) is the task of mapping mentions of entities in a text document to an entry in a large catalog of entities such as Wikipedia or another knowledge base (KB). It has also been one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (McNamee and Dang, 2009b; Ji and Grishman, 2011; Ji et al., 2014). Most works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et a"
P18-5008,N16-1150,0,0.0303098,"Missing"
P18-5008,D13-1184,1,0.896846,"Missing"
P18-5008,Q15-1036,0,0.03164,"Missing"
P18-5008,I11-1029,0,0.0695698,"Missing"
P18-5008,W12-3016,0,0.0546427,"Missing"
P18-5008,D12-1082,0,0.0679824,"Missing"
P18-5008,P17-2085,1,0.897868,"Missing"
P18-5008,N09-2051,0,0.0412,"Missing"
P18-5008,P04-1018,0,0.0519451,"Missing"
P18-5008,P17-1178,1,0.839536,"ems and motivate neural EL. 2.3 2.4 Coffee break: [30 minutes] 2.5 Language Universal Methods for Cross-lingual EDL [30 mins] We will then present some recent advances at developing low-cost approaches to perform crosslingual EL for 282 Wikipedia languages, such as deriving silver-standard annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links (Pan et al., 2017a). We will also introduce some recent extensions along this line of work, including extending the number of entity types from five to thousands, and its impact on other NLP applications such as Machine Translation. Neural Methods for EDL [30 mins] Various shared tasks such as TAC-KBP, ACE and CONLL, along with corpora like OntoNotes and ERE have provided the community substantial amount of annotations for both entity mention extraction (1,500+ documents) and entity linking (5,000+ query entities). Therefore supervised models have become popular again for each step of EDL. Among all of the sup"
P18-5008,N15-1026,0,0.0433028,"Missing"
P18-5008,P11-1138,1,0.757906,"We will try to provide a concise road-map of recent approaches, perspectives, and results, as well as point to some of our EL resources that are available to the research community. We live in a golden age of information, where we have access to vast amount of data in various forms: text, video and audio. Over the last few years, one of the key task that has been studied in support of natural language understanding and information extraction from text, is the task of Entity Linking (previously studied as Wikification). Entity Linking (henceforth, EL) (Bunescu and Pasca, 2006; Cucerzan, 2007; Ratinov et al., 2011) is the task of mapping mentions of entities in a text document to an entry in a large catalog of entities such as Wikipedia or another knowledge base (KB). It has also been one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (McNamee and Dang, 2009b; Ji and Grishman, 2011; Ji et al., 2014). Most works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research"
P18-5008,D12-1110,0,0.0356304,"Missing"
P18-5008,P14-6004,1,0.871881,"sca, 2006; Cucerzan, 2007; Ratinov et al., 2011) is the task of mapping mentions of entities in a text document to an entry in a large catalog of entities such as Wikipedia or another knowledge base (KB). It has also been one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) (McNamee and Dang, 2009b; Ji and Grishman, 2011; Ji et al., 2014). Most works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page i"
P18-5008,D13-1170,0,0.00580383,"Missing"
P18-5008,spitkovsky-chang-2012-cross,0,0.0838424,"Missing"
P18-5008,P14-2046,1,0.818026,"works in the literature have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page in the English Wikipedia. Beyond the motivation that drives the English EL task – knowledge acquisition and information extraction – in the crosslingual case and especially when dealing with low resource languages, the hope is to provide improved natural language understanding capabilities for the many languages for which we have 1 2 https://lorehlt.nist.gov/ http://nlp.cs.rpi.edu/kbp/2017/"
P18-5008,D12-1011,0,0.587029,"Missing"
P18-5008,D17-1007,0,0.0311032,"ACL 2014. domain linking, has to model each domain separately. We will discuss a multi-KB entity linking framework that employs one general-knowledge KB and a large set of domain-specific KBs as linking targets that extends the work from (Cucerzan, 2007, 2014a), as well as a supervised model with a large and diverse set of features to detect when a domain-specific KB matches a document targeted for entity analysis (Gao and Cucerzan, 2017). 2.7 New Tasks, Trends and Open Questions [15 mins] Here, we will address some of the new settings: multi-lingual EL for search engines (Pappu et al., 2017; Tan et al., 2017). We will discuss some open questions such as improving the title candidate generation process for situations where the corresponding titles only exist in the English Wikipedia and also investigate the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking (Tsai and Roth, 2018). We will also discuss EL for noisy data like social media (Meij et al., 2012; Guo et al., 2013). Finally, we will discuss the possibilities of extending the ideas taught in this EL tutorial to other multi-lingual IE tasks. 2.8 • Name: Dan"
P18-5008,P16-1213,0,0.0517392,"equent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page in the English Wikipedia. Beyond the motivation that drives the English EL task – knowledge acquisition and information extraction – in the crosslingual case and especially when dealing with low resource languages, the hope is to provide improved natural language understanding capabilities for the many languages for which we have 1 2 https://lorehlt.nist.gov/ http://nlp.cs.rpi.edu/kbp/2017/taskspec pilot.pdf 22 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-Tutorial Abstr"
P18-5008,N16-1072,1,0.909349,"e coverage and its frequent updates made by the community. The previous Entity Linking tutorial in ACL 2014 (Roth et al., 2014) addressed mostly EL research which have focused on English, the most prevalent language on the web and the one with the largest Wikipedia datasets. However, in the last few years research has shifted to address the EL task in other languages, some of which have very large web presence, such as Spanish (Fahrni et al., 2013; Ji et al., 2014), and Chinese (Cao et al., 2014; Shi et al., 2014) but also in others. In particular, there has been interest in cross-lingual EL (Tsai and Roth, 2016; Sil and Florian, 2016): given a mention in a foreign language document, map it to the corresponding page in the English Wikipedia. Beyond the motivation that drives the English EL task – knowledge acquisition and information extraction – in the crosslingual case and especially when dealing with low resource languages, the hope is to provide improved natural language understanding capabilities for the many languages for which we have 1 2 https://lorehlt.nist.gov/ http://nlp.cs.rpi.edu/kbp/2017/taskspec pilot.pdf 22 Proceedings of the 56th Annual Meeting of the Association for Computational Li"
P18-5008,D15-1081,1,0.908012,"Missing"
P18-5008,D14-1167,0,0.260672,"Missing"
P18-5008,C16-1127,0,0.0210845,"Missing"
P18-5008,K16-1025,0,0.0344507,"Missing"
P19-1016,C18-1139,0,0.0697448,"essfully identifies “Zheng Chenggong” as a person, it is not able to connect this name with “Koxinga” based on the expression “also known as” to further infer that “Koxinga” should also be a person. Table 5: Name tagging result comparison between the baseline model and our model. 171 4 Related Work bution of word frequency, embedding vectors usually have inconsistent reliability, and such inconsistency has been long overlooked. Meanwhile, language models such as ELMo, Flair, and BERT have shown their effectiveness on constructing representations in a context-aware manner (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018). These models are designed to better capture the context information by pre-training, while our model dynamically composes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional"
P19-1016,K18-1028,0,0.026665,"o recognize names but still absent from the current model. Word Representation Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute r"
P19-1016,D14-1082,0,0.0601337,"Missing"
P19-1016,N15-1075,0,0.0303189,"tion Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation"
P19-1016,N16-1030,0,0.0148324,"and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the p"
P19-1016,Q16-1026,0,0.519455,"el. The basis of this dynamic composition mechanism is the reliability signals that inform the model of the quality of each word embedding. Specifically, we assume that if a word occurs more frequently, its word embedding will be more fully trained as it has richer contexts and its embedding is updated more often during training. Thus, we design a set of reliability signals based on word frequency in the embedding training corpus and name tagging training corpus. 2.1 Baseline Model We adopt a state-of-the-art name tagging model LSTM-CNN (Long-short Term Memory - Convolutional Neural Network) (Chiu and Nichols, 2016) as our base model. In this architecture, the input sentence is represented as a sequence of vectors X = {x1 , ..., xL }, where xi is the vector representation of the i-th word, and L is the length of the sequence. Generally, xi is a concatenation of word embedding and character-level representation generated with a group of convolutional neural networks (CNNs) with various filter sizes from compositional character embeddings of the word. Next, the sequence X is fed into a bi-directional Recurrent Neural Network (RNN) with Longshort Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997)."
P19-1016,P18-1074,1,0.833856,"onstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark data sets in both within-genre and cross-genre settings demonstrate the effectiveness of our model and verify our intui"
P19-1016,P15-1033,0,0.026624,"Missing"
P19-1016,D18-1153,1,0.841269,"F) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve th"
P19-1016,P05-1045,0,0.00980259,"omposes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional methods leverage handcrafted features to capture textual signals and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling met"
P19-1016,P16-1101,0,0.0223755,"l random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new d"
P19-1016,P14-1146,0,0.117179,"Missing"
P19-1016,mota-grishman-2008-ne,0,0.0329385,"-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark dat"
P19-1016,W17-2612,0,0.0123742,"ods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose feat"
P19-1016,N16-1174,0,0.0614333,"Missing"
P19-1016,D14-1162,0,0.0917685,"l knowledge and common sense as additional signals into our architecture as they are important for human readers to recognize names but still absent from the current model. Word Representation Models Acknowledgments Recent advances on representation learning allow us to capture textual signals in a data-driven manner. Based on the distributional hypothesis (i.e., “a word is characterized by the company it keeps” (Harris, 1954)), embedding methods represent each word as a dense vector, while preserving their syntactic and semantic information in a context-agnostic manner (Mikolov et al., 2013; Pennington et al., 2014). Recent work shows that word embeddings can cover textual information of various levels (Artetxe et al., 2018) and improve name tagging performance significantly (Cherry and Guo, 2015). Still, due to the long-tail distriThis work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, LORELEI Program No. HR0011-15-C-0115, Air Force No. FA8650-17-C-7715, U.S. ARL NS-CTA No. W911NF-09-2-0053, and Tencent AI Lab Rhino-Bird Gift Fund. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, eith"
P19-1016,N18-1202,0,0.29,"though our model successfully identifies “Zheng Chenggong” as a person, it is not able to connect this name with “Koxinga” based on the expression “also known as” to further infer that “Koxinga” should also be a person. Table 5: Name tagging result comparison between the baseline model and our model. 171 4 Related Work bution of word frequency, embedding vectors usually have inconsistent reliability, and such inconsistency has been long overlooked. Meanwhile, language models such as ELMo, Flair, and BERT have shown their effectiveness on constructing representations in a context-aware manner (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018). These models are designed to better capture the context information by pre-training, while our model dynamically composes representations in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeli"
P19-1016,W13-3516,0,0.0320936,"Missing"
P19-1016,C16-1030,0,0.0599161,"Missing"
P19-1016,W04-1221,0,0.0386504,"ns in a reliability-aware manner. Therefore, our model and these efforts have the potential to mutually enhance each other. In addition, (Kim et al., 2016) and (Rei et al., 2016) also mix word- and character-level representations using gating mechanisms. They use a single gate to balance the representations in a reliability-agnostic way. Name Tagging Models Most existing methods treat name tagging as a sequence labeling task. Traditional methods leverage handcrafted features to capture textual signals and employ conditional random fields (CRF) to model label dependencies (Finkel et al., 2005; Settles, 2004; Leaman et al., 2008). Bi-LSTM-CRF (Huang et al., 2015) combines word embedding and handcrafted features, integrates neural networks with CRF, and shows performance boost over previous methods. LSTMCNN further utilizes CNN and illustrates the potential of capturing character-level signals (Chiu and Nichols, 2016). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven"
P19-1016,P16-2038,0,0.0178532,"6). LSTM-CRF and LSTMCNNs-CRF are proposed to get rid of hand-crafted features and demonstrate the feasibility to fully rely on representation learning to capture textual features (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018b). Recently, language modeling methods are proven effective as the representation module for name tagging (Liu et al., 2018a; Peters et al., 2018; Akbik et al., 2018). At the same time, there has been extensive research about cross-genre (Peng and Dredze, 2017), crossdomain (Pan et al., 2013; He and Sun, 2017), cross-time (Mota and Grishman, 2008), crosstask (Søgaard and Goldberg, 2016; Liu et al., 2018b), and cross-lingual (Yang et al., 2017; Lin et al., 2018) adaptation for name tagging training. Unlike these models, although we also aim to enhance the performance on new data, we achieve this by improving the generalization capability of the model so that it can work better on unknown new data instead of transferring it to a known target setting. 5 Conclusions and Future Work We propose a name tagging model that is able to dynamically compose features depending on the quality of input word embeddings. Experiments on the benchmark data sets in both within-genre and cross-g"
P19-1191,E17-1060,0,0.0353562,"he contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news"
P19-1191,W14-3348,0,0.040687,"ov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (nati"
P19-1191,W13-0108,0,0.0277008,"d graph attention (Sukhbaatar et al., 2015; Madotto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator netwo"
P19-1191,P18-1082,0,0.033721,"paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert"
P19-1191,N16-1087,0,0.0455489,"tto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et"
P19-1191,W07-2305,0,0.0841706,"Missing"
P19-1191,P16-1154,0,0.301994,"rk-to-title) follow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite training. We label the words with frequen"
P19-1191,D18-1086,0,0.0190419,"ckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interest"
P19-1191,P15-1067,0,0.0119856,"om biomedical text”, PaperRobot mistakenly extracts “prolog” as a related entity and generates an abstract “In this paper, we present a novel approach to the problem of extracting relationships among the prolog program. We present a system that uses a macromolecular binding relationships to extract the relationships between the abstracts of the entry. The results show that the system is able to extract the most important concepts in the prolog program.”. 4 Related Work Link Prediction. Translation-based approaches (Nickel et al., 2011; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015a) have been widely exploited for link prediction. Compared with these studies, we are the first to incorporate multi-head graph attention (Sukhbaatar et al., 2015; Madotto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven"
P19-1191,N18-2101,0,0.303572,"its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published pa"
P19-1191,W13-1302,0,0.0196907,"paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic writing of key paper elements for the biomedical domain, especially conclusion and future work, a"
P19-1191,D16-1128,0,0.0872029,"Missing"
P19-1191,P16-1094,0,0.0146349,"t. 3 3.1 Experiment Data We collect biomedical papers from the PMC Open Access Subset.5 To construct ground truth for new title prediction, if a human written paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We"
P19-1191,W04-1013,0,0.0826554,"Missing"
P19-1191,D16-1230,0,0.03642,"Missing"
P19-1191,W15-4640,0,0.0129704,"nt Data We collect biomedical papers from the PMC Open Access Subset.5 To construct ground truth for new title prediction, if a human written paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our fr"
P19-1191,D18-1435,1,0.843574,"Missing"
P19-1191,D18-1360,1,0.841868,"t data. All of the system generated titles are declarative sentences while human generated titles are often more engaging (e.g., “Does HPV play any role in the initiation or prognosis of endometrial Requirements to Make PaperRobot Work: Case Study on NLP Domain When a cool Natural Language Processing (NLP) system like PaperRobot is built, it’s natural to ask whether she can benefit the NLP community itself. We re-build the system based on 23,594 NLP papers from the new ACL Anthology Network (Radev et al., 2013). For knowledge extraction we apply our previous system trained for the NLP domain (Luan et al., 2018). But the results are much less satisfactory compared to the 1987 biomedical domain. Due to the small size of data, the language model is not able to effectively copy out-of-vocabulary words and thus the output is often too generic. For example, given a title “Statistics based hybrid approach to Chinese base phrase identification”, PaperRobot generates a fluent but uninformative abstract “This paper describes a novel approach to the task of Chinese-base-phrase identification. We first utilize the solid foundation for the Chinese parser, and we show that our tool can be easily extended to meet"
P19-1191,P18-1136,0,0.174988,"n and future work, and conclusion and future work-to-title) follow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite"
P19-1191,P02-1040,0,0.104014,"Missing"
P19-1191,W16-6603,1,0.855329,"encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include ge"
P19-1191,P17-1099,0,0.622639,"ow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite training. We label the words with frequency < 5 as Out-of-vo"
P19-1191,2006.amta-papers.25,0,0.221983,"Missing"
P19-1191,E17-2047,0,0.056946,"Missing"
P19-1191,P18-1151,0,0.0126333,"ure multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts base"
P19-1191,D18-1422,0,0.0136042,"i, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wa"
P19-1191,P16-1008,0,0.060528,"Missing"
P19-1191,D18-2028,0,0.0454461,"Missing"
P19-1191,D18-1112,0,0.0200447,"nce among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for t"
P19-1191,W18-6502,1,0.880435,"t generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (native speaker). Each human judge is asked to compare a system output and a human-authored string, and select the better one. 6 https://github.com/pytorch/examples/ tree/master/word_language_model 7 The perplexity scores of the language model are in the Appendix. 1985 Task Input Human Title End-to-End System Abstract System Conclusion and Future work System Title Human Abstract Diagnostic Human Conclusion and Future work Output Different Same Different Same Different Same Different Different Same Differen"
P19-1191,P18-2042,1,0.914756,"t generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (native speaker). Each human judge is asked to compare a system output and a human-authored string, and select the better one. 6 https://github.com/pytorch/examples/ tree/master/word_language_model 7 The perplexity scores of the language model are in the Appendix. 1985 Task Input Human Title End-to-End System Abstract System Conclusion and Future work System Title Human Abstract Diagnostic Human Conclusion and Future work Output Different Same Different Same Different Same Different Different Same Differen"
P19-1191,D18-1433,1,0.748118,"eep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic writing of key paper elements for the biomedical domain, especially conclusion and future work, and follow-on paper titles. 5 Conclusions"
P19-1191,D18-1356,0,0.0165433,"his is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic wr"
P19-1210,N10-1134,0,0.112306,"Missing"
P19-1210,N09-1070,0,0.304595,"Missing"
P19-1210,W04-3252,0,0.273715,"Missing"
P19-1210,P02-1040,0,0.103117,"Missing"
P19-1210,D14-1162,0,0.082454,"e, and each dimension stands for a VFOA target. The network is trained on the VFOA annotation, including the VFOA target for each frame of each participant. Then the output of all participants are concatenated. For utterance ui , the VFOA vector f i ∈ R|P |∗|F |is the sum of each frame’s VFOA outputs over the course of ui , where each dimension stands for the total duration of the attention paid to the corresponding VFOA target. Figure 3: Topic Segmentation Decoder 2.4 2.2 Meeting Transcript Encoder For an utterance ui = {w0i , w1i , . . . }, we embed each word wij using the pretrained GloVe (Pennington et al., 2014), and apply a bidirectional gated recurrent unit (GRU) (Cho et al., 2014) to obtain the encoded word representation hij . The utterance representations are the average of words. Additionally, the speaker pi is encoded into a onehot vector pi ∈ R|P |. 2.3 Meeting Summarization Decoder We build our decoder based on Pointer-Generator Network (PGN) (See et al., 2017) to copy words from the input transcript in terms of attention distribution. Different from PGN, we introduce a hierarchical attention mechanism based on the topic segmentation results, as shown in Figure 4. Topic Segmentation Decoder"
P19-1210,D15-1044,0,0.0766539,"ctive summarization methods rank and select words by constructing word co-occurrence graphs (Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Lin and Bilmes, 2010; Tixier et al., 2016b), and they are applied to meeting summarization (Liu et al., 2009, 2011; Tixier et al., 2 https://bitbucket.org/dascim/acl2018_ abssumm/src/master/data/meeting/ami 2016a; Shang et al., 2018). However, extractive summaries are often not natural and coherent with limited content coverage. Recently the neural natural language generation models boost the performance of abstractive summarization (Luong et al., 2015; Rush et al., 2015; See et al., 2017), but they are often unable to focus on topic words. Inspired by utterance clustering in extractive methods (Shang et al., 2018), we propose a hierarchical attention based on topic segmentation (Li et al., 2018). Moreover, our hierarchical attention is multi-modal to narrow down the focus by capturing participant interactions. Multi-modal features from human annotations have been proven effective at improving summarization, such as dialogue act (Goo and Chen, 2018). Instead of using human annotations, our approach utilizes a simply detectable multi-modal feature VFOA. 5 Conc"
P19-1210,P17-1099,0,0.613614,"and incoherent. Although state-of-the-art work (Shang et al., 2018) employs WordNet (Miller, 1995) to make summaries more abstractive, the quality is still far from those produced by humans, as shown in Table 1. Moreover, these methods tend to have limited content coverage by selecting salient words. On the other hand, recent years have witnessed the success of Natural Language Generation (NLG) models to generate abstractive summaries. Since human-written summaries tend to mention the exact given keywords without paraphrasing, the copy mechanism proposed by a Pointer Generator Network (PGN) (See et al., 2017) naturally fits this task. Apart from generating words from a fixed vocabulary, it also copies the words from the input. However, transcripts of multi-person meetings widely differ from traditional documents. Instead of grammatical, wellsegmented sentences, the input is often composed of ill-formed utterances. Therefore, NLG models can easily lose focus. For example, in Table 1, PGN fails to capture the keywords remote control, trendy and user-friendly. Therefore, we propose a multi-modal hierarchical attention mechanism across topic segments, utterances, and words. We learn topic segmentation"
P19-1210,P18-1062,0,0.357812,"Missing"
P19-1210,D16-1191,0,0.0491466,"Missing"
P19-1210,P16-4026,0,0.0699211,"Missing"
R09-1032,J95-2003,0,0.0479436,"racted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts and ambiguities. In addition, we study the temporal event linking task on top of IE results. In this way we extend the representation of each node in the chains from an event trigger to a structured aggregated event including fine-grained information such as event types, arguments and their roles. Compared to [5, 6], we also extend the definition of “centroid” from a word"
R09-1032,P08-1030,1,0.958566,"ents of events. temporal event chain: a list of temporally-ordered events involving the same centroid entity. Our cross-document IE task is defined as follows: Input: A test set of documents Ouput: Identify a set of centroid entities, and then for each centroid entity, link and order the events centered around it on a time line. For example, Figure 1 presents a temporal event chain involving “Toefting”. As for other ACE tasks, the ACE 2005 official evaluation scorer can produce an overall score called “ACE value” for event extraction. However, most of the ACE event extraction literature (e.g. [3]; [4]) used a simpler argument-based F-measure to evaluate ACE event extraction, and we will adapt this measure to our task. 2.3 Limitations In the ACE single-document event extraction task, each event mention is extracted from a single sentence. The results are reasonably useful for hundreds of documents. However, when we apply the same system to process much larger corpora, the net result is a very large collection of events which are: (1) Unconnected. Related events (for example, “Tony Blair’s foreign trips) appear unconnected and unordered. (2) Unranked. Event mentions are presented in the"
R09-1032,N09-2053,1,0.846917,"of events. temporal event chain: a list of temporally-ordered events involving the same centroid entity. Our cross-document IE task is defined as follows: Input: A test set of documents Ouput: Identify a set of centroid entities, and then for each centroid entity, link and order the events centered around it on a time line. For example, Figure 1 presents a temporal event chain involving “Toefting”. As for other ACE tasks, the ACE 2005 official evaluation scorer can produce an overall score called “ACE value” for event extraction. However, most of the ACE event extraction literature (e.g. [3]; [4]) used a simpler argument-based F-measure to evaluate ACE event extraction, and we will adapt this measure to our task. 2.3 Limitations In the ACE single-document event extraction task, each event mention is extracted from a single sentence. The results are reasonably useful for hundreds of documents. However, when we apply the same system to process much larger corpora, the net result is a very large collection of events which are: (1) Unconnected. Related events (for example, “Tony Blair’s foreign trips) appear unconnected and unordered. (2) Unranked. Event mentions are presented in the orde"
R09-1032,D08-1029,0,0.0188386,"> into one candidate centroid if they satisfy either of the following two conditions: • identified as coreferential by single-document coreference resolution; or • in different documents, there is a namei referring to mentioni and a namej referring to mentionj (if several names, taking the maximal name in each document), and namei and namei are equal or one is a substring of the other. Using this approach we can avoid linking “Rod Stewart” and “Martha Stewart” into the same entity. In the future we intend to exploit more advanced crossdocument person name disambiguation techniques (e.g. [11], [12]) to resolve ambiguities. 5.2 Global Entity Ranking Because the candidate entities are extracted automatically, and so may be erroneous, we want to promote those arguments which are both central to the collection (high frequency) and more likely to be accurate (high confidence). We exploit global confidence metrics to reach both of these goals. The intuition is that if an entity is involved in events frequently as well as with high extraction confidence, it is more salient. Our basic underlying hypothesis is that the salience of an entity ei should be calculated by taking into consideration bo"
R09-1032,P09-2093,1,0.825347,"el Sharon] Eventj Arguments Place [Jerusalem] Subset Subsumption Complement Table 1. Cross-document Event Aggregation Examples [Test Sentence] <person>Diller</person> started his entertainment career at <entity>ABC</entity>, where he is credited with creating the ``movie of the week&apos;&apos; concept. [Sentence from Wikipedia] <person>Diller</person> was hired by <entity> ABC</entity> in <time>1966</time> and was soon placed in charge of negotiating broadcast rights to feature films. (2) Statistical Implicit Time Prediction Furthermore, we exploited a time argument prediction approach as described in [14]. We manually labeled 40 ACE05 newswire texts and trained a MaxEnt classifier to determine whether a time argument from an event mention EMi can be propagated to the other event mention EMj. The features used include the event types of EMi and EMj, whether they are located in the same sentence, if so the number of time expressions in the sentence; whether they share coreferential arguments, if so the roles of the arguments. This predictor is able to propagate time arguments between two events which indicate some precursor/consequence, subevent or causal relation (e.g. from a “Conflict-Attack”"
R09-1032,W99-0201,0,0.0482159,"n generated to maximize diversity among the event nodes in a chain and completeness for each event node. In order to reach these goals, a simple event coreference solution is not enough. We also aggregate other relation types between two events: Subset, Subsumption and Complement as shown in Table 1. Besides using cross-document name coreference to measure the similarity between a pair of arguments, we adopted some results from ACE relation extraction, e.g. using “PART-WHOLE” relations between arguments to determine whether one event subsumes the other. Earlier work on event coreference (e.g. [15]) in the MUC program was limited to several scenarios such as terrorist attacks and management succession. In our task we are targeting wider and more fine-grained event types. 170 8. Experimental Results In this section we will describe our answer-key event chain annotation and then present experimental results. 8.1 Data and Answer-key Annotation We used 106 newswire texts from ACE 2005 training corpora as our test set. Then we extracted the top 40 ranked person names as centroid entities, and manually created temporal event chains by two steps: (1) Aggregated reference event mentions; (2) Fi"
R09-1032,N07-1042,0,0.183179,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,P08-2045,0,0.0255199,"p has combined ranking and linking for cross-document IE. Hence in this section, we present related work in other areas for ranking and linking separately. Text summarization progressed from single-document to multi-document processing by centroid based sentence linking and ranking (e.g. [5], [6]). Accurate ranking techniques such as PageRank [13] have greatly enhanced information retrieval. Recently there has been heightened interest in discovering temporal event chains, especially, the shared task evaluation TempEval [9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we"
R09-1032,P08-1090,0,0.0130347,"s for ranking and linking separately. Text summarization progressed from single-document to multi-document processing by centroid based sentence linking and ranking (e.g. [5], [6]). Accurate ranking techniques such as PageRank [13] have greatly enhanced information retrieval. Recently there has been heightened interest in discovering temporal event chains, especially, the shared task evaluation TempEval [9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge fro"
R09-1032,P05-1045,0,0.0313205,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,D07-1075,0,0.0374964,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,D09-1016,0,0.0155515,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,S07-1014,0,\N,Missing
S16-1181,D15-1198,0,0.186545,"CG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of exi"
S16-1181,W13-2322,0,0.42263,"Missing"
S16-1181,P13-2131,0,0.176692,"ng Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator trained using a word sense disambiguation system— IMS (Zhong and Ng, 2010). All these components viz., the Charniak parser, Stanford named entity tagger, ASSERT, and IMS word sense disambiguator were retrained on the OntoNotes v5.0 training 1176 data2 (Pradhan et al., 2013)3 . We use the version of CAMR described in (Wang et al., 2015a) (without the feature extensions) as the baseline. We evaluate our parser with Smatch v2.0.2 (Cai and Knight, 2013) on all the experiments. It should be noted that all the rows in Table 2 except for the last one get implicitly penalized by the scorer for lack of wikification information. 5.1 SemEval Development Set As discussed in (Wang et al., 2015a), the performance of the syntactic parser in the first stage has a high impact on the AMR parsing accuracy. We first do a sanity check to choose the best first stage parser. Here we only consider two scenarios: the Charniak parser trained on WSJ and OntoNotes, as shown in Table 1. As using the Charniak parser trained on OntoNotes yields slightly better AMR par"
S16-1181,P05-1022,0,0.0786156,"(CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics labels produced by an automatic SRL system. The rest of the paper is organized as follows. In Section 2 we briefly describe CAMR, and in Section 3 we describ"
S16-1181,P14-1134,0,0.635756,"d step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Association for Computationa"
S16-1181,P14-5010,0,0.00352027,"ly on the Semeval release (c.). In Section 5.3 we also use the full test set of release (b.) to evaluate the performance improvement made to CAMR as part of the SemEval evaluations against previously reported performance. Syntactic Parser Charniak (ON) Charniak (WSJ) P 70.76 69.88 R 60.57 60.24 F1 65.27 64.70 Table 1: AMR parsing performance on the SemEval development set (LDC2015E86) across two Charniak parser models 5 Experiments We use the official release dataset and standard train/dev/test split of SemEval Task 8 for experiments. All the sentences are preprocessed using Stanford CoreNLP (Manning et al., 2014) to get tokenization, lemma, named entity tag, POS tag. And we use the aligner that comes with JAMR (Flanigan et al., 2014) to align the sentence with its AMR graph. We then parse the tokenized sentences using Charniak parser (Charniak and Johnson, 2005)(Its phrase structure output is converted to dependency structure using a slightly modified version of the Stanford CoreNLP converter). Rich named entity tags are generated using Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator tr"
S16-1181,H05-1066,0,0.057511,"Missing"
S16-1181,N15-1119,1,0.758601,"urrent head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it against the dependent. If the argument predicted by ASSERT matches the dependent, the value of the IS ARGUMENT is set to true. Word Clusters For the semi-supervised word cluster feature, we use Brown clusters, more specifically, the 1000-class word clusters trained by Turian et al. (2010). We use prefixes of lengths 4, 6, 10 and 20 of the word’s bit-string as features. 1175 3.2 Wikification We apply an AMR based wikification system (Pan et al., 2015) which utilizes AMR to represent semantic information about entity mentions expressed in their textual context. Given an entity mention m, this system first constructs a Knowledge Graph g(m) with m at the hub and leaf nodes obtained from entity mentions reachable by AMR graph traversal from m. A subset of the leaf nodes are selected as collaborators of m. Mentions connected by AMR conjunction relations are grouped into sets of coherent mentions. For each entity mention m, an initial ranked list of entity candidates E = (e1 , . . . , en ) is generated based on a salience measure (Medelyan and W"
S16-1181,K15-1004,0,0.347794,"Missing"
S16-1181,N04-1030,1,0.146339,"e input sentence is in the verbalization list. semantic role labeling: wants, want-01, ARG0: the boy, ARG1: the girl to believe him wants boy ARG1 girl believe him For action NEXT- NODE-want-01 EQ FRAMESET: true Figure 2: An example of semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For actions that predict the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by ASSERT (Pradhan et al., 2004). For example, in the partial graph in Figure 2, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is equal to node wants’s frameset want-01 as predicted by ASSERT, the value of feature EQ FRAMESET is set to true. 2) IS ARGUMENT. For actions that predict the edge label, we check whether ASSERT predicts that the current dependent is an argument of the current head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it agai"
S16-1181,W13-3516,1,0.868503,"Missing"
S16-1181,D15-1136,0,0.326487,"s used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Asso"
S16-1181,P10-1040,0,0.0357655,"EQ FRAMESET is set to true. 2) IS ARGUMENT. For actions that predict the edge label, we check whether ASSERT predicts that the current dependent is an argument of the current head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it against the dependent. If the argument predicted by ASSERT matches the dependent, the value of the IS ARGUMENT is set to true. Word Clusters For the semi-supervised word cluster feature, we use Brown clusters, more specifically, the 1000-class word clusters trained by Turian et al. (2010). We use prefixes of lengths 4, 6, 10 and 20 of the word’s bit-string as features. 1175 3.2 Wikification We apply an AMR based wikification system (Pan et al., 2015) which utilizes AMR to represent semantic information about entity mentions expressed in their textual context. Given an entity mention m, this system first constructs a Knowledge Graph g(m) with m at the hub and leaf nodes obtained from entity mentions reachable by AMR graph traversal from m. A subset of the leaf nodes are selected as collaborators of m. Mentions connected by AMR conjunction relations are grouped into sets of cohe"
S16-1181,P15-2141,1,0.766369,"014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial"
S16-1181,N15-1040,1,0.829287,"014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial"
S16-1181,P10-4014,0,0.0295827,"g. And we use the aligner that comes with JAMR (Flanigan et al., 2014) to align the sentence with its AMR graph. We then parse the tokenized sentences using Charniak parser (Charniak and Johnson, 2005)(Its phrase structure output is converted to dependency structure using a slightly modified version of the Stanford CoreNLP converter). Rich named entity tags are generated using Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator trained using a word sense disambiguation system— IMS (Zhong and Ng, 2010). All these components viz., the Charniak parser, Stanford named entity tagger, ASSERT, and IMS word sense disambiguator were retrained on the OntoNotes v5.0 training 1176 data2 (Pradhan et al., 2013)3 . We use the version of CAMR described in (Wang et al., 2015a) (without the feature extensions) as the baseline. We evaluate our parser with Smatch v2.0.2 (Cai and Knight, 2013) on all the experiments. It should be noted that all the rows in Table 2 except for the last one get implicitly penalized by the scorer for lack of wikification information. 5.1 SemEval Development Set As discussed in (Wa"
W04-0705,W98-1118,1,0.888321,"Missing"
W04-0705,C02-1025,0,0.0486199,"t was made to select or cluster documents. 6 Scores are still computed on the 153 test documents ; the retrieved documents are excluded from the scoring. 8.4 Comparison to Cache Model Some named entity systems use a name cache, in which tokens or complete names which have been previously assigned a tag are available as features in tagging the remainder of a document. Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002). Our system, while more complex, makes use of a richer set of global features, involving the detailed structure of individual mentions, and in particular makes use of both name – name and name – nominal relations. We have compared the performance of our method (applied to single documents) with a voted cache model, which takes into account the number of times a particular name has been previously assigned each type of tag: System baseline voted cache current Precision 88.8 87.6 92.2 Recall 90.5 92.8 89.6 F 89.1 90.1 90.9 Table 9. Comparison with voted cache Compared to a simple voted cache mo"
W04-0705,C96-1079,1,0.547286,"Missing"
W04-0705,W98-1120,1,0.879101,"Missing"
W04-0705,M95-1005,0,0.239404,"Missing"
W04-0705,C02-1080,0,0.0588911,"Missing"
W04-0705,C02-1012,0,\N,Missing
W04-0705,W03-0430,0,\N,Missing
W04-0705,A97-1029,0,\N,Missing
W06-0206,A97-1029,0,0.0300663,"performance as corpus size increases without performance reaching an upper bound. Recent work has replicated their work on thesaurus extraction (Curran and Moens, 2002) and is-a relation extraction (Ravichandran et al., 2004), showing that collecting data over a very large corpus significantly improves system performance. However, (Curran, 2002) and (Curran and Osborne, 2002) claimed that the choice of statistical model is more important than relying upon large corpora. 3 4 Baseline Multi-lingual Name Tagger Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al, 1997). Then it uses best-first search to generate NBest hypotheses, and also computes the margin – the difference between the log probabilities of the top two hypotheses. This is used as a rough measure of confidence in our name tagging.1 In processing Chinese, to take advantage of name structures, we do name structure parsing using an extended HMM which includes a larger number of states (14). This new HMM can handle name prefixes and suffixes, and transliterated foreign names separately. We also augmented the HMM model with a set of post-processing rules to correct some omissions and systematic e"
W06-0206,W99-0613,0,0.0187463,"beled data, and how the size and relevance of data impact the performance. 2 Prior Work This work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Ando and Zhang, 2005) 48 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 48–55, c Sydney, July 2006. 2006 Association for Computational Linguistics that all focus on reducing annotation requirements. For the specific task of named entity annotation, some researchers have emphasized the creation of taggers from minimal seed sets (Strzalkowski and Wang, 1996; Collins and Singer, 1999; Lin et al., 2003) while another line of inquiry (which we are pursuing) has sought to improve on high-performance baseline taggers (Miller et al., 2004). Banko and Brill (2001) suggested that the development of very large training corpora may be most effective for progress in empirical natural language processing. Their experiments show a logarithmic trend in performance as corpus size increases without performance reaching an upper bound. Recent work has replicated their work on thesaurus extraction (Curran and Moens, 2002) and is-a relation extraction (Ravichandran et al., 2004), showing t"
W06-0206,P02-1030,0,0.0103549,"on of taggers from minimal seed sets (Strzalkowski and Wang, 1996; Collins and Singer, 1999; Lin et al., 2003) while another line of inquiry (which we are pursuing) has sought to improve on high-performance baseline taggers (Miller et al., 2004). Banko and Brill (2001) suggested that the development of very large training corpora may be most effective for progress in empirical natural language processing. Their experiments show a logarithmic trend in performance as corpus size increases without performance reaching an upper bound. Recent work has replicated their work on thesaurus extraction (Curran and Moens, 2002) and is-a relation extraction (Ravichandran et al., 2004), showing that collecting data over a very large corpus significantly improves system performance. However, (Curran, 2002) and (Curran and Osborne, 2002) claimed that the choice of statistical model is more important than relying upon large corpora. 3 4 Baseline Multi-lingual Name Tagger Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al, 1997). Then it uses best-first search to generate NBest hypotheses, and also computes the margin – the difference between the log probabilities of the top t"
W06-0206,W02-2008,0,0.0372141,"Missing"
W06-0206,P01-1005,0,0.0714479,"estigated whether we can improve the system by simply using a lot of unlabeled data. By dramatically increasing the size of the corpus with unlabeled data, we did get a significant improvement compared to the baseline system. But we found that adding off-topic unlabeled data sometimes makes the performance worse. Then we tried to select relevant documents from the unlabeled data in advance, and got clear further improvements. We also obtained significant improvement by self-training (bootstrapping on the test data) without any additional unlabeled data. Therefore, in contrast to the claim in (Banko and Brill, 2001), we concluded that, for some applications, effective use of large unlabeled corpora demands good data selection measures. We propose and quantify some effective measures to select documents and sentences in this paper. The rest of this paper is structured as follows. Section 2 briefly describes the efforts made by previous researchers to use semi-supervised learning as well as the work of (Banko and Brill, 2001). Section 3 presents our baseline name tagger. Section 4 describes the motivation for our approach while Section 5 presents the details of two semi-supervised learning methods. Section"
W06-0206,N04-1043,0,\N,Missing
W06-0206,C96-2157,0,\N,Missing
W06-0206,W02-1029,0,\N,Missing
W06-0206,N04-1038,0,\N,Missing
W06-0206,P05-1051,1,\N,Missing
W06-0206,P05-1001,0,\N,Missing
W06-0206,W04-0705,1,\N,Missing
W06-3607,A97-1029,0,0.0374317,"o apply the baseline name tagger to generate N-Best multiple hypotheses for each sentence; the results from subsequent components are then exploited to re-rank these hypotheses and the new top hypothesis is output as the final result. In our name re-ranking model, each hypothesis is an NE tagging of the entire sentence. For example, “&lt;PER&gt;John&lt;/PER&gt; was born in &lt;GPE&gt;New York&lt;/GPE&gt;.” is one hypothesis for the sentence “John was born in New York”. We apply a HMM tagger to identify four named entity types: Person, GPE, Organization and Location. The HMM tagger generally follows the Nymble model (Bikel et al, 1997), and uses bestfirst search to generate N-Best hypotheses. It also computes the “margin”, which is the difference between the log probabilities of the top two hypotheses. This is used as a rough measure of confidence in the top hypothesis. A large margin indicates greater confidence that the first hypothesis is correct. The margin also determines the number of hypotheses (N) that we will store. Using cross-validation on the training data, we determine the value of N required to include the best hypothesis, as a function of the margin. We then divide the margin into ranges of values, and set a"
W06-3607,P05-1022,0,0.0910128,"Missing"
W06-3607,P05-1023,0,0.0119745,"king techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to Ralph Grishman Dept. of Computer Science grishman@cs.nyu.edu improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a single component in order to bring to"
W06-3607,P05-1051,1,0.949896,"and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to Ralph Grishman Dept. of Computer Science grishman@cs.nyu.edu improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving r"
W06-3607,P06-2055,1,0.843278,"Missing"
W06-3607,W03-0402,0,0.0199274,"w the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to Ralph Grishman Dept. of Computer Science grishman@cs.nyu.edu improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some"
W06-3607,W04-2401,0,\N,Missing
W06-3607,C02-1151,0,\N,Missing
W06-3607,C96-1079,1,\N,Missing
W06-3607,J05-1003,0,\N,Missing
W06-3607,P02-1034,0,\N,Missing
W06-3607,P02-1062,0,\N,Missing
W06-3607,P05-1024,0,\N,Missing
W06-3607,P05-1033,0,\N,Missing
W09-1704,W06-0901,0,0.205307,"raction, the ‘classical’ information extraction (IE) task, has progressed from Message Understanding Conference (MUC)-style single template extraction to the more comprehensive multi-lingual Automatic Content Extraction (ACE) extraction including more fine-grained types. This extension has made event extraction more widely applicable in many NLP tasks including crosslingual document retrieval (Hakkani-Tur et al., 2007) and question answering (Schiffman et al., 2007). Various supervised learning approaches 1 have been explored for ACE multi-lingual event extraction (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Tan et al., 2008; Chen and Ji, 2009). All of these previous literatures showed that one main bottleneck of event extraction lies in low recall. It’s a challenging task to recognize the different forms in which an event may be expressed, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its internal algorithms or annotating additional data. As for a separate research theme, extensive techniques have been used to produce word clusters"
W09-1704,P05-1074,0,0.23705,"forms in which an event may be expressed, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its internal algorithms or annotating additional data. As for a separate research theme, extensive techniques have been used to produce word clusters or paraphrases from large unlabeled corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999, Barzilay and McKeown, 2001; Lin and Pantel, 2001; Ibrahim et al., 2003; Pang et al., 2003). For example, (Bannard and Callison-Burch, 2005) and (Callison-Burch, 2008) described a method to extract paraphrases from largely available bilingual corpora. The resulting clusters contain words with similar semantic information and therefore can be useful to augment a small amount of annotated data. We will automatically extract cross-lingual predicate clusters using two different approaches based on bilingual parallel corpora and cross-lingual IE respectively; and then use the derived clusters to improve event extraction. We propose a new learning method called inductive learning to exploit the derived predicate clusters. For each test"
W09-1704,P01-1008,0,0.222498,"ain bottleneck of event extraction lies in low recall. It’s a challenging task to recognize the different forms in which an event may be expressed, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its internal algorithms or annotating additional data. As for a separate research theme, extensive techniques have been used to produce word clusters or paraphrases from large unlabeled corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999, Barzilay and McKeown, 2001; Lin and Pantel, 2001; Ibrahim et al., 2003; Pang et al., 2003). For example, (Bannard and Callison-Burch, 2005) and (Callison-Burch, 2008) described a method to extract paraphrases from largely available bilingual corpora. The resulting clusters contain words with similar semantic information and therefore can be useful to augment a small amount of annotated data. We will automatically extract cross-lingual predicate clusters using two different approaches based on bilingual parallel corpora and cross-lingual IE respectively; and then use the derived clusters to improve event extraction. We"
W09-1704,D08-1021,0,0.133651,"d, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its internal algorithms or annotating additional data. As for a separate research theme, extensive techniques have been used to produce word clusters or paraphrases from large unlabeled corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999, Barzilay and McKeown, 2001; Lin and Pantel, 2001; Ibrahim et al., 2003; Pang et al., 2003). For example, (Bannard and Callison-Burch, 2005) and (Callison-Burch, 2008) described a method to extract paraphrases from largely available bilingual corpora. The resulting clusters contain words with similar semantic information and therefore can be useful to augment a small amount of annotated data. We will automatically extract cross-lingual predicate clusters using two different approaches based on bilingual parallel corpora and cross-lingual IE respectively; and then use the derived clusters to improve event extraction. We propose a new learning method called inductive learning to exploit the derived predicate clusters. For each test document, a background docu"
W09-1704,N09-2053,1,0.684423,"IE) task, has progressed from Message Understanding Conference (MUC)-style single template extraction to the more comprehensive multi-lingual Automatic Content Extraction (ACE) extraction including more fine-grained types. This extension has made event extraction more widely applicable in many NLP tasks including crosslingual document retrieval (Hakkani-Tur et al., 2007) and question answering (Schiffman et al., 2007). Various supervised learning approaches 1 have been explored for ACE multi-lingual event extraction (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Tan et al., 2008; Chen and Ji, 2009). All of these previous literatures showed that one main bottleneck of event extraction lies in low recall. It’s a challenging task to recognize the different forms in which an event may be expressed, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its internal algorithms or annotating additional data. As for a separate research theme, extensive techniques have been used to produce word clusters or paraphrases from large unlabeled corpora (Brown et al."
W09-1704,H05-1022,0,0.0642791,"Missing"
W09-1704,J05-1004,0,0.0250697,"Missing"
W09-1704,W98-1119,0,0.131768,"Missing"
W09-1704,N03-1024,0,0.163692,"g task to recognize the different forms in which an event may be expressed, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its internal algorithms or annotating additional data. As for a separate research theme, extensive techniques have been used to produce word clusters or paraphrases from large unlabeled corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999, Barzilay and McKeown, 2001; Lin and Pantel, 2001; Ibrahim et al., 2003; Pang et al., 2003). For example, (Bannard and Callison-Burch, 2005) and (Callison-Burch, 2008) described a method to extract paraphrases from largely available bilingual corpora. The resulting clusters contain words with similar semantic information and therefore can be useful to augment a small amount of annotated data. We will automatically extract cross-lingual predicate clusters using two different approaches based on bilingual parallel corpora and cross-lingual IE respectively; and then use the derived clusters to improve event extraction. We propose a new learning method called inductive learning to explo"
W09-1704,P04-1053,0,0.018549,"hange the algorithms of the baseline system, our approach of inductive learning is more flexible. 8 Acknowledgments Related Work Our approach of extracting predicate clusters is related to some prior work on paraphrase or word cluster discovery, either from mono-lingual parallel corpora (e.g. Barzilay and McKeown, 2001; Lin and Pantel, 2001; Ibrahim et al., 2003; Pang et al., 2003) or cross-lingual parallel corpora (e.g. Bannard and Callison-Burch, 2005; Callison-Burch, 2008). Shinyama and Sekine (2003) presented an approach of extracting paraphrases using names, dates and numbers as anchors. Hasegawa et al. (2004) described a paraphrase discovery approach based on clustering concurrent name pairs. Several recent studies have stressed the benefits of using paraphrases or word clusters to improve IE components. For example, (Miller et al., 2004) proved that word clusters can significantly improve English name tagging. The idea of using predicates in the same cluster for candidate trigger replacement is similar to Ge et al.(1998) who used local context replacement for pronoun resolution. To the best of our knowledge, our work presented the first experiment of using cross-lingual predicate paraphrases for"
W09-1704,W03-1608,0,0.154759,"all. It’s a challenging task to recognize the different forms in which an event may be expressed, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its internal algorithms or annotating additional data. As for a separate research theme, extensive techniques have been used to produce word clusters or paraphrases from large unlabeled corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999, Barzilay and McKeown, 2001; Lin and Pantel, 2001; Ibrahim et al., 2003; Pang et al., 2003). For example, (Bannard and Callison-Burch, 2005) and (Callison-Burch, 2008) described a method to extract paraphrases from largely available bilingual corpora. The resulting clusters contain words with similar semantic information and therefore can be useful to augment a small amount of annotated data. We will automatically extract cross-lingual predicate clusters using two different approaches based on bilingual parallel corpora and cross-lingual IE respectively; and then use the derived clusters to improve event extraction. We propose a new learning method called inducti"
W09-1704,W06-0206,1,0.831728,"Missing"
W09-1704,P08-1030,1,0.839696,"neration For each event mention in a test document, the baseline event tagger produces the following local confidence value: • 6.2 • For each predicatei ∈ C, we replace trigger with predicatei in S to generate new sentence S’, and add S’ into BD. In this way we can propagate highly consistent and frequent triggers and arguments with high global confidence to override other, lower confidence, extraction results. 7 7.1 For each background document BD, we apply the baseline event extraction and get a set of background events. We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. This approach is based on the premise that many events will be reported multiple times from different sources in different forms. This naturally occurs in the test document and the background 32 Experimental Results Data and Scoring Metric We used ACE2005 English and Chinese training corpora to evaluate our approach. Table 1 shows the number of documents used for training, development and blind testing. Language English Chinese Training Set 525 500 Developm"
W09-1704,P99-1005,0,0.0515725,"Missing"
W09-1704,N04-1043,0,0.0732174,"Missing"
W09-1704,J03-1002,0,0.00460544,"Missing"
W09-1704,N07-1067,0,0.0181825,"this approach obtained significant improvement over a state-of-the-art bilingual (English and Chinese) event extraction system. 1 Introduction Event extraction, the ‘classical’ information extraction (IE) task, has progressed from Message Understanding Conference (MUC)-style single template extraction to the more comprehensive multi-lingual Automatic Content Extraction (ACE) extraction including more fine-grained types. This extension has made event extraction more widely applicable in many NLP tasks including crosslingual document retrieval (Hakkani-Tur et al., 2007) and question answering (Schiffman et al., 2007). Various supervised learning approaches 1 have been explored for ACE multi-lingual event extraction (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Tan et al., 2008; Chen and Ji, 2009). All of these previous literatures showed that one main bottleneck of event extraction lies in low recall. It’s a challenging task to recognize the different forms in which an event may be expressed, given the limited amount of training data. The goal of this paper is to improve the performance of a bilingual (English and Chinese) state-of-the-art event extraction system without accessing its intern"
W09-1704,W03-1609,0,0.0220362,"in the supervisedlearning procedure of the baseline event tagger. However in the situation where we cannot directly change the algorithms of the baseline system, our approach of inductive learning is more flexible. 8 Acknowledgments Related Work Our approach of extracting predicate clusters is related to some prior work on paraphrase or word cluster discovery, either from mono-lingual parallel corpora (e.g. Barzilay and McKeown, 2001; Lin and Pantel, 2001; Ibrahim et al., 2003; Pang et al., 2003) or cross-lingual parallel corpora (e.g. Bannard and Callison-Burch, 2005; Callison-Burch, 2008). Shinyama and Sekine (2003) presented an approach of extracting paraphrases using names, dates and numbers as anchors. Hasegawa et al. (2004) described a paraphrase discovery approach based on clustering concurrent name pairs. Several recent studies have stressed the benefits of using paraphrases or word clusters to improve IE components. For example, (Miller et al., 2004) proved that word clusters can significantly improve English name tagging. The idea of using predicates in the same cluster for candidate trigger replacement is similar to Ge et al.(1998) who used local context replacement for pronoun resolution. To th"
W09-1704,N04-1033,0,0.0116991,"igger-set2. For any Chinese trigger ch-trigger in ch-triggerset1, if its corresponding translation en-trigger in en-trigger-set1 is the same as that in en-triggerset2, then we add en-trigger into the cluster anchored by ch-trigger. We apply the English and Chinese IE systems as described in (Grishman et al., 2005; Chen and Ji, 2009). Both cross-lingual IE pipelines need machine translation to translate Chinese documents (for English IE) or project the extraction results from Chinese IE into English. We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. 4.3 Derived Cross-lingual Predicate Clusters Applying the above two approaches we obtained 438 English predicate clusters and 543 Chinese predicate clusters. For example, for a trigger “伤(injure)”, we can get the following two predicate clusters with their frequency in the parallel corpora: 伤Æ {injured:99 injuries:96 injury:76 wounded:38 wounding:28 injuring:14 wounds:7 killed:4 died:2 mutilated:1 casualties:1 chop:1 killing:1 shot:1}. injured Æ {受伤:1624 重伤:102 伤:99 轻伤:29 伤 势:23 炸:12 打伤:10 爆炸:6 伤害:3 死亡:2 冲突:1 亡:1 烫伤:1 损失:1 出席:1 登陆:1 致残:1 自残:1 } We can see that the predicat"
W09-1704,J92-4003,0,\N,Missing
W09-1704,P93-1024,0,\N,Missing
W09-2209,N09-2053,1,0.613103,"paper, we focus on discussing trigger labeling and argument labeling. In the following example, Mike got married in 2008. The event extraction system should identify “married” as the event trigger which indicates the event type of “Life” and subtype of “Marry”. Furthermore, it should detect “Mike” and “2008” as arguments in which “Mike” has a role of “Person” and “2008” has a role of “Time-Within”. 2.2 A Pipeline of Event Extraction Our pipeline framework of event extraction includes trigger labeling, argument labeling and post-processing, similar to (Grishman et al., 2005), (Ahn, 2006) and (Chen and Ji, 2009). We depict the framework as Figure 1. Pre-processing Trigger labeling Trigger identification Trigger classification 2 Event Extraction 2.1 Argument labeling Task Definition and Terminology Argument identification The event extraction that we address in this paper is specified in the Automatic Content Extraction (ACE) 1 program. The ACE 2005 Evaluation defines the following terminology for the event extraction task: • event trigger: the word that most clearly expresses an event’s occurrence • event argument: an entity, a temporal expression or a value that plays a certain role in the event ins"
W09-2209,W06-0901,0,0.0841034,"essing). In this paper, we focus on discussing trigger labeling and argument labeling. In the following example, Mike got married in 2008. The event extraction system should identify “married” as the event trigger which indicates the event type of “Life” and subtype of “Marry”. Furthermore, it should detect “Mike” and “2008” as arguments in which “Mike” has a role of “Person” and “2008” has a role of “Time-Within”. 2.2 A Pipeline of Event Extraction Our pipeline framework of event extraction includes trigger labeling, argument labeling and post-processing, similar to (Grishman et al., 2005), (Ahn, 2006) and (Chen and Ji, 2009). We depict the framework as Figure 1. Pre-processing Trigger labeling Trigger identification Trigger classification 2 Event Extraction 2.1 Argument labeling Task Definition and Terminology Argument identification The event extraction that we address in this paper is specified in the Automatic Content Extraction (ACE) 1 program. The ACE 2005 Evaluation defines the following terminology for the event extraction task: • event trigger: the word that most clearly expresses an event’s occurrence • event argument: an entity, a temporal expression or a value that plays a certa"
W09-2209,N04-1038,0,0.0161867,"Missing"
W09-2209,W06-0206,1,0.852111,"Missing"
W09-2209,P91-1017,0,0.0716823,"Missing"
W09-2209,D08-1063,0,0.031487,"erall performance of our event extraction system? And if these errors are compounded in event extraction, can the cross-lingual semi-co-training alleviate the impact? To investigate all these issues, we use 159 texts from LDC Chinese Treebank English Parallel corpus to conduct cross-lingual semi-co-training. The experimental results are summarized in Figure 9 and Figure 10. For monolingual self-training on the bitexts, we conduct experiments exactly as section 4.2 except that the entities are tagged by the IE system and the 72 inference rules to enhance both entity extraction and translation. Zitouni and Florian (2008) applied English mention detection on translated texts and added the results as additional features to improve mention detection in other languages. In this paper we share the similar idea of importing evidences from English with richer resources to improve extraction in other languages. However, to the best of our knowledge this is the first work of incorporating cross-lingual feedback to improve the event extraction task. More importantly, it is the first attempt of combining cross-lingual projection with bootstrapping methods, which can avoid the efforts of designing sophisticated inference"
W09-2209,W99-0613,0,0.0598961,"Missing"
W09-2209,W04-2405,0,0.0274169,"ument Labeling P R F 64.3 59.4 61.8 49.2 34.7 40.7 78.8 48.3 59.9 60.6 34.3 43.8 59.2 59.4 59.3 51.6 59.5 55.3 75.2 74.6 74.9 58.6 60.9 59.7 Table 1.Performance of Two Monolingual Event Extraction Systems and Human Annotators 3 Bootstrapping Event Extraction 3.1 General Bootstrapping Algorithm Bootstrapping algorithms have attracted much attention from researchers because a large number of unlabeled examples are available and can be utilized to boost the performance of a system trained on a small set of labeled examples. The general bootstrapping algorithm is depicted in Figure 2, similar to (Mihalcea, 2004). Self-training and Co-training are two most commonly used bootstrapping methods. A typical self-training process is described as follows: it starts with a set of training examples and builds a classifier with the full integrated feature set. The classifier is then used to label an additional portion of the unlabeled examples. Among the resulting labeled examples, put the most confident ones into the training set, and re-train the classifier. This iterates until a certain condition is satisfied (e.g., all the unlabeled examples have been labeled, or it reaches a certain number of iterations)."
W09-2209,P05-1001,0,0.0201475,"Missing"
W09-2209,N04-1043,0,\N,Missing
W09-2423,P98-1013,0,0.0111712,"Missing"
W09-2423,P01-1017,0,0.22112,"Missing"
W09-2423,N06-1024,0,0.0607349,"Missing"
W09-2423,W04-3228,0,0.0260109,"Missing"
W09-2423,P06-2055,1,0.767512,"Missing"
W09-2423,C02-1122,0,0.0190732,"rse of eight years or so. In contrast, the Chinese and Japanese systems are newer and considerably less time was spent developing them. Thus they currently do not represent as many regularizations. One obstacle is that we do not currently use subcategorization dictionaries for either language, while we have several for English. In particular, these would be helpful in predicting and filling relative clause and others gaps. We are considering auto153 matically acquiring simple dictionaries by recording frequently occurring argument types of verbs over a larger corpus, e.g., along the lines of (Kawahara and Kurohashi, 2002). In addition, existing Japanese dictionaries such as the IPAL (monolingual) dictionary (technology Promotion Agency, 1987) or previously acquired case information reported in (Kawahara and Kurohashi, 2002). Finally, we are investigating several avenues for using this system output for Machine Translation (MT) including: (1) aiding word alignment for other MT system (Wang et al., 2007); and (2) aiding the creation various MT models involving analyzed text, e.g., (Gildea, 2004; Shen et al., 2008). Acknowledgments This work was supported by NSF Grant IIS0534700 Structure Alignment-based MT. Refe"
W09-2423,W04-2705,1,0.83884,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,meyers-etal-2004-cross,1,0.822383,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,W07-1529,1,0.799455,"Missing"
W09-2423,W04-2703,0,0.139121,"Missing"
W09-2423,I05-4002,0,0.0390298,"Missing"
W09-2423,J05-1004,0,0.21622,"Missing"
W09-2423,W05-0302,1,0.852675,"els; we have a different set of relational labels; and finally, our approach is designed to be compatible with the Penn Treebank framework and therefore, Penn-Treebankbased parsers. In addition, the expansion of our theory is governed more by available resources than by the underlying theory. As our main goal is to use our system to regularize data, we freely incorporate any analysis that fits this goal. Over time, we have found ways of incorporating Named Entities, PropBank, NomBank and the Penn Discourse Treebank. Our agenda also includes incorporating the results of other research efforts (Pustejovsky et al., 2005). For each sentence, we generate a feature structure (FS) representing our most complete analysis. We distill a subset of this information into a dependency structure governed by theoretical assumptions, e.g., about identifying functors of phrases. Each GLARF dependency is between a functor and an argument, where the functor is the head of a phrase, conjunction, complementizer, or other function word. We have built applications that use each of these two representations, e.g., the dependency representation is used in (Shinyama, 2007) and the FS representation is used in (K. Parton and K. R. Mc"
W09-2423,P08-1066,0,0.0264245,"Missing"
W09-2423,P03-1010,0,0.0528115,"Missing"
W09-2423,D07-1077,0,0.0600045,"Missing"
W09-2423,J08-2004,1,0.877787,"Missing"
W09-2423,J93-2004,0,\N,Missing
W09-2423,W08-2121,1,\N,Missing
W09-2423,J03-4003,0,\N,Missing
W09-2423,P09-1048,1,\N,Missing
W09-2423,C98-1013,0,\N,Missing
W09-2423,W09-1201,1,\N,Missing
W09-3019,P03-1010,0,0.0268446,"Missing"
W09-3019,P01-1017,0,0.0441748,"our grammars of Japanese and Chinese do not currently.; (5) a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automati"
W09-3019,J02-3001,0,0.015508,"a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generat"
W09-3019,J08-2004,1,0.84166,"inese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammati"
W09-3019,W09-1201,1,0.825301,"Missing"
W09-3019,D09-1087,1,0.855247,"Missing"
W09-3019,P06-2055,1,0.900802,"Missing"
W09-3019,W08-2121,1,\N,Missing
W09-3019,J03-4003,0,\N,Missing
W09-3019,W04-0413,1,\N,Missing
W09-3019,W09-2423,1,\N,Missing
W09-3107,P02-1051,0,0.287052,"Missing"
W09-3107,N09-2053,1,0.802352,"ts type and time argument (e.g. the “Arrest/2001-06-25” link in Figure 1). As we will see in the next section, these attributes are the key to discover name translations from the information networks because they are language-independent. English Corpora Chinese Corpora English IE Attribute Conversion English Information Network 4 Graph Traverse based on Confidence Estimation bootstrapping High-Confidence Name Pairs Figure 2. Name Translation Mining Overview 3 3.1 Information Network Creation Bilingual Information Extraction We apply a state-of-the-art bilingual information extraction system (Chen and Ji, 2009; Ji and Grishman, 2008) to extract ACE1 types of entities, relations and events from the comparable corpora. Both systems include name tagging, 2 1 Information Network Alignment After creating the information networks from each language, we automatically align them to discover name translation pairs. The general idea is that starting from a small seed set of common name pairs, we can rely on the link attributes to align their related names. Then the new name translations are added to the seed set for the next iteration. We repeat this bootstrapping procedure until no new translations are prod"
W09-3107,N04-1036,0,0.331748,"Missing"
W09-3107,P08-1030,1,0.783131,"(e.g. the “Arrest/2001-06-25” link in Figure 1). As we will see in the next section, these attributes are the key to discover name translations from the information networks because they are language-independent. English Corpora Chinese Corpora English IE Attribute Conversion English Information Network 4 Graph Traverse based on Confidence Estimation bootstrapping High-Confidence Name Pairs Figure 2. Name Translation Mining Overview 3 3.1 Information Network Creation Bilingual Information Extraction We apply a state-of-the-art bilingual information extraction system (Chen and Ji, 2009; Ji and Grishman, 2008) to extract ACE1 types of entities, relations and events from the comparable corpora. Both systems include name tagging, 2 1 Information Network Alignment After creating the information networks from each language, we automatically align them to discover name translation pairs. The general idea is that starting from a small seed set of common name pairs, we can rely on the link attributes to align their related names. Then the new name translations are added to the seed set for the next iteration. We repeat this bootstrapping procedure until no new translations are produced. We start from name"
W09-3107,P06-1010,0,0.125633,"eness of this task on automatically mining name translation pairs. Starting from a small set of seeds, we design a novel approach to acquire name translation pairs in a bootstrapping framework. The experimental results show this approach can generate highly accurate name translation pairs for persons, geopolitical and organization entities. 1 Introduction Accurate name translation is crucial to many cross-lingual information processing tasks such as information retrieval (e.g. Ji et al., 2008). Recently there has been heightened interest in discovering name pairs from comparable corpora (e.g. Sproat et al., 2006; Klementiev and Roth, 2006). By comparable corpora we mean texts that are about similar topics, but are not in general translations of each other. These corpora are naturally available, for example, many news agencies release multi-lingual news articles on the same day. There are no document-level or sentence-level alignments across languages, but important facts such as names, relations and events in one language in such corpora tend to co-occur with their counterparts in the other. Arequipa 库瓦斯 Sibling Leader 2. 蒙特西诺斯 Arrest/2001-06-25 Located 3. 卡西俄 1. National Intelligence Service Arrest/"
W09-3107,N06-1011,0,\N,Missing
W09-3208,W99-0201,0,0.825412,"Missing"
W09-3208,W06-1633,0,0.0331184,"-Within) the past EM2 EM3 EM4 EM5 EM6 EM7 3 3.1 Included event mentions {EM1,EM2,EM3,EM4,EM5} {EM6} {EM7} Table 3. Event coreference results 2 Event Coreference Resolution Spectral Graph Clustering as We view the event coreference space as an undirected weighted graph in which the nodes represent all the event mentions in a document and the edge weights indicate the coreference confidence between two event mentions. In real implementation, we initially construct different graphs for separate event types 2 , such that, in each graph, all the event mentions have the same event type. Similar to (Nicolae and Nicolae, 2006), we formally define a framework for event coreference resolution.  = ݓ 3 2 Method 1: Computing a Coreference Formula Obviously, the trigger pair and the argument sets owned by two event mentions carry much information about whether one event mention corefers with the other. Based on a corpus, we compute the statistics about event mention pairs (with the same event type) listed in Table 4. Let  .  ݎݎݐbe the trigger in  ,  (ݐݏ.  )ݎݎݐbe the stem of the trigger in  ,  (ݐݎݓ. ݎݎݐ,  .  )ݎݎݐbe the semantic similarity between the two triggers in"
W09-3208,W97-1311,0,0.894943,"Missing"
W09-3208,H05-1004,0,\N,Missing
W09-4303,J84-3009,0,0.636015,"Missing"
W09-4303,M95-1005,0,0.694471,"ated the performance of the four event attribute classification models using the ground truth event mentions and system generated event mentions respectively. The evaluation metrics we adopted in this set of experiments are Precision (P), Recall (R) and F-Measure (F). We then validated our agglomerative clustering algorithm for the event coreference resolution using the ground truth event mentions and system generated event mentions respectively. The evaluation metrics we adopted in this set of experiments are three conventional metrics for entity coreference resolution, namely, MUC F-Measure [4], B-Cubed F-Measure [5] and ECM F-Measure [6]. We conducted all the experiments by ten times ten-fold cross validation and measured significance with the Wilcoxon signed rank test. 4.2 Performance of the Four Event Attribute Classification Models Table 3 shows that the majority of event mentions are POSITIVE (5162/5349=0.965), ASSERTED (4002/5349 =0.748), SPECIFIC (4145/5349=0.775) and PAST (2720/5349=0.509). • the embedding verb of the trigger if any Table 3. Statistics of the four event attributes in the corpus • a boolean feature indicating whether a negative word exists (not, no, cannot or"
W09-4303,H05-1004,0,0.81095,"ute classification models using the ground truth event mentions and system generated event mentions respectively. The evaluation metrics we adopted in this set of experiments are Precision (P), Recall (R) and F-Measure (F). We then validated our agglomerative clustering algorithm for the event coreference resolution using the ground truth event mentions and system generated event mentions respectively. The evaluation metrics we adopted in this set of experiments are three conventional metrics for entity coreference resolution, namely, MUC F-Measure [4], B-Cubed F-Measure [5] and ECM F-Measure [6]. We conducted all the experiments by ten times ten-fold cross validation and measured significance with the Wilcoxon signed rank test. 4.2 Performance of the Four Event Attribute Classification Models Table 3 shows that the majority of event mentions are POSITIVE (5162/5349=0.965), ASSERTED (4002/5349 =0.748), SPECIFIC (4145/5349=0.775) and PAST (2720/5349=0.509). • the embedding verb of the trigger if any Table 3. Statistics of the four event attributes in the corpus • a boolean feature indicating whether a negative word exists (not, no, cannot or a word ending with n’t) ahead of the trigger"
W09-4303,W99-0201,0,0.735799,"Missing"
W09-4303,W97-1311,0,0.917586,"Missing"
W09-4303,W06-0901,0,0.0508141,"Missing"
W09-4303,W09-3208,1,0.621638,"Missing"
W09-4303,P08-1030,1,0.771583,"Missing"
W09-4303,N09-2053,1,0.512576,"Missing"
W09-4303,W09-2209,1,0.883091,"Missing"
W10-2301,E03-1020,0,0.105599,"res set mapping MUC (Vilain et al.,1995), B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 the coreference likelihood between two mentions. Nicolae and Nicolae (2006) proposed a new quality measure named BESTCUT which is to optimize the sum of “correctly” placed vertices in the graph. The BESTCUT algorithm works by performing recursive bisection (similar to Kernighan-Lin algorithm) and in each iteration"
W10-2301,W06-3812,0,0.0287107,"3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 the coreference likelihood between two mentions. Nicolae and Nicolae (2006) proposed a new quality measure named BESTCUT which is to optimize the sum of “correctly” placed vertices in the graph. The BESTCUT algorithm works by performing recursive bisection (similar to Kernighan-Lin algorithm) and in each iteration, it searches the best cut that leads to partition into halves. They compared BESTCUT algor"
W10-2301,W06-1633,0,0.0763948,"r one reason, the four constraints can Evaluation Measures set mapping MUC (Vilain et al.,1995), B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 the coreference likelihood between two mentions. Nicolae and Nicolae (2006) proposed a new quality measure named BESTCUT which is to optimize the sum of “correctly” placed vertices in the graph. The BESTCUT algorithm works by performing recursive bisection (simil"
W10-2301,W08-2005,0,0.0538282,"Missing"
W10-2301,P02-1014,0,0.0173432,"opic. One of the most prevalent approaches for coreference resolution is to follow a two-step procedure: (1) a classification step that computes how likely one mention corefers with the other and (2) a clustering step that groups the mentions into clusters such that all mentions in a cluster refer to the same entity. In the past years, NLP researchers have explored and enriched this methodogy from various directions (either in classification or clustering step). Unfortunately, most of the proposed clustering algorithms, e.g., closest-first clustering (Soon et al., 2001), bestfirst clustering (Ng and Cardie, 2002), suffer from a drawback: an instant decision is made (in greedy style) when considering two mentions are coreferent or not, therefore, the algorithm makes no attempt to search through the space of all possible clusterings, which results in a suboptimal clustering (Luo et al., 2004). Various approaches have been proposed to alleviate this problem, of which graph clustering methodology is one of the most promising solutions. The problem of coreference resolution can be modeled as a graph such that the vertex represents a mention, and the edge weight carries 3.2 Word Clustering Word clustering i"
W10-2301,J01-4004,0,0.0759229,"Missing"
W10-2301,H05-1004,0,0.033256,"Missing"
W10-2301,P04-1018,0,0.0229832,"luster refer to the same entity. In the past years, NLP researchers have explored and enriched this methodogy from various directions (either in classification or clustering step). Unfortunately, most of the proposed clustering algorithms, e.g., closest-first clustering (Soon et al., 2001), bestfirst clustering (Ng and Cardie, 2002), suffer from a drawback: an instant decision is made (in greedy style) when considering two mentions are coreferent or not, therefore, the algorithm makes no attempt to search through the space of all possible clusterings, which results in a suboptimal clustering (Luo et al., 2004). Various approaches have been proposed to alleviate this problem, of which graph clustering methodology is one of the most promising solutions. The problem of coreference resolution can be modeled as a graph such that the vertex represents a mention, and the edge weight carries 3.2 Word Clustering Word clustering is a problem defined as clustering a set of words (e.g., nouns, verbs) into groups so that similar words are in the same cluster. Word clustering is a major technique that can benefit many NLP tasks, e.g., thesaurus construction, text classification, and word sense disambiguation. Wo"
W10-2301,W06-1664,0,0.0147007,"AF (Luo, 2005) Table 3. Summary of Evaluation Measures 3 Applying Graph Clustering to NLP A variety of structures in NLP can be naturally represented as graphs, e.g., co-occurrence graphs, coreference graphs, word/sentence/ document graphs. In recent years, there have been an increasing amount of interests in applying graphbased clustering to some NLP problems, e.g., document clustering (Zhong and Ghosh, 2004), summarization (Zha, 2002), coreference resolution (Nicolae and Nicolae, 2006), word sense disambiguation (Dorow and Widdows, 2003; Véronis, 2004; Agirre et al., 2007), word clustering (Matsuo et al., 2006; Biemann, 2006). Many authors chose one or two their favorite graph clustering algorithms and claimed the effectiveness by comparing with supervised algorithms (which need expensive annotations) or other non5 the coreference likelihood between two mentions. Nicolae and Nicolae (2006) proposed a new quality measure named BESTCUT which is to optimize the sum of “correctly” placed vertices in the graph. The BESTCUT algorithm works by performing recursive bisection (similar to Kernighan-Lin algorithm) and in each iteration, it searches the best cut that leads to partition into halves. They compar"
W10-2301,M95-1005,0,0.126552,"Missing"
W10-2301,W09-3208,1,0.890905,"Missing"
W10-2301,W09-4303,1,0.835595,"uctance impose quality within clusters, but not inter-cluster quality; bicriteria takes both into considerations Evaluates the quality of clustering with respect to a randomized graph Drawbacks: (1) It requires global knowledge of the graph’s topology, i.e., the number of edges. Clauset (2005) proposed an improved measure Local Modularity. (2) Resolution limit problem: it fails to identify clusters smaller than a certain scale. Ruan and Zhang (2008) proposed an improved measure HQcut. (3) It fails to distinguish good from bad clustering between different graphs with the same modularity value. Chen et al. (2009) proposed an improved measure Max-Min Modularity Table 1. Summary of Quality Measures Category divisive cut-based spectral multilevel random shortest path agglomerative Algorithms Kernighan-Lin algorithm (Kernighan and Lin, 1970) cut-clustering algorithm (Flake et al., 2003) unnormalized spectral clustering (Luxburg, 2006) normalized spectral clustering I (Luxburg, 2006; Shi and Malik, 2000) normalized spectral clustering II (Luxburg, 2006; Ng, 2002) iterative conductance cutting (ICC) (Kannan et al.,2000) geometric MST clustering (GMC) (Brandes et al., 2007) modularity oriented (White and Smy"
W10-2301,W06-1669,0,\N,Missing
W10-2301,N03-1032,0,\N,Missing
W11-1215,P98-1069,0,0.238684,"6)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the"
W11-1215,P08-1030,1,0.887201,"Missing"
W11-1215,Y09-1024,1,0.840324,"on each other, so we can improve the results by improving the “coherence” of the story (i.e. consistency among all generated answers - query proﬁles). We use feature f2 to check whether the same answer was generated for conﬂicting slots, such as per:parents and per:children. Compared to traditional QA tasks, slot ﬁlling is a more ﬁne-grained task in which diﬀerent slots are expected to obtain semantically diﬀerent answers. Therefore, we explored semantic constraints in both local and global contexts. For example, we utilized bilingual name gazetteers from ACE training corpora, Google n-grams (Ji and Lin, 2009) and the geonames website 3 to encode features f6, f8 and f9; The org:top members/employees slot requires a system to distinguish whether a person member/ employee is in the top position, thus we encoded f10 for this purpose. The knowledge used in our baseline pipelines is relatively static – it is not updated during the 3 http://www.geonames.org/statistics/ 115 extraction process. Achieving high performance for cross-lingual slot ﬁlling requires that we take a broader view, one that looks outside a single document or a single language in order to exploit global knowledge. Fortunately, as more"
W11-1215,W09-3107,1,0.883354,"disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a document which supports the correctness of this answer. KBP 2010 (Ji et al., 2010) deﬁnes 26 ty"
W11-1215,N06-1011,0,0.0324019,"knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along w"
W11-1215,O08-3001,0,0.0379743,"ms for improving crosslingual document retrieval. Many previous validation methods for crosslingual QA, such as those organized by Cross Language Evaluation Forum (Vallin et al., 2005), focused on local information which involves only the query and answer (e.g. (Kwork and Deng, 2006)), keyword translation (e.g. (Mitamura et al., 2006)) and surface patterns (e.g. (Soubbotin and Soubbotin, 2001)). Some global validation approaches considered information redundancy based on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al."
W11-1215,P09-1113,0,0.0373131,"ion 3.3.2 Monolingual Slot Filling We applied a state-of-the-art bilingual slot ﬁlling system (Chen et al., 2010) to process bilingual comparable corpora. This baseline system includes a supervised ACE IE pipeline and a bottom-up pattern matching pipeline. The IE pipeline includes relation extraction and event extraction based on maximum entropy models that incorporate diverse lexical, syntactic, semantic and ontological knowledge. The extracted ACE relations and events are then mapped to KBP slot ﬁlls. In pattern matching, we extract and rank patterns based on a distant supervision approach (Mintz et al., 2009) that uses entity-attribute pairs from Wikipedia Infoboxes and Freebase (Bollacker et al., 2008). We set a low threshold to include more answer candidates, and then a series of ﬁltering steps to reﬁne and improve the overall pipeline results. The ﬁltering steps include removing answers which have inappropriate entity types or have inappropriate dependency paths to the entities. 3.3.3 Document and Name Translation English Candidate Answers Figure 1: Overview of Baseline Crosslingual Slot Filling Pipelines 1 http://www.itl.nist.gov/iad/mig/tests/ace/ 112 We use a statistical, phrase-based MT sys"
W11-1215,W06-1905,0,0.0311885,"on we also demonstrate that these two approaches are complementary and can be used to boost each other’s results in a statistical rescoring model with global evidence from large comparable corpora. Hakkani-Tur et al. (2007) described a ﬁltering mechanism using two crosslingual IE systems for improving crosslingual document retrieval. Many previous validation methods for crosslingual QA, such as those organized by Cross Language Evaluation Forum (Vallin et al., 2005), focused on local information which involves only the query and answer (e.g. (Kwork and Deng, 2006)), keyword translation (e.g. (Mitamura et al., 2006)) and surface patterns (e.g. (Soubbotin and Soubbotin, 2001)). Some global validation approaches considered information redundancy based on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stresse"
W11-1215,P03-1021,0,0.0127194,"Missing"
W11-1215,C10-2109,0,0.154114,"Missing"
W11-1215,D09-1016,0,0.0253525,"ased on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regard"
W11-1215,P99-1067,0,0.0820529,"ng (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a docu"
W11-1215,C04-1089,0,0.145328,"rabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a document which supports"
W11-1215,P06-1112,0,0.0206846,"idation methods for crosslingual QA, such as those organized by Cross Language Evaluation Forum (Vallin et al., 2005), focused on local information which involves only the query and answer (e.g. (Kwork and Deng, 2006)), keyword translation (e.g. (Mitamura et al., 2006)) and surface patterns (e.g. (Soubbotin and Soubbotin, 2001)). Some global validation approaches considered information redundancy based on shallow statistics including cooccurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fu"
W11-1215,P06-1010,0,0.0288872,"et al., 2008), deeper knowledge from dependency parsing (e.g. (Shen et al., 2006)) or logic reasoning (e.g. (Harabagiu et al., 2005)). However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing"
W11-1215,C04-1127,0,0.277434,"Workshop on Building and Using Comparable Corpora, pages 110–119, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics provides substantial improvement over each individual baseline system and even state-of-theart monolingual slot ﬁlling systems. Compared to previous methods of exploiting comparable corpora, our approach is novel in multiple aspects because it exploits knowledge from: (1) both local and global statistics; (2) both languages; and (3) both shallow and deep analysis. 2 Related Work Sudo et al. (2004) found that for a crosslingual single-document IE task, source language extraction and fact translation performed notably better than machine translation and target language extraction. We observed the same results. In addition we also demonstrate that these two approaches are complementary and can be used to boost each other’s results in a statistical rescoring model with global evidence from large comparable corpora. Hakkani-Tur et al. (2007) described a ﬁltering mechanism using two crosslingual IE systems for improving crosslingual document retrieval. Many previous validation methods for cr"
W11-1215,E09-1091,0,0.0620664,"e limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation. Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish111 man, 2008). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009). To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task. 3 Experimental Setup 3.1 Task Deﬁnition The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB). Along with each slot answer, the system must provide the ID of a document which supports the correctness of this answer. KBP 2010 (Ji et al., 2010) d"
W11-1215,N04-1033,0,0.0573646,"Missing"
W11-1215,C98-1066,0,\N,Missing
W11-1215,P02-1054,0,\N,Missing
W11-2206,P02-1051,0,0.0836097,"ACE 2007 entity translation training corpus which includes 119 Chinese-English document pairs. Table 5 shows the number of correct and unique pairs mined pairs from each of the above approaches, as well as how these name mining methods can be augmented using the infobox name mining described in this paper. The names mined from our approach greatly extend the total number of correct translations with only a small number of conflicting name translations. 7 Related Work Most of the previous name translation work combined supervised transliteration approaches with Language Model based re-scoring (Al-Onaizan and Knight, 2002; Huang et al., 2004; Huang, 2005). Our goal of addressing name translation for a large number of languages is similar to the panlingual lexical translation project (Etzioni et al., 2007). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarit"
W11-2206,W09-1604,0,0.0278916,"2007). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarity or document similarity to re-score candidate name translation pairs. Some recent cross-lingual information access work explored attribute mining from Wikipedia pages. For example, Bouma et al. (2009) aligned attributes in Wikipedia infoboxes based on cross-page links. Navigli and Ponzetto (2010) built a multilingual semantic network by integrating the crosslingual Wikipedia page links and WordNet. Ji et Automatic Manual Method (1) Bitexts (2) Comparable Corpora (3) Patterns for Web Mining (4) Bilingual Gazetteer (5) ACE2007 Training Data # Name Pairs 2,451 288 194 59,886 1,541 Infobox Mining # New # Conflicting 8,673 78 8,780 13 8799 0 8,689 74 8,718 52 Table 5: Name Pairs Mined Using Previous Methods al. (2009) described various approaches to automatically mine name translation pairs fro"
W11-2206,P10-1087,0,0.0220335,"Missing"
W11-2206,P98-1069,0,0.0807051,"ly a small number of conflicting name translations. 7 Related Work Most of the previous name translation work combined supervised transliteration approaches with Language Model based re-scoring (Al-Onaizan and Knight, 2002; Huang et al., 2004; Huang, 2005). Our goal of addressing name translation for a large number of languages is similar to the panlingual lexical translation project (Etzioni et al., 2007). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarity or document similarity to re-score candidate name translation pairs. Some recent cross-lingual information access work explored attribute mining from Wikipedia pages. For example, Bouma et al. (2009) aligned attributes in Wikipedia infoboxes based on cross-page links. Navigli and Ponzetto (2010) built a multilingual semantic network by integrating the crosslingual Wikipedia page links and Wo"
W11-2206,N04-1036,0,0.0961143,"raining corpus which includes 119 Chinese-English document pairs. Table 5 shows the number of correct and unique pairs mined pairs from each of the above approaches, as well as how these name mining methods can be augmented using the infobox name mining described in this paper. The names mined from our approach greatly extend the total number of correct translations with only a small number of conflicting name translations. 7 Related Work Most of the previous name translation work combined supervised transliteration approaches with Language Model based re-scoring (Al-Onaizan and Knight, 2002; Huang et al., 2004; Huang, 2005). Our goal of addressing name translation for a large number of languages is similar to the panlingual lexical translation project (Etzioni et al., 2007). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarity or document simila"
W11-2206,W09-3107,1,0.8863,"reasonable amounts of data. 1. Name Pair Mining from Bitexts Within each sentence pair in a parallel corpus, we ran an HMM based bilingual name tagger (references omitted for anonymous review). If the types of the name tags on both sides are identical, we extract the name pairs from this sentence. Then at the corpus-wide level, we count the frequency for each name pair, and only keep the name pairs that are frequent enough. The corpora used for this approach were all DARPA GALE MT training corpora. 2. Comparable Corpora We implemented an information extraction driven approach as described in Ji (2009) to extract name pairs from comparable corpora. This approach is based on extracting information graphs from each language and align names by a graph traverse algorithm. The corpora used for this approach were 2000 English documents and 2000 Chinese documents from the Gigaword corpora. 3. Using patterns for Web mining We constructed heuristic patterns such as parenthetical structure “Chinese name (English name)” (Lin et al., 2008) to extract name pairs from web data with mixed Chinese and En50 glish. We used about 1,000 web pages for this experiment. 4. Bilingual Gazetteer We exploited an LDC"
W11-2206,P08-1113,0,0.0713458,"Missing"
W11-2206,P10-1023,0,0.0166783,"roat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarity or document similarity to re-score candidate name translation pairs. Some recent cross-lingual information access work explored attribute mining from Wikipedia pages. For example, Bouma et al. (2009) aligned attributes in Wikipedia infoboxes based on cross-page links. Navigli and Ponzetto (2010) built a multilingual semantic network by integrating the crosslingual Wikipedia page links and WordNet. Ji et Automatic Manual Method (1) Bitexts (2) Comparable Corpora (3) Patterns for Web Mining (4) Bilingual Gazetteer (5) ACE2007 Training Data # Name Pairs 2,451 288 194 59,886 1,541 Infobox Mining # New # Conflicting 8,673 78 8,780 13 8799 0 8,689 74 8,718 52 Table 5: Name Pairs Mined Using Previous Methods al. (2009) described various approaches to automatically mine name translation pairs from aligned phrases (e.g. cross-lingual Wikipedia title links) or aligned sentences (bi-texts). G e"
W11-2206,P99-1067,0,0.222967,"conflicting name translations. 7 Related Work Most of the previous name translation work combined supervised transliteration approaches with Language Model based re-scoring (Al-Onaizan and Knight, 2002; Huang et al., 2004; Huang, 2005). Our goal of addressing name translation for a large number of languages is similar to the panlingual lexical translation project (Etzioni et al., 2007). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarity or document similarity to re-score candidate name translation pairs. Some recent cross-lingual information access work explored attribute mining from Wikipedia pages. For example, Bouma et al. (2009) aligned attributes in Wikipedia infoboxes based on cross-page links. Navigli and Ponzetto (2010) built a multilingual semantic network by integrating the crosslingual Wikipedia page links and WordNet. Ji et"
W11-2206,C04-1089,0,0.154053,"name translations. 7 Related Work Most of the previous name translation work combined supervised transliteration approaches with Language Model based re-scoring (Al-Onaizan and Knight, 2002; Huang et al., 2004; Huang, 2005). Our goal of addressing name translation for a large number of languages is similar to the panlingual lexical translation project (Etzioni et al., 2007). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarity or document similarity to re-score candidate name translation pairs. Some recent cross-lingual information access work explored attribute mining from Wikipedia pages. For example, Bouma et al. (2009) aligned attributes in Wikipedia infoboxes based on cross-page links. Navigli and Ponzetto (2010) built a multilingual semantic network by integrating the crosslingual Wikipedia page links and WordNet. Ji et Automatic Manual M"
W11-2206,E09-1091,0,0.0712685,"f correct translations with only a small number of conflicting name translations. 7 Related Work Most of the previous name translation work combined supervised transliteration approaches with Language Model based re-scoring (Al-Onaizan and Knight, 2002; Huang et al., 2004; Huang, 2005). Our goal of addressing name translation for a large number of languages is similar to the panlingual lexical translation project (Etzioni et al., 2007). Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007). However, most of these approaches needed large amount of seeds and suffered from information extraction errors, and thus relied on phonetic similarity or document similarity to re-score candidate name translation pairs. Some recent cross-lingual information access work explored attribute mining from Wikipedia pages. For example, Bouma et al. (2009) aligned attributes in Wikipedia infoboxes based on cross-page links. Navigli and Ponzetto (2010) built a multilingual semantic network by integrating the crosslingu"
W11-2206,D10-1042,0,\N,Missing
W11-2206,C98-1066,0,\N,Missing
W11-2206,H05-1055,0,\N,Missing
W12-3005,P11-1115,1,0.852747,"Missing"
W12-3005,N06-1020,0,0.0379031,"es by dataset implied information. We used a variety of lexical and syntactic features after document annotation and sentence retrieval to generate a feature set for supervised learning. 3.1 Relabeling The temporal class labels, start, end, hold, range and irrelevant, are used to inform the final aggregation that is done for each entity in the KB. In order improve the accuracy and of the training instances and incorporate local context that distance supervision does not capture, we used self-training, a semisupervised learning method that has been used to label data for tasks such as parsing (Mcclosky et al., 2006). Using a small set of human annotations, or seed examples, we iteratively labels the partitioned unlabeled set, retaining only the confident labels for retraining the classifier in each round. However, the size of the training dataset resulted in a prohibitively large, sparse feature space. We perform two step in order to generate a more parsimonious classification model that can be used for self-training: (1) feature elimination to identify a minimal set of model features, followed by (2) relabeling using the reduced feature set and a lasso regression classifier. Feature elimination: First,"
W12-3005,P09-1113,0,0.0167866,"ument collection. This source corpus consists of over one million documents that have been collected from a variety of national and international newswire sources and less formal digital publications. The CUNY TSF system shown in 2 ran several parallel submissions, two that varied only in how the classifier is trained. The methods used to develop the system are described in more detail in previous work (Li et al., 2012). In order to obtain a large amount of data to train a classifier for labeling temporal instances, we extended a general distance supervision framework for relation extraction (Mintz et al., 2009) and modify the assumption to consider the value of a temporal expression that additionally cooccurs. That is, for a known query, q, attribute, a, and time range, [tbegin , tend ], sentences in a corpus where q,a, and a temporal expression t co-occur can be automatically labeled with the classes start, end, hold, range or irrelevant for training purposes using a mapping based on the following heuristic rules and on the value of t:   t = tbegin , start      end t = tend , coocurq,a,t = tbegin > t &lt; tend , hold    t = tbegin ∧ tend , range     (t &lt; tbegin ) ∨ (t > tend ), irr. As"
W14-5411,J92-4003,0,0.0537765,"ous, or because of spam being injected into trending conversations to make it visible. For example, the tweet “Hurricane Kitty: http://t.co/cdIexE3” is an advertisement, which is not topically related to Irene. (ii) Deep semantic analysis of the content, especially for images. We rely on distinct terms to refer to the same concept. More extensive semantic analyses of text can help identify those terms, possibly enhancing the propagation process. For example, we can explore existing text dictionaries such as WordNet (Miller, 1995) to mine synonym/hypernym/hyponym relations, and Brown clusters (Brown et al., 1992) to mine other types of relations in order to enrich the concepts extracted from images. 7 Conclusion and Future Work In this paper, we propose a comprehensive information ranking approach which facilitates measurement on crossmedia/cross-genre informativeness based on a novel multi-media information network representation MiNet. We establish links via information extraction method from text and images and verification with Wikipedia. In addition, we propose similarity measurement on intra-media and cross-media using transfer learning techniques. We also introduce a novel En-Tri-Hits algorithm"
W14-5411,D11-1071,1,0.84272,"n Semantic Role Labeling (SRL) (Pradhan et al., 2008). For example, given the sentence “In North Carolina, 10 counties are being evacuated.”, the “evacuation” event is not included in ACE. However, the SRL system can successfully detect the predicate (“evacuated”) and its semantic roles (“10 counties” and “North Carolina”). These argument heads and predicates are added into the meta-information network as vertices, and edges are added between each predicate-argument pairs. We merge entity mentions across tweets and web documents based on a cross-document entity clustering system described in (Chen and Ji, 2011). Moreover, for the same type of nodes from the SRL system, we also merge them by string matching across documents. 3.2 Concept Extraction from Images We also developed a concept modeling approach by extending the similar framework in previous work (Tsai et al., 2012), Probabilistic Logical Tree (PLT), to extract semantic concepts from images. PLT integrates the logical and statistical inferences in a unifying framework where the existing primitive concepts are connected into a potentially unlimited vocabulary of high-level concepts by basic logical operations. In contrast, most existing image"
W14-5411,N10-1125,0,0.0131903,"ch and adopted some heterogenous content analysis, nevertheless, the connection between image and the keywords are still arbitrarily determined by the users, thus it is still far from optimal. Active solutions which attempt to summarize information only focused on single data modalities. For example, Zanzotto et al. (2011) provided a comprehensive comparison about summarization methods for tweets. Zhao et al. (2011) developed a context-sensitive topical PageRank (Brin and Page, 1998) method to extract topical key phrase from Twitter as a way to summarize twitter content. As a new prospective, Feng and Lapata (2010) used LDA to annotate images, but this does not firmly integrate the information across different data types. Huang et al. (2012) presented a tweet ranking approach but only focused on single data modality (i.e., text). Other conventional solutions towards analyzing the relationship or links between the instances have long been proposed and applied, such as PageRank (Brin and Page, 1998)and VisualRank (Jing and Baluja, 2008). The former is excessively used in heterogeneous networks (i.e., webpages and resources) but they are mainly based on linkage itself. VisualRank, which is based on PageRan"
W14-5411,P13-1008,1,0.827007,"problem from two angles to balance the trade-off between quality and granularity/annotation cost. On one hand, to reveal deep semantics in meta-information network, we focus on achieving high-quality extraction for pre-defined finegrained types such as those in NIST Automatic Content Extraction (ACE) 1 . For example, a “Person/Individual” node may include attributes such as “Birth-Place”, and a “Organization/Employee” node may include attributes such as “City-of-Headquarter”. These two nodes may be connected via a “Employment/End-Position” link. We apply an Information Extraction (IE) system (Li et al., 2013) to extract entities, relations and events defined in ACE2005. There are 7 types of entities, 18 types of relations and 33 types of events. This system is based on a joint framework using structured perceptron with efficient beam-search and incorporating diverse lexical, syntactic, semantic and ontological features. We convert the IE output into the graph structured representation of meta-information network by mapping each entity as a node, and link entity nodes by semantic relations or events they are involved. For example, the relations between entities are naturally mapped to links in the"
W14-5411,J08-2006,0,0.0477321,"Example: SRL Contents IE Tweets Sentences of web documents type concept Images Multimedia information Networks Wikipedia Figure 1: An example of meta-information network. Sentence: “Bill Read, Hurricane Center director, said that flooding from Irene killed at least one person in Puerto Rico” argument is an entity, we also add an “Event Argument” link between the event trigger and the entity, such as the link between “Irene” and “killed”. On the other hand, in order to enrich the meta-information network, we extract more coarse-grained salient fact types based on Semantic Role Labeling (SRL) (Pradhan et al., 2008). For example, given the sentence “In North Carolina, 10 counties are being evacuated.”, the “evacuation” event is not included in ACE. However, the SRL system can successfully detect the predicate (“evacuated”) and its semantic roles (“10 counties” and “North Carolina”). These argument heads and predicates are added into the meta-information network as vertices, and edges are added between each predicate-argument pairs. We merge entity mentions across tweets and web documents based on a cross-document entity clustering system described in (Chen and Ji, 2011). Moreover, for the same type of no"
W14-5411,D11-1061,0,0.0306061,"when typing the search query as most search engines do. However, without prior knowledge or due to the word limit, it is never trivial for the users to establish a satisfied ranking list for topics which attract more public attention. Recent changes on some Google Search have integrated image search and adopted some heterogenous content analysis, nevertheless, the connection between image and the keywords are still arbitrarily determined by the users, thus it is still far from optimal. Active solutions which attempt to summarize information only focused on single data modalities. For example, Zanzotto et al. (2011) provided a comprehensive comparison about summarization methods for tweets. Zhao et al. (2011) developed a context-sensitive topical PageRank (Brin and Page, 1998) method to extract topical key phrase from Twitter as a way to summarize twitter content. As a new prospective, Feng and Lapata (2010) used LDA to annotate images, but this does not firmly integrate the information across different data types. Huang et al. (2012) presented a tweet ranking approach but only focused on single data modality (i.e., text). Other conventional solutions towards analyzing the relationship or links between t"
W14-5411,P11-1039,0,0.0281622,"the word limit, it is never trivial for the users to establish a satisfied ranking list for topics which attract more public attention. Recent changes on some Google Search have integrated image search and adopted some heterogenous content analysis, nevertheless, the connection between image and the keywords are still arbitrarily determined by the users, thus it is still far from optimal. Active solutions which attempt to summarize information only focused on single data modalities. For example, Zanzotto et al. (2011) provided a comprehensive comparison about summarization methods for tweets. Zhao et al. (2011) developed a context-sensitive topical PageRank (Brin and Page, 1998) method to extract topical key phrase from Twitter as a way to summarize twitter content. As a new prospective, Feng and Lapata (2010) used LDA to annotate images, but this does not firmly integrate the information across different data types. Huang et al. (2012) presented a tweet ranking approach but only focused on single data modality (i.e., text). Other conventional solutions towards analyzing the relationship or links between the instances have long been proposed and applied, such as PageRank (Brin and Page, 1998)and Vis"
W15-3810,W09-1309,0,\N,Missing
W15-3810,W14-1211,0,\N,Missing
W16-1004,W13-2322,1,0.735417,"me. RED also labels a causal and temporal relation between the two events, &quot;BEFORE/PRECONDITIONS&quot;, showing that the quitting event leads to, but does not directly cause, the replacement, and a temporal CONTAINS relation linking quit to Wednesday.     Event 1: quit - BEFORE DOCTIME, Actual Modality Event 2: replace - AFTER DOCTIME, Actual Modality Relation 1: quit BEFORE/ PRECONDITIONS replace Relation 2: Wednesday CONTAINS quit Although RED does not annotate the arguments of events, it is intended to be combined with semantic role annotations such as PropBank (Bonial et al., 2014) or AMR (Banarescu et al., 2013), which would provide the argument information. For this example, the quit and replace events would also be given the predicate argument structures below: quit.01 Arg0: Media Tycoon Barry Diller Arg1: as chief of Vivendi Universal Entertainment ArgM-TMP: on Wednesday replace.01 Arg2: Parent company chairman JeanRene Fourtou Arg1: Diller ArgM-MOD: will ArgM-PRD: as chief executive of US unit. EER: The following events are connected by Condition and Temporality relations:   Event 1 (Personnel.EndPosition): quit Event 2 (Personnel.StartPosition): replace 34 A preliminary analysis of the Rich ER"
W16-1004,W14-2903,1,0.927744,"Missing"
W16-1004,D12-1045,0,0.048737,"y – along with 21 sense-based subtypes (or relation senses), as shown in Table 1. Events involved in a relation play certain roles. For example, an Attack event and an Injure event in a Contingency_Causality will play Cause and Result roles respectively. Figure 1 shows more information about types and roles. Figure 1: Roles and examples specific to fine-grained event-event relation subtypes. 2.5 Richer Event Descriptions (RED) 3 RED annotation (Ikuta et al., 2014) marks all events in a document, as well as certain relations between those events. RED combines coreference (Pradhan et al., 2007; Lee et al., 2012) and THYME Temporal Relations annotation (Styler et al., 2014) to provide a thorough representation of entities, events and their relations. The RED schema also goes beyond prior annotations of coreference or temporal relations by also annotating subevent structure, cause-effect relations and reporting relations. Guidelines for RED annotation can be found at https://github.com/timjogorman/RicherEventDescr iption/blob/master/guidelines.md. 31 Annotation Data Annotated Features and The representation of events and the scope of annotation vary across the different annotation approaches. Table 2 c"
W16-1004,W15-0809,1,0.776814,"verlapping data set could be used to explore how the differences in annotation procedure lead to differences in decisions about event granularity. 2.3 Event Nugget (EN) An Event Nugget is a tuple of an event trigger, classification of event type and subtype, and realis attribute. It is similar to an event mention in ERE, but arguments are not labelled. EN annotation in 2014 focused on event nuggets (expanded triggers) only, and followed the same taxonomy of 33 event types and subtypes as Light ERE. However, instead of tagging minimal extent as the trigger, EN allowed multi-word event nuggets (Mitamura et al., 2015). Multi-word event nuggets can be either continuous or discontinuous, and are based on the goal of marking the maximal extent of a semantically meaningful unit to express the event in a sentence. EN also added a realis attribute for each event mention. The realis attribute labels each event as Actual, Generic, or Other. TAC KBP 2014 conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute. 30 Table 1: EER event relation types. In 2015,"
W16-1004,W15-0812,1,0.92996,"e taggable, and entity subtypes are not labeled). The event ontology of Light ERE is similar to ACE, with slight modification and reduction, and there is strict coreference of events within documents (Aguilar et al., 2014). As in ACE, the annotation of each event mention includes the identification of a trigger, the labeling of the event type, subtype, and participating event argument entities and time expressions. Simplifying from ACE, only attested actual events are annotated (no irrealis events or arguments). Rich ERE annotation expands on both the inventories and taggability of Light ERE (Song et al., 2015). Rich ERE Entity annotation adds nonspecific entities and nominal head marking, in addition to adding a distinction between Location and Facility entity types. Rich ERE Relation annotation doubles the Light ERE ontology to twenty relation subtypes, and also adds future, hypothetical, and 28 conditional relations. A new category of argument fillers was added for Rich ERE, to allow arguments that are not taggable as entities to be used as fillers for specific relation and event subtypes. For each event mention, Rich ERE labels the event type and subtype, its realis attribute, any of its argumen"
W16-1004,W16-1005,1,0.820435,"event relation types. In 2015, TAC KBP ran an open evaluation on EN that was expanded to three evaluation tasks: Event Nugget Detection, Event Nugget Detection and Coreference, and Event Nugget Coreference. Full Event Nugget Coreference is identified when two or more Event Nuggets refer to the same event. EN annotation in 2015 followed the Rich ERE event taxonomy, which added 5 event types and subtypes to make a total of 38 event types and subtypes, and also followed the Rich ERE guidelines on trigger extents, which adopted the minimal extent rule and disallowed discontinuous event triggers (Song et al., 2016). Annotation of Event Nugget Coreference adopted the concept of Event Hopper as in Rich ERE. 2.4 Event-Event Relations (EER) EER annotation focuses on relations between events in the ERE/ACE taxonomy, both within document and cross-document (Hong et al., 2016). Our general goal is to construct event-centric knowledge networks, where each node is an event and the edges effectively capture the relations between any two events. EER includes five main types of event relations – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 sense-based subtypes (or relation senses)"
W16-1701,P08-1030,1,0.776377,"d texts. Most previous IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 201"
W16-1701,P03-2030,0,0.0231323,"aki et al., 2004; Radev, 2000), which focus on the relatedness between two sentences, by tackling a full document or multiple documents. We adopted some terminology (e.g., Causality and Expansion) from the taxonomy of discourse relations (Miltsakaki et al., 2004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al.,"
W16-1701,D12-1045,0,0.110855,"Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco"
W16-1701,P10-1143,0,0.0939859,"ilar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgeme"
W16-1701,S13-2002,0,0.0743148,"004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity"
W16-1701,P13-1008,1,0.805039,"ic Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Event 1 Event 2 T"
W16-1701,W09-3208,1,0.81815,"ture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.r"
W16-1701,D14-1198,1,0.856623,"event. Event mention: a phrase or sentence within which an event is described, including a trigger and a set of arguments. Event: a set of coreferential event mentions within one document. We define the event-event relation task as the annotation of all applicable logical relations between two events. For example, as illustrated in Figure 1, the following events are connected by Condition and Temporality relations: Event 1: Media tycoon Barry Diller on Wednesday quit as chief of Vivendi Universal EntertainThe ultimate goal of Information Extraction (IE) is to construct “Information Networks” (Li et al., 2014) from unstructured texts. Most previous IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extrac"
W16-1701,W09-4303,1,0.782356,"etworks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_r"
W16-1701,P10-1081,0,0.0395486,"IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 2016 Association for Computa"
W16-1701,W13-1903,0,0.0219813,"ope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the"
W16-1701,D11-1027,0,0.0372204,"alysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, and Bernardo Magnin"
W16-1701,miltsakaki-etal-2004-penn,0,0.0955379,"kes room for the successor. Conjunction Sub-event Sub-event Sub-event / e2 / e5 o / e6 Conjunction Conjunction Figure 2: A hierarchical event network 3.2 Contingency and Comparison A Contingency relation indicates either an event leading to the emergence (Causality) or serving as a triggering condition (Conditional) of another event. Comparison relations indicate deeper logical contrasts between relations. Opposition indicates a relation in which two events are mutually contradictory, and unlikely to be both true. This has some similarity to Contrast.Opposition in the Penn Discourse Treebank (Miltsakaki et al., 2004) or specific annotations of opposition (Feltracco et al., 2015; Takabatake et al., 2015). Negation indicates that while two events could both be true, one shows that the other is no longer true. Competition shows that two events are contrasting versions of the same underlying “event” (e.g., retreat versus escape in disorder). Event-Event Relation Schema Our event-event relation schema includes 5 main Types – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 Subtypes as shown in Table 1. Table 1 also demonstrates Roles. Events involved in a relation play certain ro"
W16-1701,D12-1062,0,0.042122,"formation Extraction (IE) is to construct “Information Networks” (Li et al., 2014) from unstructured texts. Most previous IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceeding"
W16-1701,C14-1198,0,0.0192457,"et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, and Bernardo Magnini. 2015. Opposition relations among verb frames."
W16-1701,ovchinnikova-etal-2010-data,0,0.0262188,"entences, by tackling a full document or multiple documents. We adopted some terminology (e.g., Causality and Expansion) from the taxonomy of discourse relations (Miltsakaki et al., 2004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tone"
W16-1701,W11-0419,0,0.0362447,"events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand"
W16-1701,W00-1009,0,0.0938791,"gn the inventory here with other ongoing efforts to annotate within-document event-event relations. Table 5 shows a mapping between a subset of the relations proposed here and those used in the Richer Event Descriptions (RED) (Ikuta et al., 2014). Other similar resources – such as Penn Discourse Treebank (Miltsakaki et al., 2004) – could also be used. 5 RED RED (Ikuta et al., 2014) is in general more coarsegrained and has fewer types and subtypes. Event-event relations differ from textual entailment (Dagan et al., 2013) or discourse relations (Soricut and Marcu, 2003; Miltsakaki et al., 2004; Radev, 2000), which focus on the relatedness between two sentences, by tackling a full document or multiple documents. We adopted some terminology (e.g., Causality and Expansion) from the taxonomy of discourse relations (Miltsakaki et al., 2004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus"
W16-1701,W13-4004,0,0.0210329,"aints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, and Bernardo Magnini. 2015. Opposition re"
W16-1701,N03-1030,0,0.386271,"en two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Event 1 Event 2 T ype End-Position Start Position T rigger quit replace P erson Barry Diller Jean-Rene P osition chief chief executive Organization Vivendi U.E. U.S. unit t Event2 o Contingency.Condition T emporality.Before-After Boston M arathon Bombings Sub-event o bombings O e4 o Event1 ment. Event 2:"
W16-1701,W15-0813,0,0.0250267,"onjunction Conjunction Figure 2: A hierarchical event network 3.2 Contingency and Comparison A Contingency relation indicates either an event leading to the emergence (Causality) or serving as a triggering condition (Conditional) of another event. Comparison relations indicate deeper logical contrasts between relations. Opposition indicates a relation in which two events are mutually contradictory, and unlikely to be both true. This has some similarity to Contrast.Opposition in the Penn Discourse Treebank (Miltsakaki et al., 2004) or specific annotations of opposition (Feltracco et al., 2015; Takabatake et al., 2015). Negation indicates that while two events could both be true, one shows that the other is no longer true. Competition shows that two events are contrasting versions of the same underlying “event” (e.g., retreat versus escape in disorder). Event-Event Relation Schema Our event-event relation schema includes 5 main Types – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 Subtypes as shown in Table 1. Table 1 also demonstrates Roles. Events involved in a relation play certain roles. For example, an Attack event and an Injure event in a Contingency.Causality will pl"
W16-1701,S13-2001,0,0.0549046,"t representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with othe"
W16-1701,D15-1020,1,0.85988,"hus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, a"
W16-1701,P11-1113,1,\N,Missing
W16-1701,W15-0803,0,\N,Missing
W16-1701,Q14-1012,1,\N,Missing
W16-1701,W14-2903,1,\N,Missing
W16-2701,P02-1051,1,0.755998,"tity Linking and projecting resources from related languages obtained comparable performance as the method using the same amount of training pairs in the original languages without Entity Linking.1 1 Introduction In Machine Translation and Cross-lingual Information Extraction tasks, an important problem is translating out-of-vocabulary words, mostly names. For some names, we can perform transliteration (Knight and Graehl, 1997; Knight and Graehl, 1998), namely converting them to their approximate phonetic equivalents. Previous methods have generally followed the two-step approach proposed by (Al-Onaizan and Knight, 2002): 1 The transliteration systems are publicly available for research purpose at http://nlp.cs.rpi.edu/transliteration/ Generating transliteration hypotheses based on phoneme, grapheme or correspondence, and validating or re-ranking hypotheses using language modeling (Oh and Isahara, 2007) or Information Extraction from the target language (Ji et al., 2009). In this paper, we focus on back-transliteration from languages lacking in Natural Language Processing (NLP) resources to English for two reasons: (1) In NLP tasks such as name tagging, we can take advantage of rich English resources by trans"
W16-2701,C04-1086,0,0.0292958,", some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of r"
W16-2701,W09-2010,0,0.0125686,"l., 2015). Scores of various data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a Mention Context Graph Gm for all entity menti"
W16-2701,P16-1038,1,0.915003,"nt vowel, namely schwa, of its preceding consonant in many Indic scripts. In the light of this fact, it is possible to transfer words or transliteration rules across related languages and thereby avoid collecting extra training data for each language. Therefore, we propose a Unicode namebased projection scheme that transfers IL words to their character equivalents in a high-resource related language so that we can apply the transliteration model trained for the high-resource language. This method is very similar to our recent work on building grapheme-to-phoneme models across related scripts (Deri and Knight, 2016). In Unicode character code charts5 , most vowels, consonants and signs are assigned a name with the following format: 5 http://unicode.org/charts SCRIPT TYPE NAME For example, Bengali independent vowel “অ”, dependent vowel sign “ ু” and consonant “ক” are named BENGALI LETTER A, BENGALI VOWEL SIGN U and BENGALI LETTER KA, respectively. Utilizing these Unicode character names as a bridge, our approach consists of the following steps: (1) For a low resource language L, select its related language L′ whose transliteration pairs can be extracted from existing resources with minimal effort; (2) Con"
W16-2701,P09-1014,0,0.019406,"d in (Nicolai et al., 2015). Scores of various data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a Mention Context Gra"
W16-2701,P06-2025,0,0.0245592,"ve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally,"
W16-2701,N15-1151,0,0.031529,"t of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related languages. Similar cross-lingual projection methods based on data/annotation transfer have also been exploited for other Natural Language Processing tasks, including relation extraction, data annotation, entity recognition, and grapheme-tophoneme models (Xia and Lewis, 2007; Padó and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015; Deri and Knight, 2016). 9 Conclusions and Future Work For many names we need to know the real-world entities they refer to before generating their correct transliterations. In this paper we developed a novel context-aware name transliteration approach by leveraging Entity Linking and related language projection. Experiments have demonstrated that our approach can significantly enhance the transliteration performance. In the future we will explore more knowledge from the KB such as types and properties of entities to improve disambiguation and transliteration. We will also aim to incorporate"
W16-2701,2010.iwslt-papers.7,0,0.0300468,"for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but e"
W16-2701,I08-6006,0,0.0271832,"hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sp"
W16-2701,N07-1047,0,0.193094,"rs in the infobox of “Usher (Singer)”, we take “Usher” and “Lackey” as transliterations for “亚瑟” and “雷基” respectively. Challenge 4: Lack of Training Pairs. Statistical transliteration models usually rely on thousands of name pairs for training. However, it might be costly to collect required training data for lowresource languages. To address this issue, we propose a simple but effective method which transliterates names in a low-resource language using a model trained on one of its similar languages by means of a character mapping table derived from Unicode charts. ment model (m2m-aligner) (Jiampojamarn et al., 2007) to segment and align each transliteration pair in the training data. 2. Transliteration. For each name in the test set, we apply a joint source-channel model (JSCM) (Li et al., 2004) to generate a list of transliteration hypotheses, where the probabilities of n-grams of transliteration unit pairs are estimated from the alignment result. 3. Linking. We link each transliteration hypothesis to an English KB using a languageindependent entity linker (Wang et al., 2015). If context exists, we apply collective inference to link multiple related names simultaneously. 4. Hypotheses Correction. Finall"
W16-2701,P08-1103,0,0.0221752,"us data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a Mention Context Graph Gm for all entity mentions M = {m1 , m2 , ..., mn }"
W16-2701,W09-3504,0,0.0953896,"ne translation) are reported in (Nicolai et al., 2015). Scores of various data representation methods, namely P (character), M (character+boundary marker), T (bigram), and M+T (bigram+boundary marker), are reported in (Kunchukuttan and Bhattacharyya, 2015) . In our experiments, we estimate the conditional probability P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) from the alignment result generated by the many-to-many alignment model (m2m-aligner). Originally designed for letter-to-phoneme conversion, the m2m-aligner has also been used in previous transliteration-related tasks (Jiampojamarn and Kondrak, 2009; Jiampojamarn et al., 2009; Dou et al., 2009; Cook and Stevenson, 2009; Jiampojamarn et al., 2008). We apply the m2malingner to the training data to obtain segmentations and alignments. For languages with a large Figure 2: Collective Inference based Entity Linking. If the context is available, the linker adopts an unsupervised collective inference approach which links multiple entity mentions simultaneously and selects corresponding entity candidates which are most strongly connected in the KB as the final linking results. Figure 2 shows the workflow of Collective Inference based Entity Linking. It first constructs a M"
W16-2701,C00-1056,0,0.0550885,"o “door”, “net”, “dream” and “dew”, respectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise translitera"
W16-2701,C10-1064,0,0.0221027,"he specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related languages. Similar cross-lingual projection methods based on data/annotation transfer have also been exploited for other Natural Language Processing tasks, including relation extraction, data annotation, entity recognition, and grapheme-tophoneme models (Xia and Lewis, 2007; Padó and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015; Deri and Knight, 2016). 9 Conclusions and Future Work For many names we need to know the real-world entities they refer to before generating their correct transliterations. In this paper we developed a novel context-aware name transliteration approach by leveraging Entity Linking and related language projection. Experiments have demonstrated that our approach can significantly enhance the transliteration performance. In the future we will explore more knowledge from the KB such as types and properties of entities to improve disambiguation and transliteration. We will"
W16-2701,P97-1017,1,0.568462,"nroe”, a famous American actress, where “门”, “罗”, “梦” and “露” refer to “door”, “net”, “dream” and “dew”, respectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the"
W16-2701,W15-3912,0,0.0263527,"ration unit pairs ⟨s, t⟩k = ⟨xi xi+1 ...xi+p , yj yj+1 ...yj+q ⟩, where each s or t corresponds to one or more source or target characters, respectively. The JSCM is an ngram model defined as P (S, T ) = P (⟨s, t⟩1 , ⟨s, t⟩2 , ..., ⟨s, t⟩K ) = P (α, β, γ) = K ∏ P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) 4 Entity Linking k=1 We can formulate forward-transliteration and back-transliteration as β¯ = arg max P (α, β, γ) β,γ α ¯ = arg max P (α, β, γ) α,γ Table 2 shows that the performance of JSCM on forward-transliteration (English to foreign language) is comparable with state-of-the-art (Nicolai et al., 2015; Kunchukuttan and Bhattacharyya, 2015) on the NEWS2015 development sets, thereby showing it is a simple but effective model. Target Hindi Kannada Bengali Tamil Hebrew Thai DTL 43.5 32.7 37.1 38.5 61.3 36.2 SEQ 40.4 35.7 37.8 34.4 56.6 35.8 SMT 36.8 28.1 34.9 29.3 53.1 30.6 P 38.8 27.6 35.4 28.6 54.6 - M 41.0 32.7 38.2 32.4 56.4 - T 37.0 28.9 34.5 31.4 54.4 - M+T 40.5 30.4 36.4 33.4 54.5 - number of characters, training pairs may not cover all characters. As a fallback option, we extend the m2m-aligner’s output with pronunciations or romanizations of characters out of the training data. For example, if the Chinese character 孔 (kǒng"
W16-2701,P04-1021,0,0.552024,"sually rely on thousands of name pairs for training. However, it might be costly to collect required training data for lowresource languages. To address this issue, we propose a simple but effective method which transliterates names in a low-resource language using a model trained on one of its similar languages by means of a character mapping table derived from Unicode charts. ment model (m2m-aligner) (Jiampojamarn et al., 2007) to segment and align each transliteration pair in the training data. 2. Transliteration. For each name in the test set, we apply a joint source-channel model (JSCM) (Li et al., 2004) to generate a list of transliteration hypotheses, where the probabilities of n-grams of transliteration unit pairs are estimated from the alignment result. 3. Linking. We link each transliteration hypothesis to an English KB using a languageindependent entity linker (Wang et al., 2015). If context exists, we apply collective inference to link multiple related names simultaneously. 4. Hypotheses Correction. Finally, we revise each hypothesis using the surface forms of the linked entities, and merge and rank the revised hypotheses. The detailed techniques for each step will be presented in the"
W16-2701,W15-3911,0,0.0747924,"ent γ with K transliteration unit pairs ⟨s, t⟩k = ⟨xi xi+1 ...xi+p , yj yj+1 ...yj+q ⟩, where each s or t corresponds to one or more source or target characters, respectively. The JSCM is an ngram model defined as P (S, T ) = P (⟨s, t⟩1 , ⟨s, t⟩2 , ..., ⟨s, t⟩K ) = P (α, β, γ) = K ∏ P (⟨s, t⟩k |⟨s, t⟩k−1 k−n+1 ) 4 Entity Linking k=1 We can formulate forward-transliteration and back-transliteration as β¯ = arg max P (α, β, γ) β,γ α ¯ = arg max P (α, β, γ) α,γ Table 2 shows that the performance of JSCM on forward-transliteration (English to foreign language) is comparable with state-of-the-art (Nicolai et al., 2015; Kunchukuttan and Bhattacharyya, 2015) on the NEWS2015 development sets, thereby showing it is a simple but effective model. Target Hindi Kannada Bengali Tamil Hebrew Thai DTL 43.5 32.7 37.1 38.5 61.3 36.2 SEQ 40.4 35.7 37.8 34.4 56.6 35.8 SMT 36.8 28.1 34.9 29.3 53.1 30.6 P 38.8 27.6 35.4 28.6 54.6 - M 41.0 32.7 38.2 32.4 56.4 - T 37.0 28.9 34.5 31.4 54.4 - M+T 40.5 30.4 36.4 33.4 54.5 - number of characters, training pairs may not cover all characters. As a fallback option, we extend the m2m-aligner’s output with pronunciations or romanizations of characters out of the training data. For ex"
W16-2701,C02-1099,0,0.0621322,"spectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the spec"
W16-2701,I05-1040,0,0.0367448,"es, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related"
W16-2701,2007.mtsummit-papers.47,0,0.0465972,"t problem is translating out-of-vocabulary words, mostly names. For some names, we can perform transliteration (Knight and Graehl, 1997; Knight and Graehl, 1998), namely converting them to their approximate phonetic equivalents. Previous methods have generally followed the two-step approach proposed by (Al-Onaizan and Knight, 2002): 1 The transliteration systems are publicly available for research purpose at http://nlp.cs.rpi.edu/transliteration/ Generating transliteration hypotheses based on phoneme, grapheme or correspondence, and validating or re-ranking hypotheses using language modeling (Oh and Isahara, 2007) or Information Extraction from the target language (Ji et al., 2009). In this paper, we focus on back-transliteration from languages lacking in Natural Language Processing (NLP) resources to English for two reasons: (1) In NLP tasks such as name tagging, we can take advantage of rich English resources by transliterating a name to English. Our analysis of 986 transliteration pairs from the Named Entities Workshop 2015 (NEWS2015) 2 Bengali development set shows that 574 English names can be found in the DBpedia 3 , while only 47 Bengali names exist in the same knowledge base (KB). (2) Back-tran"
W16-2701,W03-1508,0,0.100687,"ebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into"
W16-2701,P98-2220,0,0.220149,"“罗”, “梦” and “露” refer to “door”, “net”, “dream” and “dew”, respectively. For celebrities with corresponding entities in the KB, the collective inference method we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to"
W16-2701,D15-1081,1,0.885331,"one of its similar languages by means of a character mapping table derived from Unicode charts. ment model (m2m-aligner) (Jiampojamarn et al., 2007) to segment and align each transliteration pair in the training data. 2. Transliteration. For each name in the test set, we apply a joint source-channel model (JSCM) (Li et al., 2004) to generate a list of transliteration hypotheses, where the probabilities of n-grams of transliteration unit pairs are estimated from the alignment result. 3. Linking. We link each transliteration hypothesis to an English KB using a languageindependent entity linker (Wang et al., 2015). If context exists, we apply collective inference to link multiple related names simultaneously. 4. Hypotheses Correction. Finally, we revise each hypothesis using the surface forms of the linked entities, and merge and rank the revised hypotheses. The detailed techniques for each step will be presented in the following sections. 2 Approach Overview Figure 1 illustrates the overall framework of our approach, which consists of four steps as follows. 1. Training. We employ a many-to-many align3 Transliteration Hypotheses Generation We use the joint source-channel model (JSCM) proposed in (Li et"
W16-2701,N07-1057,0,0.0174264,"e transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambiguity. Additionally, to tackle the data sparsity challenge in low-resource languages, we propose a simple but effective cross-lingual projection approach to take advantage of resources in related languages. Similar cross-lingual projection methods based on data/annotation transfer have also been exploited for other Natural Language Processing tasks, including relation extraction, data annotation, entity recognition, and grapheme-tophoneme models (Xia and Lewis, 2007; Padó and Lapata, 2009; Kim et al., 2010; Faruqui and Kumar, 2015; Deri and Knight, 2016). 9 Conclusions and Future Work For many names we need to know the real-world entities they refer to before generating their correct transliterations. In this paper we developed a novel context-aware name transliteration approach by leveraging Entity Linking and related language projection. Experiments have demonstrated that our approach can significantly enhance the transliteration performance. In the future we will explore more knowledge from the KB such as types and properties of entities to improve di"
W16-2701,C04-1103,0,0.0395382,"we employ can resolve the ambiguity and hence generate correct transliterations, while it does not work for out-of-KB ones. In order to transliterate such out-of-KB names, some of their properties, such as gender, need to be inferred from the text. 8 Related Work In terms of transliteration unit, existing machine transliteration models can be classified into three categories, phoneme-based (Knight and Graehl, 1997; Lee and Choi, 1998; Wan and Verspoor, 1998; Jung et al., 2000; Meng et al., 2001; Oh and Choi, 2002; Virga and Khudanpur, 2003; Gao et al., 2005), grapheme-based (Li et al., 2004; Zhang et al., 2004; Ekbal et al., 2006; Ganesh et al., 2008; Das et al., 2009; Chinnakotla et al., 2010; Finch and Sumita, 2010), and hybrid (Al-Onaizan and Knight, 2002; Bilac and Tanaka, 2004; Oh and Choi, 2005; Oh et al., 2006; Kim et al., 1999). Since names are inherently associated with entities, it is natural to leverage entity linking to improve name transliteration. To the best of our knowledge, this is the first study using entity linking results to revise transliteration hypotheses. We also take the specific context of a name into consideration to improve the quality of entity linking and reduce ambig"
W16-4503,E09-1003,0,0.0327559,"1) UN Secretary-General Ban Ki-moon appointed “Red” from the Angry Birds as Honorary Ambassador for Green. 2) 联合国秘书长潘基文 United Nations Secretary-General Ban Ki-moon 任命 “愤怒的小鸟” 中的 红色 小鸟 为 绿色荣誉大使. appoint angry bird from red bird as Honorary Ambassador for green culture Large-scale comparable corpora generally contain rich and diverse bilingual translation examples, such as phrase-level equivalents as well as aligned words. Therefore, so far, such corpora have been admitted to be extremely useful in training translation models. During the past decades, great effort has been made by researchers (Rauf et al 2009, Skadina et al 2012, Santanu et al 2014 and Ann et al 2014) to construct and expand the corpora. They fulfilled the goal mainly by using cross-language content similarity measurement techniques. Lexical information, topic model, knowledge base and domain-specific terminology have all been proven to be effective in the acquisition of document-level equivalents (Talvensaari et al 2007, Li et al 2010, Zhu et al 2013 and Hashemi et al 2014). 联合国秘书长潘基文 任命 “ 愤怒的小鸟 ” 中的红色小鸟为绿色 荣誉大使. UN SecretaryGeneral Ban Ki-moon appointed “Red” from the Angry Birds as Honorary Ambassador for Green. (See the Englis"
W16-4503,C10-1073,0,0.0446355,"Missing"
W16-4503,C10-2054,0,0.0721983,"Missing"
W16-4503,su-babych-2012-development,0,0.0412757,"Missing"
W16-4503,W14-3357,0,0.0504273,"Missing"
W16-4503,N12-1065,0,0.0243816,"Missing"
W16-4503,W11-1205,0,0.0564479,"Missing"
W16-4503,W14-1009,0,0.0167428,"ppointed “Red” from the Angry Birds as Honorary Ambassador for Green. 2) 联合国秘书长潘基文 United Nations Secretary-General Ban Ki-moon 任命 “愤怒的小鸟” 中的 红色 小鸟 为 绿色荣誉大使. appoint angry bird from red bird as Honorary Ambassador for green culture Large-scale comparable corpora generally contain rich and diverse bilingual translation examples, such as phrase-level equivalents as well as aligned words. Therefore, so far, such corpora have been admitted to be extremely useful in training translation models. During the past decades, great effort has been made by researchers (Rauf et al 2009, Skadina et al 2012, Santanu et al 2014 and Ann et al 2014) to construct and expand the corpora. They fulfilled the goal mainly by using cross-language content similarity measurement techniques. Lexical information, topic model, knowledge base and domain-specific terminology have all been proven to be effective in the acquisition of document-level equivalents (Talvensaari et al 2007, Li et al 2010, Zhu et al 2013 and Hashemi et al 2014). 联合国秘书长潘基文 任命 “ 愤怒的小鸟 ” 中的红色小鸟为绿色 荣誉大使. UN SecretaryGeneral Ban Ki-moon appointed “Red” from the Angry Birds as Honorary Ambassador for Green. (See the English translation in (2)) Figure 1: Similar"
W16-4503,N10-1063,0,0.0175779,"The recent work seeks to use topic model to improve CLIR. The key issue which is mainly considered in this case is to precisely calculate the similarity between the translations and the documents in the target language. Preiss et al (2012) transformed the topic models in the source language to the target language, and measured the similarity at the level of topic. Zhu et al (2013) utilized the bilingual LDA model and structural information in similarity measurement. Besides, knowledge base like Wikipedia has been proven to be useful for the discovery of bilingual equivalents (Ni et al., 2009, Smith et al., 2010). Otero et al (2010) used Wikipedia categories as the restriction to detect the equivalents within small-scale reliable candidates. Skadinaa et al (2012) proposed a method to merge the comparable corpora respectively obtained from news stories, Wikipedia articles and domain-specific documents. 3 Methodology First of all, we present the methodological framework. Then we introduce the crucial part of the image-image search engine, i.e., SIFT based image similarity measurement. Finally, we list the preprocessing methods for collecting and processing raw data. 3.1 Cross-Media Information Retrieval"
W16-4503,P13-2050,0,0.110047,"words. Therefore, so far, such corpora have been admitted to be extremely useful in training translation models. During the past decades, great effort has been made by researchers (Rauf et al 2009, Skadina et al 2012, Santanu et al 2014 and Ann et al 2014) to construct and expand the corpora. They fulfilled the goal mainly by using cross-language content similarity measurement techniques. Lexical information, topic model, knowledge base and domain-specific terminology have all been proven to be effective in the acquisition of document-level equivalents (Talvensaari et al 2007, Li et al 2010, Zhu et al 2013 and Hashemi et al 2014). 联合国秘书长潘基文 任命 “ 愤怒的小鸟 ” 中的红色小鸟为绿色 荣誉大使. UN SecretaryGeneral Ban Ki-moon appointed “Red” from the Angry Birds as Honorary Ambassador for Green. (See the English translation in (2)) Figure 1: Similar images and their captions in English and Chinese news websites (In this case, we would like to believe that an English journalist and a Chinese peer both attended the ceremony and took the photos from different perspectives, and then released them in the domestic news stories) ————  corresponding author This work is licenced under a Creative Commons Attribution 4.0 Internat"
W16-4503,skadina-etal-2012-collecting,0,\N,Missing
W16-5602,baccianella-etal-2010-sentiwordnet,0,0.0384124,"ess features and news values they encode. the Twitter NLP toolkit (Ritter et al., 2011; Ritter et al., 2012b) which was designed specifically for tweet content. The syntax tree features were calculated using the Stanford parser (Manning et al., 2014) trained using the English caseless model (de Marneffe et al., 2006). The premise behind using the parse tree as a feature is that more complex speech is more likely to be newsworthy because it is well composed. Sentiment terms were determined based on lexical matching from gazetteers(Hu and Liu, 2004; Taboada and Grieve, 2004; Wiebe et al., 2004; Baccianella et al., 2010; Joshi et al., 2011) and compiled into one sentiment dictionary (Li et al., 2012). Normalized stopword usage is important for both composition and human interest particularly because of the structure of a tweet. Since tweets are short and contain few words, if a tweet uses a high proportion of stopwords, it likely doesn’t have many novel terms that would contain human interest. The remaining features are encoded based on the principle that generally recognized names and events are important for detecting topically familiar and important materials. 2.2 Newsworthiness Identification There are t"
W16-5602,de-marneffe-etal-2006-generating,0,0.136089,"Missing"
W16-5602,P11-4022,0,0.019088,"es they encode. the Twitter NLP toolkit (Ritter et al., 2011; Ritter et al., 2012b) which was designed specifically for tweet content. The syntax tree features were calculated using the Stanford parser (Manning et al., 2014) trained using the English caseless model (de Marneffe et al., 2006). The premise behind using the parse tree as a feature is that more complex speech is more likely to be newsworthy because it is well composed. Sentiment terms were determined based on lexical matching from gazetteers(Hu and Liu, 2004; Taboada and Grieve, 2004; Wiebe et al., 2004; Baccianella et al., 2010; Joshi et al., 2011) and compiled into one sentiment dictionary (Li et al., 2012). Normalized stopword usage is important for both composition and human interest particularly because of the structure of a tweet. Since tweets are short and contain few words, if a tweet uses a high proportion of stopwords, it likely doesn’t have many novel terms that would contain human interest. The remaining features are encoded based on the principle that generally recognized names and events are important for detecting topically familiar and important materials. 2.2 Newsworthiness Identification There are two tasks to complete"
W16-5602,Y12-1013,1,0.865142,"tter et al., 2012b) which was designed specifically for tweet content. The syntax tree features were calculated using the Stanford parser (Manning et al., 2014) trained using the English caseless model (de Marneffe et al., 2006). The premise behind using the parse tree as a feature is that more complex speech is more likely to be newsworthy because it is well composed. Sentiment terms were determined based on lexical matching from gazetteers(Hu and Liu, 2004; Taboada and Grieve, 2004; Wiebe et al., 2004; Baccianella et al., 2010; Joshi et al., 2011) and compiled into one sentiment dictionary (Li et al., 2012). Normalized stopword usage is important for both composition and human interest particularly because of the structure of a tweet. Since tweets are short and contain few words, if a tweet uses a high proportion of stopwords, it likely doesn’t have many novel terms that would contain human interest. The remaining features are encoded based on the principle that generally recognized names and events are important for detecting topically familiar and important materials. 2.2 Newsworthiness Identification There are two tasks to complete to identify newsworthy, salient content. The first is to iden"
W16-5602,C12-1104,0,0.0726368,"Missing"
W16-5602,P14-5010,0,0.00444356,"s. The non-syntactic features listed in Table 1 are calculated as the number of words in the tweet and the normalized features are calculated as the ratio of the number of sentiment words to the total number of words in the tweet not including stopwords. The named entities and slangs were extracted using 12 News Value 2 4 3 3 3 1, 4 1, 4 2, 4 2 2 2 Table 1: Newsworthiness features and news values they encode. the Twitter NLP toolkit (Ritter et al., 2011; Ritter et al., 2012b) which was designed specifically for tweet content. The syntax tree features were calculated using the Stanford parser (Manning et al., 2014) trained using the English caseless model (de Marneffe et al., 2006). The premise behind using the parse tree as a feature is that more complex speech is more likely to be newsworthy because it is well composed. Sentiment terms were determined based on lexical matching from gazetteers(Hu and Liu, 2004; Taboada and Grieve, 2004; Wiebe et al., 2004; Baccianella et al., 2010; Joshi et al., 2011) and compiled into one sentiment dictionary (Li et al., 2012). Normalized stopword usage is important for both composition and human interest particularly because of the structure of a tweet. Since tweets"
W16-5602,D11-1141,0,0.0582494,"erest meaning it must affect many people. Using Galtung and Ruge’s metaphor of a signal for news, these principles should indicate a strong signal or spike in news. The non-syntactic features listed in Table 1 are calculated as the number of words in the tweet and the normalized features are calculated as the ratio of the number of sentiment words to the total number of words in the tweet not including stopwords. The named entities and slangs were extracted using 12 News Value 2 4 3 3 3 1, 4 1, 4 2, 4 2 2 2 Table 1: Newsworthiness features and news values they encode. the Twitter NLP toolkit (Ritter et al., 2011; Ritter et al., 2012b) which was designed specifically for tweet content. The syntax tree features were calculated using the Stanford parser (Manning et al., 2014) trained using the English caseless model (de Marneffe et al., 2006). The premise behind using the parse tree as a feature is that more complex speech is more likely to be newsworthy because it is well composed. Sentiment terms were determined based on lexical matching from gazetteers(Hu and Liu, 2004; Taboada and Grieve, 2004; Wiebe et al., 2004; Baccianella et al., 2010; Joshi et al., 2011) and compiled into one sentiment dictiona"
W16-5602,C14-1083,0,0.020444,"ets posted on a daily basis, automatic tweet ranking has become an important task to assist fast information distillation. Previous work (Inouye and Kalita, 2011; Yang et al., 2011; Liu et al., 2012; Ren et al., 2013; Shou et al., 2013; Chua and Asur, 2013; Chang et al., 2013) focused on selecting informative tweets based on a variety of criteria such as readability, author’s influˇ ence and users’ interest. In addition, (Stajner et al., 2013) studied selection of user’s responses to news by optimizing an objective function which jointly models the messages’ utility scores and their entropy. (Wei and Gao, 2014) proposed using learningto-rank techniques to help conduct single-document Tweet ranking is also related to the previous work concerning tweet summarization which summarized important information from tweet streams. Modern summarization approaches rank sentences or phrases from informal genres such as social media, web forums, and micro-blogging sites. Some methods determine semantic relations of manually annotated hashtags and user replies using social network and web document graphs (e.g., (Huang et al., 2012)). Our goal is to accomplish ranking of micro-blogging content using natural langua"
W16-5602,J04-3002,0,0.0772585,"Table 1: Newsworthiness features and news values they encode. the Twitter NLP toolkit (Ritter et al., 2011; Ritter et al., 2012b) which was designed specifically for tweet content. The syntax tree features were calculated using the Stanford parser (Manning et al., 2014) trained using the English caseless model (de Marneffe et al., 2006). The premise behind using the parse tree as a feature is that more complex speech is more likely to be newsworthy because it is well composed. Sentiment terms were determined based on lexical matching from gazetteers(Hu and Liu, 2004; Taboada and Grieve, 2004; Wiebe et al., 2004; Baccianella et al., 2010; Joshi et al., 2011) and compiled into one sentiment dictionary (Li et al., 2012). Normalized stopword usage is important for both composition and human interest particularly because of the structure of a tweet. Since tweets are short and contain few words, if a tweet uses a high proportion of stopwords, it likely doesn’t have many novel terms that would contain human interest. The remaining features are encoded based on the principle that generally recognized names and events are important for detecting topically familiar and important materials. 2.2 Newsworthiness"
W16-5602,C12-1076,1,\N,Missing
W17-5603,P05-1074,0,0.0960262,"ork has much better generalizability. Although in this work, we only trained and tested our framework on short paraphrases, our model can be further applied to any longer phrases. We demonstrate the effectiveness of our framework on various phrase similarity tasks. Results show that our model can achieve state-of-the-art performance on capturing semantics of phrases. 2 Paraphrase Database PPDB (Ganitkevitch et al., 2013) is a database which contains hundreds of millions of English paraphrase pairs extracted from bilingual parallel corpora. It is constructed with the bilingual pivoting method (Bannard and Callison-Burch, 2005). Namely if two English phrases are translated to the same foreign phrase, then the two English phrases are considered to be paraphrases. PPDB comes with 6 pre-packaged sizes: S to XXXL 1 . In our work, to ensure efficiency and correctness, we only used the smallest and most accurate S package. To generate training data, we filtered out the paraphrases (p1 , p2 ) where 1. p1 is identical to p2 2. p1 or p2 contains any non-letter characters except spaces 3. p1 or p2 contains words which are not contained in our trained word embeddings 4. p1 and p2 are both single words After such a filtering st"
W17-5603,D14-1082,0,0.0223535,"aphrase database and present a pair-wise gated recurrent units (pairwiseGRU) framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although compositional models can be trained unsupervisedly with auto encoders such as the Re"
W17-5603,P16-2011,1,0.819934,"this work, we propose to take advantage of largescaled paraphrase database and present a pair-wise gated recurrent units (pairwiseGRU) framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although compositional models"
W17-5603,N13-1092,0,0.10053,"Missing"
W17-5603,D13-1170,0,0.252266,"to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although compositional models can be trained unsupervisedly with auto encoders such as the Recursive Auto Encoder (Socher et al., 2011), such models ignore contexts and actual"
W17-5603,S13-2007,0,0.0287982,"y size. Figure 2: Performances of PGRU trained under different configurations as well as the performance of AVG. els. Moreover, for each training set size, while we observe obvious performance gain from increasing k from 9 to 29, the gain from further increasing k to 99 is more moderate. Considering the amount of additional computation required, we conclude that it is not worth the computation efforts to increase k even further. 3.2 Phrase Similarity Tasks Datasets Following Yu and Dredze (2015), we evaluated our model on human annotated datasets including SemEval2013 Task 5(a) (SemEval2013) (Korkontzelos et al., 2013) and the noun-modifer problem in Turney2012 (Turney2012) (Turney, 2012). SemEval2013 is a task to classify a phrase pair as either semantically similar or dissimilar. Turney2012(5) is a task to select the most semantically similar word to the given bigram phrase among 5 candidate words. In order to test the model’s sensitivity to word orders, extended from Turney2012(5), Turney2012(10) reverse the bigram and add it to the original bigram side. Thus the model needs to choose a bigram from these two bigrams and also choose the most semantically similar word from 5 candidates. Examples for these"
W17-5603,P15-1150,0,0.0256618,"work can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although compositional models can be trained unsupervisedly with auto encoders such as the Recursive Auto Encoder (Socher et al., 2011), such models ignore contexts and actual usages of phrases and thus cannot fully capture the semant"
W17-5603,N16-1030,0,0.037048,"e easily generalized to all phrases. In this work, we propose to take advantage of largescaled paraphrase database and present a pair-wise gated recurrent units (pairwiseGRU) framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AF"
W17-5603,D15-1201,0,0.0192439,"we need embeddings for phrases. For example, in Information Extraction, we need representations for multi-word entity mentions, and in Question Answering, we may need representations for even longer question and answer phrases. Generally, there are two types of models to learn phrase emmbeddings: noncompositional models and compositional models. Noncompositional models treat phrases as single information units while ignoring their components and structures. Embeddings of phrases can thus be learned with general word embedding learning techniques (Mikolov et al., 2013; Yin and Sch¨utze, 2014; Yazdani et al., 2015), however, such methods are not scalable to all English phrases and suffer from data sparsity. On the other hand, compositional models derives a phrase’s embedding from the embeddings of its component words (Socher et al., 2012; Mikolov et al., 2013; Yu and Dredze, 2015; Poliak et al., 2017). Previous work have shown good results from compositional models which simply used predefined functions such as element-wise addition (Mikolov et al., 2013). However, such methods ignore word orders and cannot capture complex linguistic phenomena. Other studies on compositional models learn complex composi"
W17-5603,P14-3006,0,0.0292372,"Missing"
W17-5603,E17-2081,0,0.0368016,"Missing"
W17-5603,Q15-1017,0,0.606198,"learn phrase emmbeddings: noncompositional models and compositional models. Noncompositional models treat phrases as single information units while ignoring their components and structures. Embeddings of phrases can thus be learned with general word embedding learning techniques (Mikolov et al., 2013; Yin and Sch¨utze, 2014; Yazdani et al., 2015), however, such methods are not scalable to all English phrases and suffer from data sparsity. On the other hand, compositional models derives a phrase’s embedding from the embeddings of its component words (Socher et al., 2012; Mikolov et al., 2013; Yu and Dredze, 2015; Poliak et al., 2017). Previous work have shown good results from compositional models which simply used predefined functions such as element-wise addition (Mikolov et al., 2013). However, such methods ignore word orders and cannot capture complex linguistic phenomena. Other studies on compositional models learn complex composition functions from data. For instance, the Recursive Neural Network (Socher et al., 2012) finds all linguistically plausible phrases in a sentence and recursively compose phrase embedding from subphrase embeddings with learned matrix/tensor transformations. Since compo"
W17-5603,C14-1220,0,0.0182983,"to all phrases. In this work, we propose to take advantage of largescaled paraphrase database and present a pair-wise gated recurrent units (pairwiseGRU) framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although co"
W17-5603,D12-1110,0,0.0592651,"Generally, there are two types of models to learn phrase emmbeddings: noncompositional models and compositional models. Noncompositional models treat phrases as single information units while ignoring their components and structures. Embeddings of phrases can thus be learned with general word embedding learning techniques (Mikolov et al., 2013; Yin and Sch¨utze, 2014; Yazdani et al., 2015), however, such methods are not scalable to all English phrases and suffer from data sparsity. On the other hand, compositional models derives a phrase’s embedding from the embeddings of its component words (Socher et al., 2012; Mikolov et al., 2013; Yu and Dredze, 2015; Poliak et al., 2017). Previous work have shown good results from compositional models which simply used predefined functions such as element-wise addition (Mikolov et al., 2013). However, such methods ignore word orders and cannot capture complex linguistic phenomena. Other studies on compositional models learn complex composition functions from data. For instance, the Recursive Neural Network (Socher et al., 2012) finds all linguistically plausible phrases in a sentence and recursively compose phrase embedding from subphrase embeddings with learned"
W17-5603,P14-1011,0,0.0168974,"e-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although compositional models can be trained unsupervisedly with auto encoders such as the Recursive Auto Encoder (Socher et al., 2011), such models ignore contexts and actual usages of phrases and thus cannot fully capture the semantics of phrases. Some previous work train compositional models for a specific task, such as Sentiment Analysis (Socher et al.,"
W17-5603,P15-1109,0,0.0153556,"sent a pair-wise gated recurrent units (pairwiseGRU) framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks. 1 Introduction Continuous vector representations of words, also known as word embeddings, have been used as features for all kinds of NLP tasks such as Information Extraction (Lample et al., 2016; Zeng et al., 2014; Feng et al., 2016; Huang et al., 2016), Semantic Parsing (Chen and Manning, 2014; Zhou and Xu, 2015; Konstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although compositional models can be trained unsupervisedly with auto encoders such as the Recursive Auto Encode"
W17-5603,D11-1014,0,0.428375,"onstas et al., 2017), Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014; Tai et al., 2015), Question Answering (Tellex et al., 2003; Kumar et al., 2015) and machine translation (Cho et al., 2014; Zhang et al., 2014) and have yielded stateof-the-art results. However, single word embed16 Proceedings of the Workshop on Curation and Applications of Parallel and Comparable Corpora, pages 16–23, c Taipei, Taiwan, November 27–December 1, 2017. 2017 AFNLP 2.1 Although compositional models can be trained unsupervisedly with auto encoders such as the Recursive Auto Encoder (Socher et al., 2011), such models ignore contexts and actual usages of phrases and thus cannot fully capture the semantics of phrases. Some previous work train compositional models for a specific task, such as Sentiment Analysis (Socher et al., 2013b; Kalchbrenner et al., 2014; Kim, 2014) or syntactic parsing (Socher et al., 2010). But these methods require large amounts of human annotated data. Moreover, the embeddings obtained will be biased to a specific task and thus will not be applicable for other tasks. A more general source of training data which does not require human annotation is plain text through lan"
W17-5603,P16-1025,1,\N,Missing
W17-5603,P17-1014,0,\N,Missing
W18-0516,C16-1167,0,0.0852799,"China. 2 3 Approach Related Work Figure 1 shows the overall architecture of our approach. For a query and the definition of a candidate Chengyu, we first apply a word segmentation tool jieba2 to segment query and definition into words, and apply a Bi-LSTM network to encode each word with a contextual embedding. In order to better capture the correlation between a query and a Chengyu, we further compare the representations of the Chengyu definition and the contextual embedding of each word in the query, and Our Chengyu cloze test task is similar to reading comprehension (Hermann et al., 2015; Cui et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Seo et al., 2016). However, it’s more challenging because the context includes a sentence instead of a paragraph, the Chengyu phrase itself does not convey its figurative meaning, and there are many more candidate answers. Very few Natural Language Processing techniques have been applied to understand or recommend Chengyu. 2 155 https://github.com/fxsjy/jieba Figure 1: Architecture Overview Training With the weighted sum vector representation of the query R, we apply a softmax function to compute the probability of each candidate Chengyu dj to be fille"
W18-0516,P16-1086,0,0.0155462,"ure 1 shows the overall architecture of our approach. For a query and the definition of a candidate Chengyu, we first apply a word segmentation tool jieba2 to segment query and definition into words, and apply a Bi-LSTM network to encode each word with a contextual embedding. In order to better capture the correlation between a query and a Chengyu, we further compare the representations of the Chengyu definition and the contextual embedding of each word in the query, and Our Chengyu cloze test task is similar to reading comprehension (Hermann et al., 2015; Cui et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Seo et al., 2016). However, it’s more challenging because the context includes a sentence instead of a paragraph, the Chengyu phrase itself does not convey its figurative meaning, and there are many more candidate answers. Very few Natural Language Processing techniques have been applied to understand or recommend Chengyu. 2 155 https://github.com/fxsjy/jieba Figure 1: Architecture Overview Training With the weighted sum vector representation of the query R, we apply a softmax function to compute the probability of each candidate Chengyu dj to be filled into the slot. take the weighted sum o"
W18-0516,D11-1090,0,0.0601244,"ete and vivid plot, and the characters have distinct personalities. But it's like ____some sentences need to be further polished. ⽩璧微瑕 (white jade with a little blemish) 洁⽩的⽟上有些⼩斑点。⽐喻很好的⼈或物有些⼩缺点，美中不⾜。 A flaw in a white jade. It's a metaphor for a good person or a good thing with a little defect. Table 1: Chengyu Examples Chung (2009) studied a subset of Chinese figurative language, focusing on Chinese five elements and body part terms. Limited efforts have used Chengyu dictionaries to expand Chinese emotion lexicon (Xu et al., 2010) and improve Chinese word segmentation (Chan and Chong, 2008; Sun and Xu, 2011; Wang and Xu, 2017). Chengyus differ from metaphors in other languages (Tsvetkov et al., 2014; Shutova, 2010) because they do not follow the grammatical structure and syntax of the modern Chinese. attentive neural network architecture to select the most appropriate Chengyu to fit in the slot context of each query. We first encode query sentence and Chengyu definitions using a bi-directional long short-term memory (Bi-LSTM) network (Hochreiter and Schmidhuber, 1997). To better capture the correlation between the query and the definition, we apply a soft attention to assign a weight to each wor"
W18-0516,P14-1024,0,0.016641,"e sentences need to be further polished. ⽩璧微瑕 (white jade with a little blemish) 洁⽩的⽟上有些⼩斑点。⽐喻很好的⼈或物有些⼩缺点，美中不⾜。 A flaw in a white jade. It's a metaphor for a good person or a good thing with a little defect. Table 1: Chengyu Examples Chung (2009) studied a subset of Chinese figurative language, focusing on Chinese five elements and body part terms. Limited efforts have used Chengyu dictionaries to expand Chinese emotion lexicon (Xu et al., 2010) and improve Chinese word segmentation (Chan and Chong, 2008; Sun and Xu, 2011; Wang and Xu, 2017). Chengyus differ from metaphors in other languages (Tsvetkov et al., 2014; Shutova, 2010) because they do not follow the grammatical structure and syntax of the modern Chinese. attentive neural network architecture to select the most appropriate Chengyu to fit in the slot context of each query. We first encode query sentence and Chengyu definitions using a bi-directional long short-term memory (Bi-LSTM) network (Hochreiter and Schmidhuber, 1997). To better capture the correlation between the query and the definition, we apply a soft attention to assign a weight to each word in the query sentence, and predict a matching score for each candidate Chengyu. Our system s"
W18-0516,I08-4018,0,0.0132766,"novel includes a complete and vivid plot, and the characters have distinct personalities. But it's like ____some sentences need to be further polished. ⽩璧微瑕 (white jade with a little blemish) 洁⽩的⽟上有些⼩斑点。⽐喻很好的⼈或物有些⼩缺点，美中不⾜。 A flaw in a white jade. It's a metaphor for a good person or a good thing with a little defect. Table 1: Chengyu Examples Chung (2009) studied a subset of Chinese figurative language, focusing on Chinese five elements and body part terms. Limited efforts have used Chengyu dictionaries to expand Chinese emotion lexicon (Xu et al., 2010) and improve Chinese word segmentation (Chan and Chong, 2008; Sun and Xu, 2011; Wang and Xu, 2017). Chengyus differ from metaphors in other languages (Tsvetkov et al., 2014; Shutova, 2010) because they do not follow the grammatical structure and syntax of the modern Chinese. attentive neural network architecture to select the most appropriate Chengyu to fit in the slot context of each query. We first encode query sentence and Chengyu definitions using a bi-directional long short-term memory (Bi-LSTM) network (Hochreiter and Schmidhuber, 1997). To better capture the correlation between the query and the definition, we apply a soft attention to assign a"
W18-0516,P16-1223,0,0.144257,"Missing"
W18-0516,I17-1017,0,0.0613552,", and the characters have distinct personalities. But it's like ____some sentences need to be further polished. ⽩璧微瑕 (white jade with a little blemish) 洁⽩的⽟上有些⼩斑点。⽐喻很好的⼈或物有些⼩缺点，美中不⾜。 A flaw in a white jade. It's a metaphor for a good person or a good thing with a little defect. Table 1: Chengyu Examples Chung (2009) studied a subset of Chinese figurative language, focusing on Chinese five elements and body part terms. Limited efforts have used Chengyu dictionaries to expand Chinese emotion lexicon (Xu et al., 2010) and improve Chinese word segmentation (Chan and Chong, 2008; Sun and Xu, 2011; Wang and Xu, 2017). Chengyus differ from metaphors in other languages (Tsvetkov et al., 2014; Shutova, 2010) because they do not follow the grammatical structure and syntax of the modern Chinese. attentive neural network architecture to select the most appropriate Chengyu to fit in the slot context of each query. We first encode query sentence and Chengyu definitions using a bi-directional long short-term memory (Bi-LSTM) network (Hochreiter and Schmidhuber, 1997). To better capture the correlation between the query and the definition, we apply a soft attention to assign a weight to each word in the query sente"
W18-0516,O09-4005,0,0.0132754,"rning his paw in the process, the monkey gobbles them up. It's used to describe a person used unwittingly or unwillingly by another to accomplish the other's own purpose with his own risk but gets nothing. Metaphor 这篇⼩说情节完整⽣动，⼈物性格鲜 明，但____，个别语句还⽋推敲。 This novel includes a complete and vivid plot, and the characters have distinct personalities. But it's like ____some sentences need to be further polished. ⽩璧微瑕 (white jade with a little blemish) 洁⽩的⽟上有些⼩斑点。⽐喻很好的⼈或物有些⼩缺点，美中不⾜。 A flaw in a white jade. It's a metaphor for a good person or a good thing with a little defect. Table 1: Chengyu Examples Chung (2009) studied a subset of Chinese figurative language, focusing on Chinese five elements and body part terms. Limited efforts have used Chengyu dictionaries to expand Chinese emotion lexicon (Xu et al., 2010) and improve Chinese word segmentation (Chan and Chong, 2008; Sun and Xu, 2011; Wang and Xu, 2017). Chengyus differ from metaphors in other languages (Tsvetkov et al., 2014; Shutova, 2010) because they do not follow the grammatical structure and syntax of the modern Chinese. attentive neural network architecture to select the most appropriate Chengyu to fit in the slot context of each query. We"
W18-0516,C10-1136,0,0.119875,"thing. Metaphor 这篇⼩说情节完整⽣动，⼈物性格鲜 明，但____，个别语句还⽋推敲。 This novel includes a complete and vivid plot, and the characters have distinct personalities. But it's like ____some sentences need to be further polished. ⽩璧微瑕 (white jade with a little blemish) 洁⽩的⽟上有些⼩斑点。⽐喻很好的⼈或物有些⼩缺点，美中不⾜。 A flaw in a white jade. It's a metaphor for a good person or a good thing with a little defect. Table 1: Chengyu Examples Chung (2009) studied a subset of Chinese figurative language, focusing on Chinese five elements and body part terms. Limited efforts have used Chengyu dictionaries to expand Chinese emotion lexicon (Xu et al., 2010) and improve Chinese word segmentation (Chan and Chong, 2008; Sun and Xu, 2011; Wang and Xu, 2017). Chengyus differ from metaphors in other languages (Tsvetkov et al., 2014; Shutova, 2010) because they do not follow the grammatical structure and syntax of the modern Chinese. attentive neural network architecture to select the most appropriate Chengyu to fit in the slot context of each query. We first encode query sentence and Chengyu definitions using a bi-directional long short-term memory (Bi-LSTM) network (Hochreiter and Schmidhuber, 1997). To better capture the correlation between the quer"
W18-4203,J14-2006,0,0.245424,"s 1967 book, “The Death Of A President”. the first character of 胡 锦 涛 can be decomposed into two characters 古月; and 古月 is a famous actor playing Mao Zedong. Both Mao and Hu acted as the former chairman of China. Angel historical book Air Force One terrorism 古月 (Gu Yue) drama 胡锦涛 (Hu Jintao) politics Table 4: Domain Mapping Examples for Other Purposes 3 System Encoding and Decoding 3.1 Cipher for Encoding Traditional text encryption techniques focus on alphabetic substitution or transposition based on lexical level (Franceschini and Mukherjee, 1996; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural"
W18-4203,N15-1021,1,0.826946,"rical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim is to create an amusing new form that is understandable by a reader, e.g., frenemy for an entity that is both friend and enemy. Doing this well requires fusion at the phonetic level followed by an appropriate choice of spelling. Machines cannot yet process such created neologisms. This portmanteau generation approach (Deri and Knight, 2015) can be used to encode other types of neologisms 10 11 https://en.wikipedia.org/wiki/Leet https://en.wikipedia.org/wiki/Martian_language 28 in both English and Chinese. We observe the words embedded are usually semantically and phonetically compati"
W18-4203,N15-1180,1,0.862423,"hography, so the coded messages are more easily remembered and adopted. An encoded message is as follows. • Chinese: 明天下午三点到鼓楼大街集合。 • English translation: Let’s gather at the Bell Tower Street at 3pm tomorrow. • Pronounce English in Chinese phonetic system (pinyin): Laici galete ate de beier taoer sijute aite teli piaimu temoluo. • Code by spelling out pinyin: 来此盖乐特爱特得贝尔套儿思聚特爱特特例皮埃姆特摩罗。 (3) Poetry passwords. (Greene et al., 2010) build the first statistical machine translation system to translate poetry. They subsequently apply poetry generation techniques to the problem of password security (Ghazvininejad and Knight, 2015). In this work, the machine first assigns a random 60-bit password to a user. Because the user cannot remember a random sequence of 0’s and 1’s, the machine converts the bit sequence into a more memorable iambic tetrameter couplet (e.g., The legendary Japanese // subsidiaries overseas). The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is maintained. Brennan et al. (2012) propose three methods to create adversarial passages: obfuscation, imitation, and translation. They find manual circumvention methods work very well, while automated"
W18-4203,D10-1051,1,0.77301,"ion (MT) to make the results more challenging to decode because it is difficult to recover from MT errors, and (2) polish the output by using common words or phrases in the user’s own orthography, so the coded messages are more easily remembered and adopted. An encoded message is as follows. • Chinese: 明天下午三点到鼓楼大街集合。 • English translation: Let’s gather at the Bell Tower Street at 3pm tomorrow. • Pronounce English in Chinese phonetic system (pinyin): Laici galete ate de beier taoer sijute aite teli piaimu temoluo. • Code by spelling out pinyin: 来此盖乐特爱特得贝尔套儿思聚特爱特特例皮埃姆特摩罗。 (3) Poetry passwords. (Greene et al., 2010) build the first statistical machine translation system to translate poetry. They subsequently apply poetry generation techniques to the problem of password security (Ghazvininejad and Knight, 2015). In this work, the machine first assigns a random 60-bit password to a user. Because the user cannot remember a random sequence of 0’s and 1’s, the machine converts the bit sequence into a more memorable iambic tetrameter couplet (e.g., The legendary Japanese // subsidiaries overseas). The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is ma"
W18-4203,W13-0908,0,0.0374128,"Missing"
W18-4203,P13-1107,1,0.836828,"between encoding and decoding objectives. This opens an unexplored area of coded language processing. 1 2 https://freedomhouse.org/report/freedom-net/freedom-net-2016 https://en.wikipedia.org/wiki/Internet_censorship_in_China 23 Proceedings of the First Workshop on Natural Language Processing for Internet Freedom, pages 23–33 Santa Fe, New Mexico, USA, August 20, 2018. 2 Human Encoding 2.1 Categories of Coded Language Netizens create and use obfuscated language for a variety of purposes. Discussing sensitive information and evading censorship. Code words widely exist in Chinese social media (Huang et al., 2013; Zhang et al., 2014; Zhang et al., 2015). Bamman et al. (2012) automatically discover politically sensitive terms from Chinese tweets based on message deletion analysis. When Chinese netizens talk about the former politician 周永康 (Zhou Yongkang), they use a coded word 康师傅 (Master Kang), a brand of instant noodles whose Chinese spellings share one character 康 (kang). The Enron emails3 also include many code words, such as dinosaur referring to an illegal stock. Masking illegal activity. In the dark web for human trafficking, arms dealing, and drug dealing, research chemical or RC is euphemistic"
W18-4203,jabbari-etal-2008-using,0,0.0134371,"The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is maintained. Brennan et al. (2012) propose three methods to create adversarial passages: obfuscation, imitation, and translation. They find manual circumvention methods work very well, while automated translation methods are not effective. Potash et al. (2015) develop a GhostWriter system that can take a given artist’s rap lyrics and generate similar yet unique lyrics. Other recent work detects word obfuscation in adversarial communication (Roussinov et al., 2007; Fong et al., 2008; Jabbari et al., 2008; Deshmukh et al., 2014; Agarwal and Sureka, 2015) using existing commonsense KBs such as ConceptNet (Agarwal and Sureka, 2015). 3.3 Entity Encoding and Decoding Huang et al. (2013), Zhang et al. (2014) and Zhang et al. (2015) study a problem of encoding and decoding entity morph, which is a special case of coded name alias to hide the original entities for expressing strong sentiment or evading censorship in Chinese social media. They propose a variety of novel approaches to automatically encode proper and interesting morphs (Zhang et al., 2014), including Phonetic Substitution, Spelling Deco"
W18-4203,P95-1034,1,0.591728,"l level (Franceschini and Mukherjee, 1996; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim is to create an amusing new form that is understandable by a reader, e.g., frenemy for an entity that is both friend and enemy. Doing this well requires fusion at the phonetic level followed by an appropriate choice of spelling. Machines cannot yet process such created neologisms. This portmanteau"
W18-4203,W11-1202,1,0.80527,"l book Air Force One terrorism 古月 (Gu Yue) drama 胡锦涛 (Hu Jintao) politics Table 4: Domain Mapping Examples for Other Purposes 3 System Encoding and Decoding 3.1 Cipher for Encoding Traditional text encryption techniques focus on alphabetic substitution or transposition based on lexical level (Franceschini and Mukherjee, 1996; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim"
W18-4203,P98-1116,1,0.170803,"96; Venkateswaran and Sundaram, 2010), synonyms (Chang and Clark, 2014) or image-adaptive public watermarking (Sun et al., 2008). Cipher systems can also be potentially developed and mutated to encode messages, including compare sophisticated ciphers such as historical ones (Knight et al., 2011) and simple mutations such as Leet10 and Martian script11 . Further strategies need to be developed to make them easy and fun for target human comprehension and widespread adoption, and difficult for automatic decoding. 3.2 Natural Language Generation for Encoding Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998) lay the foundation for statistical natural language generation, and they recently apply these techniques to the generation of creative language: (1) Portmanteau neologism creation. They fuse existing English words to create novel ones (Deri and Knight, 2015). The aim is to create an amusing new form that is understandable by a reader, e.g., frenemy for an entity that is both friend and enemy. Doing this well requires fusion at the phonetic level followed by an appropriate choice of spelling. Machines cannot yet process such created neologisms. This portmanteau generation approach (Deri and Kn"
W18-4203,N07-1025,0,0.117845,"Missing"
W18-4203,N15-1119,1,0.878102,"Missing"
W18-4203,D15-1221,0,0.0296214,"ns a random 60-bit password to a user. Because the user cannot remember a random sequence of 0’s and 1’s, the machine converts the bit sequence into a more memorable iambic tetrameter couplet (e.g., The legendary Japanese // subsidiaries overseas). The mapping between bit sequences and poems is reversible, so the security of the randomly-assigned password is maintained. Brennan et al. (2012) propose three methods to create adversarial passages: obfuscation, imitation, and translation. They find manual circumvention methods work very well, while automated translation methods are not effective. Potash et al. (2015) develop a GhostWriter system that can take a given artist’s rap lyrics and generate similar yet unique lyrics. Other recent work detects word obfuscation in adversarial communication (Roussinov et al., 2007; Fong et al., 2008; Jabbari et al., 2008; Deshmukh et al., 2014; Agarwal and Sureka, 2015) using existing commonsense KBs such as ConceptNet (Agarwal and Sureka, 2015). 3.3 Entity Encoding and Decoding Huang et al. (2013), Zhang et al. (2014) and Zhang et al. (2015) study a problem of encoding and decoding entity morph, which is a special case of coded name alias to hide the original entit"
W18-4203,P14-2046,1,0.839502,"11 https://en.wikipedia.org/wiki/Leet https://en.wikipedia.org/wiki/Martian_language 28 in both English and Chinese. We observe the words embedded are usually semantically and phonetically compatible, such as “frenemy (friend + enemy)”. They are also terse, representative, expressive, interesting and easy to remember. For example, the following short phrases are created to refer to good men and bad men respectively: “暖男 (warm + man)” and “渣男 (dirt + man)”. (2) Dynamic phrasebooks. Tourists frequently carry phonetic phrasebooks that allow them to say things in a language they do not know. In (Shi et al., 2014), we developed a system that accepts text entered by a user (e.g., Chinese), translates the text (e.g., into English), then converts the translation into a phonetic spelling in the user’s own orthography (e.g., Chinese). This system let users say anything they want, even if it is not in any phrasebook, using their own voice. For example, if a Chinese visitor to the United States wants to say 早上好 (meaning Good morning), they enter this phrase, and the system tells them to instead say 古德莫宁 (pronounced gu-de-mo-ning). This user is not required to know any English, but can still be understood by m"
W18-4203,W13-0906,0,0.0390039,"Missing"
W18-4203,P95-1026,0,0.202486,"Missing"
W18-4203,P14-2115,1,0.664896,"d decoding objectives. This opens an unexplored area of coded language processing. 1 2 https://freedomhouse.org/report/freedom-net/freedom-net-2016 https://en.wikipedia.org/wiki/Internet_censorship_in_China 23 Proceedings of the First Workshop on Natural Language Processing for Internet Freedom, pages 23–33 Santa Fe, New Mexico, USA, August 20, 2018. 2 Human Encoding 2.1 Categories of Coded Language Netizens create and use obfuscated language for a variety of purposes. Discussing sensitive information and evading censorship. Code words widely exist in Chinese social media (Huang et al., 2013; Zhang et al., 2014; Zhang et al., 2015). Bamman et al. (2012) automatically discover politically sensitive terms from Chinese tweets based on message deletion analysis. When Chinese netizens talk about the former politician 周永康 (Zhou Yongkang), they use a coded word 康师傅 (Master Kang), a brand of instant noodles whose Chinese spellings share one character 康 (kang). The Enron emails3 also include many code words, such as dinosaur referring to an illegal stock. Masking illegal activity. In the dark web for human trafficking, arms dealing, and drug dealing, research chemical or RC is euphemistically used to discuss"
W18-4203,P15-1057,1,0.737244,"s. This opens an unexplored area of coded language processing. 1 2 https://freedomhouse.org/report/freedom-net/freedom-net-2016 https://en.wikipedia.org/wiki/Internet_censorship_in_China 23 Proceedings of the First Workshop on Natural Language Processing for Internet Freedom, pages 23–33 Santa Fe, New Mexico, USA, August 20, 2018. 2 Human Encoding 2.1 Categories of Coded Language Netizens create and use obfuscated language for a variety of purposes. Discussing sensitive information and evading censorship. Code words widely exist in Chinese social media (Huang et al., 2013; Zhang et al., 2014; Zhang et al., 2015). Bamman et al. (2012) automatically discover politically sensitive terms from Chinese tweets based on message deletion analysis. When Chinese netizens talk about the former politician 周永康 (Zhou Yongkang), they use a coded word 康师傅 (Master Kang), a brand of instant noodles whose Chinese spellings share one character 康 (kang). The Enron emails3 also include many code words, such as dinosaur referring to an illegal stock. Masking illegal activity. In the dark web for human trafficking, arms dealing, and drug dealing, research chemical or RC is euphemistically used to discuss psychoactive chemica"
W18-6502,D10-1049,0,0.138916,"Missing"
W18-6502,P15-1034,0,0.0613092,"Missing"
W18-6502,W13-2322,1,0.842586,"Missing"
W18-6502,N16-1087,0,0.0704839,"Missing"
W18-6502,D16-1006,0,0.0363935,"Missing"
W18-6502,P17-1017,0,0.205356,"te content into slots (Kukich, 1983; Cawsey et al., 1997; Angeli et al., 2010; Duma and Klein, 2013; Konstas and Lapata, 2013a; Flanigan et al., 2016a). These methods can generate high-quality descriptions but heavily rely on information redundancy to create templates. The second category is to directly generate a sequence of words using language model (Belz, 2008; Chen and Mooney, 2008; Liang et al., 2009; Angeli et al., 2010; Konstas and Lapata, 2012a,b, 2013a,b; Mahapatra et al., 2016) or deep neural networks (Sutskever et al., 2011; Wen et al., 2015; Kiddon et al., 2016; Mei et al., 2016; Gardent et al., 2017b; Wiseman et al., 2017; Wang et al., 2018; Song et al., 2018). Several studies (Lebret et al., 2016; Chisholm et al., 2017; Kaffee et al., 2018a,b; Liu et al., 2018; Sha et al., 2018) generate a person’s biography from an input structure, which are closely related to our task. However, instead of modeling the input structure as a sequence of facts and generating one sentence only, we introduce a table position self-attention, inspired from structure attention (Lin et al., 2017; Kim et al., 2017; Vaswani et al., 2017; Shen et al., 2018a,b), to capture the dependencies among facts and generate"
W18-6502,E17-1060,0,0.0844058,"Missing"
W18-6502,P16-1154,0,0.0527383,"sition-aware representation of each slot type and value, and update their context vectors L∗t and L∗v in Equation 1 as: L∗s = L∗v = 2.3 Xn i=1 Xn i=1 where P (y t ) is the prediction probability of the ground truth token y. λ is a hyperparameter. 3 αit s∗i 3.1 αit v∗i Traditional sequence-to-sequence models predict a target sequence by only selecting words from a vocabulary with a fixed size. However, in our task, we regard the slot value as a single information unit. Therefore, there is a certain amount of outof-vocabulary (OOV) words during the test phase. Inspired by the pointer-generator (Gu et al., 2016; See et al., 2017), which is designed to automatically locate particular source words and directly copy them into the target sequence, we design a structure-aware generator as follows. We first obtain a source attention distribution of all unique input slot values. Since one particular slot value may occur in the structure input for many times, we aggregate the attention weights for each unique slot value vj from αt and obtain its agj by gregated source attention distribution Psource X Data Using person and animal entities as case studies, we create a new dataset based on Wikipedia dump (2018"
W18-6502,P16-1014,0,0.0533797,"Missing"
W18-6502,W14-3348,0,0.158359,"Missing"
W18-6502,N18-2101,0,0.0901125,"Missing"
W18-6502,W13-0108,0,0.0911938,"Missing"
W18-6502,D11-1142,0,0.0918571,"Missing"
W18-6502,D16-1032,0,0.0374598,"Missing"
W18-6502,P15-1002,0,0.0745402,"Missing"
W18-6502,P12-1039,0,0.124605,"Missing"
W18-6502,P17-2100,0,0.0574529,"Missing"
W18-6502,N12-1093,0,0.0734882,"Missing"
W18-6502,W16-6624,0,0.0433841,"Missing"
W18-6502,D13-1157,0,0.0549744,"Missing"
W18-6502,N16-1086,0,0.0808646,"Missing"
W18-6502,P83-1022,0,0.671208,"Missing"
W18-6502,D16-1128,0,0.20676,"Missing"
W18-6502,D12-1094,0,0.0232392,"Missing"
W18-6502,P09-1011,0,0.313582,"Missing"
W18-6502,P02-1040,0,0.101095,"Missing"
W18-6502,W16-6603,1,0.886983,"Missing"
W18-6502,W04-1013,0,0.109953,"Missing"
W18-6502,P17-1099,0,0.67687,"equences and applies a sequence to sequence (seq2seq) framework (Cho et al., 2014) for generation. However, the task of describing structured knowledge is fundamentally different from creative writing, because we need to cover the knowledge elements contained in the input KB, and the goal of generation is mainly to clearly describe the semantic connections among these knowledge elements in an accurate and coherent way. The seq2seq model fails to capture such connections and tends to generate wrong information (e.g., Thailand in Table 2). To address this challenge, we choose a pointer network (See et al., 2017) to copy slot values directly from the 11 resentations of these fields as li = [si , vi , r1 , rˆ1 ], and obtain L = [l1 , l2 , ..., ln ]. We attempted to apply the average of L as the representation for the input KB. However, such flat representation vectors fail to capture the structured contextual information in the entire KB. Therefore, we apply a bi-directional Gated Recurrent Unit (GRU) encoder (Cho et al., 2014) on L to produce the encoder hidden states H = [h1 , h2 , ..., hn ], where hi is a hidden state for li . Decoder with Slot-aware Attention The decoder is a forward GRU network wi"
W18-6502,Q18-1005,0,0.0276597,"7) (3,6) (4,5) (5,4) (5,4) (5,4) Slot Value Table Position Attention pgen Table Position Vocabulary Distribution Final Distribution Bidirectional GRU &lt;SOS&gt; Zsolt Laczkó ( born Figure 1: KB-to-Language Generation Model Overview 2.2 a sequence of row index embeddings R = 0 0 0 [r1 , r2 , ..., rn ] with random initialization, where 0 ri = [ri ; ˆri ]. We model the inter-dependencies among slots as a latent structure, where for each position i we assume it has a latent in-link and an out-link to denote where it is linked to or from. This assumption is similar to the structure attention applied in Liu and Lapata (2018), which assumes each word within a sentence can be a parent node or a child node in a latent tree structure. For each pair of slots i and j, we compute the attention score fij as follows: Table Position Self-attention Although the sequence-to-sequence attention model takes into account the information of input triples, it still encodes the structured knowledge as sequential facts while ignoring the correlations between facts. In our task, multiple interdependent slots should be described within one sentence. For example, in Table 1, the sport team Israel women’s national football team should b"
W18-6502,D18-1435,1,0.872665,"Missing"
W18-6502,I17-1086,1,0.896453,"Missing"
W18-6502,P18-1150,0,0.0509342,"Missing"
W18-6502,P18-2042,1,0.903298,"Missing"
W18-6502,D15-1199,0,0.069351,"Missing"
W18-6502,D18-1433,1,0.89042,"Missing"
W18-6502,P10-1013,0,0.052207,"Missing"
W18-6502,N13-1107,0,0.0347677,"Missing"
W19-2804,W16-2711,0,0.0264167,"Missing"
W19-2804,P18-4001,1,0.883374,"Missing"
W19-2804,ploch-etal-2012-gerned,0,0.0358137,"Missing"
Y09-1024,W09-1116,1,0.851404,"Missing"
Y09-1024,W99-0611,0,0.0356644,"will further boost the performance because the difficult cases tackled by these two methods are complementary. 7 Related Work Our method exhibits a fundamental advantage over supervised learning algorithm (including Boschee et al., 2005; Ji and Grishman, 2006; Zitouni and Florian, 2008) as it does not require costly hand-labeled training data. It thrives on web-scale Google n-gram data and discovers semantic knowledge corresponding to the task of mention detection. The use of gender information stems from a lot of prior work on pronoun resolution. Most of these methods (e.g. Ge et al., 1998; Cardie and Wagstaff, 1999) encoded the gender information as hard constraints. Hale and Charniak (1998) obtained gender statistics by using an anaphora algorithm on a large corpus. Bergsma et al. (2005, 2009a) mined gender information from the web and parsed corpora and incorporated gender probabilities as additional features in supervised learning. To the best of our knowledge, this is the first work on exploiting gender information for mention detection and in an unsupervised learning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguation (Bergsma et al., 2009b). L"
Y09-1024,P03-1001,0,0.0141265,"additional features in supervised learning. To the best of our knowledge, this is the first work on exploiting gender information for mention detection and in an unsupervised learning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguation (Bergsma et al., 2009b). Limited prior work has used manually constructed knowledge resources such as WordNet for Animacy Discovery (Evans and Orasan, 2000). Our offline strategy for acquiring gender and animacy information for online mention detection is similar to that for question answering described in Fleischman et al. (2003). And our approach of using pronoun context to improve mention detection is similar to the idea of refining name tagging based on coreference feedback in (Ji et al., 2005). 8 Conclusion Using mention detection as a case study, we have demonstrated that unsupervised learning methods can achieve comparable performance for some particular tasks if we discover semantic knowledge corresponding to each task. Our method harnesses the probabilistic lexical properties such as gender and animacy discovered from web-scale n-grams, and therefore can identify more rare mentions than the traditional supervi"
Y09-1024,W98-1119,0,0.0248626,"covered knowledge will further boost the performance because the difficult cases tackled by these two methods are complementary. 7 Related Work Our method exhibits a fundamental advantage over supervised learning algorithm (including Boschee et al., 2005; Ji and Grishman, 2006; Zitouni and Florian, 2008) as it does not require costly hand-labeled training data. It thrives on web-scale Google n-gram data and discovers semantic knowledge corresponding to the task of mention detection. The use of gender information stems from a lot of prior work on pronoun resolution. Most of these methods (e.g. Ge et al., 1998; Cardie and Wagstaff, 1999) encoded the gender information as hard constraints. Hale and Charniak (1998) obtained gender statistics by using an anaphora algorithm on a large corpus. Bergsma et al. (2005, 2009a) mined gender information from the web and parsed corpora and incorporated gender probabilities as additional features in supervised learning. To the best of our knowledge, this is the first work on exploiting gender information for mention detection and in an unsupervised learning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguatio"
Y09-1024,W06-0206,1,0.704209,"y annotated corpora and gazetteers. Keywords: Knowledge Discovery, Mention Detection, N-Grams, Gender, Animacy 1 Introduction The task of detecting entity mentions (references to entities) is very important to the downstream processing of information extraction such as coreference resolution and event extraction. Entity mentions can be divided into name mentions (e.g. “John Smith”), nominal mentions (e.g. “president”) and pronouns (e.g. “he”, “she”). Typical mention detection systems are based on supervised learning (Boschee et al., 2005; Zitouni and Florian, 2008) or semisupervised learning (Ji and Grishman, 2006). Achieving really high performance for mention detection requires deep semantic knowledge and large costly hand-labeled data. Many systems also exploited lexical gazetteers such as census data with gender information. However, such knowledge is relatively static (it is not updated during the extraction process), expensive to construct, and doesn’t include any probabilistic information. Mention detection is by definition a semantic task: for example, a phrase is a person mention if it refers to a real-world person entity. We should thus expect a successful mention detection system to exploit w"
Y09-1024,H05-1003,1,0.838874,"ning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguation (Bergsma et al., 2009b). Limited prior work has used manually constructed knowledge resources such as WordNet for Animacy Discovery (Evans and Orasan, 2000). Our offline strategy for acquiring gender and animacy information for online mention detection is similar to that for question answering described in Fleischman et al. (2003). And our approach of using pronoun context to improve mention detection is similar to the idea of refining name tagging based on coreference feedback in (Ji et al., 2005). 8 Conclusion Using mention detection as a case study, we have demonstrated that unsupervised learning methods can achieve comparable performance for some particular tasks if we discover semantic knowledge corresponding to each task. Our method harnesses the probabilistic lexical properties such as gender and animacy discovered from web-scale n-grams, and therefore can identify more rare mentions than the traditional supervised learning methods based on limited and static semantic resources. Also as an unsupervised learning approach it performs surprisingly well especially on recall. We have"
Y09-1024,D08-1063,0,0.110374,"e-art supervised learning methods which require manually annotated corpora and gazetteers. Keywords: Knowledge Discovery, Mention Detection, N-Grams, Gender, Animacy 1 Introduction The task of detecting entity mentions (references to entities) is very important to the downstream processing of information extraction such as coreference resolution and event extraction. Entity mentions can be divided into name mentions (e.g. “John Smith”), nominal mentions (e.g. “president”) and pronouns (e.g. “he”, “she”). Typical mention detection systems are based on supervised learning (Boschee et al., 2005; Zitouni and Florian, 2008) or semisupervised learning (Ji and Grishman, 2006). Achieving really high performance for mention detection requires deep semantic knowledge and large costly hand-labeled data. Many systems also exploited lexical gazetteers such as census data with gender information. However, such knowledge is relatively static (it is not updated during the extraction process), expensive to construct, and doesn’t include any probabilistic information. Mention detection is by definition a semantic task: for example, a phrase is a person mention if it refers to a real-world person entity. We should thus expect"
Y10-1027,D08-1021,0,0.0363569,"Missing"
Y10-1027,P08-2017,0,0.025065,"can annotate event arguments efficiently and effectively by correcting SRL output. In order to evaluate the robustness of our approach, we experimented with both traditional news domain and a new domain of carbon sequestration. We expect that our method will enable developing annotated event corpora rapidly, and thus lead IE techniques to higher performance and broader applicability. In the future we will focus on filtering the remaining noise in trigger clustering by adding more distributional constraints. In addition, we plan to use more advanced cost-conscious active learning methods (e.g. Haertel et al., 2008) to further speed up our annotation procedures, because our task fits naturally into a hierarchical framework with multiple annotation sub-tasks. We are also interested in applying domain adaptation techniques on SRL in order to boost event annotation quality for non-news domains. Acknowledgement This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053, the U.S. NSF CAREER Award under Grant IIS-0953149, Google, Inc., DARPA GALE Program, CUNY Research Enhancement Program, PSC-CUNY Research Program, Faculty Publication Program and GRTI Prog"
Y10-1027,N06-2015,0,0.0176551,"DP as a 235 236 Regular Papers vector whose dimensions correspond to the words wi in the corpus vocabulary, and whose cell values are the log-likelihood ratio of w (or X) and wi, as in McDonald (2000). Then, each profile is biased toward each of the word senses s of w (or sense r of X), one sense at a time, and each of the biased profiles of w is compared with each of the biased profiles of X, using cosine of these vectors. The final similarity score of w and X is the score of the highest scoring sense-biased DPs. Words senses were determined using the technique and data in Mahammad and Hirst (2006). For each w we use the 50 top ranking candidates X above similarity score threshold 0.10, after filtering (shallowly approximated) textually entailing candidates: if all words of w also appear in same order in X, we filter X out. For example, for ``decided&apos;&apos;, the candidate ``decided quickly&apos;&apos; is filtered out. The semantic similarity between triggers u and v can be estimated by calculating the above similarity (vector distance) between their DPs. Then, u and v are grouped into the same cluster if the distance is shorter than a threshold. For example, an “abandon” cluster with similarity thresh"
Y10-1027,ji-etal-2010-annotating,1,0.813221,"new view of IE by considering it as a more fine-grained version of semantic role labeling (SRL). For each novel event type, we identify relevant and salient sentences involving new event triggers, and then correct errors based on uncertainty estimation, and finally map semantic roles into event argument roles based on semantic frame descriptions. We will demonstrate that our approach can make novel event discovery and annotation much more feasible (Section 5) by testing on three corpora from different domains: ACE, OntoNotes (Hovy et al., 2006) and a corpus of carbon sequestration literature (Ji et al., 2010). 2 2.1 Approach Overview Event Definition An event is a specific occurrence involving participants, and can be frequently described as a change of state (LDC, 2005). An event includes the following elements: Event type: a particular event class, such as “Life/Be-Born”, “Business-Transaction”, etc. Event trigger: the main word which most clearly expresses an event occurrence Event arguments: the mentions that are involved in an event (participants) For example, the sentence “the US-led coalition troops are reportedly thrusting into the second Iraqi city of Basra.” includes a “Movement_Transpor"
Y10-1027,W09-1704,1,0.922588,"ic similarity between triggers u and v can be estimated by calculating the above similarity (vector distance) between their DPs. Then, u and v are grouped into the same cluster if the distance is shorter than a threshold. For example, an “abandon” cluster with similarity threshold of 0.1 includes: {abandon, retire from, blight, quit, pull out, abducted, disuse, call off, end, withdraw, …} 3.3 Crosslingually derived Trigger Clustering In addition to the first clustering approach, we exploit a cross-lingual clustering algorithm based on sentence-aligned bilingual parallel texts, a.k.a. bitexts (Ji, 2009) to discover additional event trigger clusters. The general idea is that if two words w and u on the bitext’s source side are aligned with the same word on the target side with high confidence, then they should be grouped into the same cluster. In this paper we use Chinese-English bitexts from DARPA GALE program1. For each Chinese trigger, we search its aligned English words in order to construct a cluster including possible English trigger words. Then we acquire Chinese triggers from the other direction and continue the iterations. The word alignment was obtained by running Giza++ (Och and Ne"
Y10-1027,P09-1116,0,0.0738456,"Missing"
Y10-1027,D09-1040,1,0.821096,"09), together with the 1233 English triggers and 852 Chinese triggers in ACE05 training corpora as our ‘pivot’ event triggers. In order to minimize the impact of word alignment errors and some other noise, we conduct lemmatization based on WordNet (Fellbaum, 1998), and filter out stop-words (Fox, 1992), numbers and punctuations, time expressions and other function words that are not helpful in trigger clustering. 3.2 Monolingually derived Trigger Clustering The first trigger clustering approach is based on distributional semantic similarity measures over a monolingual, source-language corpus (Marton et al., 2009; Marton, 2010). We constructed a monolingual English corpus of about 500 million words, consisting of all English Gigaword documents from 2004 and 2008 (LDC2009T13). With this corpus, we used essentially the earlier technique described in Marton (2010). Its outline is as follows: For each word (or word sequence) of interest w, collect all contexts L w R in which w appear in our monolingual corpus, and then collect paraphrase candidates: all word sequences X up to 6 token long appearing in the same L X R contexts. Then, rank the candidates X by their semantic similarity to w, as estimated by a"
Y10-1027,W06-1605,0,0.0210812,"Missing"
Y10-1027,J03-1002,0,0.00247528,"(Ji, 2009) to discover additional event trigger clusters. The general idea is that if two words w and u on the bitext’s source side are aligned with the same word on the target side with high confidence, then they should be grouped into the same cluster. In this paper we use Chinese-English bitexts from DARPA GALE program1. For each Chinese trigger, we search its aligned English words in order to construct a cluster including possible English trigger words. Then we acquire Chinese triggers from the other direction and continue the iterations. The word alignment was obtained by running Giza++ (Och and Ney, 2003). From each cluster we filter out those trigger pairs with frequency (in bitexts) less than some threshold (we have tried frequency threshold of 1, 2, 3 and 4 separately). For example, “announce” is not an ACE-type event, but we can get its cluster as follows: {宣布, 通告，断言，宣告，声明，预报，传达，阐明，提出，显露，显示，陈述}  {announce, declare, herald, proclaim, set forth, set out, state, unveil, convey, affirm, assert} 3.4 Event Cluster Ranking Let E denote the unlabeled corpus from which we want to discover novel event types and annotate event arguments. We apply a high-performance entity extraction system (Grishman"
Y10-1027,J05-1004,0,0.0122398,"Correction & Role Mapping Novel Event Annotations Figure 1. Novel Event Discovery and Annotation Pipeline 3 Novel Event Type Discovery After pre-processing the text resources (Section 3.1), we continue with automatically detecting candidate event types based on trigger clustering (Sections 3.2 and 3.3), and then detecting novel event types based on cluster ranking (Section 3.4). 3.1 Pre-processing We apply two open-domain state-of-the-art automatic trigger clustering methods to discover event trigger clusters. We consider a collection of 3065 English verbs and 4865 Chinese verbs in PropBank (Palmer et al., 2005, Xue and Palmer, 2009), together with the 1233 English triggers and 852 Chinese triggers in ACE05 training corpora as our ‘pivot’ event triggers. In order to minimize the impact of word alignment errors and some other noise, we conduct lemmatization based on WordNet (Fellbaum, 1998), and filter out stop-words (Fox, 1992), numbers and punctuations, time expressions and other function words that are not helpful in trigger clustering. 3.2 Monolingually derived Trigger Clustering The first trigger clustering approach is based on distributional semantic similarity measures over a monolingual, sour"
Y10-1027,N03-1024,0,0.0625279,"r permeability. In these and many other cases, SRL failed to classify such arguments mainly because of some domain-specific features such as argument heads (e.g. “rates” appear very rarely as an “ARG0” in news domain). We expect to get further improvement after we incorporate some domainspecific knowledge such as high-frequency terminology lexicons into the SRL system. 6 Related Work A number of previous studies have described extensive techniques to cluster words or word sequences from large unlabeled corpora (e.g. , Lin and Wu, 2009), monolingual parallel corpora (e.g. Lin and Pantel, 2001; Pang et al., 2003) and bilingual parallel corpora (e.g. Callison-Burch et al., 2008; Ji, 2009). Stevenson and Joanis (2003) applied semi-supervised learning for verb class discovery. We chose the parallel corpora discovery method and hybrid distributional clustering method because our target trigger list is a relatively closed set. Parallel corpora are likely to yield higher quality due to the human linguistic knowledge implicit in sentence alignment, but it is limited in size and vocabulary. Monolingual corpora are not as limited, so can cover more out-of-vocabulary terms, and might equal or out-perform parall"
Y10-1027,J08-2006,0,0.0315091,") less than some threshold (we have tried frequency threshold of 1, 2, 3 and 4 separately). For example, “announce” is not an ACE-type event, but we can get its cluster as follows: {宣布, 通告，断言，宣告，声明，预报，传达，阐明，提出，显露，显示，陈述}  {announce, declare, herald, proclaim, set forth, set out, state, unveil, convey, affirm, assert} 3.4 Event Cluster Ranking Let E denote the unlabeled corpus from which we want to discover novel event types and annotate event arguments. We apply a high-performance entity extraction system (Grishman et al., 2005) and a state-of-the-art SRL system (including syntactic parsing) (Pradhan et al., 2008) to pre-process E. For each candidate trigger cluster C, we gather it together with the entities in E as a query, and then use information retrieval methods to obtain related sentences for this query. For any word/phrase v  C, if an entity e is identified by SRL as an argument of v in a sentence s, then s is the related sentence. If v is not tagged as a trigger for any existing event types, we consider s as a novel-event related sentence. Let nsk ( v , e ) be the kth novel-event related sentence, and si ( v , e ) be the ith related sentence for v and e; compute the salience of C as follows. s"
Y10-1027,W03-0410,0,0.0333845,"use of some domain-specific features such as argument heads (e.g. “rates” appear very rarely as an “ARG0” in news domain). We expect to get further improvement after we incorporate some domainspecific knowledge such as high-frequency terminology lexicons into the SRL system. 6 Related Work A number of previous studies have described extensive techniques to cluster words or word sequences from large unlabeled corpora (e.g. , Lin and Wu, 2009), monolingual parallel corpora (e.g. Lin and Pantel, 2001; Pang et al., 2003) and bilingual parallel corpora (e.g. Callison-Burch et al., 2008; Ji, 2009). Stevenson and Joanis (2003) applied semi-supervised learning for verb class discovery. We chose the parallel corpora discovery method and hybrid distributional clustering method because our target trigger list is a relatively closed set. Parallel corpora are likely to yield higher quality due to the human linguistic knowledge implicit in sentence alignment, but it is limited in size and vocabulary. Monolingual corpora are not as limited, so can cover more out-of-vocabulary terms, and might equal or out-perform parallel corpora methods if given a large enough monolingual corpus. Both methods do not require any supervisio"
Y10-1027,C00-2136,0,\N,Missing
Y10-1027,N07-1070,0,\N,Missing
Y10-1027,P06-2094,0,\N,Missing
Y12-1013,W11-0705,0,0.23689,"Negative 5691 2323 381 Table 2: Statistics of Data Sets 4 Linguistic-based Approach In this section, we present our baseline approach using only linguistic features. 4.1 Pre-processing We have applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum posts). • Replace URLs with “@URL”. • Replace @username with “@USERNAME”. • Replace negation words with “NOT” based on the list derived from the LIWCLexicon (Pennebaker et al., 2001). • Normalize slang words (e.g. “LOL” to “laugh out loud”) (Agarwal et al., 2011). • Spelling correction using WordNet (Fellbaum, 2005) (e.g. “cooooool” to “cool”) In addition, each document has been tokenized and annotated with Part-of-speech tags (Toutanova et al., 2003). 129 4.2 Target and Issue Detection After pre-processing, the first step is to detect documents which include popular targets and issues. A popular target is an entity that users frequently discuss, such as a product (e.g. “Iphone4”), a person (e.g. “Ron Paul”) or an organization (e.g. “Red Cross”). A popular issue is a related aspect associated with a target, such as “display function” or “economic issu"
Y12-1013,baccianella-etal-2010-sentiwordnet,0,0.0354325,"pectively. 4.3 Sentiment Detection We have developed a supervised learning model based on Support Vector Machines to classify sentiment labels for each document (a post, a tweet message or a movie review document), incorporating several features such as N-grams, POS, various lexicons, punctuation, capitalization (see Table 3). Feature N-grams Part-of-Speech Gazetteer Word Cluster Punctuation Capitalization Description All unique unigrams, bigrams and trigrams Part-Of-Speech tags generated by Stanford Parser (Toutanova et al., 2003) Lexical matching based on (Joshi et al., 2011), SentiWordNet (Baccianella et al., 2010), Subjectivity Lexicon (Wiebe et al., 2004), Inquirer (Stone et al., 1966), Taboada (Taboada and Grieve, 2004), UICLexicon (Hu and Liu, 2004), LIWCLexicon (Pennebaker et al., 2001) Use synset information provided by Wordnet to expand the entries of each gazettteer; Lexical matching based on the expanded gazetteers Whether the document includes any exclamation mark or question mark Unique words which include all capitalized letters Table 3: Linguistic Features Used in the Baseline System The classification results are normalized to probability based confidence values via a sigmoid kernel functi"
Y12-1013,W10-2914,0,0.0388359,"low confidence values. 6 Remaining Challenges Although the proposed approach based on social cognitive theories has significantly enhanced the we would need to identify agreement/disagreement relations among posts. 0.75 Baseline +Hypothesis 1 +Hypothesis 1+2 +Hypothesis 1+2+3 Accuracy 0.70 0.65 0.60 0.55 0.5 0.6 0.7 0.8 Threshold Figure 4: Impact of Parameters performance of sentiment analysis, some challenges remain. We analyze the major sources of the remaining errors as follows. Sarcasm Detection. For both tweets and forum posts, some remaining errors require accurate detection of sarcasm (Davidov et al., 2010; GonzalezIbanez et al., 2011). For example, “LOL..remember Obama chastising business’s for going to Vegas. Vegas would have cost a third of what these locations costs. But hey, no big deal... ” contains sarcasm, which leads our system to misclassify this post. Domain-specific Latent Sentiments. The same word or phrase might indicate completely different sentiments in various domains. For example, “big” usually indicates positive sentiment, but it indicates negative sentiment in the following sentence: “tell me how the big government, big bank backing, war mongering Obama differs from Bush?”."
Y12-1013,P11-2102,1,0.839681,"Missing"
Y12-1013,P11-1038,0,0.0255817,"problem. We also used a more traditional set for sentiment analysis — the movie review polarity data set shared by (Pang et al., 2002) — to highlight the challenges of more informal texts. Table 2 summarizes the statistics of data sets used for each genre. All experiments in this paper are based on three-fold cross-validation. Genre Review Tweet Forum Positive 5691 2323 381 Negative 5691 2323 381 Table 2: Statistics of Data Sets 4 Linguistic-based Approach In this section, we present our baseline approach using only linguistic features. 4.1 Pre-processing We have applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum posts). • Replace URLs with “@URL”. • Replace @username with “@USERNAME”. • Replace negation words with “NOT” based on the list derived from the LIWCLexicon (Pennebaker et al., 2001). • Normalize slang words (e.g. “LOL” to “laugh out loud”) (Agarwal et al., 2011). • Spelling correction using WordNet (Fellbaum, 2005) (e.g. “cooooool” to “cool”) In addition, each document has been tokenized and annotated with Part-of-speech tags (Toutanova et al., 2003). 129 4.2 Target and Issue Detect"
Y12-1013,D10-1121,0,0.0463535,"Missing"
Y12-1013,H05-1003,1,0.872191,"Missing"
Y12-1013,P11-1016,0,0.242892,"Missing"
Y12-1013,P11-4022,0,0.0663571,"d one another in an online discussion forum using a signed network representation of participant interaction. In contrast, we are interested in discovering the opinions of participants toward a public figure in light of their stance on various political issues. Sentiment Analysis can be categorized into targetindependent and target-dependent. The targetindependent work mainly focused on exploring various local linguistic features and incorporating them into supervised learning based systems (Pang and Lee, 2004; Zhao et al., 2008; Narayanan et al., 2009) or unsupervised learning based systems (Joshi et al., 2011). Recent target-dependent work has focused on automatically extracting sentiment expressions for a given target (Godbole et al., 2007; Chen et al., 2012), or incorporating target-dependent features into sentiment analysis (Liu et al., 2005; Jiang et al., 2011). In this paper we focus on the task of jointly extracting sentiment, target and issue in order to provide richer and more concrete evidence to describe and predict the attitudes of online users. This bares similarity to the idea of modeling aspect rating in product reviews (Titov and McDonald, 2008; Wang et al., 2011). When sentiment ana"
Y12-1013,D09-1019,0,0.0223555,"Missing"
Y12-1013,pak-paroubek-2010-twitter,0,0.45822,"Missing"
Y12-1013,P04-1035,0,0.0429906,"Missing"
Y12-1013,W02-1011,0,0.011769,"l!!! #GOPFAIL I also find it interesting that so many people ridicule Ron Paul's foreign policy yet the people that are directly affected by it, our troops, support Ron Paul more than any other GOP candidate combined and more than Obama. Obama screwed up by not fixing the economy first and leaving health care reform for a second term. Table 1: Sentiment Examples of Different Genres negative sentiments as opposed to neutral, therefore we only focus on the polarity classification problem. We also used a more traditional set for sentiment analysis — the movie review polarity data set shared by (Pang et al., 2002) — to highlight the challenges of more informal texts. Table 2 summarizes the statistics of data sets used for each genre. All experiments in this paper are based on three-fold cross-validation. Genre Review Tweet Forum Positive 5691 2323 381 Negative 5691 2323 381 Table 2: Statistics of Data Sets 4 Linguistic-based Approach In this section, we present our baseline approach using only linguistic features. 4.1 Pre-processing We have applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum"
Y12-1013,W11-2207,0,0.17712,"Missing"
Y12-1013,P08-1036,0,0.0926009,"Missing"
Y12-1013,N03-1033,0,0.0293076,"ave applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum posts). • Replace URLs with “@URL”. • Replace @username with “@USERNAME”. • Replace negation words with “NOT” based on the list derived from the LIWCLexicon (Pennebaker et al., 2001). • Normalize slang words (e.g. “LOL” to “laugh out loud”) (Agarwal et al., 2011). • Spelling correction using WordNet (Fellbaum, 2005) (e.g. “cooooool” to “cool”) In addition, each document has been tokenized and annotated with Part-of-speech tags (Toutanova et al., 2003). 129 4.2 Target and Issue Detection After pre-processing, the first step is to detect documents which include popular targets and issues. A popular target is an entity that users frequently discuss, such as a product (e.g. “Iphone4”), a person (e.g. “Ron Paul”) or an organization (e.g. “Red Cross”). A popular issue is a related aspect associated with a target, such as “display function” or “economic issue”. We have applied a state-of-the-art English entity extraction system (Li et al., 2012; Ji et al., 2005) that includes name tagging and coreference resolution to detect name variants from ea"
Y12-1013,J04-3002,0,0.0429006,"ped a supervised learning model based on Support Vector Machines to classify sentiment labels for each document (a post, a tweet message or a movie review document), incorporating several features such as N-grams, POS, various lexicons, punctuation, capitalization (see Table 3). Feature N-grams Part-of-Speech Gazetteer Word Cluster Punctuation Capitalization Description All unique unigrams, bigrams and trigrams Part-Of-Speech tags generated by Stanford Parser (Toutanova et al., 2003) Lexical matching based on (Joshi et al., 2011), SentiWordNet (Baccianella et al., 2010), Subjectivity Lexicon (Wiebe et al., 2004), Inquirer (Stone et al., 1966), Taboada (Taboada and Grieve, 2004), UICLexicon (Hu and Liu, 2004), LIWCLexicon (Pennebaker et al., 2001) Use synset information provided by Wordnet to expand the entries of each gazettteer; Lexical matching based on the expanded gazetteers Whether the document includes any exclamation mark or question mark Unique words which include all capitalized letters Table 3: Linguistic Features Used in the Baseline System The classification results are normalized to probability based confidence values via a sigmoid kernel function (Wu et al., 2004). 4.4 Results and Analy"
Y12-1013,W03-1017,0,0.220023,"Missing"
Y12-1013,D08-1013,0,0.0324631,"Missing"
