2020.aacl-main.42,D16-1156,0,0.0260597,"s hypothesis is termed ‘noun bias’ in language acquisition (Gentner, 1982, 2006; Waxman et al., 2013), through which the early acquisition of nouns is attributed to nouns referring to observable objects. However, the models in Shi et al. (2019) also rely on language-specific branching bias to outperform other text-based models, and images are encoded by pretrained object classifiers trained with large datasets, with no ablation to show the benefit of visual information for unsupervised parsing. Visual information has also been used for joint training of prepositional phrase attachment models (Christie et al., 2016) suggesting that visual information may contain semantic information to help disambiguate prepositional phrase attachment. 3 Grounded Grammar Induction Model The full grounded grammar induction model used in these experiments, ImagePCFG, consists of two parts: a word-based PCFG induction model and a vision model, as shown in Figure 2. The two parts have their own objective functions. The PCFG induction model, called NoImagePCFG when trained by itself, can be trained by maximizing the marginal probability P(σ) of sentences σ. This part functions similarly to previously proposed PCFG induction m"
2020.aacl-main.42,D19-1161,0,0.0171426,"able to use visual information for proposing noun phrases, gathering useful information from images for unknown words, and achieving better performance at prepositional phrase attachment prediction.1 1 (a) friend as companion (b) friend as condiment Figure 1: Examples of disambiguating information provided by images for the prepositional phrase attachment of the sentence Mary eats spaghetti with a friend (Gokcen et al., 2018). Introduction Recent grammar induction models are able to produce accurate grammars and labeled parses with raw text only (Jin et al., 2018b, 2019; Kim et al., 2019b,a; Drozdov et al., 2019), providing evidence against the poverty of the stimulus argument (Chomsky, 1965), and showing that many linguistic distinctions like lexical and phrasal categories can be directly induced from raw text statistics. However, as computational-level models of human syntax acquisition, they lack semantic, pragmatic and environmental information which human learners seem to use (Gleitman, 1990; Pinker and MacWhinney, 1987; Tomasello, 2003). This paper proposes novel grounded neuralnetwork-based models of grammar induction which take into account information extracted from images in learning. Perfor"
2020.aacl-main.42,W16-3210,0,0.0716253,"Missing"
2020.aacl-main.42,P19-1031,0,0.0758317,"er distribution of high-frequency word types: positive effect One potential advantage of using visual information in induction is to ground nouns and noun phrases. For example, if images like in Figure 1 are consistently presented to models with sentences describing spaghetti, the models may learn the categorize words and phrases which could be linked with objects in images as nominal units and then bootstrap other lexical categories. However, in the test languages above, a narrow set of very high fre6 The multilingual parsing accuracy for all languages used in this work has been validated in Fried et al. (2019) and verified in Shi et al. (2019). 7 https://translate.google.com/. 8 PCFG induction models where a grammar is induced generally perform better in parsing evaluation than sequence models where only syntactic structures are induced (Kim et al., 2019a; Jin et al., 2019). 400 MSCOCO Models Multi30k English** English** German** French** F1 RH F1 RH F1 RH F1 RH Left-branching Right-branching PRPN ON-LSTM VG-NSL+H VG-NSL+H+F 23.3 21.4 52.5±2.6 45.5±3.3 53.3±0.2 54.4±0.4 - 22.6 11.3 30.8±17.9 38.7±12.7 38.7±0.2 - - 34.7 12.1 31.5±8.9 34.9±12.3 38.3±0.2 - - 19.0 11.0 27.5±7.0 27.7±5.6 38.1±0.6 - - No"
2020.aacl-main.42,N18-5011,0,0.0502331,"Missing"
2020.aacl-main.42,P14-2118,0,0.0264278,"Linguistics 2 Related work Existing unsupervised PCFG inducers exploit naturally-occurring cognitive and developmental constraints, such as punctuation as a proxy to prosody (Seginer, 2007), human memory constraints (Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018b), and morphology (Jin and Schuler, 2019), to regulate the posterior of grammars which are known to be extremely multimodal (Johnson et al., 2007). Models in Shi et al. (2019) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure (Hill et al., 2014; Hill and Korhonen, 2014). Additionally, visual information is observed to provide grounding for words describing concrete objects, helping to identify and categorize such words. This hypothesis is termed ‘noun bias’ in language acquisition (Gentner, 1982, 2006; Waxman et al., 2013), through which the early acquisition of nouns is attributed to nouns referring to observable objects. However, the models in Shi et al. (2019) also rely on language-specific branching bias to outperform other text-based models, and images are encoded by pretrained object classifiers trained with large datasets, with no ablation to show the"
2020.aacl-main.42,Q14-1023,0,0.0270409,"for Computational Linguistics 2 Related work Existing unsupervised PCFG inducers exploit naturally-occurring cognitive and developmental constraints, such as punctuation as a proxy to prosody (Seginer, 2007), human memory constraints (Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018b), and morphology (Jin and Schuler, 2019), to regulate the posterior of grammars which are known to be extremely multimodal (Johnson et al., 2007). Models in Shi et al. (2019) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure (Hill et al., 2014; Hill and Korhonen, 2014). Additionally, visual information is observed to provide grounding for words describing concrete objects, helping to identify and categorize such words. This hypothesis is termed ‘noun bias’ in language acquisition (Gentner, 1982, 2006; Waxman et al., 2013), through which the early acquisition of nouns is attributed to nouns referring to observable objects. However, the models in Shi et al. (2019) also rely on language-specific branching bias to outperform other text-based models, and images are encoded by pretrained object classifiers trained with large datasets, wi"
2020.aacl-main.42,D18-1292,1,0.800431,"information shows that the grounded models are able to use visual information for proposing noun phrases, gathering useful information from images for unknown words, and achieving better performance at prepositional phrase attachment prediction.1 1 (a) friend as companion (b) friend as condiment Figure 1: Examples of disambiguating information provided by images for the prepositional phrase attachment of the sentence Mary eats spaghetti with a friend (Gokcen et al., 2018). Introduction Recent grammar induction models are able to produce accurate grammars and labeled parses with raw text only (Jin et al., 2018b, 2019; Kim et al., 2019b,a; Drozdov et al., 2019), providing evidence against the poverty of the stimulus argument (Chomsky, 1965), and showing that many linguistic distinctions like lexical and phrasal categories can be directly induced from raw text statistics. However, as computational-level models of human syntax acquisition, they lack semantic, pragmatic and environmental information which human learners seem to use (Gleitman, 1990; Pinker and MacWhinney, 1987; Tomasello, 2003). This paper proposes novel grounded neuralnetwork-based models of grammar induction which take into account in"
2020.aacl-main.42,Q18-1016,1,0.629069,"information shows that the grounded models are able to use visual information for proposing noun phrases, gathering useful information from images for unknown words, and achieving better performance at prepositional phrase attachment prediction.1 1 (a) friend as companion (b) friend as condiment Figure 1: Examples of disambiguating information provided by images for the prepositional phrase attachment of the sentence Mary eats spaghetti with a friend (Gokcen et al., 2018). Introduction Recent grammar induction models are able to produce accurate grammars and labeled parses with raw text only (Jin et al., 2018b, 2019; Kim et al., 2019b,a; Drozdov et al., 2019), providing evidence against the poverty of the stimulus argument (Chomsky, 1965), and showing that many linguistic distinctions like lexical and phrasal categories can be directly induced from raw text statistics. However, as computational-level models of human syntax acquisition, they lack semantic, pragmatic and environmental information which human learners seem to use (Gleitman, 1990; Pinker and MacWhinney, 1987; Tomasello, 2003). This paper proposes novel grounded neuralnetwork-based models of grammar induction which take into account in"
2020.aacl-main.42,P19-1234,1,0.68363,"hetti, the models may learn the categorize words and phrases which could be linked with objects in images as nominal units and then bootstrap other lexical categories. However, in the test languages above, a narrow set of very high fre6 The multilingual parsing accuracy for all languages used in this work has been validated in Fried et al. (2019) and verified in Shi et al. (2019). 7 https://translate.google.com/. 8 PCFG induction models where a grammar is induced generally perform better in parsing evaluation than sequence models where only syntactic structures are induced (Kim et al., 2019a; Jin et al., 2019). 400 MSCOCO Models Multi30k English** English** German** French** F1 RH F1 RH F1 RH F1 RH Left-branching Right-branching PRPN ON-LSTM VG-NSL+H VG-NSL+H+F 23.3 21.4 52.5±2.6 45.5±3.3 53.3±0.2 54.4±0.4 - 22.6 11.3 30.8±17.9 38.7±12.7 38.7±0.2 - - 34.7 12.1 31.5±8.9 34.9±12.3 38.3±0.2 - - 19.0 11.0 27.5±7.0 27.7±5.6 38.1±0.6 - - NoImagePCFG ImagePrePCFG ImagePCFG 60.0±8.2 55.6±7.5 55.1±2.7 47.6±10.0 42.3±7.3 42.5±1.5 59.4±7.7 47.0±7.0 48.2±4.9 51.6±8.5 40.5±7.2 40.5±5.0 48.1±5.2 46.2±7.4 47.0±5.5 53.7±5.2 51.1±8.0 51.8±8.4 44.3±5.1 42.6±10.3 43.6±5.5 43.8±5.2 43.4±10.8 44.5±6.3 Table 1: Averages"
2020.aacl-main.42,P19-1235,1,0.822817,"ub.com/ajdagokcen/ madlyambiguous-repo 396 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 396–408 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related work Existing unsupervised PCFG inducers exploit naturally-occurring cognitive and developmental constraints, such as punctuation as a proxy to prosody (Seginer, 2007), human memory constraints (Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018b), and morphology (Jin and Schuler, 2019), to regulate the posterior of grammars which are known to be extremely multimodal (Johnson et al., 2007). Models in Shi et al. (2019) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure (Hill et al., 2014; Hill and Korhonen, 2014). Additionally, visual information is observed to provide grounding for words describing concrete objects, helping to identify and categorize such words. This hypothesis is termed ‘noun bias’ in language acquisition (Gentner, 1982, 2006; Waxman et al., 2013), through which the early acquisition"
2020.aacl-main.42,2020.iwpt-1.15,1,0.698601,"lts from models proposed in this paper — NoImagePCFG, ImagePrePCFG and ImagePCFG — are compared with published results from Shi et al. (2019), which include PRPN (Shen et al., 2018), ON-LSTM (Shen et al., 2019) as well as the grounded VG-NSL models which uses either head final bias (VG-NSL+H) or head final bias and Fasttext embeddings (VG-NSL+H+F) as inductive biases from external sources. All of these models only induce unlabeled structures and have been evaluated with unlabeled F1 scores. We additionally report the labeled evaluation score Recall-Homogeneity (Rosenberg and Hirschberg, 2007; Jin and Schuler, 2020) for better comparison between the proposed models. All evaluation is done on Viterbi parse trees of the test set from 5 different runs. Details about hyper-parameters and results on development data sets can be found in the appendix. However, importantly, the tuned hyperparameters for the grammar induction model are the same across the three proposed models, which facilitates direct comparisons among these models to determine the effect of visual information on induction. 6.1 Standard set: no replication of effect for visual information Both unlabeled and labeled evaluation results are shown"
2020.aacl-main.42,N07-1018,0,0.20694,"f the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 396–408 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related work Existing unsupervised PCFG inducers exploit naturally-occurring cognitive and developmental constraints, such as punctuation as a proxy to prosody (Seginer, 2007), human memory constraints (Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018b), and morphology (Jin and Schuler, 2019), to regulate the posterior of grammars which are known to be extremely multimodal (Johnson et al., 2007). Models in Shi et al. (2019) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure (Hill et al., 2014; Hill and Korhonen, 2014). Additionally, visual information is observed to provide grounding for words describing concrete objects, helping to identify and categorize such words. This hypothesis is termed ‘noun bias’ in language acquisition (Gentner, 1982, 2006; Waxman et al., 2013), through which the early acquisition of nouns is attributed to nouns referring to observable objects. However, the models in Shi et al. (2019"
2020.aacl-main.42,P19-1228,0,0.2553,"Missing"
2020.aacl-main.42,N19-1114,0,0.282587,"e grounded models are able to use visual information for proposing noun phrases, gathering useful information from images for unknown words, and achieving better performance at prepositional phrase attachment prediction.1 1 (a) friend as companion (b) friend as condiment Figure 1: Examples of disambiguating information provided by images for the prepositional phrase attachment of the sentence Mary eats spaghetti with a friend (Gokcen et al., 2018). Introduction Recent grammar induction models are able to produce accurate grammars and labeled parses with raw text only (Jin et al., 2018b, 2019; Kim et al., 2019b,a; Drozdov et al., 2019), providing evidence against the poverty of the stimulus argument (Chomsky, 1965), and showing that many linguistic distinctions like lexical and phrasal categories can be directly induced from raw text statistics. However, as computational-level models of human syntax acquisition, they lack semantic, pragmatic and environmental information which human learners seem to use (Gleitman, 1990; Pinker and MacWhinney, 1987; Tomasello, 2003). This paper proposes novel grounded neuralnetwork-based models of grammar induction which take into account information extracted from"
2020.aacl-main.42,P18-1249,0,0.0173005,"lihood, syntactic-visual loss and image reconstruction loss (Equation 14): 5 Details of these models can be found in the cited work and the appendix. L(σ, m) = −P(σ) + L(em , eσ ) + L(m). 399 (15) 6 Experiment methods Experiments described in this paper use the MSCOCO caption data set (Lin et al., 2015) and the Multi30k dataset (Elliott et al., 2016), which contains pairs of images and descriptions of images written by human annotators. Captions in the MSCOCO data set are in English, whereas captions in the Multi30k dataset are in English, German and French. Captions are automatically parsed (Kitaev and Klein, 2018) to generate a version of the reference set with constituency trees.6 In addition to these datasets with captions generated by human annotators, we automatically translate the English captions into Chinese, Polish and Korean using Google Translate,7 and parse the resulting translations into constituency trees, which are then used in experiments to probe the interactions between visual information and grammar induction. Results from models proposed in this paper — NoImagePCFG, ImagePrePCFG and ImagePCFG — are compared with published results from Shi et al. (2019), which include PRPN (Shen et al"
2020.aacl-main.42,P15-1036,0,0.0270781,"at the ImagePCFG model in which visual information is supplied during train11 The unlabeled evaluation results can be found in the appendix. Prepositional phrase attachment Finally, visual information may provide semantic information to resolve structural ambiguities. Word quintuples such as (a) hotel caught fire during (a) storm were extracted from English Wikipedia and the attachment locations were automatically labeled either as ‘n’ for low attachment, where the prepositional phrase adjoins the direct object, or ‘v’ for high attachment, where the prepositional phrase adjoins the main verb (Nakashole and Mitchell, 2015). 168 test items are selected by human annotators for evaluation, within which 119 are sentences with high attached PPs and 49 are with low attached PPs. For evaluation of PP attachment with induced trees, one test item is labeled correct when the induced tree puts the main verb and the direct object into one constituent and it is labeled as ‘v’. For example, if the induced tree has caught fire as a constituent, it counts as correct for the above example with high attachment. Low attachment trees must have a constituent with the direct object and the prepositional phrase. For example, for the"
2020.aacl-main.42,D16-1004,0,0.0960494,"his work can be found at https://github.com/ lifengjin/imagepcfg. 2 https://github.com/ajdagokcen/ madlyambiguous-repo 396 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 396–408 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related work Existing unsupervised PCFG inducers exploit naturally-occurring cognitive and developmental constraints, such as punctuation as a proxy to prosody (Seginer, 2007), human memory constraints (Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018b), and morphology (Jin and Schuler, 2019), to regulate the posterior of grammars which are known to be extremely multimodal (Johnson et al., 2007). Models in Shi et al. (2019) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure (Hill et al., 2014; Hill and Korhonen, 2014). Additionally, visual information is observed to provide grounding for words describing concrete objects, helping to identify and categorize such words. This hypothesis is termed ‘noun bias’ in language acquisition"
2020.aacl-main.42,D07-1043,0,0.35424,"tion and grammar induction. Results from models proposed in this paper — NoImagePCFG, ImagePrePCFG and ImagePCFG — are compared with published results from Shi et al. (2019), which include PRPN (Shen et al., 2018), ON-LSTM (Shen et al., 2019) as well as the grounded VG-NSL models which uses either head final bias (VG-NSL+H) or head final bias and Fasttext embeddings (VG-NSL+H+F) as inductive biases from external sources. All of these models only induce unlabeled structures and have been evaluated with unlabeled F1 scores. We additionally report the labeled evaluation score Recall-Homogeneity (Rosenberg and Hirschberg, 2007; Jin and Schuler, 2020) for better comparison between the proposed models. All evaluation is done on Viterbi parse trees of the test set from 5 different runs. Details about hyper-parameters and results on development data sets can be found in the appendix. However, importantly, the tuned hyperparameters for the grammar induction model are the same across the three proposed models, which facilitates direct comparisons among these models to determine the effect of visual information on induction. 6.1 Standard set: no replication of effect for visual information Both unlabeled and labeled evalu"
2020.aacl-main.42,P07-1049,0,0.194079,"entation and translated datasets used in this work can be found at https://github.com/ lifengjin/imagepcfg. 2 https://github.com/ajdagokcen/ madlyambiguous-repo 396 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 396–408 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related work Existing unsupervised PCFG inducers exploit naturally-occurring cognitive and developmental constraints, such as punctuation as a proxy to prosody (Seginer, 2007), human memory constraints (Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018b), and morphology (Jin and Schuler, 2019), to regulate the posterior of grammars which are known to be extremely multimodal (Johnson et al., 2007). Models in Shi et al. (2019) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure (Hill et al., 2014; Hill and Korhonen, 2014). Additionally, visual information is observed to provide grounding for words describing concrete objects, helping to identify and categorize such words. This hypoth"
2020.aacl-main.42,C16-1092,1,0.903465,"Missing"
2020.aacl-main.42,P19-1180,0,0.487895,"al Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 396–408 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related work Existing unsupervised PCFG inducers exploit naturally-occurring cognitive and developmental constraints, such as punctuation as a proxy to prosody (Seginer, 2007), human memory constraints (Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018b), and morphology (Jin and Schuler, 2019), to regulate the posterior of grammars which are known to be extremely multimodal (Johnson et al., 2007). Models in Shi et al. (2019) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure (Hill et al., 2014; Hill and Korhonen, 2014). Additionally, visual information is observed to provide grounding for words describing concrete objects, helping to identify and categorize such words. This hypothesis is termed ‘noun bias’ in language acquisition (Gentner, 1982, 2006; Waxman et al., 2013), through which the early acquisition of nouns is attributed to nouns referring to observable objects. However, the models in Shi et al. (2019) also rely on language-speci"
2020.coling-main.404,W19-4828,0,0.0199437,"e of the above systems integrate explicit coreference tracking into a full generative model of natural language. As a result, while such systems may be amenable to studying coreference retrieval processes in human comprehenders, they cannot be used to test the present hypothesis that coreference information guides comprehenders’ overall expectations about upcoming words. Also of relevance are recent high-performance large-scale language models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2018), which have been argued to learn coreference information (Clark et al., 2019). However, these models rely on powerful deep neural networks that are difficult to interpret, and it is generally not possible to manipulate their access to 4588 coreference information in order to evaluate the impact of this information on human expectations. Ji et al. (2017) propose an incremental generative model that explicitly tracks coreference but the joint objectives are not depedendent on each other, meaning that they do not model word expectation as dependent on the coreference decision. 3 Background The models used in this paper obtain surprisal predictors from an incremental proce"
2020.coling-main.404,N19-1423,0,0.0184799,"em without external resources using bidirectional LSTMs and an attention mechanism for head finding. However, none of the above systems integrate explicit coreference tracking into a full generative model of natural language. As a result, while such systems may be amenable to studying coreference retrieval processes in human comprehenders, they cannot be used to test the present hypothesis that coreference information guides comprehenders’ overall expectations about upcoming words. Also of relevance are recent high-performance large-scale language models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2018), which have been argued to learn coreference information (Clark et al., 2019). However, these models rely on powerful deep neural networks that are difficult to interpret, and it is generally not possible to manipulate their access to 4588 coreference information in order to evaluate the impact of this information on human expectations. Ji et al. (2017) propose an incremental generative model that explicitly tracks coreference but the joint objectives are not depedendent on each other, meaning that they do not model word expectation as dependent on the corefe"
2020.coling-main.404,N16-1024,0,0.0195492,"ssible working memory store states qt at every time step t, each of which consist of a bounded number D of nested derivation fragments adt /bdt (partial phrase structure trees). Each derivation fragment at a given depth d spans a part of a possible derivation tree from some apex sign adt (syntactic category in the case of a purely syntactic parser) to some base sign bdt in its right progeny. Previous work has shown large tree-annotated resources such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 such fragments (Schuler et al., 2010). Like other incremental parsers (Dyer et al., 2016; Hale et al., 2018; Jin and Schuler, 2020) this model makes a series of probabilistic decisions which incrementally define a syntactic structure. The left-corner parsing model defines new words wt and store states qt by first making a lexical decision `t (related to hypothesizing a preterminal above wt and possibly attaching it to an existing derivation fragment) and then making a grammatical decision gt (related to hypothesizing some minimal binary branch that subsumes both wt and wt+1 and possibly attaching it to an existing derivation fragment) after encountering each word:1 P(wt qt |qt−1"
2020.coling-main.404,J95-2003,0,0.669109,"ng a model of discourse referents and their relations is a core function of human language understanding and depends on the ability to recognize when linguistic expressions corefer. Identifying the referent of a linguistic expression (coreference resolution) is a well-established task in natural language processing, and a sizeable psycholinguistic literature has explored the computations that support coreference resolution in humans. Theories of coreference processing generally agree on a critical role played by searching associative memory stores for plausible referents (Greene et al., 1992; Grosz et al., 1995; Gordon, 1998; Almor, 1999; Ariel, 2001), based on findings that coreference is more easily established to more salient or activated referents, as evidenced by human reading latencies (Gordon, 1998; Cunnings et al., 2014). Nonetheless, some coreference-related phenomena are not obviously related to search. Consider the following example: Sally frightened her brother because Sally/she/he... Numerous studies indicate that he in the boldfaced slot is more difficult to process (i.e. read more slowly) than she despite the fact that he corefers unambiguously to her brother, the most recently activa"
2020.coling-main.404,P18-1254,0,0.0205644,"ry store states qt at every time step t, each of which consist of a bounded number D of nested derivation fragments adt /bdt (partial phrase structure trees). Each derivation fragment at a given depth d spans a part of a possible derivation tree from some apex sign adt (syntactic category in the case of a purely syntactic parser) to some base sign bdt in its right progeny. Previous work has shown large tree-annotated resources such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 such fragments (Schuler et al., 2010). Like other incremental parsers (Dyer et al., 2016; Hale et al., 2018; Jin and Schuler, 2020) this model makes a series of probabilistic decisions which incrementally define a syntactic structure. The left-corner parsing model defines new words wt and store states qt by first making a lexical decision `t (related to hypothesizing a preterminal above wt and possibly attaching it to an existing derivation fragment) and then making a grammatical decision gt (related to hypothesizing some minimal binary branch that subsumes both wt and wt+1 and possibly attaching it to an existing derivation fragment) after encountering each word:1 P(wt qt |qt−1 ) = X `t ,gt P(`t |"
2020.coling-main.404,N01-1021,0,0.714486,"arcelona, Spain (Online), December 8-13, 2020 referent is not necessarily the most recent. It is thus not straightforward to account for these effects solely on the basis of retrieval difficulty. Various accounts of these and related coreference phenomena exist, such as the informational load hypothesis of Almor (1999), which posits a trade-off between the information content of referring expressions and their linguistic complexity. However, to our knowledge, little prior work has explored the role coreference might play in more general expectation-based theories of human sentence processing (Hale, 2001; Levy, 2008; van Schijndel et al., 2013; Rasmussen and Schuler, 2018), according to which comprehension difficulty is driven by mechanisms that incrementally reallocate cognitive resources between competing interpretations of the unfolding sentence. Expectation-based theories have been highly successful in providing broad-coverage explanations of sentence processing difficulty, but have so far been evaluated using local lexical (n-gram) or syntactic (parser-based) contexts (Demberg and Keller, 2008; Frank and Bod, 2011; Smith and Levy, 2013, inter alia). We show here that they can also be a u"
2020.coling-main.404,P13-2121,0,0.0272386,"Low-level control predictors include: Word Length, the length of a word measured in characters, and Story Position, a measure of the proportion of the story completed, ranging from 0 to 1 and calculated as current sentence index divided by total sentence count. Story Position is meant to control for order effects and task learning and habituation (Baayen et al., 2017; Jaffe et al., 2018). Three surprisal predictors are also included: NgramSurp is the incremental surprisal estimate based on a 5-gram language model trained on the Gigaword corpus (Graff and Cieri, 2003) using the KenLM toolkit (Heafield et al., 2013). It is meant to account for word predictability based on local lexical context alone. NoCorefSurp is the baseline incremental surprisal estimate that comes from the parser described above in Section 3.3. Surprisal is calculated as the difference in log probability from the previous timestep to the current timestep as allocated on the beam (see Equation 1). CorefSurp is the incremental surprisal estimate that comes from augmenting the parser with coreference resolution, as described in Section 4. Because the effect of interest may be delayed, it is standard psycholinguistic practice to conside"
2020.coling-main.404,W18-0101,1,0.797008,"orted evidence of the need for both memory-related and expectation-related mechanisms in accounting for the full range of human sentence processing phenomena (Levy et al., 2013). 2 Previous Work Coreference resolution has been extensively studied, both as a psycholinguistic phenomenon and as a natural language processing task. Psycholinguistic investigations have largely focused on the memory retrieval mechanisms that allow comprehenders to identify the referents of linguistic expressions (Greene et al., 1992; Grosz et al., 1995; Gordon, 1998; Almor, 1999; Ariel, 2001; Kehler and Rohde, 2015; Jaffe et al., 2018). Some computational models have attempted to implement these theories in order to solve the coreference resolution task. For example, Wiemer-Hastings and Iacucci (2001) implement Gordon (1998), Tetreault (2002) is inspired by Centering Theory (Grosz et al., 1995), and Webster and Nothman (2016) draw on the Accessibility Hierarchy (Ariel, 2001) for feature design. Webster and Curran (2014) propose an incremental coreference resolution system inspired by Centering and Accessibility theories by maintaining a ranked stack of entities supposed to reflect the reader’s mental state. Kehler and Rohde"
2020.coling-main.404,D17-1195,0,0.0158202,"hat coreference information guides comprehenders’ overall expectations about upcoming words. Also of relevance are recent high-performance large-scale language models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2018), which have been argued to learn coreference information (Clark et al., 2019). However, these models rely on powerful deep neural networks that are difficult to interpret, and it is generally not possible to manipulate their access to 4588 coreference information in order to evaluate the impact of this information on human expectations. Ji et al. (2017) propose an incremental generative model that explicitly tracks coreference but the joint objectives are not depedendent on each other, meaning that they do not model word expectation as dependent on the coreference decision. 3 Background The models used in this paper obtain surprisal predictors from an incremental processing model based on a left-corner parser, trained on corpora annotated with both syntactic and coreference information (Weischedel et al., 2012), which produces a simple incremental probabilistic account of sentence processing by making a single lexical attachment decision and"
2020.coling-main.404,2020.iwpt-1.6,1,0.7942,"at every time step t, each of which consist of a bounded number D of nested derivation fragments adt /bdt (partial phrase structure trees). Each derivation fragment at a given depth d spans a part of a possible derivation tree from some apex sign adt (syntactic category in the case of a purely syntactic parser) to some base sign bdt in its right progeny. Previous work has shown large tree-annotated resources such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 such fragments (Schuler et al., 2010). Like other incremental parsers (Dyer et al., 2016; Hale et al., 2018; Jin and Schuler, 2020) this model makes a series of probabilistic decisions which incrementally define a syntactic structure. The left-corner parsing model defines new words wt and store states qt by first making a lexical decision `t (related to hypothesizing a preterminal above wt and possibly attaching it to an existing derivation fragment) and then making a grammatical decision gt (related to hypothesizing some minimal binary branch that subsumes both wt and wt+1 and possibly attaching it to an existing derivation fragment) after encountering each word:1 P(wt qt |qt−1 ) = X `t ,gt P(`t |qt−1 ) · P(wt |qt−1 `t )"
2020.coling-main.404,D17-1018,0,0.0250153,"tering and Accessibility theories by maintaining a ranked stack of entities supposed to reflect the reader’s mental state. Kehler and Rohde (2015) offer a Bayesian account of pronominal reference that highlights an asymmetry between pronoun production and interpretation. Other computational models are less directly inspired by human cognition but achieve strong performance on the coreference resolution task. Wiseman et al. (2016) use LSTMs to track each entity cluster, where the hidden state is updated with each mention that is linked to the cluster, allowing for use of entity-level features. Lee et al. (2017) propose an end-to-end coreference resolution system without external resources using bidirectional LSTMs and an attention mechanism for head finding. However, none of the above systems integrate explicit coreference tracking into a full generative model of natural language. As a result, while such systems may be amenable to studying coreference retrieval processes in human comprehenders, they cannot be used to test the present hypothesis that coreference information guides comprehenders’ overall expectations about upcoming words. Also of relevance are recent high-performance large-scale langu"
2020.coling-main.404,P14-2050,0,0.0121909,"such as gap-fillers, relative and interrogative pronoun antecedents, extraposed or heavy-shifted items, and passive subjects. Vectors for syntactic predicates and arguments may be dense, generated by neural network models in order to maximize prediction accuracy, but experiments described in this paper will adopt a sparse encoding generated by explicit features in order to perform a clean ablation with a coreference model and avoid confounds related to hyperparameter tuning. In this sparse encoding, a vector for a referent signified by a sign contains a set of referential context indicators (Levy and Goldberg, 2014), each consisting of a predicate constant and a path of up to three labeled associations that connect that predicate to that referent. 4590 For example, pour might be a referential context for the eventuality of a pour predicate, and pour 2 might be a referential context for the second argument of a pour predicate. The model is then augmented to generate not only match decisions and category labels for each lexical and grammatical decision, but also sparse binary vectors for these referential contexts. Match decisions are then estimated using logistic regression on pairs of referential context"
2020.coling-main.404,J93-2004,0,0.0688958,"ry time step, and because it makes a fixed number of decisions after every word. The model maintains a distribution over possible working memory store states qt at every time step t, each of which consist of a bounded number D of nested derivation fragments adt /bdt (partial phrase structure trees). Each derivation fragment at a given depth d spans a part of a possible derivation tree from some apex sign adt (syntactic category in the case of a purely syntactic parser) to some base sign bdt in its right progeny. Previous work has shown large tree-annotated resources such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 such fragments (Schuler et al., 2010). Like other incremental parsers (Dyer et al., 2016; Hale et al., 2018; Jin and Schuler, 2020) this model makes a series of probabilistic decisions which incrementally define a syntactic structure. The left-corner parsing model defines new words wt and store states qt by first making a lexical decision `t (related to hypothesizing a preterminal above wt and possibly attaching it to an existing derivation fragment) and then making a grammatical decision gt (related to hypothesizing some minimal binary branch that subsumes both"
2020.coling-main.404,C12-1130,1,0.876342,"Missing"
2020.coling-main.404,N18-1202,0,0.0157548,"coreference resolution system without external resources using bidirectional LSTMs and an attention mechanism for head finding. However, none of the above systems integrate explicit coreference tracking into a full generative model of natural language. As a result, while such systems may be amenable to studying coreference retrieval processes in human comprehenders, they cannot be used to test the present hypothesis that coreference information guides comprehenders’ overall expectations about upcoming words. Also of relevance are recent high-performance large-scale language models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2018), which have been argued to learn coreference information (Clark et al., 2019). However, these models rely on powerful deep neural networks that are difficult to interpret, and it is generally not possible to manipulate their access to 4588 coreference information in order to evaluate the impact of this information on human expectations. Ji et al. (2017) propose an incremental generative model that explicitly tracks coreference but the joint objectives are not depedendent on each other, meaning that they do not model word expectatio"
2020.coling-main.404,J01-2004,0,0.0608116,"erated by a processing model at consecutive time steps: def S(wt ) = − log P(wt |w1..t−1 ) = − log P(w1..t ) + log P(w1..t−1 ) (1) These prefix probabilities can be calculated by marginalizing over hidden states in the forward probabilities of an incremental processing model: X P(w1..t ) = P(w1..t qt ) (2) qt These forward probabilities are in turn defined recursively using a transition model: X def P(w1..t qt ) = P(wt qt |qt−1 ) · P(w1..t−1 qt−1 ) (3) qt−1 3.2 Left-corner Parsing The transition model described in this paper is based on a probabilistic left-corner parser (Johnson-Laird, 1983; Roark, 2001; van Schijndel et al., 2013; Rasmussen and Schuler, 2018), in part because it requires a bounded amount of working memory at every time step, and because it makes a fixed number of decisions after every word. The model maintains a distribution over possible working memory store states qt at every time step t, each of which consist of a bounded number D of nested derivation fragments adt /bdt (partial phrase structure trees). Each derivation fragment at a given depth d spans a part of a possible derivation tree from some apex sign adt (syntactic category in the case of a purely syntactic parse"
2020.coling-main.404,J10-1001,1,0.842474,"Missing"
2020.coling-main.404,C14-1201,0,0.0234991,"he memory retrieval mechanisms that allow comprehenders to identify the referents of linguistic expressions (Greene et al., 1992; Grosz et al., 1995; Gordon, 1998; Almor, 1999; Ariel, 2001; Kehler and Rohde, 2015; Jaffe et al., 2018). Some computational models have attempted to implement these theories in order to solve the coreference resolution task. For example, Wiemer-Hastings and Iacucci (2001) implement Gordon (1998), Tetreault (2002) is inspired by Centering Theory (Grosz et al., 1995), and Webster and Nothman (2016) draw on the Accessibility Hierarchy (Ariel, 2001) for feature design. Webster and Curran (2014) propose an incremental coreference resolution system inspired by Centering and Accessibility theories by maintaining a ranked stack of entities supposed to reflect the reader’s mental state. Kehler and Rohde (2015) offer a Bayesian account of pronominal reference that highlights an asymmetry between pronoun production and interpretation. Other computational models are less directly inspired by human cognition but achieve strong performance on the coreference resolution task. Wiseman et al. (2016) use LSTMs to track each entity cluster, where the hidden state is updated with each mention that"
2020.coling-main.404,P16-2070,0,0.0127205,"as a natural language processing task. Psycholinguistic investigations have largely focused on the memory retrieval mechanisms that allow comprehenders to identify the referents of linguistic expressions (Greene et al., 1992; Grosz et al., 1995; Gordon, 1998; Almor, 1999; Ariel, 2001; Kehler and Rohde, 2015; Jaffe et al., 2018). Some computational models have attempted to implement these theories in order to solve the coreference resolution task. For example, Wiemer-Hastings and Iacucci (2001) implement Gordon (1998), Tetreault (2002) is inspired by Centering Theory (Grosz et al., 1995), and Webster and Nothman (2016) draw on the Accessibility Hierarchy (Ariel, 2001) for feature design. Webster and Curran (2014) propose an incremental coreference resolution system inspired by Centering and Accessibility theories by maintaining a ranked stack of entities supposed to reflect the reader’s mental state. Kehler and Rohde (2015) offer a Bayesian account of pronominal reference that highlights an asymmetry between pronoun production and interpretation. Other computational models are less directly inspired by human cognition but achieve strong performance on the coreference resolution task. Wiseman et al. (2016) u"
2020.coling-main.404,N16-1114,0,0.0198982,"ster and Nothman (2016) draw on the Accessibility Hierarchy (Ariel, 2001) for feature design. Webster and Curran (2014) propose an incremental coreference resolution system inspired by Centering and Accessibility theories by maintaining a ranked stack of entities supposed to reflect the reader’s mental state. Kehler and Rohde (2015) offer a Bayesian account of pronominal reference that highlights an asymmetry between pronoun production and interpretation. Other computational models are less directly inspired by human cognition but achieve strong performance on the coreference resolution task. Wiseman et al. (2016) use LSTMs to track each entity cluster, where the hidden state is updated with each mention that is linked to the cluster, allowing for use of entity-level features. Lee et al. (2017) propose an end-to-end coreference resolution system without external resources using bidirectional LSTMs and an attention mechanism for head finding. However, none of the above systems integrate explicit coreference tracking into a full generative model of natural language. As a result, while such systems may be amenable to studying coreference retrieval processes in human comprehenders, they cannot be used to t"
2020.iwpt-1.15,P07-1094,0,0.0894496,"rules in the grammar. However, only 17,020 unique rules are found in the corpus, showing the high sparsity of attested rules. In other frameworks like Combinatory Categorial Grammar (Steedman, 2002) where lexical categories can be in the thousands, the number of attested lexical categories is still small compared to all possible ones. The Dirichlet concentration hyperparameter β in the model controls the probability of a sampled multinomial distribution concentrating its probability mass on only a few items. Previous work using similar models usually sets this value low (Johnson et al., 2007; Goldwater and Griffiths, 2007; Grac¸a et al., 2009; Jin et al., 2018b) to prefer sparse grammars (i.e. grammars in which most of the probability mass is allocated to a small number of rules), with good results. The prediction based on the preference of sparsity is that the best β value should be much lower than 1. Figure 1a shows unlabeled F1 scores with different β values on Adam.1 Contrary to the prediction, grammar accuracy peaks at high values for β when measured using unlabeled F1. However, these grammars with high unlabeled F1 are almost purely right-branching grammars, which performs very well on English child-dire"
2020.iwpt-1.15,P19-1234,1,0.914165,"h-bounding as an implementation of human memory constraints in grammar inducers is still effective with labeled evaluation on multilingual transcribed child-directed utterances. 1 Introduction Recent work in probabilistic context-free grammar (PCFG) induction has shown that it is possible to learn accurate grammars from raw text (Jin et al., 2018b, 2019; Kim et al., 2019), which is significant in addressing the issue of the poverty of the stimulus (Chomsky, 1965, 1980) in linguistics. Although phrasal categories and morphosyntactic features can be induced from raw text (Jin and Schuler, 2019; Jin et al., 2019), most unsupervised parsing work has been evaluated using unlabeled parsing accuracy scores (Seginer, 2007; Ponvert et al., 2011; Jin et al., 2018b; Shen et al., 2018, 2019; Shi et al., 2019). This is potentially distortative because children and adults can distinguish categories of phrases and clauses (Tomasello and Olguin, 1993; Valian, 1986; Kemp et al., 2005; Pine et al., 2013), and much of acquisition modeling research has been directed at simulating the development of abstract linguistic categories in first language acquisition (Bannard et al., 2009; Perfors et al., 2011; Kwiatkowski et"
2020.iwpt-1.15,D18-1292,1,0.508267,"rasal labels, an essential part of a grammar. Experiments in this work using a labeled evaluation metric, RH, show that linguistically motivated predictions about grammar sparsity and use of categories can only be revealed through labeled evaluation. Furthermore, depth-bounding as an implementation of human memory constraints in grammar inducers is still effective with labeled evaluation on multilingual transcribed child-directed utterances. 1 Introduction Recent work in probabilistic context-free grammar (PCFG) induction has shown that it is possible to learn accurate grammars from raw text (Jin et al., 2018b, 2019; Kim et al., 2019), which is significant in addressing the issue of the poverty of the stimulus (Chomsky, 1965, 1980) in linguistics. Although phrasal categories and morphosyntactic features can be induced from raw text (Jin and Schuler, 2019; Jin et al., 2019), most unsupervised parsing work has been evaluated using unlabeled parsing accuracy scores (Seginer, 2007; Ponvert et al., 2011; Jin et al., 2018b; Shen et al., 2018, 2019; Shi et al., 2019). This is potentially distortative because children and adults can distinguish categories of phrases and clauses (Tomasello and Olguin, 1993"
2020.iwpt-1.15,P19-1235,1,0.775689,"tion. Furthermore, depth-bounding as an implementation of human memory constraints in grammar inducers is still effective with labeled evaluation on multilingual transcribed child-directed utterances. 1 Introduction Recent work in probabilistic context-free grammar (PCFG) induction has shown that it is possible to learn accurate grammars from raw text (Jin et al., 2018b, 2019; Kim et al., 2019), which is significant in addressing the issue of the poverty of the stimulus (Chomsky, 1965, 1980) in linguistics. Although phrasal categories and morphosyntactic features can be induced from raw text (Jin and Schuler, 2019; Jin et al., 2019), most unsupervised parsing work has been evaluated using unlabeled parsing accuracy scores (Seginer, 2007; Ponvert et al., 2011; Jin et al., 2018b; Shen et al., 2018, 2019; Shi et al., 2019). This is potentially distortative because children and adults can distinguish categories of phrases and clauses (Tomasello and Olguin, 1993; Valian, 1986; Kemp et al., 2005; Pine et al., 2013), and much of acquisition modeling research has been directed at simulating the development of abstract linguistic categories in first language acquisition (Bannard et al., 2009; Perfors et al., 20"
2020.iwpt-1.15,N07-1018,0,0.322826,"and ternary branching rules in the grammar. However, only 17,020 unique rules are found in the corpus, showing the high sparsity of attested rules. In other frameworks like Combinatory Categorial Grammar (Steedman, 2002) where lexical categories can be in the thousands, the number of attested lexical categories is still small compared to all possible ones. The Dirichlet concentration hyperparameter β in the model controls the probability of a sampled multinomial distribution concentrating its probability mass on only a few items. Previous work using similar models usually sets this value low (Johnson et al., 2007; Goldwater and Griffiths, 2007; Grac¸a et al., 2009; Jin et al., 2018b) to prefer sparse grammars (i.e. grammars in which most of the probability mass is allocated to a small number of rules), with good results. The prediction based on the preference of sparsity is that the best β value should be much lower than 1. Figure 1a shows unlabeled F1 scores with different β values on Adam.1 Contrary to the prediction, grammar accuracy peaks at high values for β when measured using unlabeled F1. However, these grammars with high unlabeled F1 are almost purely right-branching grammars, which performs"
2020.iwpt-1.15,P07-1049,0,0.103086,"d evaluation on multilingual transcribed child-directed utterances. 1 Introduction Recent work in probabilistic context-free grammar (PCFG) induction has shown that it is possible to learn accurate grammars from raw text (Jin et al., 2018b, 2019; Kim et al., 2019), which is significant in addressing the issue of the poverty of the stimulus (Chomsky, 1965, 1980) in linguistics. Although phrasal categories and morphosyntactic features can be induced from raw text (Jin and Schuler, 2019; Jin et al., 2019), most unsupervised parsing work has been evaluated using unlabeled parsing accuracy scores (Seginer, 2007; Ponvert et al., 2011; Jin et al., 2018b; Shen et al., 2018, 2019; Shi et al., 2019). This is potentially distortative because children and adults can distinguish categories of phrases and clauses (Tomasello and Olguin, 1993; Valian, 1986; Kemp et al., 2005; Pine et al., 2013), and much of acquisition modeling research has been directed at simulating the development of abstract linguistic categories in first language acquisition (Bannard et al., 2009; Perfors et al., 2011; Kwiatkowski et al., 2012; Abend et al., 2017; Jin et al., 2018b). Recent work proposed a labeled parsing accuracy evaluat"
2020.iwpt-1.15,C16-1092,1,0.769086,"Missing"
2020.iwpt-1.15,P19-1228,0,0.116786,"l part of a grammar. Experiments in this work using a labeled evaluation metric, RH, show that linguistically motivated predictions about grammar sparsity and use of categories can only be revealed through labeled evaluation. Furthermore, depth-bounding as an implementation of human memory constraints in grammar inducers is still effective with labeled evaluation on multilingual transcribed child-directed utterances. 1 Introduction Recent work in probabilistic context-free grammar (PCFG) induction has shown that it is possible to learn accurate grammars from raw text (Jin et al., 2018b, 2019; Kim et al., 2019), which is significant in addressing the issue of the poverty of the stimulus (Chomsky, 1965, 1980) in linguistics. Although phrasal categories and morphosyntactic features can be induced from raw text (Jin and Schuler, 2019; Jin et al., 2019), most unsupervised parsing work has been evaluated using unlabeled parsing accuracy scores (Seginer, 2007; Ponvert et al., 2011; Jin et al., 2018b; Shen et al., 2018, 2019; Shi et al., 2019). This is potentially distortative because children and adults can distinguish categories of phrases and clauses (Tomasello and Olguin, 1993; Valian, 1986; Kemp et al"
2020.iwpt-1.15,P18-1249,0,0.0301191,"0.2 0.5 1.0 5.0 β (b) RH scores with βs Figure 2: Different evaluation metrics on the WSJ20Dev dataset with different β values. Data and hyperparameters Experiments here use transcribed child-directed utterances from the CHILDES corpus (Macwhinney, 1992) in three languages with more than 15,000 sentences each. English hand-annotated constituency trees are taken from the Adam and Eve portions of the Brown Corpus (Brown, 1973). Mandarin (Tong, Deng et al., 2018) and German (Leo, Behrens, 2006) data are collected from CHILDES with reference trees automatically generated using the stateof-the-art Kitaev and Klein (2018) parser. Disfluencies are removed, and only sentences spoken by caregivers are kept in the data. Models are run 10 times with 700 iterations with random seeds following previous work (Jin et al., 2018a). The last sampled grammar is used to generate Viterbi parses for all sentences. All punctuation is retained during induction and then removed in evaluation. Significance testing uses permutation tests on concatenations of Viterbi trees from all test runs. We use Adam for exploratory experiments and the other three sets for confirmatory experiments. 146 0.25 0.70 0.20 0.65 0.5 RH 0.30 0.75 RH Un"
2020.iwpt-1.15,E12-1024,0,0.0179654,"n et al., 2019), most unsupervised parsing work has been evaluated using unlabeled parsing accuracy scores (Seginer, 2007; Ponvert et al., 2011; Jin et al., 2018b; Shen et al., 2018, 2019; Shi et al., 2019). This is potentially distortative because children and adults can distinguish categories of phrases and clauses (Tomasello and Olguin, 1993; Valian, 1986; Kemp et al., 2005; Pine et al., 2013), and much of acquisition modeling research has been directed at simulating the development of abstract linguistic categories in first language acquisition (Bannard et al., 2009; Perfors et al., 2011; Kwiatkowski et al., 2012; Abend et al., 2017; Jin et al., 2018b). Recent work proposed a labeled parsing accuracy evaluation metric called Recall-V-Measure (RVM) as a method for evaluating unsupervised grammar inducers (Jin et al., 2019), but this metric counts categories as incorrect if they are finergrained than reference categories or if they represent binarizations of n-ary branches in reference trees, which may be linguistically acceptable. We therefore further modify it to Recall-Homogeneity (RH) calculated as the homogeneity (Rosenberg and Hirschberg, 2007) of the labels of matching constituents of the induced"
2020.iwpt-1.15,J93-2004,0,0.0696485,"all fall into the confines of a gold category. RVM, on the other hand, would penalize both underproposing and overproposing categories compared to the ones in the annotation, but the gold categories, like nouns and verbs, are defined on a very high level that languages almost always further specify, represented usually as subcategories or features in linguistic theories. Unary branches in gold and predicted trees are removed, and the top category is used as the category for the constituent. 4 4.1 Experiments Experiment 1: Labeled evaluation shows preference of grammar sparsity Penn Treebank (Marcus et al., 1993), there are 73 unique nonterminal categories. In theory, there can be more than 28 million possible unary, binary and ternary branching rules in the grammar. However, only 17,020 unique rules are found in the corpus, showing the high sparsity of attested rules. In other frameworks like Combinatory Categorial Grammar (Steedman, 2002) where lexical categories can be in the thousands, the number of attested lexical categories is still small compared to all possible ones. The Dirichlet concentration hyperparameter β in the model controls the probability of a sampled multinomial distribution concen"
2020.iwpt-1.15,P11-1108,0,0.0606322,"Missing"
2020.iwpt-1.15,D07-1043,0,0.819262,"age acquisition (Bannard et al., 2009; Perfors et al., 2011; Kwiatkowski et al., 2012; Abend et al., 2017; Jin et al., 2018b). Recent work proposed a labeled parsing accuracy evaluation metric called Recall-V-Measure (RVM) as a method for evaluating unsupervised grammar inducers (Jin et al., 2019), but this metric counts categories as incorrect if they are finergrained than reference categories or if they represent binarizations of n-ary branches in reference trees, which may be linguistically acceptable. We therefore further modify it to Recall-Homogeneity (RH) calculated as the homogeneity (Rosenberg and Hirschberg, 2007) of the labels of matching constituents of the induced and gold trees, weighted by unlabeled recall. This work uses transcribed child-directed utterances from multiple languages as input to a grammar inducer with hyperparameters tuned using either unlabeled F1 or labeled RH. Results show that: (1) the induced grammars capture the preference of sparse concentrations in human grammars only when using labeled evaluation; (2) grammar accuracy increases as the number of labels grows only when using labeled evaluation; (3) depth-bounding (Jin et al., 2018a, limiting center embedding) is still effect"
2020.iwpt-1.15,J10-1001,1,0.744121,"Missing"
2020.iwpt-1.15,P19-1180,0,0.0926639,"n Recent work in probabilistic context-free grammar (PCFG) induction has shown that it is possible to learn accurate grammars from raw text (Jin et al., 2018b, 2019; Kim et al., 2019), which is significant in addressing the issue of the poverty of the stimulus (Chomsky, 1965, 1980) in linguistics. Although phrasal categories and morphosyntactic features can be induced from raw text (Jin and Schuler, 2019; Jin et al., 2019), most unsupervised parsing work has been evaluated using unlabeled parsing accuracy scores (Seginer, 2007; Ponvert et al., 2011; Jin et al., 2018b; Shen et al., 2018, 2019; Shi et al., 2019). This is potentially distortative because children and adults can distinguish categories of phrases and clauses (Tomasello and Olguin, 1993; Valian, 1986; Kemp et al., 2005; Pine et al., 2013), and much of acquisition modeling research has been directed at simulating the development of abstract linguistic categories in first language acquisition (Bannard et al., 2009; Perfors et al., 2011; Kwiatkowski et al., 2012; Abend et al., 2017; Jin et al., 2018b). Recent work proposed a labeled parsing accuracy evaluation metric called Recall-V-Measure (RVM) as a method for evaluating unsupervised gram"
2020.iwpt-1.6,J13-4008,0,0.0203692,"e voice following exposure to sentence stimuli, and Miller and Isard (1964) found that subjects were not able to understand sentences with deeply nested center-embedded structures, such as: (i) The cart [RC the horse [RC the man bought ] pulled ] broke. as easily as non-center-embedded control sentences, despite being composed of familiar rules. 48 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 48–61 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics memory operations (van Schijndel and Schuler, 2015). Demberg et al. (2013) propose a parser which is also able to produce prefix probabilities for tree-adjoining grammars. All of these statistical parsers lag behind state-of-the-art parsers in parsing accuracy, because of psycholinguistic constraints like incrementality and because they use less expressive statistical models. sentence, ‘The rat that the cat that the dog chased ate nibbled the malt,’ consistent with findings that humans have more difficulty understanding the latter sentence. Left-corner parsers also define a fixed set of probabilistic decisions at each word, which naturally paces the surprisal measur"
2020.iwpt-1.6,D12-1033,0,0.0223946,"k memory not only more closely implements human working memory limits in a model designed to calculate cognitive predictors, other work (Jin et al., 2018) shows that it also helps limit search space for unsupervised grammar acquisition. Related work Incremental generative constituent parsers are able to process sentences in time order and provide psycholinguistically predictive measures like syntactic surprisal and entropy reduction (Levy, 2008; Hale, 2001, 2006), which in turn are used in psycholinguistic experiments for probing effects of syntax on behavioral data (Demberg and Keller, 2008; Demberg et al., 2012; van Schijndel and Schuler, 2015). Statistical incremental parsers like ones proposed by Roark (2001) and van Schijndel et al. (2013) are based on contextfree grammars. The Roark (2001) parser builds syntactic structures top-down incrementally and has been used in studies for calculating surprisal (Demberg and Keller, 2008; Roark et al., 2009; Frank, 2009). Left-corner parsers (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983) are often used to model limits on center embedding (Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994; Lewis and Vasishth, 2005). van Schijndel et al."
2020.iwpt-1.6,N16-1024,0,0.520326,"isal estimates and can be constrained to use a bounded stack. Results show that accuracy gains of neural parsers can be reliably extended to psycholinguistic modeling without risk of distortion due to unbounded working memory. 1 Introduction Syntactic surprisal has been shown to have an effect on human sentence processing, and can be calculated from prefix probabilities of generative incremental parsers (Hale, 2001; Levy, 2008), making it a useful baseline predictor when looking for effects of other factors, like limits of memory or attention. Recent work in generative neural network parsing (Dyer et al., 2016; Hale et al., 2018) has shown that generative parsers based on neural networks are more accurate than earlier statistical generative parsers, and can be used for surprisal calculation. Although a typical shift-reduce neural network parser like that used by Hale et al. (2018) 1 Specifically, Bransford and Franks (1971) found that subjects were not reliably able to recall word-order information such as whether sentences were in passive or active voice following exposure to sentence stimuli, and Miller and Isard (1964) found that subjects were not able to understand sentences with deeply nested"
2020.iwpt-1.6,E17-1064,0,0.0167489,"to the top category are illustrated in Figure 5 in the appendix. The probability of a bottom vector is also deterministic and modeled as an indicator function equal to one when the vectors are as defined by a set of LSTMs over dependent top and bottom store vectors, depending on the previous bottom category and current top category decisions: to make such decisions perfectly by memorizing the training data. This model therefore stops the cross-entropy training when parsing performance starts to decrease on a development set, and switches to use the REINFORCE algorithm (Williams, 1992; Le and Fokkens, 2017) to finetune the model with sequence level supervision. The loss becomes:4 λ 2 ||θ ||− Eq01..T ∼Pθ (q01..T |w1..T ) 2 ˆ log Pθ (q0 |w1..T ) (16) (F(q01..T , q1..T ) − b) 1..T Lθ0 (w1..T q1..T ) = 1..D 0 0 1..D )= |ct−1 ht−1 a1..D P(b1..D t t−1 bt−1 ct−1 ht−1 wt ct ht pt at bt at  ¯ ¯ d¯ d−1  Jbd−1  ¯ t =LSTMθQ0 [ wt ,at−1 ,bt−1 ,E δbt ]K·ψd−1       if bt−1 =⊥,at =⊥     d¯ Jbt =E δbt K·ψd¯ if bt−1 =⊥,at ,⊥      ¯ ¯  d d  Jbt =LSTMθQ0 [ wt ,bt−1 ,E δbt ]K·ψd¯ if bt−1 ,⊥,at =⊥     Jbd+1 ¯ =E δ K·ψ ¯ if b ,⊥,a ,⊥ t bt t−1 d+1 Pθ (w1..T q0 (13) t where d¯ = argmaxd {adt−1 ,"
2020.iwpt-1.6,N01-1021,0,0.936375,"istical parsers exist, but are less accurate than neural parsers in predicting reading times. This paper describes a neural incremental generative parser that is able to provide accurate surprisal estimates and can be constrained to use a bounded stack. Results show that accuracy gains of neural parsers can be reliably extended to psycholinguistic modeling without risk of distortion due to unbounded working memory. 1 Introduction Syntactic surprisal has been shown to have an effect on human sentence processing, and can be calculated from prefix probabilities of generative incremental parsers (Hale, 2001; Levy, 2008), making it a useful baseline predictor when looking for effects of other factors, like limits of memory or attention. Recent work in generative neural network parsing (Dyer et al., 2016; Hale et al., 2018) has shown that generative parsers based on neural networks are more accurate than earlier statistical generative parsers, and can be used for surprisal calculation. Although a typical shift-reduce neural network parser like that used by Hale et al. (2018) 1 Specifically, Bransford and Franks (1971) found that subjects were not reliably able to recall word-order information such"
2020.iwpt-1.6,H94-1020,0,0.0824707,"Missing"
2020.iwpt-1.6,P18-1254,0,0.535574,"can be constrained to use a bounded stack. Results show that accuracy gains of neural parsers can be reliably extended to psycholinguistic modeling without risk of distortion due to unbounded working memory. 1 Introduction Syntactic surprisal has been shown to have an effect on human sentence processing, and can be calculated from prefix probabilities of generative incremental parsers (Hale, 2001; Levy, 2008), making it a useful baseline predictor when looking for effects of other factors, like limits of memory or attention. Recent work in generative neural network parsing (Dyer et al., 2016; Hale et al., 2018) has shown that generative parsers based on neural networks are more accurate than earlier statistical generative parsers, and can be used for surprisal calculation. Although a typical shift-reduce neural network parser like that used by Hale et al. (2018) 1 Specifically, Bransford and Franks (1971) found that subjects were not reliably able to recall word-order information such as whether sentences were in passive or active voice following exposure to sentence stimuli, and Miller and Isard (1964) found that subjects were not able to understand sentences with deeply nested center-embedded stru"
2020.iwpt-1.6,C92-2065,0,0.695722,"of syntax on behavioral data (Demberg and Keller, 2008; Demberg et al., 2012; van Schijndel and Schuler, 2015). Statistical incremental parsers like ones proposed by Roark (2001) and van Schijndel et al. (2013) are based on contextfree grammars. The Roark (2001) parser builds syntactic structures top-down incrementally and has been used in studies for calculating surprisal (Demberg and Keller, 2008; Roark et al., 2009; Frank, 2009). Left-corner parsers (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983) are often used to model limits on center embedding (Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994; Lewis and Vasishth, 2005). van Schijndel et al. (2013) proposed an incremental parser that takes working memory constraints into account, and is able to produce probabilistic measures as well as predictions about working 2 Kuncoro et al. (2018) calls the in-order tree traversal a left-corner traversal. In order to avoid confusion, we refer to it as in-order tree traversal, and save the name left-corner traversal for the Johnson-Laird (1983) formulation, which traverses trees from left to right with a bounded stack. 49 Step Stack [] [the] 1 [NP/NP &gt;] [NP/NP ⊥] [NP] 2 [NP/NP &gt;]"
2020.iwpt-1.6,P13-2121,0,0.178277,"Missing"
2020.iwpt-1.6,J01-2004,0,0.694052,"ve predictors, other work (Jin et al., 2018) shows that it also helps limit search space for unsupervised grammar acquisition. Related work Incremental generative constituent parsers are able to process sentences in time order and provide psycholinguistically predictive measures like syntactic surprisal and entropy reduction (Levy, 2008; Hale, 2001, 2006), which in turn are used in psycholinguistic experiments for probing effects of syntax on behavioral data (Demberg and Keller, 2008; Demberg et al., 2012; van Schijndel and Schuler, 2015). Statistical incremental parsers like ones proposed by Roark (2001) and van Schijndel et al. (2013) are based on contextfree grammars. The Roark (2001) parser builds syntactic structures top-down incrementally and has been used in studies for calculating surprisal (Demberg and Keller, 2008; Roark et al., 2009; Frank, 2009). Left-corner parsers (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983) are often used to model limits on center embedding (Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994; Lewis and Vasishth, 2005). van Schijndel et al. (2013) proposed an incremental parser that takes working memory constraints into account, and is able"
2020.iwpt-1.6,D09-1034,0,0.0470372,"e psycholinguistically predictive measures like syntactic surprisal and entropy reduction (Levy, 2008; Hale, 2001, 2006), which in turn are used in psycholinguistic experiments for probing effects of syntax on behavioral data (Demberg and Keller, 2008; Demberg et al., 2012; van Schijndel and Schuler, 2015). Statistical incremental parsers like ones proposed by Roark (2001) and van Schijndel et al. (2013) are based on contextfree grammars. The Roark (2001) parser builds syntactic structures top-down incrementally and has been used in studies for calculating surprisal (Demberg and Keller, 2008; Roark et al., 2009; Frank, 2009). Left-corner parsers (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983) are often used to model limits on center embedding (Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994; Lewis and Vasishth, 2005). van Schijndel et al. (2013) proposed an incremental parser that takes working memory constraints into account, and is able to produce probabilistic measures as well as predictions about working 2 Kuncoro et al. (2018) calls the in-order tree traversal a left-corner traversal. In order to avoid confusion, we refer to it as in-order tree traversal, and save the name"
2020.iwpt-1.6,P80-1024,0,0.594078,"This potentially sentence-length stack may be used by the neural parser to maintain explicit in-order representations of all previously parsed words. In contrast, humans seem to have a bounded working memory, demonstrated by inhibited performance on word recall in multiclause sentences (Bransford and Franks, 1971), and on center-embedded sentences (Miller and Isard, 1964).1 Explicit storage of this long parsing history may improve parsing accuracy, but it also risks distorting the predictions of the model when used as a statistical control in psycholinguistic experiments. Left-corner parsers (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983) have been argued to provide human-like limits on working memory, because the stack memory requirements of this kind of parser do not grow unboundedly in linguistically common cases of left- or right recursion, only in linguistically rare cases of center recursion. For example, a left corner parser would require only one memory element to process the right recursive sentence, ‘The dog chased the cat that ate the rat that nibbled the malt,’ but would require three elements to process the center recursive Syntactic surprisal has been shown to have an effect on human sentenc"
2020.iwpt-1.6,W13-2605,1,0.87732,"Missing"
2020.iwpt-1.6,N15-1183,1,0.872321,"Missing"
2020.iwpt-1.6,J10-1001,1,0.904773,"m. The left-corner model evaluated in the experiments also applies bounding to the stack memory and uses a relatively liberal maximum depth of 5 derivation fragments (10 tree nodes), reflecting the fact that remembering more than 10 items faithfully at once is highly unlikely in sentence processing due to working memory limits in humans. There are two sets of constraints for different use cases for the parser to prune parses on the beam.3 The basic set only drops a parse when the Use of stack memory Stack memory depth increases only when a left nonterminal child of a right child is generated (Schuler et al., 2010) as a center-embedded structure is generated. In the current transition system, the pjb decision at the previous time step (pjbt−1 ) and the pja decision at the current time step (pjat ) together decide how depth of a parse will change: • if pjbt−1 = null and pjat = null, then the 3 51 Please see the supplemental materials for details. • a bottom vector bdt ∈ Rn for each depth d ∈ {1..D}, and • cell and hidden vectors c0t , h0t ∈ Rn for a decision LSTM. parse reaches depth 0 before the end of the sentence. This set is used by the parser when psycholinguistic measures are needed. The extended s"
2020.iwpt-1.6,D17-1178,0,0.0701372,"etwork models (Choe and Charniak, 2016; Dyer et al., 2016; Kitaev and Klein, 2018). Dyer et al. (2016) propose a generative neural model for top-down incremental parsing but use it only as a reranker for a discriminative parser. Extensions to the Dyer et al. (2016) model allow the parser to do in-order tree traversal (Liu and Zhang, 2017; Kuncoro et al., 2018).2 However, the in-order transition system has a bias towards left children of constituents, which is not desirable when the model is used to calculate prefix probabilities. This issue was addressed by using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018) or variable sized beam search (Crabb´e et al., 2019) and successfully predict brain imaging data. However, all of these parsers do not limit the number of stack elements the parser has direct access to at any timestep, which in some cases can be equal to number of derivational decisions made up to the current timestep. This behavior of unbounded stack does not match what we know about human working memory and is undesirable for calculating predictors like probabilistically-weighted emdedding depth (Wu et al., 2010). The model described in this paper avoids these problems b"
2020.iwpt-1.6,P15-1032,0,0.0205625,"ld and hypothesized decision sequences to parsing F-scores, and bˆ is a global running average of F scores of all sampled trees. After the model is trained, the parser uses beam search to find the approximate best parse. A large beam width is desirable because it provides more accurate parses and straightforward ways to calculate psycholinguistic measures like surprisal which requires marginalization. (14) Training and Parsing The proposed model here is a generative model for sequence prediction with no forward context, therefore ideally it should be trained with a structured training scheme (Weiss et al., 2015). However since it is expensive to search a wide beam in training with a neural network, this model uses a two-stage training scheme. The model is first trained to minimize a cross-entropy loss objective with an l2 regularization term, defined by: λ Lθ (w1..T q1..T ) = − log Pθ (w1..T q1..T ) + ||θ||2 2 (15) Q where Pθ (w1..T q1..T ) = t Pθ (wt qt |qt−1 ), and λ is an l2 regularization strength hyper-parameter. Training with the local cross-entropy objective quickly leads to overfitting, because the left-corner parsing decisions can be ambiguous at early parts of the sentence, and the objectiv"
2020.iwpt-1.6,P10-1121,1,0.742255,"sue was addressed by using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018) or variable sized beam search (Crabb´e et al., 2019) and successfully predict brain imaging data. However, all of these parsers do not limit the number of stack elements the parser has direct access to at any timestep, which in some cases can be equal to number of derivational decisions made up to the current timestep. This behavior of unbounded stack does not match what we know about human working memory and is undesirable for calculating predictors like probabilistically-weighted emdedding depth (Wu et al., 2010). The model described in this paper avoids these problems by using a left-corner transition system, which uses a bounded pushdown store and a fixed set of probabilistic decisions per word. The bounded stack memory not only more closely implements human working memory limits in a model designed to calculate cognitive predictors, other work (Jin et al., 2018) shows that it also helps limit search space for unsupervised grammar acquisition. Related work Incremental generative constituent parsers are able to process sentences in time order and provide psycholinguistically predictive measures like"
2020.lrec-1.132,J08-4004,0,0.291135,"ulations. But with three or more quantifiers in an article, their relationships constrain one another, because scopings must be transitive and acyclic. This violates the independence assumptions of the de-facto standard κ statistics (Cohen, 1960; Davies and Fleiss, 1982), which are defined in terms of individual classifications rather than whole scopings, and so it invalidates their models of chance agreement. The granularity mismatch is particularly bad when there are many mutually constraining relationships, as in these multisentence articles. For a more appropriate IAA statistic, we follow Artstein and Poesio (2008) and Skjærholt (2014) and adopt Krippendoff’s α (Hayes and Krippendorff, 2007). Krippendorff’s α defines observed disagreement between two codings of an item in terms of a distance function, and determines expected disagreement by using the same function in an exhaustive permutation test, measuring the distance between codings of different items. Crucially, α is agnostic as to which distance function is employed, as long as it is a metric. 9 Freedom to select a distance metric addresses the problem of IAA over annotations with internal structure, such as scopings. The metric can compare annota"
2020.lrec-1.132,basile-etal-2012-developing,0,0.0564799,"Missing"
2020.lrec-1.132,D11-1111,0,0.0494435,"Missing"
2020.lrec-1.132,W13-0203,0,0.0184769,"overwhelmingly in part–whole relationships, so that a very simple heuristic scoping (Schuler and Wheeler, 2014) rivals both the predictions of a sophisticated machine learning system (Manshadi et al., 2013) and QuanText’s inter-annotator agreement (Manshadi et al., 2012). Though this confirms the value of world knowledge for scope prediction, again it limits chances to generalize from the annotated data. Furthermore, the QuanText sentences have been edited to be understandable out of the blue, which prevents any investigation of text coherence or other discourse influences on interpretation. Evang and Bos (2013) extracted from the Groningen Meaning Bank (Basile et al., 2012) all occurrences of PP modifiers with one of every, each, all quantifying either the modificand or the prepositional object, and annotated the scoping between the modificand and the PP object. They acknowledge that the narrowly selected syntax and the purely binary annotation limit what can be learned and even what can be annotated. Furthermore, finding only 456 examples in a million-word corpus suggests that the GMB’s genres are poorly suited for a scope corpus. The present work thus represents an improvement on previous scope co"
2020.lrec-1.132,J03-1004,0,0.322031,"scope-bearers, including indefinites, definite descriptions, and generics; the first to embrace the complexities added by negation, modals, or sentential adverbs; and the basis of the first attempt to statistically predict quantifier scope over such complex materials (Manshadi and Allen, 2011). In addition to scoping itself, QuanText annotates related phenomena such as collective and distributive readings, partitives, and type/token distinctions (Manshadi et al., 2012). QuanText sentences routinely contain three or more scopebearers. Not every genre shares this tendency. In WSJ, for example, Higgins and Sadock (2003) found a mere 61 sentences with three quantifiers from their list, and 12 sentences with four. But having more than two scopebearers required QuanText to adopt a more complex annotation scheme than previous projects, and this incurred some problems in the methodology for comparing annotations with one another or with machine predictions (see Section 6.1.). The principal objects of the domain—characters, words, lines, and files—are overwhelmingly in part–whole relationships, so that a very simple heuristic scoping (Schuler and Wheeler, 2014) rivals both the predictions of a sophisticated machin"
2020.lrec-1.132,P10-1004,0,0.0282412,"functional programming language such as Haskell or Ocaml as a way to evaluate input statements as goals in some world model. The corpus described in this paper contains a large number of quantifiers and interesting scoping configurations, and is presented specifically as a resource for quantifier scope disambiguation systems, and also more generally as an object of linguistic study. 2. Related Work The semantic task of determining all possible scopal readings of a sentence can be addressed with compositional rules, and the task of identifying the weakest readings can be done algorithmically (Koller and Thater, 2010). However, the pragmatic task of identifying the preferred scoping has not, as yet, been reduced to a general-purpose algorithm.1 To model it statistically requires training data that incorporate the relevant psycholinguistic cues: text coherence (Dwivedi, 2013), linear order of scope-bearers, syntactic structure, choice of quantifiers (as each vs. every), and presumptions of background knowledge (AnderBois et al., 2012) similar to those found in an explanation. 1 Highly successful algorithms are available for certain special cases (Evang and Bos, 2013; Schuler and Wheeler, 2014). VanLehn (197"
2020.lrec-1.132,W11-1108,0,0.027813,"iles. (1) Print every line of the file that starts with a digit followed by punctuation. Sentences were derived from tutorials, help documents, a survey of computer users, and crowdsourced descriptions of example data manipulations (Manshadi et al., 2011). QuanText is the first scope corpus to consider all NP chunks as candidate scope-bearers, including indefinites, definite descriptions, and generics; the first to embrace the complexities added by negation, modals, or sentential adverbs; and the basis of the first attempt to statistically predict quantifier scope over such complex materials (Manshadi and Allen, 2011). In addition to scoping itself, QuanText annotates related phenomena such as collective and distributive readings, partitives, and type/token distinctions (Manshadi et al., 2012). QuanText sentences routinely contain three or more scopebearers. Not every genre shares this tendency. In WSJ, for example, Higgins and Sadock (2003) found a mere 61 sentences with three quantifiers from their list, and 12 sentences with four. But having more than two scopebearers required QuanText to adopt a more complex annotation scheme than previous projects, and this incurred some problems in the methodology fo"
2020.lrec-1.132,P11-2025,0,0.365303,"though without mentioning its quantity. The genre continues to limit the usability of the texts as training data for a general-purpose system. QuanText, by Manshadi et al. (2013), is to our knowledge the most thoroughly developed corpus of scope annotations. It consists of 500 imperative sentences similar to Example (1), giving instructions for manipulating text files. (1) Print every line of the file that starts with a digit followed by punctuation. Sentences were derived from tutorials, help documents, a survey of computer users, and crowdsourced descriptions of example data manipulations (Manshadi et al., 2011). QuanText is the first scope corpus to consider all NP chunks as candidate scope-bearers, including indefinites, definite descriptions, and generics; the first to embrace the complexities added by negation, modals, or sentential adverbs; and the basis of the first attempt to statistically predict quantifier scope over such complex materials (Manshadi and Allen, 2011). In addition to scoping itself, QuanText annotates related phenomena such as collective and distributive readings, partitives, and type/token distinctions (Manshadi et al., 2012). QuanText sentences routinely contain three or mor"
2020.lrec-1.132,manshadi-etal-2012-annotation,0,0.0221647,"ourced descriptions of example data manipulations (Manshadi et al., 2011). QuanText is the first scope corpus to consider all NP chunks as candidate scope-bearers, including indefinites, definite descriptions, and generics; the first to embrace the complexities added by negation, modals, or sentential adverbs; and the basis of the first attempt to statistically predict quantifier scope over such complex materials (Manshadi and Allen, 2011). In addition to scoping itself, QuanText annotates related phenomena such as collective and distributive readings, partitives, and type/token distinctions (Manshadi et al., 2012). QuanText sentences routinely contain three or more scopebearers. Not every genre shares this tendency. In WSJ, for example, Higgins and Sadock (2003) found a mere 61 sentences with three quantifiers from their list, and 12 sentences with four. But having more than two scopebearers required QuanText to adopt a more complex annotation scheme than previous projects, and this incurred some problems in the methodology for comparing annotations with one another or with machine predictions (see Section 6.1.). The principal objects of the domain—characters, words, lines, and files—are overwhelmingly"
2020.lrec-1.132,J93-2004,0,0.0700786,"true) (λy prop fish x)) This allows annotated scope associations to include and interact with negation. 5. Annotation Procedure The annotated data in this corpus is drawn from encyclopedia articles in a 2014 dump of Simple English Wikipedia. The selected articles are those whose title appears most frequently in the full text of the dump, plus a random sample Hand-corrected Automatic Syntactic Annotation Prior to semantic annotation, the corpus is automatically segmented and parsed, using the Petrov and Klein (2007) parser trained on the Nguyen et al. (2012) reannotation of the Penn Treebank (Marcus et al., 1993), into a generalized categorial grammar markup. This markup distinguishes composition operations for arguments, modifiers and various non-local constructions such as filler-gap constructions, each of which selectively constrains restrictors or nuclear scopes of quantifiers, depending on the operation. In general, meanings of modifier predicates are applied to restrictors of quantifiers associated with modificands, and meanings of non-modifier predicates are applied to nuclear scopes of quantifiers associated with arguments. These marked-up operations are then used to define a set of elementary"
2020.lrec-1.132,C12-1130,1,0.789161,"Missing"
2020.lrec-1.132,N07-1051,0,0.0097263,"estrictor or nuclear scope: (22) Amphibians are not fish. gen (λ x prop amphibian x) (λ x count= 0 (λy true) (λy prop fish x)) This allows annotated scope associations to include and interact with negation. 5. Annotation Procedure The annotated data in this corpus is drawn from encyclopedia articles in a 2014 dump of Simple English Wikipedia. The selected articles are those whose title appears most frequently in the full text of the dump, plus a random sample Hand-corrected Automatic Syntactic Annotation Prior to semantic annotation, the corpus is automatically segmented and parsed, using the Petrov and Klein (2007) parser trained on the Nguyen et al. (2012) reannotation of the Penn Treebank (Marcus et al., 1993), into a generalized categorial grammar markup. This markup distinguishes composition operations for arguments, modifiers and various non-local constructions such as filler-gap constructions, each of which selectively constrains restrictors or nuclear scopes of quantifiers, depending on the operation. In general, meanings of modifier predicates are applied to restrictors of quantifiers associated with modificands, and meanings of non-modifier predicates are applied to nuclear scopes of quantifier"
2020.lrec-1.132,S14-1018,1,0.83416,"every genre shares this tendency. In WSJ, for example, Higgins and Sadock (2003) found a mere 61 sentences with three quantifiers from their list, and 12 sentences with four. But having more than two scopebearers required QuanText to adopt a more complex annotation scheme than previous projects, and this incurred some problems in the methodology for comparing annotations with one another or with machine predictions (see Section 6.1.). The principal objects of the domain—characters, words, lines, and files—are overwhelmingly in part–whole relationships, so that a very simple heuristic scoping (Schuler and Wheeler, 2014) rivals both the predictions of a sophisticated machine learning system (Manshadi et al., 2013) and QuanText’s inter-annotator agreement (Manshadi et al., 2012). Though this confirms the value of world knowledge for scope prediction, again it limits chances to generalize from the annotated data. Furthermore, the QuanText sentences have been edited to be understandable out of the blue, which prevents any investigation of text coherence or other discourse influences on interpretation. Evang and Bos (2013) extracted from the Groningen Meaning Bank (Basile et al., 2012) all occurrences of PP modif"
2020.lrec-1.132,P14-1088,0,0.0159053,"e quantifiers in an article, their relationships constrain one another, because scopings must be transitive and acyclic. This violates the independence assumptions of the de-facto standard κ statistics (Cohen, 1960; Davies and Fleiss, 1982), which are defined in terms of individual classifications rather than whole scopings, and so it invalidates their models of chance agreement. The granularity mismatch is particularly bad when there are many mutually constraining relationships, as in these multisentence articles. For a more appropriate IAA statistic, we follow Artstein and Poesio (2008) and Skjærholt (2014) and adopt Krippendoff’s α (Hayes and Krippendorff, 2007). Krippendorff’s α defines observed disagreement between two codings of an item in terms of a distance function, and determines expected disagreement by using the same function in an exhaustive permutation test, measuring the distance between codings of different items. Crucially, α is agnostic as to which distance function is employed, as long as it is a metric. 9 Freedom to select a distance metric addresses the problem of IAA over annotations with internal structure, such as scopings. The metric can compare annotations at the proper g"
2020.lrec-1.132,D09-1152,0,0.0685692,"Missing"
2021.acl-long.290,P15-1033,0,0.0218555,"WebText dataset (∼8M web documents). In addition, three incremental parsing models were used to calculate surprisal estimates: • RNNGSurp (Hale et al., 2018; Dyer et al., 2016): An LSTM-based model with explicit phrase structure, trained on Sections 02 to 21 of the WSJ corpus. • vSLCSurp (van Schijndel et al., 2013): A leftcorner parser based on a PCFG with subcategorized syntactic categories (Petrov et al., 2006), trained on a generalized categorial grammar reannotation of Sections 02 to 21 of the WSJ corpus. • JLCSurp (Jin and Schuler, 2020): A neural leftcorner parser based on stack LSTMs (Dyer et al., 2015), trained on Sections 02 to 21 of the WSJ corpus. 5.2 Procedures The set of self-paced reading times from the Natural Stories Corpus after applying the same data exclusion criteria as Experiment 1 provided the response variable for the regression models. In addition to the full dataset, regression models were 6 Please refer to the appendix for surprisal calculation, outof-vocabulary handling, and re-initialization procedures. also fitted to a ‘no out-of-vocabulary (No-OOV)’ version of the dataset, in which observations corresponding to out-of-vocabulary words for the LSTM language model with t"
2021.acl-long.290,N16-1024,0,0.0255895,"nguage models were used to calculate surprisal estimates at each word.6 • GLSTMSurp (Gulordava et al., 2018): A twolayer LSTM model trained on ∼80M tokens of the English Wikipedia. • JLSTMSurp (Jozefowicz et al., 2016): A twolayer LSTM model with CNN character inputs trained on ∼800M tokens of the 1B Word Benchmark (Chelba et al., 2014). • GPT2Surp (Radford et al., 2019): GPT-2 XL, a 48-layer decoder-only transformer model trained on the WebText dataset (∼8M web documents). In addition, three incremental parsing models were used to calculate surprisal estimates: • RNNGSurp (Hale et al., 2018; Dyer et al., 2016): An LSTM-based model with explicit phrase structure, trained on Sections 02 to 21 of the WSJ corpus. • vSLCSurp (van Schijndel et al., 2013): A leftcorner parser based on a PCFG with subcategorized syntactic categories (Petrov et al., 2006), trained on a generalized categorial grammar reannotation of Sections 02 to 21 of the WSJ corpus. • JLCSurp (Jin and Schuler, 2020): A neural leftcorner parser based on stack LSTMs (Dyer et al., 2015), trained on Sections 02 to 21 of the WSJ corpus. 5.2 Procedures The set of self-paced reading times from the Natural Stories Corpus after applying the same d"
2021.acl-long.290,L18-1012,0,0.434112,"periment 1: Effect of Character Model In order to assess the influence of the characterbased word generation model over the baseline word generation model on the predictive quality of surprisal estimates, linear mixed-effects models containing common baseline predictors and one or more surprisal predictors were fitted to self-paced reading times. Subsequently, a series of likelihood ratio tests were conducted in order to evaluate the relative contribution of each surprisal predictor to regression model fit. 3749 4.1 The first experiment described in this paper used the Natural Stories Corpus (Futrell et al., 2018), which contains self-paced reading times from 181 subjects that read 10 naturalistic stories consisting of 10,245 tokens. The data were filtered to exclude observations corresponding to sentenceinitial and sentence-final words, observations from subjects who answered fewer than four comprehension questions correctly, and observations with durations shorter than 100 ms or longer than 3000 ms. This resulted in a total of 768,584 observations, which were subsequently partitioned into an exploratory set of 383,906 observations and a held-out set of 384,678 observations. The partitioning allows mo"
2021.acl-long.290,N19-1004,0,0.0224579,"Missing"
2021.acl-long.290,W18-0102,0,0.182684,"models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought. 1 Introduction and Related Work Expectation-based theories of sentence processing (Hale, 2001; Levy, 2008) posit that processing difficulty is determined by predictability in context. In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013). However, surprisal itself makes no representational assumptions about sentence processing, leaving open the question of how best to estimate its underlying probability model. In natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017). Character models have been shown not only to alleviate problems with out-of-vocabulary words but also to embody morphological information available at the subword level. For this reason, th"
2021.acl-long.290,N18-1108,0,0.259999,"ted against surprisal predictors calculated from a number of other large-scale pretrained language models and smaller parser-based models. To compare the predictive power of surprisal estimates from different language models on equal footing, we calculated the increase in loglikelihood (∆LL) to a baseline regression model as a result of including a surprisal predictor, following recent work (Goodkind and Bicknell, 2018; Hao et al., 2020). 5.1 Surprisal Estimates from Other Models A total of three pretrained language models were used to calculate surprisal estimates at each word.6 • GLSTMSurp (Gulordava et al., 2018): A twolayer LSTM model trained on ∼80M tokens of the English Wikipedia. • JLSTMSurp (Jozefowicz et al., 2016): A twolayer LSTM model with CNN character inputs trained on ∼800M tokens of the 1B Word Benchmark (Chelba et al., 2014). • GPT2Surp (Radford et al., 2019): GPT-2 XL, a 48-layer decoder-only transformer model trained on the WebText dataset (∼8M web documents). In addition, three incremental parsing models were used to calculate surprisal estimates: • RNNGSurp (Hale et al., 2018; Dyer et al., 2016): An LSTM-based model with explicit phrase structure, trained on Sections 02 to 21 of the"
2021.acl-long.290,N01-1021,0,0.360917,"neration probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought. 1 Introduction and Related Work Expectation-based theories of sentence processing (Hale, 2001; Levy, 2008) posit that processing difficulty is determined by predictability in context. In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013). However, surprisal itself makes no representational assumptions about sentence processing, leaving open the question of how best to estimate its underlying probability model. In natural language processing (NLP) applications, the use of character models has been"
2021.acl-long.290,P18-1254,0,0.10229,"out-of-vocabulary words but also to embody morphological information available at the subword level. For this reason, they have been extensively used to model morphological processes (Elsner et al., 2019; Kann and Schütze, 2016) or incorporate morphological information into models of syntactic acquisition (Jin et al., 2019). Nonetheless, the use of character models has been slow to catch on in psycholinguistic surprisal estimation, which has recently focused on evaluating largescale language models that make predictions at the word level (e.g. Futrell et al. 2019; Goodkind and Bicknell 2018; Hale et al. 2018; Hao et al. 2020). This raises the question of whether incorporating character-level information into an incremental processing model will result in surprisal estimates that better characterize predictability in context. To answer this question, this paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model.1 The proposed model defines a process of generating a word from an underlying lemma and a morphological rule, which allows the processing model to capture the predictability of a given word form in a fine-grai"
2021.acl-long.290,2020.cmcl-1.10,0,0.133096,"words but also to embody morphological information available at the subword level. For this reason, they have been extensively used to model morphological processes (Elsner et al., 2019; Kann and Schütze, 2016) or incorporate morphological information into models of syntactic acquisition (Jin et al., 2019). Nonetheless, the use of character models has been slow to catch on in psycholinguistic surprisal estimation, which has recently focused on evaluating largescale language models that make predictions at the word level (e.g. Futrell et al. 2019; Goodkind and Bicknell 2018; Hale et al. 2018; Hao et al. 2020). This raises the question of whether incorporating character-level information into an incremental processing model will result in surprisal estimates that better characterize predictability in context. To answer this question, this paper presents a character model that can be used to estimate word generation probabilities in a structural parser-based processing model.1 The proposed model defines a process of generating a word from an underlying lemma and a morphological rule, which allows the processing model to capture the predictability of a given word form in a fine-grained manner. Regres"
2021.acl-long.290,P13-2121,0,0.229396,"Missing"
2021.acl-long.290,P19-1234,1,0.843231,"st to estimate its underlying probability model. In natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017). Character models have been shown not only to alleviate problems with out-of-vocabulary words but also to embody morphological information available at the subword level. For this reason, they have been extensively used to model morphological processes (Elsner et al., 2019; Kann and Schütze, 2016) or incorporate morphological information into models of syntactic acquisition (Jin et al., 2019). Nonetheless, the use of character models has been slow to catch on in psycholinguistic surprisal estimation, which has recently focused on evaluating largescale language models that make predictions at the word level (e.g. Futrell et al. 2019; Goodkind and Bicknell 2018; Hale et al. 2018; Hao et al. 2020). This raises the question of whether incorporating character-level information into an incremental processing model will result in surprisal estimates that better characterize predictability in context. To answer this question, this paper presents a character model that can be used to estim"
2021.acl-long.290,2020.iwpt-1.6,1,0.748631,"019): GPT-2 XL, a 48-layer decoder-only transformer model trained on the WebText dataset (∼8M web documents). In addition, three incremental parsing models were used to calculate surprisal estimates: • RNNGSurp (Hale et al., 2018; Dyer et al., 2016): An LSTM-based model with explicit phrase structure, trained on Sections 02 to 21 of the WSJ corpus. • vSLCSurp (van Schijndel et al., 2013): A leftcorner parser based on a PCFG with subcategorized syntactic categories (Petrov et al., 2006), trained on a generalized categorial grammar reannotation of Sections 02 to 21 of the WSJ corpus. • JLCSurp (Jin and Schuler, 2020): A neural leftcorner parser based on stack LSTMs (Dyer et al., 2015), trained on Sections 02 to 21 of the WSJ corpus. 5.2 Procedures The set of self-paced reading times from the Natural Stories Corpus after applying the same data exclusion criteria as Experiment 1 provided the response variable for the regression models. In addition to the full dataset, regression models were 6 Please refer to the appendix for surprisal calculation, outof-vocabulary handling, and re-initialization procedures. also fitted to a ‘no out-of-vocabulary (No-OOV)’ version of the dataset, in which observations corres"
2021.acl-long.290,W16-2010,0,0.0178264,"lf makes no representational assumptions about sentence processing, leaving open the question of how best to estimate its underlying probability model. In natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017). Character models have been shown not only to alleviate problems with out-of-vocabulary words but also to embody morphological information available at the subword level. For this reason, they have been extensively used to model morphological processes (Elsner et al., 2019; Kann and Schütze, 2016) or incorporate morphological information into models of syntactic acquisition (Jin et al., 2019). Nonetheless, the use of character models has been slow to catch on in psycholinguistic surprisal estimation, which has recently focused on evaluating largescale language models that make predictions at the word level (e.g. Futrell et al. 2019; Goodkind and Bicknell 2018; Hale et al. 2018; Hao et al. 2020). This raises the question of whether incorporating character-level information into an incremental processing model will result in surprisal estimates that better characterize predictability in"
2021.acl-long.290,Q17-1026,0,0.0323269,"dictability in context. In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013). However, surprisal itself makes no representational assumptions about sentence processing, leaving open the question of how best to estimate its underlying probability model. In natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017). Character models have been shown not only to alleviate problems with out-of-vocabulary words but also to embody morphological information available at the subword level. For this reason, they have been extensively used to model morphological processes (Elsner et al., 2019; Kann and Schütze, 2016) or incorporate morphological information into models of syntactic acquisition (Jin et al., 2019). Nonetheless, the use of character models has been slow to catch on in psycholinguistic surprisal estimation, which has recently focused on evaluating largescale language models that make predictions at"
2021.acl-long.290,J10-1001,1,0.742723,"y a bounded amount of working memory, in keeping with experimental observations of human memory limits (Miller and Isard, 1963). The transition model maintains a distribution over possible working memory store states qt at every time step t, each of which consists of a bounded number D of nested derivation fragments adt /bdt . Each derivation fragment spans a part of a derivation tree from some apex node adt lacking a base node bdt yet to come. Previous work has shown that large annotated corpora such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 of such fragments (Schuler et al., 2010). At each time step, a left-corner parsing model generates a new word wt and a new store state qt Thus, the parser creates a hierarchically organized sequence of derivation fragments and joins these fragments up whenever expectations are satisfied. In order to update the store state based on the lexical and grammatical decisions, derivation fragments above the most recent nonterminal node are carried forward, and derivation fragments below it are set to null (⊥):  0 0 0 0   Jadt , bdt = adt−1 , bdt−1 K if d0 &lt; d  D  Y   d0 d0 def P(qt |. . .) = Jat , bt = agt , bgt K if d0 = d     0"
2021.acl-long.290,P16-1162,0,0.0895549,"Missing"
2021.acl-long.290,N19-1413,0,0.0179441,"hat the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought. 1 Introduction and Related Work Expectation-based theories of sentence processing (Hale, 2001; Levy, 2008) posit that processing difficulty is determined by predictability in context. In support of this position, predictability quantified through surprisal has been shown to correlate with behavioral measures of word processing difficulty (Goodkind and Bicknell, 2018; Hale, 2001; Levy, 2008; Shain, 2019; Smith and Levy, 2013). However, surprisal itself makes no representational assumptions about sentence processing, leaving open the question of how best to estimate its underlying probability model. In natural language processing (NLP) applications, the use of character models has been popular for several years (Al-Rfou et al., 2019; Kim et al., 2016; Lee et al., 2017). Character models have been shown not only to alleviate problems with out-of-vocabulary words but also to embody morphological information available at the subword level. For this reason, they have been extensively used to mode"
2021.acl-long.290,J93-2004,0,0.0783728,"ine a fixed number of decisions at every time step and also require only a bounded amount of working memory, in keeping with experimental observations of human memory limits (Miller and Isard, 1963). The transition model maintains a distribution over possible working memory store states qt at every time step t, each of which consists of a bounded number D of nested derivation fragments adt /bdt . Each derivation fragment spans a part of a derivation tree from some apex node adt lacking a base node bdt yet to come. Previous work has shown that large annotated corpora such as the Penn Treebank (Marcus et al., 1993) do not require more than D = 4 of such fragments (Schuler et al., 2010). At each time step, a left-corner parsing model generates a new word wt and a new store state qt Thus, the parser creates a hierarchically organized sequence of derivation fragments and joins these fragments up whenever expectations are satisfied. In order to update the store state based on the lexical and grammatical decisions, derivation fragments above the most recent nonterminal node are carried forward, and derivation fragments below it are set to null (⊥):  0 0 0 0   Jadt , bdt = adt−1 , bdt−1 K if d0 &lt; d  D  Y"
2021.acl-long.290,C12-1130,1,0.853336,"Missing"
2021.acl-long.290,P06-1055,0,0.00776035,"el with CNN character inputs trained on ∼800M tokens of the 1B Word Benchmark (Chelba et al., 2014). • GPT2Surp (Radford et al., 2019): GPT-2 XL, a 48-layer decoder-only transformer model trained on the WebText dataset (∼8M web documents). In addition, three incremental parsing models were used to calculate surprisal estimates: • RNNGSurp (Hale et al., 2018; Dyer et al., 2016): An LSTM-based model with explicit phrase structure, trained on Sections 02 to 21 of the WSJ corpus. • vSLCSurp (van Schijndel et al., 2013): A leftcorner parser based on a PCFG with subcategorized syntactic categories (Petrov et al., 2006), trained on a generalized categorial grammar reannotation of Sections 02 to 21 of the WSJ corpus. • JLCSurp (Jin and Schuler, 2020): A neural leftcorner parser based on stack LSTMs (Dyer et al., 2015), trained on Sections 02 to 21 of the WSJ corpus. 5.2 Procedures The set of self-paced reading times from the Natural Stories Corpus after applying the same data exclusion criteria as Experiment 1 provided the response variable for the regression models. In addition to the full dataset, regression models were 6 Please refer to the appendix for surprisal calculation, outof-vocabulary handling, and"
2021.acl-long.290,D17-1178,0,0.0236887,"Missing"
2021.cmcl-1.28,W16-4102,0,0.0146463,"research has sought to incorporate propositional cated to studying differential patterns of processing content information into various complexity metdifficulty in order to shed light on the latent mecha- rics. A prominent approach in this line of renism behind online processing. As it is now well- search has been to quantify complexity based on established that processing difficulty can be ob- the compatibility between a predicate and its arguserved in behavioral responses (e.g. reading times, ments (i.e. thematic fit, Baroni and Lenci 2010, eye movements, and event-related potentials), re- Chersoni et al. 2016, Padó et al. 2009). Howcent psycholinguistic work has tried to account for ever, these complexity metrics can only be evalthese variables by regressing various predictors of uated at a coarse per-sentence level or at critical interest. Most notably, in support of expectation- regions of constructed stimuli where predicates and based theories of sentence processing (Hale, 2001; arguments are revealed, making them less suitable Levy, 2008), predictability in context has been for studying online processing. A more distribu241 Proceedings of the Workshop on Cognitive Modeling and Computational Li"
2021.cmcl-1.28,L18-1012,0,0.0565557,"rsers used beam search decoding with a beam width of 5,000 to return the most likely sequence of parsing decisions. The unlabeled WSJ bracketing F1 scores from both parsers are presented in the WSJ22 and WSJ23 columns of the vS et al. and Full model rows of Table 1.5 The results show that the two parsers achieve comparable performance on WSJ22 and WSJ23, indicating that the current processing model is a reasonable model of syntactic parsing. 4.2 Cross-Domain Linguistic Accuracy The two parsers were also evaluated on the Natural In order to assess the parsing performance of the Stories Corpus (Futrell et al., 2018). This corpus content-sensitive processing model outlined in Secconsists of 10 naturalistic stories (10,245 tokens) tion 2, a linguistic accuracy evaluation was conadapted from existing texts such as fairy tales and ducted on the development set and test set (i.e. secshort stories. As can be seen in the NS column of tions 22 and 23 respectively) of the Wall Street the vS et al. and Full model rows of Table 1, parsing Journal (WSJ) corpus of the English Penn Treeaccuracy on this corpus is substantially lower. This bank (Marcus et al., 1993). The performance of the is likely due to the “deceptiv"
2021.cmcl-1.28,W18-0102,0,0.026126,"Missing"
2021.cmcl-1.28,N01-1021,0,0.787631,"itional Content and Syntactic Category Information in Sentence Processing Byung-Doh Oh Department of Linguistics The Ohio State University oh.531@osu.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Abstract quantified through the information-theoretical measure of surprisal (Shannon, 1948). Although there Expectation-based theories of sentence prohas been empirical support for n-gram, PCFG, and cessing posit that processing difficulty is deterLSTM surprisal in the literature (Goodkind and mined by predictability in context. While preBicknell, 2018; Hale, 2001; Levy, 2008; Shain, dictability quantified via surprisal has gained 2019; Smith and Levy, 2013), as surprisal makes empirical support, this representation-agnostic minimal assumptions about linguistic representameasure leaves open the question of how to tions that are built during processing, this leaves best approximate the human comprehender’s open the question of how to best estimate the hulatent probability model. This work presents an incremental left-corner parser that incorman language comprehender’s latent probability porates information about both propositional model. content and syn"
2021.cmcl-1.28,P13-2121,0,0.0346641,"Missing"
2021.cmcl-1.28,J93-2004,0,0.0740521,"assess the parsing performance of the Stories Corpus (Futrell et al., 2018). This corpus content-sensitive processing model outlined in Secconsists of 10 naturalistic stories (10,245 tokens) tion 2, a linguistic accuracy evaluation was conadapted from existing texts such as fairy tales and ducted on the development set and test set (i.e. secshort stories. As can be seen in the NS column of tions 22 and 23 respectively) of the Wall Street the vS et al. and Full model rows of Table 1, parsing Journal (WSJ) corpus of the English Penn Treeaccuracy on this corpus is substantially lower. This bank (Marcus et al., 1993). The performance of the is likely due to the “deceptively naturalistic” nacontent-sensitive processing model is compared to ture of the Natural Stories Corpus; this corpus was the incremental left-corner parser of van Schijndel designed to over-represent rare words and syntacet al. (2013), which is based on a PCFG with subtic constructions, therefore representing a different categorized syntactic categories from the Berkeley “syntactic domain” from the WSJ corpus. Interlatent variable inducer (Petrov et al., 2006). estingly, the content-sensitive processing model The content-sensitive process"
2021.cmcl-1.28,P10-1021,0,0.0426676,"l attachment decision `t adt−1 ⇒ d a`t adt−1 bt−1 wt b) grammatical attachment decision gt ⇒ d adt−1 adt−1 bdt−1 bt−1 adt−1 agt bt−1 a`t wt ⇒ d bgt a`t ⇒ bdt−1 adt−1 bdt−1 agt a`t bgt m`t = 1 mgt = 1 m`t = 0 mgt = 0 Figure 1: Left-corner parser operations: a) lexical match (m`t =1) and no-match (m`t =0) operations, creating new apex a`t , and b) grammatical match (mgt =1) and no-match (mgt =0) operations, creating new apex agt and base bgt . tional approach has also been explored that relies on word co-occurrence to calculate the semantic coherence between each word and its preceding context (Mitchell et al., 2010; Sayeed et al., 2015). Although these models allow more fine-grained perword metrics to be calculated, their dependence on an aggregate context vector makes it difficult to distinguish ‘gist’ or topic information from propositional content. Unlike these models, our approach seeks to incorporate propositional content by augmenting a generative and incremental parser to build an ongoing representation of predicate context vectors, which is based on a categorial grammar formalism that captures both local and non-local predicateargument structure. This processing model can be used to estimate per"
2021.cmcl-1.28,C12-1130,1,0.846156,"Missing"
2021.cmcl-1.28,P06-1055,0,0.0540327,"WSJ22 WSJ23 NS 85.20 84.60 81.64 75.63 84.08 82.45 79.86 74.45 69.60 71.64 69.88 64.19 Table 1: Bracketing F1 scores on sentences with 40 or fewer words for the incremental parsing models. WSJ: Wall Street Journal, NS: Natural Stories. parameters, the average performance of the contentsensitive processing model trained using three different random seeds is reported. Likewise, the left-corner parser of van Schijndel et al. (2013) was trained on the same generalized categorial grammar reannotation of sections 02 to 21 of the WSJ corpus, using four iterations of the split-merge-smooth algorithm (Petrov et al., 2006). Both parsers used beam search decoding with a beam width of 5,000 to return the most likely sequence of parsing decisions. The unlabeled WSJ bracketing F1 scores from both parsers are presented in the WSJ22 and WSJ23 columns of the vS et al. and Full model rows of Table 1.5 The results show that the two parsers achieve comparable performance on WSJ22 and WSJ23, indicating that the current processing model is a reasonable model of syntactic parsing. 4.2 Cross-Domain Linguistic Accuracy The two parsers were also evaluated on the Natural In order to assess the parsing performance of the Stories"
2021.cmcl-1.28,P15-1074,0,0.0142945,"t adt−1 ⇒ d a`t adt−1 bt−1 wt b) grammatical attachment decision gt ⇒ d adt−1 adt−1 bdt−1 bt−1 adt−1 agt bt−1 a`t wt ⇒ d bgt a`t ⇒ bdt−1 adt−1 bdt−1 agt a`t bgt m`t = 1 mgt = 1 m`t = 0 mgt = 0 Figure 1: Left-corner parser operations: a) lexical match (m`t =1) and no-match (m`t =0) operations, creating new apex a`t , and b) grammatical match (mgt =1) and no-match (mgt =0) operations, creating new apex agt and base bgt . tional approach has also been explored that relies on word co-occurrence to calculate the semantic coherence between each word and its preceding context (Mitchell et al., 2010; Sayeed et al., 2015). Although these models allow more fine-grained perword metrics to be calculated, their dependence on an aggregate context vector makes it difficult to distinguish ‘gist’ or topic information from propositional content. Unlike these models, our approach seeks to incorporate propositional content by augmenting a generative and incremental parser to build an ongoing representation of predicate context vectors, which is based on a categorial grammar formalism that captures both local and non-local predicateargument structure. This processing model can be used to estimate per-word surprisal predic"
2021.cmcl-1.28,N19-1413,0,0.0163351,"ether as predictors. In order to mitigate this problem, the difference between the surprisal predictors from the ablated model and those from the full model (∆ConSurp, ∆CatSurp) were also calculated as predictors that represent the contribution of the full model over an ablated model. All 7 Although word frequency is also often included as a baseline predictor in the form of unigram surprisal, it was excluded in the current study in light of results showing no significant effect of unigram surprisal over and above 5-gram surprisal when predicting reading times from the Natural Stories Corpus (Shain, 2019). 246 6 predictors were centered and scaled prior to model fitting. 5.3 Likelihood Ratio Testing Two sets of nested linear mixed-effects models were fitted to reading times in the held-out set using using lme4 (Bates et al., 2015). The first set manipulated the contribution of propositional content by including ∆ConSurp in the full regression model over the base model that contains the baseline predictors and NoConSurp. Similarly, the second set manipulated the contribution of syntactic categories by including ∆CatSurp in the full regression model over a base model that contains the baseline p"
2021.findings-emnlp.285,L18-1012,0,0.0219538,"ictors can have varying amplitude and decay over time. This approach therefore allows predictor and response variables to have different temporal granularity. For model details, see Appendix A. For each fMRI experiment, two models are fit which differ minimally by the addition of a fixed effect for the predictor of interest (all models include by-subject random effects for all predictors), and correlation coefficients are calculated between each model’s predictions and the fMRI observations. The difference between correlation coeffiResponse Data SPR data comes from the Natural Stories corpus (Futrell et al., 2018) and consists of reading times from 181 participants that read 10 short narratives on Amazon’s Mechanical Turk platform. Filtering observations of &lt;100ms and &gt;3000ms, sentenceinitial and sentence-final words, and participants who answered fewer than four comprehension questions correctly resulted in 768,584 observations, which were split into fit and held-out partitions (50/50). Because likelihood ratio tests with LMER (Bates et al., 2015) require the same data for fitting and evaluation, this work fits a single regression model on the held-out partition for all SPR results. The fMRI analyses"
2021.findings-emnlp.285,J95-2003,0,0.616379,"om high-capacity neural lanCoreference resolution is a core component of lan- guage models. Linguistic task accuracy for coreference resolution shows improvements from the guage that enables comprehenders to construct extended incremental parser, further motivating its a detailed representation of referents throughout use for psycholinguistic evaluation. a discourse. Extensive prior work has explored various conditions related to coreference resoluRegression analyses are conducted using surtion that affect language processing (Greene et al., prisal estimates from the parser to determine 1992; Grosz et al., 1995; Gordon and Hendrick, whether coreference awareness helps explain mea1998; Almor, 1999; Ariel, 2001; Cunnings et al., sures of human sentence comprehension from SPR. 2014) and have often relied on constructed stim- In addition, we further evaluate whether these efuli to manipulate variables of interest. Comple- fects generalize to data from fMRI. Results from mentary studies using broad-coverage, naturalistic self-paced reading replicate Jaffe et al. (2020) by stimuli have observed coreference effects in self- showing both (1) that coreference awareness impaced reading in two instantiations:"
2021.findings-emnlp.285,P13-2121,0,0.0365035,"Missing"
2021.findings-emnlp.285,W18-0101,1,0.917851,"d on constructed stim- In addition, we further evaluate whether these efuli to manipulate variables of interest. Comple- fects generalize to data from fMRI. Results from mentary studies using broad-coverage, naturalistic self-paced reading replicate Jaffe et al. (2020) by stimuli have observed coreference effects in self- showing both (1) that coreference awareness impaced reading in two instantiations: (1) as a mem- proves the parser’s approximation of human subory effect based on the count of times an entity jective surprisal and (2) that this improvement does has been previously mentioned (Jaffe et al., 2018) not fully explain a previously reported facilitation and (2) as an expectation-based effect, operational- effect from repeated mentions, which is plausiized by surprisal estimates from a coreference- bly driven by ease of memory retrieval. Results aware incremental parser (Jaffe et al., 2020). While from fMRI support a contribution of coreferenceexpectation-based effects have been previously awareness to human surprisal estimation, but fail shown for naturalistic stimuli in self-paced reading to support a dissociable memory effect. Results 3351 Findings of the Association for Computational Li"
2021.findings-emnlp.285,2020.coling-main.404,1,0.679668,"ging (fMRI) (Smith and Levy, 2013; Shain et al., Recent evidence supports a role for corefer2020), the current study extends these findings by ence processing in guiding human expectaarguing that coreference resolution contributes to tions about upcoming words during reading, predicting human behavioral data over previous imbased on covariation between reading times plementations (e.g., surprisal) that do not model and word surprisal estimated by a coreferenceaware semantic processing model (Jaffe et al., coreference. 2020). The present study reproduces and elabThe current study elaborates on Jaffe et al. (2020) orates on this finding by (1) enabling the parser by re-examining the expectation-based effect of to process subword information that might betcoreference information using an improved baseter approximate human morphological knowlline provided by an extension of the coreferenceedge, and (2) extending evaluation of coreference effects from self-paced reading to huaware incremental parser. First, the probabilities man brain imaging data. Results show that an of coreference decisions are modeled using a mulexpectation-based processing effect of corefertilayer perceptron (MLP) model, leading to i"
2021.findings-emnlp.285,C12-1130,1,0.844333,"Missing"
2021.findings-emnlp.285,2021.acl-long.290,1,0.919441,"eading to huaware incremental parser. First, the probabilities man brain imaging data. Results show that an of coreference decisions are modeled using a mulexpectation-based processing effect of corefertilayer perceptron (MLP) model, leading to imence is still evident even in the presence of the proved generalizability over the previous system stronger psycholinguistic baseline provided by based on maximum-entropy. Additionally, the inthe subword model, and that the coreference cremental parser incorporates a character-based effect is observed in both self-paced reading word generation model (Oh et al., 2021), which and fMRI data, providing evidence of the efhas been shown to yield surprisal estimates that fect’s robustness. predict human reading times more accurately than 1 Introduction surprisal calculated from high-capacity neural lanCoreference resolution is a core component of lan- guage models. Linguistic task accuracy for coreference resolution shows improvements from the guage that enables comprehenders to construct extended incremental parser, further motivating its a detailed representation of referents throughout use for psycholinguistic evaluation. a discourse. Extensive prior work has"
2021.findings-emnlp.285,D18-1288,1,0.820761,"lover predictors (Erlich and Rayner, 1983) and likelihood ratio tests between full and ablated models, following prior work for comparability. fMRI studies of naturalistic language comprehension must contend with a slow hemodynamic response function (HRF) that causes effects on the response to spread out over several seconds (Boynton et al., 1996). This low temporal resolution of response data must be reconciled with relatively faster word-level predictors in our models. To accomplish this, the current study follows Shain et al. (2020) by using continuous-time deconvolutional regression (CDR; Shain and Schuler, 2018, 2021) to identify the HRF from fMRI data. CDR models individual predictor response functions and convolves them to generate a continuous prediction of blood oxygenation level-dependent (BOLD) signals as the combination of previous events. Since the effect of a predictor on the response variable is modeled as an impulse function, predictors can have varying amplitude and decay over time. This approach therefore allows predictor and response variables to have different temporal granularity. For model details, see Appendix A. For each fMRI experiment, two models are fit which differ minimally b"
2021.findings-emnlp.371,N19-1116,0,0.0160319,"Liang et al., 2009; port, 2014), providing some evidence against the poverty of the stimulus argument (Chomsky, 1965, 1 Code used in this work is available at https:// 1980) in language acquisition. github.com/lifengjin/charInduction. 4367 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4367–4378 November 7–11, 2021. ©2021 Association for Computational Linguistics Tu, 2012) was not as successful as models of unsupervised constituency parsing (Seginer, 2007; Ponvert et al., 2011). However, recent work from unsupervised parsing (Shen et al., 2019; Wang et al., 2019; Drozdov et al., 2019, 2020) and grammar induction (Jin et al., 2018a, 2019; Kim et al., 2019; Zhu et al., 2020; Jin and Schuler, 2020; Li et al., 2020) shows much improvement over previous results with grammars learned solely from raw text, indicating that statistical regularities relevant to syntactic acquisition can be found in word collocations. For example, Kim et al. (2019) propose a word-based neural compound PCFG induction model for accurate grammar induction on English. Zhu et al. (2020) further extend this compound PCFG induction model to jointly induce lexical dependencies using a lexicalized PCFG. Jin"
2021.findings-emnlp.371,D17-1112,0,0.016521,"PCFG induction models, the proposed language. Furthermore, the average F1 scores are model nonetheless relies on symbolic representavery similar across models compared to the aver- tions of characters, which are abstractions from poage RH scores, which indicates that models that tentially noisy perceptual input. Future work could perform similarly in terms of unlabeled evaluation explore the extent to which syntactic knowledge may produce constituent labels of varying quality. can be acquired from lower-level (e.g. phonemic or 4374 acoustic) input alone by including a word segmentation task (Elsner and Shain, 2017; Shain and Elsner, 2020) for the model. Additionally, recent work in unsupervised grammar induction (Jin and Schuler, 2020; Zhang et al., 2021) has shown that incorporating visual information in the form of images and videos helps learn constituents that denote entities or action. Adopting such grounded approaches can help answer questions about the extent to which the visual scene or other contexts contain relevant information, and eventually about the nature of input required for syntactic acquisition. Noam Chomsky. 1980. On cognitive structures and their development: A reply to Piaget. In"
2021.findings-emnlp.371,N18-1108,0,0.023804,"l to use contextualized word embeddings with subword information to allow morphological cues to influence induction. Unfortunately, the ELMo embeddings (Peters et al., 2018) used in that work are trained with a large number of tokens, which limits the value of this approach for investigating child-like syntactic acquisition. In an attempt to answer a similar question, there has also been recent interest in evaluating whether recurrent neural networks like LSTMs can learn grammar-like representations from word sequence alone. Although some initial results seemed promising (Linzen et al., 2016; Gulordava et al., 2018), later studies have shown that some of these results may not necessarily generalize to languages other than English (Ravfogel et al., 2018; Davis and van Schijndel, 2020), yielding an inconsistent picture. Moreover, studies in this line of research try to model specific syntactic phenomena such as subject-verb agreement, filler-gap dependencies, and auxiliary inversion, and therefore have a slightly different focus from grammar induction. Complementary to this approach are studies that aim to reconstruct syntactic representations from contextualized word representations (Tenney et al., 2019;"
2021.findings-emnlp.371,D18-1292,1,0.910549,"of subword information results in more accurate grammars with coherent syntactic categories that the word-based induction model has difficulty finding. Additionally, this effect is found to be amplified in morphologically richer languages that rely on functional affixes to express grammatical relations. Finally, an evaluation on multilingual treebanks shows that the model with subword information achieves stateof-the-art induction results on many languages, providing further evidence for a distributional model of syntactic acquisition. Unsupervised PCFG induction models (Johnson et al., 2007; Jin et al., 2018b) induce grammars from raw text and use those induced grammars to build linguistically meaningful hierarchical structures for sentences. Recent work on PCFG induction has adopted Bayesian or neural word-based PCFG models (Jin et al., 2018b; Kim et al., 2019; Zhu et al., 2020), which have proven to be accurate at discovering syntactic structures solely from word sequences. These results indicate that a human-like 2 Related Work grammar can be learned from distributional data (Harris, 1954; Saffran et al., 1996; Aslin and New- Early work in unsupervised PCFG induction from raw text (Johnson et"
2021.findings-emnlp.371,Q18-1016,1,0.943168,"of subword information results in more accurate grammars with coherent syntactic categories that the word-based induction model has difficulty finding. Additionally, this effect is found to be amplified in morphologically richer languages that rely on functional affixes to express grammatical relations. Finally, an evaluation on multilingual treebanks shows that the model with subword information achieves stateof-the-art induction results on many languages, providing further evidence for a distributional model of syntactic acquisition. Unsupervised PCFG induction models (Johnson et al., 2007; Jin et al., 2018b) induce grammars from raw text and use those induced grammars to build linguistically meaningful hierarchical structures for sentences. Recent work on PCFG induction has adopted Bayesian or neural word-based PCFG models (Jin et al., 2018b; Kim et al., 2019; Zhu et al., 2020), which have proven to be accurate at discovering syntactic structures solely from word sequences. These results indicate that a human-like 2 Related Work grammar can be learned from distributional data (Harris, 1954; Saffran et al., 1996; Aslin and New- Early work in unsupervised PCFG induction from raw text (Johnson et"
2021.findings-emnlp.371,P19-1234,1,0.860972,"2019, 2020) and grammar induction (Jin et al., 2018a, 2019; Kim et al., 2019; Zhu et al., 2020; Jin and Schuler, 2020; Li et al., 2020) shows much improvement over previous results with grammars learned solely from raw text, indicating that statistical regularities relevant to syntactic acquisition can be found in word collocations. For example, Kim et al. (2019) propose a word-based neural compound PCFG induction model for accurate grammar induction on English. Zhu et al. (2020) further extend this compound PCFG induction model to jointly induce lexical dependencies using a lexicalized PCFG. Jin et al. (2019) augment a PCFG induction model to use contextualized word embeddings with subword information to allow morphological cues to influence induction. Unfortunately, the ELMo embeddings (Peters et al., 2018) used in that work are trained with a large number of tokens, which limits the value of this approach for investigating child-like syntactic acquisition. In an attempt to answer a similar question, there has also been recent interest in evaluating whether recurrent neural networks like LSTMs can learn grammar-like representations from word sequence alone. Although some initial results seemed pr"
2021.findings-emnlp.371,2020.aacl-main.42,1,0.871171,"1965, 1 Code used in this work is available at https:// 1980) in language acquisition. github.com/lifengjin/charInduction. 4367 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4367–4378 November 7–11, 2021. ©2021 Association for Computational Linguistics Tu, 2012) was not as successful as models of unsupervised constituency parsing (Seginer, 2007; Ponvert et al., 2011). However, recent work from unsupervised parsing (Shen et al., 2019; Wang et al., 2019; Drozdov et al., 2019, 2020) and grammar induction (Jin et al., 2018a, 2019; Kim et al., 2019; Zhu et al., 2020; Jin and Schuler, 2020; Li et al., 2020) shows much improvement over previous results with grammars learned solely from raw text, indicating that statistical regularities relevant to syntactic acquisition can be found in word collocations. For example, Kim et al. (2019) propose a word-based neural compound PCFG induction model for accurate grammar induction on English. Zhu et al. (2020) further extend this compound PCFG induction model to jointly induce lexical dependencies using a lexicalized PCFG. Jin et al. (2019) augment a PCFG induction model to use contextualized word embeddings with subword information to al"
2021.findings-emnlp.371,N07-1018,0,0.0739143,"hat the incorporation of subword information results in more accurate grammars with coherent syntactic categories that the word-based induction model has difficulty finding. Additionally, this effect is found to be amplified in morphologically richer languages that rely on functional affixes to express grammatical relations. Finally, an evaluation on multilingual treebanks shows that the model with subword information achieves stateof-the-art induction results on many languages, providing further evidence for a distributional model of syntactic acquisition. Unsupervised PCFG induction models (Johnson et al., 2007; Jin et al., 2018b) induce grammars from raw text and use those induced grammars to build linguistically meaningful hierarchical structures for sentences. Recent work on PCFG induction has adopted Bayesian or neural word-based PCFG models (Jin et al., 2018b; Kim et al., 2019; Zhu et al., 2020), which have proven to be accurate at discovering syntactic structures solely from word sequences. These results indicate that a human-like 2 Related Work grammar can be learned from distributional data (Harris, 1954; Saffran et al., 1996; Aslin and New- Early work in unsupervised PCFG induction from raw"
2021.findings-emnlp.371,P19-1228,0,0.0394603,"Missing"
2021.findings-emnlp.371,P19-1340,0,0.13505,"ovided by Pearl and Sprouse (2013). The Korean corpus is from the Jong section (Ryu et al., 2015), which contains transcriptions of interactions between Jong and his caregivers recorded at ages from 1 year 3 months to 3 years 5 months. There were 28,620 sentences that were uttered by caregivers, which had a mean sentence length of 5.0 words. As there are no gold syntactic annotations available for this dataset, a subset of 150 sentences was annotated in order to evaluate the two models. This was done by first automatically generating silver parses using the state-of-the-art supervised parser (Kitaev et al., 2019) and subsequently correcting them according to the annotation scheme of Choi (2013).4 The NeuralChar and NeuralWord models were trained on these two corpora ten times with different random seeds, using 90 nonterminal categories and other hyperparameters tuned on the Brown Corpus portion of the Penn Treebank (see Appendix A).5 Following previous work (Seginer, 2007), punctuation marks were left in the input data 4 3 Homogeneity is positively correlated with the number of categories used, achieving a perfect score when each constituent is assigned a unique category. However, a grammar with tens"
2021.findings-emnlp.371,2020.acl-main.300,0,0.0229186,"is work is available at https:// 1980) in language acquisition. github.com/lifengjin/charInduction. 4367 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4367–4378 November 7–11, 2021. ©2021 Association for Computational Linguistics Tu, 2012) was not as successful as models of unsupervised constituency parsing (Seginer, 2007; Ponvert et al., 2011). However, recent work from unsupervised parsing (Shen et al., 2019; Wang et al., 2019; Drozdov et al., 2019, 2020) and grammar induction (Jin et al., 2018a, 2019; Kim et al., 2019; Zhu et al., 2020; Jin and Schuler, 2020; Li et al., 2020) shows much improvement over previous results with grammars learned solely from raw text, indicating that statistical regularities relevant to syntactic acquisition can be found in word collocations. For example, Kim et al. (2019) propose a word-based neural compound PCFG induction model for accurate grammar induction on English. Zhu et al. (2020) further extend this compound PCFG induction model to jointly induce lexical dependencies using a lexicalized PCFG. Jin et al. (2019) augment a PCFG induction model to use contextualized word embeddings with subword information to allow morphological"
2021.findings-emnlp.371,P09-1011,0,0.0232125,"rammars from raw text and use those induced grammars to build linguistically meaningful hierarchical structures for sentences. Recent work on PCFG induction has adopted Bayesian or neural word-based PCFG models (Jin et al., 2018b; Kim et al., 2019; Zhu et al., 2020), which have proven to be accurate at discovering syntactic structures solely from word sequences. These results indicate that a human-like 2 Related Work grammar can be learned from distributional data (Harris, 1954; Saffran et al., 1996; Aslin and New- Early work in unsupervised PCFG induction from raw text (Johnson et al., 2007; Liang et al., 2009; port, 2014), providing some evidence against the poverty of the stimulus argument (Chomsky, 1965, 1 Code used in this work is available at https:// 1980) in language acquisition. github.com/lifengjin/charInduction. 4367 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4367–4378 November 7–11, 2021. ©2021 Association for Computational Linguistics Tu, 2012) was not as successful as models of unsupervised constituency parsing (Seginer, 2007; Ponvert et al., 2011). However, recent work from unsupervised parsing (Shen et al., 2019; Wang et al., 2019; Drozdov et al., 20"
2021.findings-emnlp.371,Q16-1037,0,0.0333176,"a PCFG induction model to use contextualized word embeddings with subword information to allow morphological cues to influence induction. Unfortunately, the ELMo embeddings (Peters et al., 2018) used in that work are trained with a large number of tokens, which limits the value of this approach for investigating child-like syntactic acquisition. In an attempt to answer a similar question, there has also been recent interest in evaluating whether recurrent neural networks like LSTMs can learn grammar-like representations from word sequence alone. Although some initial results seemed promising (Linzen et al., 2016; Gulordava et al., 2018), later studies have shown that some of these results may not necessarily generalize to languages other than English (Ravfogel et al., 2018; Davis and van Schijndel, 2020), yielding an inconsistent picture. Moreover, studies in this line of research try to model specific syntactic phenomena such as subject-verb agreement, filler-gap dependencies, and auxiliary inversion, and therefore have a slightly different focus from grammar induction. Complementary to this approach are studies that aim to reconstruct syntactic representations from contextualized word representatio"
2021.findings-emnlp.371,J93-2004,0,0.0753333,"Missing"
2021.findings-emnlp.371,W09-3035,0,0.0355713,"Missing"
2021.findings-emnlp.371,N18-1202,0,0.0199241,"ed solely from raw text, indicating that statistical regularities relevant to syntactic acquisition can be found in word collocations. For example, Kim et al. (2019) propose a word-based neural compound PCFG induction model for accurate grammar induction on English. Zhu et al. (2020) further extend this compound PCFG induction model to jointly induce lexical dependencies using a lexicalized PCFG. Jin et al. (2019) augment a PCFG induction model to use contextualized word embeddings with subword information to allow morphological cues to influence induction. Unfortunately, the ELMo embeddings (Peters et al., 2018) used in that work are trained with a large number of tokens, which limits the value of this approach for investigating child-like syntactic acquisition. In an attempt to answer a similar question, there has also been recent interest in evaluating whether recurrent neural networks like LSTMs can learn grammar-like representations from word sequence alone. Although some initial results seemed promising (Linzen et al., 2016; Gulordava et al., 2018), later studies have shown that some of these results may not necessarily generalize to languages other than English (Ravfogel et al., 2018; Davis and"
2021.findings-emnlp.371,P11-1108,0,0.0880013,"Missing"
2021.findings-emnlp.371,W18-5412,0,0.0201067,"embeddings (Peters et al., 2018) used in that work are trained with a large number of tokens, which limits the value of this approach for investigating child-like syntactic acquisition. In an attempt to answer a similar question, there has also been recent interest in evaluating whether recurrent neural networks like LSTMs can learn grammar-like representations from word sequence alone. Although some initial results seemed promising (Linzen et al., 2016; Gulordava et al., 2018), later studies have shown that some of these results may not necessarily generalize to languages other than English (Ravfogel et al., 2018; Davis and van Schijndel, 2020), yielding an inconsistent picture. Moreover, studies in this line of research try to model specific syntactic phenomena such as subject-verb agreement, filler-gap dependencies, and auxiliary inversion, and therefore have a slightly different focus from grammar induction. Complementary to this approach are studies that aim to reconstruct syntactic representations from contextualized word representations (Tenney et al., 2019; Hewitt and Manning, 2019). Again, however, the use of neural language models assumes access to much more data than the input available to t"
2021.findings-emnlp.371,D07-1043,0,0.0602938,"NeuralWord models on child-directed speech corpora, which represent authentic linguistic input that child learners are exposed to. 4.1 Evaluation Metric: Recall-Homogeneity It has been argued that the ability to predict constituent labels that are consistent with linguistic annotation is a crucial part of a grammar and therefore should be accounted for in evaluation (Jin et al., 2019). This work uses Recall-Homogeneity (RH, Jin et al., 2021) as a labeled evaluation metric, which is calculated by multiplying unlabeled recall of bracketed spans in the predicted trees with the homogeneity score (Rosenberg and Hirschberg, 2007) of the predicted labels of the matching spans. Similarly to Recall-V-Measure (RVM, Jin et al., 2019), this metric is made insensitive to the branching factor of the grammar through the use of unlabeled recall. However, the use of homogeneity rather than V-measure assumes that the annotators’ decision to suppress the annotation of in-depth information such as case or subcategorization in category labels is motivated by expediency rather than linguistic theory. Therefore, RH does not penalize induced grammars for the use of categories to make more fine-grained distinctions, to the extent that i"
2021.findings-emnlp.371,P07-1049,0,0.182934,", 1954; Saffran et al., 1996; Aslin and New- Early work in unsupervised PCFG induction from raw text (Johnson et al., 2007; Liang et al., 2009; port, 2014), providing some evidence against the poverty of the stimulus argument (Chomsky, 1965, 1 Code used in this work is available at https:// 1980) in language acquisition. github.com/lifengjin/charInduction. 4367 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4367–4378 November 7–11, 2021. ©2021 Association for Computational Linguistics Tu, 2012) was not as successful as models of unsupervised constituency parsing (Seginer, 2007; Ponvert et al., 2011). However, recent work from unsupervised parsing (Shen et al., 2019; Wang et al., 2019; Drozdov et al., 2019, 2020) and grammar induction (Jin et al., 2018a, 2019; Kim et al., 2019; Zhu et al., 2020; Jin and Schuler, 2020; Li et al., 2020) shows much improvement over previous results with grammars learned solely from raw text, indicating that statistical regularities relevant to syntactic acquisition can be found in word collocations. For example, Kim et al. (2019) propose a word-based neural compound PCFG induction model for accurate grammar induction on English. Zhu et"
2021.findings-emnlp.371,2020.conll-1.15,0,0.0359885,"the proposed language. Furthermore, the average F1 scores are model nonetheless relies on symbolic representavery similar across models compared to the aver- tions of characters, which are abstractions from poage RH scores, which indicates that models that tentially noisy perceptual input. Future work could perform similarly in terms of unlabeled evaluation explore the extent to which syntactic knowledge may produce constituent labels of varying quality. can be acquired from lower-level (e.g. phonemic or 4374 acoustic) input alone by including a word segmentation task (Elsner and Shain, 2017; Shain and Elsner, 2020) for the model. Additionally, recent work in unsupervised grammar induction (Jin and Schuler, 2020; Zhang et al., 2021) has shown that incorporating visual information in the form of images and videos helps learn constituents that denote entities or action. Adopting such grounded approaches can help answer questions about the extent to which the visual scene or other contexts contain relevant information, and eventually about the nature of input required for syntactic acquisition. Noam Chomsky. 1980. On cognitive structures and their development: A reply to Piaget. In Language and learning: Th"
2021.findings-emnlp.371,D19-1098,0,0.0117806,"nson et al., 2007; Liang et al., 2009; port, 2014), providing some evidence against the poverty of the stimulus argument (Chomsky, 1965, 1 Code used in this work is available at https:// 1980) in language acquisition. github.com/lifengjin/charInduction. 4367 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4367–4378 November 7–11, 2021. ©2021 Association for Computational Linguistics Tu, 2012) was not as successful as models of unsupervised constituency parsing (Seginer, 2007; Ponvert et al., 2011). However, recent work from unsupervised parsing (Shen et al., 2019; Wang et al., 2019; Drozdov et al., 2019, 2020) and grammar induction (Jin et al., 2018a, 2019; Kim et al., 2019; Zhu et al., 2020; Jin and Schuler, 2020; Li et al., 2020) shows much improvement over previous results with grammars learned solely from raw text, indicating that statistical regularities relevant to syntactic acquisition can be found in word collocations. For example, Kim et al. (2019) propose a word-based neural compound PCFG induction model for accurate grammar induction on English. Zhu et al. (2020) further extend this compound PCFG induction model to jointly induce lexical dependencies using a"
2021.findings-emnlp.371,L18-1289,0,0.0466435,"Missing"
2021.findings-emnlp.371,xia-etal-2000-developing,0,0.0455089,"Missing"
2021.findings-emnlp.371,2021.naacl-main.119,1,0.751742,"cross models compared to the aver- tions of characters, which are abstractions from poage RH scores, which indicates that models that tentially noisy perceptual input. Future work could perform similarly in terms of unlabeled evaluation explore the extent to which syntactic knowledge may produce constituent labels of varying quality. can be acquired from lower-level (e.g. phonemic or 4374 acoustic) input alone by including a word segmentation task (Elsner and Shain, 2017; Shain and Elsner, 2020) for the model. Additionally, recent work in unsupervised grammar induction (Jin and Schuler, 2020; Zhang et al., 2021) has shown that incorporating visual information in the form of images and videos helps learn constituents that denote entities or action. Adopting such grounded approaches can help answer questions about the extent to which the visual scene or other contexts contain relevant information, and eventually about the nature of input required for syntactic acquisition. Noam Chomsky. 1980. On cognitive structures and their development: A reply to Piaget. In Language and learning: The debate between Jean Piaget and Noam Chomsky, pages 751–755. Harvard University Press, Cambridge, MA. Forrest Davis an"
2021.findings-emnlp.371,2020.tacl-1.42,0,0.336781,"press grammatical relations. Finally, an evaluation on multilingual treebanks shows that the model with subword information achieves stateof-the-art induction results on many languages, providing further evidence for a distributional model of syntactic acquisition. Unsupervised PCFG induction models (Johnson et al., 2007; Jin et al., 2018b) induce grammars from raw text and use those induced grammars to build linguistically meaningful hierarchical structures for sentences. Recent work on PCFG induction has adopted Bayesian or neural word-based PCFG models (Jin et al., 2018b; Kim et al., 2019; Zhu et al., 2020), which have proven to be accurate at discovering syntactic structures solely from word sequences. These results indicate that a human-like 2 Related Work grammar can be learned from distributional data (Harris, 1954; Saffran et al., 1996; Aslin and New- Early work in unsupervised PCFG induction from raw text (Johnson et al., 2007; Liang et al., 2009; port, 2014), providing some evidence against the poverty of the stimulus argument (Chomsky, 1965, 1 Code used in this work is available at https:// 1980) in language acquisition. github.com/lifengjin/charInduction. 4367 Findings of the Associatio"
2021.findings-emnlp.371,W10-1401,0,0.0577901,"Missing"
C02-1024,P89-1018,0,0.0327203,"constituents a1 : : : ak each cover smaller spans than those in the consequent x, the algorithm will not enter into an in nite recursion; and since there are only n2 N di erent values of x, and only 2n di erent rules that could prove any consequent x (two rule forms for = and , each with n di erent values3of k), the algorithm runs in polynomial time: (n N ). The resulting chart can then be annotated with back pointers to produce a polynomial-sized shared forest  j  ; ; j n O 1 Following 2 Following Shieber et al. (1995). Goodman (1999). j j representation of all possible grammatical trees (Billot and Lang, 1989). Traditional corpus-based parsers select preferred trees from such forests by calculating Viterbi scores for each proposed constituent, according to the recursive function: ! k Y SV (ai ) P(a1 ::: ak x) SV (x) = max 1 a ::: a s:t: 1 k a ::: ak x  i=1 j These scores can be calculated in polynomial time, using the same dynamic programming algorithm as that described for parsing. A tree can then be selected, from the top down, by expanding the highestscoring rule application for each constituent. The environment-based parser described here uses a similar mechanism to select preferred trees, but"
C02-1024,J83-2002,0,0.13124,"s) in this analysis, conjunctions can now safely be assigned the 8familiar truth-functional denotations in every case. Also, since the resulting constituent has the same number of components as the conjoined constituents, there is nothing to prevent its use as an argument in subsequent conjunction operations. A sample multi-component analysis for quanti ers is shown below, allowing material to be duplicated both to the left and to the right of a conjoined NP: some,all,no,etc. : X NPq NPq NPq NPq =NP X=NPq NPq NPq NPq =NP The lexical entry for a quanti er can be split in this n  n   n  7 Dahl and McCord (1983) propose a similar duplication mechanism to produce appropriate semantic representations for NP and other conjunctions, but for di erent reasons. 8 e.g. for the word `and': fh:::TRUE; :::TRUE; :::TRUEi; h::TRUE; ::FALSE; ::FALSEi; h::FALSE; ::TRUE; ::FALSEi; h::FALSE; ::FALSE; ::FALSEig way into a number of components, the last (or lowest) of which is not duplicated in conjunction while others may or may not be. These include a component for the quanti er NPq =NP (which will ultimately also contain a noun phrase restrictor of category NP), a component for restrictor PPs and relative clauses"
C02-1024,P94-1016,0,0.208185,"x k ; a ::: ak x on fhig in which R(x) is a lexical relation de ned for each axiom x of category equal to some subset 4of 's worst-case denotation W ( ), as de ned above. The operator on is natural (relational) join on the elds of its operands: Ao n B = e1:::emax(a;b) e1:::ea A; e1:::eb B where a; b 0; and  is a projection that removes the rst element of the result (corresponding the most recently discharged argument of the head or functor category): A = e2 :::ea e1:::ea A This interleaving of semantic evaluation and parsing for the purpose of disambiguation has much in common with that of Dowding et al. (1994), except fh i j h i2 h i2 g  fh i j h i2 g 3 Here, the score is simply equal to the number of nonempty constituents in an analysis, but other metrics are possible. 4 So a lexical relation for the constituent `lemon' of category NP would contain all and only the lemons in the environment, and a lexical relation for the constituent `falling' of category SnNP would contain a mapping from every entity in the environment to some truth value (TRUE if that entity is falling, FALSE otherwise): e.g. fhlemon1 ; T RU E i; hlemon2 ; F ALS E i; : : : g. NP[lemon] fl1 g [ ; PP:NPnNP[in] fhl1 ; l1 ig NP[lem"
C02-1024,J99-4004,0,0.0204255,"the same x constituent.2 Since the indices in every rule's antecedent constituents a1 : : : ak each cover smaller spans than those in the consequent x, the algorithm will not enter into an in nite recursion; and since there are only n2 N di erent values of x, and only 2n di erent rules that could prove any consequent x (two rule forms for = and , each with n di erent values3of k), the algorithm runs in polynomial time: (n N ). The resulting chart can then be annotated with back pointers to produce a polynomial-sized shared forest  j  ; ; j n O 1 Following 2 Following Shieber et al. (1995). Goodman (1999). j j representation of all possible grammatical trees (Billot and Lang, 1989). Traditional corpus-based parsers select preferred trees from such forests by calculating Viterbi scores for each proposed constituent, according to the recursive function: ! k Y SV (ai ) P(a1 ::: ak x) SV (x) = max 1 a ::: a s:t: 1 k a ::: ak x  i=1 j These scores can be calculated in polynomial time, using the same dynamic programming algorithm as that described for parsing. A tree can then be selected, from the top down, by expanding the highestscoring rule application for each constituent. The environment-based"
C02-1024,P01-1061,1,0.928607,"users not only answer questions but also ask questions and give instructions, is currently limited by the inadequacy of existing corpus-based disambiguation techniques. This paper explores the use of semantic and pragmatic information, in the form of the entities and relations in the interfaced application's run-time environment, as an additional source of information to guide disambiguation. In particular, this paper extends an existing parsing algorithm that calculates and compares the denotations of rival parse tree constituents in order to resolve structural ambiguity in input sentences (Schuler, 2001). The algorithm is extended to incorporate a full set of logical operators into this calculation so as to improve the accuracy of the resulting denotations { and thereby improve the accuracy of parsing { without increasing the complexity of the overall algorithm beyond polynomial time (both in terms of the length of the input and the number of entities in the environment model). This parsimony is achieved by localizing certain kinds of semantic relations during parsing, particularly those between quanti ers and their restrictor and body arguments  The author would like to thank David Chiang,"
C08-1072,P06-1021,0,0.344521,"epair has focused on acoustic cues such as pauses and prosodic contours for detecting repairs, which could then be excised from the text for improved transcription. Recent work has also looked at the possible contribution of higher-level cues, including syntactic structure, in the detection of speech repair. Some of this work is inspired by Levelt’s (1983) investigation of the syntactic and semantic variables in speech repairs, particularly his well-formedness rule, which states that the reparandum and alteration of a repair typically have the same consitituent label, similar to coordination. Hale et al. (2006) use Levelt’s well-formedness rule to justify an annotation scheme where unfinished categories (marked X-UNF) have the UNF label appended to all ancestral category labels all the way up to the top-most constituent beneath an EDITED label (EDITED labels denoting a reparandum). They reason that this should prevent grammar rules of finished constituents in the corpus from corrupting the grammar of unfinished constituents. While this annotation proves helpful, it also leads to the unfortunate result that a large reparandum requires several special repair rules to be applied, even though the actual"
C08-1072,P98-1101,0,0.610877,"orm rewrites syntax trees, turning all right branching structure into left branching structure, and leaving left branching structure as is. As a result, constituent structure can be explicitly built from the bottom up during incremental recognition. This arrangement is well-suited to recognition of speech with errors, because it allows constituent structure to be built up using fluent speech rules until the moment of interruption, at which point a special repair rule may be applied. Before transforming the trees in the grammar into right-corner trees, trees are binarized in the same manner as Johnson (1998b) and Klein and Manning (2003).2 Binarized trees are then transformed into right-corner trees using transform rules similar to those described by Johnson (1998a). In this transform, all right branching sequences in each tree are transformed into left branching sequences of symbols of the form A1 /A2 , denoting an incomplete instance of category A1 lacking an instance of category A2 to the right.3 Rewrite rules for the right-corner transform are shown below, first flattening right-branching structure: Another recent approach to this problem (Johnson and Charniak, 2004) uses a tree-adjoining gr"
C08-1072,C08-1099,1,0.8063,"se tree), the different parsing strategy of an HHMM parser and the close connection of this system with a probabilistic model of semantics present potential benefits to the recognition of disfluent speech. 574 First, by using a depth-limited stack, this model better adheres to psycholinguistically observed short term memory limits that the human parser and generator are likely to obey (Cowan, 2001; Miller, 1956). The use of a depth-limited stack is enabled by the right-corner transform’s property of transforming right expansions to left expansions, which minimizes stack usage. Corpus studies (Schuler et al., 2008) suggest that broad coverage parsing can be achieved via this transform using only four stack elements. In practical terms this means that the model is less likely than a standard CYK parser to spend time and probability mass on analyses that conflict with the memory limits humans appear to be constrained by when generating and understanding speech. Second, this model is part of a more general framework that incorporates a model of referential semantics into the parsing model of the HHMM (Schuler et al., in press). While the framework evaluated in this paper models only the syntactic contribut"
C08-1072,J98-4004,0,\N,Missing
C08-1072,N01-1016,0,\N,Missing
C08-1072,P03-1054,0,\N,Missing
C08-1072,J09-3001,1,\N,Missing
C08-1072,C98-1098,0,\N,Missing
C08-1072,P08-2027,1,\N,Missing
C08-1072,P04-1005,0,\N,Missing
C08-1099,J01-2004,0,0.280126,"Finally, studies of short-term memory capacity sugc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. gest human language processing operates within a severely constrained short-term memory store — possibly restricted to as few as four distinct elements (Miller, 1956; Cowan, 2001). The first two observations may be taken to endorse existing probabilistic beam-search models which maintain multiple competing analyses, pruned by contextual preferences and dead ends (e.g. Roark, 2001). But the last observation on memory bounds imposes a restriction that until now has not been evaluated in a corpus study. Can a simple, useful human-like processing model be defined using these constraints? This paper describes a relatively simple model of language as a factored statistical time-series process that meets all three of the above desiderata; and presents corpus evidence that this model is sufficient to parse naturally occurring sentences using humanlike bounds on memory. The remainder of this paper is organized as follows: Section 2 describes some current approaches to increment"
C08-1099,W05-1513,0,0.0141637,"ch are not independently cognitively motivated. Cascaded finite-state automata, as in FASTUS (Hobbs et al., 1996), also make use of a bounded stack, but stack levels in such systems are typically dedicated to particular syntactic operations: e.g. a word group level, a phrasal level, and a clausal level. As a result, some amount of constituent structure may overflow its dedicated level, and be sacrificed (for example, prepositional phrase attachment may be left underspecified). Finite-state equivalent parsers (and thus, bounded-stack parsers) have asymptotically linear run time. Other parsers (Sagae and Lavie, 2005) have achieved linear runtime complexity with unbounded stacks in incremental parsing by using a greedy strategy, pursuing locally most probable shift or reduce operations, conditioned on multiple surrounding words. But without an explicit bounded stack it is difficult to connect these models to concepts in a psycholinguistic model. Abney and Johnson (1991) explore left-corner parsing as a memory model, but again only in terms of (complete) syntactic constituents. The approach explored here is similar, but the transform is reversed to allow the recognizer to store recognized structure rather t"
C08-1099,P98-1101,0,0.5998,"form, which accounts partially recognized phrases and clauses as incomplete constituents, lacking one instance of another constituent yet to come. In particular, this study will use the trees in the Penn Treebank Wall Street Journal (WSJ) corpus (Marcus et al., 1994) as a data set. In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the corpus is subjected to another, pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. 3.2 Right-Corner Transform Phrase structure trees are recognized in this framework in a right-corner form that can be mapped to and from ordinary phrase structure via reversible transform rules, similar to those described by Johnson (1998a). This transformed grammar constrains memory usage in left-to-rig"
C08-1099,P03-1054,0,0.0960988,"s partially recognized phrases and clauses as incomplete constituents, lacking one instance of another constituent yet to come. In particular, this study will use the trees in the Penn Treebank Wall Street Journal (WSJ) corpus (Marcus et al., 1994) as a data set. In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the corpus is subjected to another, pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. 3.2 Right-Corner Transform Phrase structure trees are recognized in this framework in a right-corner form that can be mapped to and from ordinary phrase structure via reversible transform rules, similar to those described by Johnson (1998a). This transformed grammar constrains memory usage in left-to-right traversal to a bound consist"
C08-1099,J98-4004,0,\N,Missing
C08-1099,J93-2004,0,\N,Missing
C08-1099,J09-3001,1,\N,Missing
C08-1099,C98-1098,0,\N,Missing
C12-1130,W02-1015,0,0.0199114,"f alternative syntactic representations on some down-stream task such as recovering unbounded dependencies. As such, the reannotation rules used in this process are defined to apply in a single top-down pass, pulling arguments and modifiers out of constituents until only a lexical head remains at the bottom. This single-pass architecture allows the rules for reannotating various linguistic phenomena to be relatively modular, so that they can be independently manipulated and evaluated. The reannotation rules described in this paper are also similar to reannotation rules in the ‘wsjsed’ system (Blaheta, 2002), but that system was evaluated as an error-correction tool, rather than for making theoretically-motivated changes to syntactic analyses. The reannotated grammar described in this paper exploits the rich traces and function labels in the Treebank that are typically ignored in parsing. These annotations are transformed into a categorial grammar representation of filler-gap constructions that makes use of ‘gap’ arguments, similar to the ‘slash’ arguments used in HPSG. Other approaches also exploit the trace and function labels of the Penn Treebank. Campbell (2004) proposed recovering trace info"
C12-1130,P06-4020,0,0.0420019,"69.0 57.5 26.4 38.8 54.6 Table 1: Unbounded dependency results compared to those of other systems studied by Rimell et al. (2009) and Nivre et al. (2010) over a variety of constructions: object extraction from relative clauses (Obj RC), object extraction from reduced relative clauses (Obj Red), subject extraction from relative clauses (Sbj RC), free relatives (Free), object wh-questions (Obj Q), right node raising (RNR), and subject extraction from embedded clauses (Sbj Embed). Evaluated parsers are C&C (Clark and Curran, 2007), Enju (Miyao and Tsujii, 2005), DCU (Cahill et al., 2004), Rasp (Briscoe et al., 2006), Stanford (Klein and Manning, 2003), MST (McDonald, 2006), Malt (Nivre et al., 2006a,b). This system used the Berkley parser (Petrov and Klein, 2007) run on the reannotated categorial grammar. Due to differences between the de Marneffe et al. (2006) dependency representation and that of the current system, some deterministic modifications were required for evaluation against the Rimell et al. (2009) corpus.11 1. If the hypothesized target of a dependency is a conjunction, the dependencies to each of its conjuncts are hypothesized instead; 2. If the target of a dependency is a relativizer or a"
C12-1130,P04-1041,0,0.0323741,"3 This system 52.7 69.2 68.4 69.0 57.5 26.4 38.8 54.6 Table 1: Unbounded dependency results compared to those of other systems studied by Rimell et al. (2009) and Nivre et al. (2010) over a variety of constructions: object extraction from relative clauses (Obj RC), object extraction from reduced relative clauses (Obj Red), subject extraction from relative clauses (Sbj RC), free relatives (Free), object wh-questions (Obj Q), right node raising (RNR), and subject extraction from embedded clauses (Sbj Embed). Evaluated parsers are C&C (Clark and Curran, 2007), Enju (Miyao and Tsujii, 2005), DCU (Cahill et al., 2004), Rasp (Briscoe et al., 2006), Stanford (Klein and Manning, 2003), MST (McDonald, 2006), Malt (Nivre et al., 2006a,b). This system used the Berkley parser (Petrov and Klein, 2007) run on the reannotated categorial grammar. Due to differences between the de Marneffe et al. (2006) dependency representation and that of the current system, some deterministic modifications were required for evaluation against the Rimell et al. (2009) corpus.11 1. If the hypothesized target of a dependency is a conjunction, the dependencies to each of its conjuncts are hypothesized instead; 2. If the target of a dep"
C12-1130,P04-1082,0,0.018332,"n rules in the ‘wsjsed’ system (Blaheta, 2002), but that system was evaluated as an error-correction tool, rather than for making theoretically-motivated changes to syntactic analyses. The reannotated grammar described in this paper exploits the rich traces and function labels in the Treebank that are typically ignored in parsing. These annotations are transformed into a categorial grammar representation of filler-gap constructions that makes use of ‘gap’ arguments, similar to the ‘slash’ arguments used in HPSG. Other approaches also exploit the trace and function labels of the Penn Treebank. Campbell (2004) proposed recovering trace information in a post-process following parsing. In contrast, the sample reannotation described in this paper incorporates filler-gap annotations into the corpus on which the parser is trained. Collins (1997), connected fillers with gaps and introduced a ‘TRACE’ category into the training data. Since this annotation does not change the constituent structure of the corpus, the addition of this gap annotation did not result in improved parsing results for Collins’ gap-annotating model (model 3) as compared to the non-gap-annotating model (model 2). However, these are t"
C12-1130,J07-4004,0,0.29379,"etation models are based on PCFGs, trained on syntactic annotations from the Penn Treebank (Marcus et al., 1993). These often recover dependencies as a post-process to parsing, and often are not able to retrieve unbounded dependencies if they are optimized on syntactic representations that leave these dependencies out. Categorial grammars, on the other hand, have well-defined unbounded dependency representations based on functor-argument relations in a small and easily-learnable set of composition operations. Such grammars — in particular, Combinatory Categorial Grammar (CCG) (Steedman, 2000; Clark and Curran, 2007) — do well on unbounded dependency recovery tasks (Rimell et al., 2009) but not as well as models based on Head Driven Phrase-structure Grammar (HPSG) (Pollard and Sag, 1994; Miyao and Tsujii, 2005), given the same training. This may be attributed to implicit tradeoffs in many categorial frameworks that minimize the number of composition operations at the expense of large numbers of possible categories for each lexical item, which may lead to sparse data effects in training. HPSG models, in contrast, maintain a relatively large number of composition operations and a relatively small set of pos"
C12-1130,P97-1003,0,0.218306,"loits the rich traces and function labels in the Treebank that are typically ignored in parsing. These annotations are transformed into a categorial grammar representation of filler-gap constructions that makes use of ‘gap’ arguments, similar to the ‘slash’ arguments used in HPSG. Other approaches also exploit the trace and function labels of the Penn Treebank. Campbell (2004) proposed recovering trace information in a post-process following parsing. In contrast, the sample reannotation described in this paper incorporates filler-gap annotations into the corpus on which the parser is trained. Collins (1997), connected fillers with gaps and introduced a ‘TRACE’ category into the training data. Since this annotation does not change the constituent structure of the corpus, the addition of this gap annotation did not result in improved parsing results for Collins’ gap-annotating model (model 3) as compared to the non-gap-annotating model (model 2). However, these are the kind of unbounded dependencies that need to be recovered in the evaluation described in this paper. 3 Generalized Categorial Grammar This paper explores the use of a generalized categorial grammar which has the transparent predicate"
C12-1130,de-marneffe-etal-2006-generating,0,0.117035,"Missing"
C12-1130,W03-1008,0,0.040348,"es such as the Penn Treebank are often stripped out, makes it very difficult for parsers to recognize this type of dependency correctly. While difficult to parse, this type of dependency is vital to the meaning of the sentence and of great importance in applications such as question answering and information extraction. Rich grammar formalisms such as CCG (Steedman, 2000) have been shown to provide very accurate syntactic dependency extraction. This is thought to be because these formalisms explicitly model long-distance filler-gap dependencies inherent in relative clauses and interrogatives (Gildea and Hockenmaier, 2003). But formal systems like CCG, especially as reflected in reannotated Treebanks like CCGbank (Hockenmaier, 2003), comprise analyses of many phenomena: right node raising, light verbs, subject control, object control, coordinating conjunctions, etc., among which filler-gap constructions are just one component. Some of these analyses may be more easily automatically learnable (and thus more robust) than others. For example, CCG analyzes filler-gap constructions using essentially the same mechanism as right-side complementation, turning gaps into right-seeking functors using a special backward cr"
C12-1130,P06-1063,0,0.0410106,"Missing"
C12-1130,P03-1054,0,0.0361291,": Unbounded dependency results compared to those of other systems studied by Rimell et al. (2009) and Nivre et al. (2010) over a variety of constructions: object extraction from relative clauses (Obj RC), object extraction from reduced relative clauses (Obj Red), subject extraction from relative clauses (Sbj RC), free relatives (Free), object wh-questions (Obj Q), right node raising (RNR), and subject extraction from embedded clauses (Sbj Embed). Evaluated parsers are C&C (Clark and Curran, 2007), Enju (Miyao and Tsujii, 2005), DCU (Cahill et al., 2004), Rasp (Briscoe et al., 2006), Stanford (Klein and Manning, 2003), MST (McDonald, 2006), Malt (Nivre et al., 2006a,b). This system used the Berkley parser (Petrov and Klein, 2007) run on the reannotated categorial grammar. Due to differences between the de Marneffe et al. (2006) dependency representation and that of the current system, some deterministic modifications were required for evaluation against the Rimell et al. (2009) corpus.11 1. If the hypothesized target of a dependency is a conjunction, the dependencies to each of its conjuncts are hypothesized instead; 2. If the target of a dependency is a relativizer or a relative pronoun, the predicate it"
C12-1130,J93-2004,0,0.0590686,"ng unbounded dependency recovery task (Rimell et al., 2009; Nivre et al., 2010). KEYWORDS: Grammar and formalisms, Semantics. Proceedings of COLING 2012: Technical Papers, pages 2125–2140, COLING 2012, Mumbai, December 2012. 2125 1 Introduction Accurate recovery of predicate-argument dependencies is vital for interpretation tasks like information extraction and question answering, and unbounded dependencies may account for a significant portion of the dependencies in any given text. Many current interpretation models are based on PCFGs, trained on syntactic annotations from the Penn Treebank (Marcus et al., 1993). These often recover dependencies as a post-process to parsing, and often are not able to retrieve unbounded dependencies if they are optimized on syntactic representations that leave these dependencies out. Categorial grammars, on the other hand, have well-defined unbounded dependency representations based on functor-argument relations in a small and easily-learnable set of composition operations. Such grammars — in particular, Combinatory Categorial Grammar (CCG) (Steedman, 2000; Clark and Curran, 2007) — do well on unbounded dependency recovery tasks (Rimell et al., 2009) but not as well a"
C12-1130,P05-1011,0,0.318924,"e to retrieve unbounded dependencies if they are optimized on syntactic representations that leave these dependencies out. Categorial grammars, on the other hand, have well-defined unbounded dependency representations based on functor-argument relations in a small and easily-learnable set of composition operations. Such grammars — in particular, Combinatory Categorial Grammar (CCG) (Steedman, 2000; Clark and Curran, 2007) — do well on unbounded dependency recovery tasks (Rimell et al., 2009) but not as well as models based on Head Driven Phrase-structure Grammar (HPSG) (Pollard and Sag, 1994; Miyao and Tsujii, 2005), given the same training. This may be attributed to implicit tradeoffs in many categorial frameworks that minimize the number of composition operations at the expense of large numbers of possible categories for each lexical item, which may lead to sparse data effects in training. HPSG models, in contrast, maintain a relatively large number of composition operations and a relatively small set of possible lexical categories, which are then used in a wider set of contexts. Can categorial grammars, which have well-studied semantic representations and are well suited for interpretation, obtain bet"
C12-1130,nivre-etal-2006-maltparser,0,0.0510444,"her systems studied by Rimell et al. (2009) and Nivre et al. (2010) over a variety of constructions: object extraction from relative clauses (Obj RC), object extraction from reduced relative clauses (Obj Red), subject extraction from relative clauses (Sbj RC), free relatives (Free), object wh-questions (Obj Q), right node raising (RNR), and subject extraction from embedded clauses (Sbj Embed). Evaluated parsers are C&C (Clark and Curran, 2007), Enju (Miyao and Tsujii, 2005), DCU (Cahill et al., 2004), Rasp (Briscoe et al., 2006), Stanford (Klein and Manning, 2003), MST (McDonald, 2006), Malt (Nivre et al., 2006a,b). This system used the Berkley parser (Petrov and Klein, 2007) run on the reannotated categorial grammar. Due to differences between the de Marneffe et al. (2006) dependency representation and that of the current system, some deterministic modifications were required for evaluation against the Rimell et al. (2009) corpus.11 1. If the hypothesized target of a dependency is a conjunction, the dependencies to each of its conjuncts are hypothesized instead; 2. If the target of a dependency is a relativizer or a relative pronoun, the predicate it modifies is used in its place; and 3. If the sou"
C12-1130,W06-2933,0,0.0211987,"her systems studied by Rimell et al. (2009) and Nivre et al. (2010) over a variety of constructions: object extraction from relative clauses (Obj RC), object extraction from reduced relative clauses (Obj Red), subject extraction from relative clauses (Sbj RC), free relatives (Free), object wh-questions (Obj Q), right node raising (RNR), and subject extraction from embedded clauses (Sbj Embed). Evaluated parsers are C&C (Clark and Curran, 2007), Enju (Miyao and Tsujii, 2005), DCU (Cahill et al., 2004), Rasp (Briscoe et al., 2006), Stanford (Klein and Manning, 2003), MST (McDonald, 2006), Malt (Nivre et al., 2006a,b). This system used the Berkley parser (Petrov and Klein, 2007) run on the reannotated categorial grammar. Due to differences between the de Marneffe et al. (2006) dependency representation and that of the current system, some deterministic modifications were required for evaluation against the Rimell et al. (2009) corpus.11 1. If the hypothesized target of a dependency is a conjunction, the dependencies to each of its conjuncts are hypothesized instead; 2. If the target of a dependency is a relativizer or a relative pronoun, the predicate it modifies is used in its place; and 3. If the sou"
C12-1130,C10-1094,0,0.231975,"er describes a categorial grammar which, like other categorial grammars, imposes a small, uniform, and easily learnable set of semantic composition operations based on functor-argument relations, but like HPSG, is generalized to limit the number of categories used to those needed to enforce grammatical constraints. The paper also describes a novel reannotation system used to map existing resources based on Government and Binding Theory, like the Penn Treebank, into this categorial representation. This grammar is evaluated on an existing unbounded dependency recovery task (Rimell et al., 2009; Nivre et al., 2010). KEYWORDS: Grammar and formalisms, Semantics. Proceedings of COLING 2012: Technical Papers, pages 2125–2140, COLING 2012, Mumbai, December 2012. 2125 1 Introduction Accurate recovery of predicate-argument dependencies is vital for interpretation tasks like information extraction and question answering, and unbounded dependencies may account for a significant portion of the dependencies in any given text. Many current interpretation models are based on PCFGs, trained on syntactic annotations from the Penn Treebank (Marcus et al., 1993). These often recover dependencies as a post-process to par"
C12-1130,P06-1055,0,0.1325,"otation system described in this paper, a version of the Penn Treebank was annotated with a generalized categorial grammar as described in Section 3. This required about 150 reannotation rules, with about 20 rules for each of noun phrases, verb phrases, prepositional phrases, adjectival phrases, and adverbial phrases, and about 50 rules for various sentence types. This reannotation was applied to Sections 02 to 21 of the WSJ portion of the Penn Treebank (Marcus et al., 1993). The resulting corpus then underwent three iterations of latent variable annotation, using the split-merge algorithm of Petrov et al. (2006).8 Each iteration of this algorithm splits the categories in the training corpus randomly in two, runs the inside-outside algorithm (expectation maximization) on the resulting trees to maximize the probability of the training set, then merges those categories that contributed the least to the maximization. The categorial grammar type-constructing operators were then used to extract semantic dependency relations for each parsed sentence. This evaluation used development and test sets from Rimell et al. (2009) for tuning the annotation mappings and testing, respectively. The de Marneffe et al. ("
C12-1130,N07-1051,0,0.864784,"lly distinguish between analyses of individual phenomena, or between entire formal systems, on the basis of learnability in a down-stream unbounded dependency recovery task. In particular, this paper describes a method for implementing syntactic analyses of various phenomena through automatic reannotation rules, which operate deterministically on a corpus like the Penn Treebank (Marcus et al., 1993) to produce a corpus with desired syntactic analyses. This reannotated corpus is then used to define a probabilistic grammar which is automatically annotated with additional latent variable values (Petrov and Klein, 2007) to introduce distinctions based on distributions of words and syntactic categories that increase the probability of the corpus (and improve the accuracy of parsing on held-out data), but do not affect the calculation of dependency structure. Dependency structures can then be extracted from parse trees produced by this grammar in a deterministic post-process, and mapped to the dependency representation used by Rimell et al. (2009) for evaluation. 2.2 Reannotation The reannotation process described in this paper is similar in purpose to efforts to convert Penn Treebank annotations into other gr"
C12-1130,D09-1085,0,0.087971,"given text. This paper describes a categorial grammar which, like other categorial grammars, imposes a small, uniform, and easily learnable set of semantic composition operations based on functor-argument relations, but like HPSG, is generalized to limit the number of categories used to those needed to enforce grammatical constraints. The paper also describes a novel reannotation system used to map existing resources based on Government and Binding Theory, like the Penn Treebank, into this categorial representation. This grammar is evaluated on an existing unbounded dependency recovery task (Rimell et al., 2009; Nivre et al., 2010). KEYWORDS: Grammar and formalisms, Semantics. Proceedings of COLING 2012: Technical Papers, pages 2125–2140, COLING 2012, Mumbai, December 2012. 2125 1 Introduction Accurate recovery of predicate-argument dependencies is vital for interpretation tasks like information extraction and question answering, and unbounded dependencies may account for a significant portion of the dependencies in any given text. Many current interpretation models are based on PCFGs, trained on syntactic annotations from the Penn Treebank (Marcus et al., 1993). These often recover dependencies as"
C16-1092,D11-1059,0,0.0457986,"Missing"
C16-1092,P99-1065,0,0.685096,"Missing"
C16-1092,P02-1017,0,0.519781,"rarchical syntax through distributional statistics alone, by modeling two widely-accepted features of human language acquisition and sentence processing that have not been simultaneously modeled by any existing grammar induction algorithm: (1) a left-corner parsing strategy and (2) limited working memory capacity. To model realistic input to human language learners, we evaluate our system on a corpus of child-directed speech rather than typical newswire corpora. Results beat or closely match those of three competing systems. 1 Introduction The success of statistical grammar induction systems (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Christodoulopoulos et al., 2012) seems to suggest that sufficient statistical information is available in language to allow grammar acquisition on this basis alone, as has been argued for word segmentation (Saffran et al., 1999). But existing grammar induction systems make unrealistic assumptions about human learners, such as the availability of part-of-speech information and access to an indexaddressable parser chart, which are not independently cognitively motivated. This paper explores the possibility that a memory-limited incremental left-corner parse"
C16-1092,P04-1061,0,0.434513,"Missing"
C16-1092,P11-1108,0,0.460587,"statistics alone, by modeling two widely-accepted features of human language acquisition and sentence processing that have not been simultaneously modeled by any existing grammar induction algorithm: (1) a left-corner parsing strategy and (2) limited working memory capacity. To model realistic input to human language learners, we evaluate our system on a corpus of child-directed speech rather than typical newswire corpora. Results beat or closely match those of three competing systems. 1 Introduction The success of statistical grammar induction systems (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Christodoulopoulos et al., 2012) seems to suggest that sufficient statistical information is available in language to allow grammar acquisition on this basis alone, as has been argued for word segmentation (Saffran et al., 1999). But existing grammar induction systems make unrealistic assumptions about human learners, such as the availability of part-of-speech information and access to an indexaddressable parser chart, which are not independently cognitively motivated. This paper explores the possibility that a memory-limited incremental left-corner parser, of the sort independently motivate"
C16-1092,C92-1032,0,0.55902,"Missing"
C16-1092,J10-1001,1,0.916984,"Missing"
C16-1092,P07-1049,0,0.840112,"distributional statistics alone, by modeling two widely-accepted features of human language acquisition and sentence processing that have not been simultaneously modeled by any existing grammar induction algorithm: (1) a left-corner parsing strategy and (2) limited working memory capacity. To model realistic input to human language learners, we evaluate our system on a corpus of child-directed speech rather than typical newswire corpora. Results beat or closely match those of three competing systems. 1 Introduction The success of statistical grammar induction systems (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Christodoulopoulos et al., 2012) seems to suggest that sufficient statistical information is available in language to allow grammar acquisition on this basis alone, as has been argued for word segmentation (Saffran et al., 1999). But existing grammar induction systems make unrealistic assumptions about human learners, such as the availability of part-of-speech information and access to an indexaddressable parser chart, which are not independently cognitively motivated. This paper explores the possibility that a memory-limited incremental left-corner parser, of the sort"
C98-2187,P95-1023,0,0.0166447,"e is restricted to take place at no more than one distinct node. We show that in this case the parsing problem for TAG can be solved in worst case timc O(nS). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound"
C98-2187,P94-1022,0,0.630794,"he proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which re1176 the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G --= (N, Z, I, A, S),"
C98-2187,J94-2002,1,0.874825,"lved in worst case timc O(nS). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which re1176 the proposed vast majority been cur"
C98-2187,J94-1004,0,0.0252243,"az)..., (1) where X n is any projection of category X , y m a z is the maximal projection of Y, and the order of the constituents is variable. 3 A c o m p l e m e n t auxiliary tree, on the other hand, introduces a lexical head that subcategorizes for the tree's foot node and assigns it a thematic role. The structure of a complement auxiliary tree may be described as: xmax ._+ . . . yO ... xmax ... (2) where X max is the maximal projection Of some category X , and y 0 is the lexical projection 2The s a m e linguistic distinction is used in the conception of 'modifier' and 'predicative' trees (Schabes and Shieber, 1994), b u t Schabes and Shieber give the trees special properties in the calculation of derivation structures, which we do not. aThe CFG-like notation is taken directly from (Kroch, 1989), where it is used to specify labels at the root and frontier nodes of a tree without placing constraints on the internal structure. 1180 of some category Y, whose maximal projection dominates X max . From this we make the following observations: 1. Because it does not assign a theta role to its foot node, an athematic auxiliary tree may adjoin at any projection of a category, which we take to designate any adjunc"
C98-2187,J95-4002,0,0.508785,". A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which re1176 the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G --= ("
C98-2187,P93-1017,0,\N,Missing
D18-1288,L18-1012,0,0.308716,"tes a given response variable. GAM generalizes linear models by allowing the response variable to be computed as the sum of smooth functions of one or more predictors. In both approaches, responses are modeled as conditionally independent of preceding observations of predictors unless spillover terms are added, with the attendant drawbacks discussed in Section 1. To make this point more forcefully, take for example Shain et al. (2016), who find significant effects of constituent wrap-up (p = 2.33e14) and dependency locality (p = 4.87e-10) in the 2680 Natural Stories self-paced reading corpus (Futrell et al., 2018). They argue that this constitutes the first strong evidence of memory effects in broadcoverage sentence processing. However, it turns out that when one baseline predictor — probabilistic context free grammar (PCFG) surprisal — is spilled over one position, the reported effects disappear: p = 0.816 for constituent wrap-up and p = 0.370 for dependency locality. Thus, a reasonable but ultimately inaccurate assumption about baseline effect timecourses can have a dramatic impact on the conclusions supported by the statistical model. DTSR offers a way forward by bringing temporal diffusion under di"
D18-1288,P13-2121,0,0.266248,"Missing"
D18-1288,W16-4106,1,0.891245,"Missing"
D18-1288,N15-1183,1,0.911087,"Missing"
D18-1292,N10-1083,0,0.140493,"Missing"
D18-1292,D13-1195,0,0.0218808,"score calculation The complexity of the inside algorithm is cubic on the length of the sentence because it has to iterate over all start points i, all end points j and all split points k of a span. For a dense PCFG with a large number of states, the explicit looping is undesirable, especially when it can be formulated as matrix multiplication. The split point loop is therefore replaced with a matrix multiplication in order to take advantage of highly optimized GPU linear algebra packages like cuBLAS and cuSPARSE, whereas previous work explores how to parse efficiently on GPUs (Johnson, 2011; Canny et al., 2013; Hall et al., 2014). Inside likelihoods are propagated using a copy V0 of the inside likelihood tensor V with the first and second indices reversed: V0[ j,i,c] = V[i, j,c] (17) This reversal allows the sum over split points k ∈ {i+1, ..., j−1} to be calculated as a product of contiguous matrices, which can be efficiently implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, which seek to maximiz"
D18-1292,P07-3008,0,0.0398876,"t either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983). Left-corner parsers require only minimal stack memory to process left-branching and right-branching structures, but require an extra stack element to process each center embedding in a structure. For example,"
D18-1292,P14-1020,0,0.0181031,"he complexity of the inside algorithm is cubic on the length of the sentence because it has to iterate over all start points i, all end points j and all split points k of a span. For a dense PCFG with a large number of states, the explicit looping is undesirable, especially when it can be formulated as matrix multiplication. The split point loop is therefore replaced with a matrix multiplication in order to take advantage of highly optimized GPU linear algebra packages like cuBLAS and cuSPARSE, whereas previous work explores how to parse efficiently on GPUs (Johnson, 2011; Canny et al., 2013; Hall et al., 2014). Inside likelihoods are propagated using a copy V0 of the inside likelihood tensor V with the first and second indices reversed: V0[ j,i,c] = V[i, j,c] (17) This reversal allows the sum over split points k ∈ {i+1, ..., j−1} to be calculated as a product of contiguous matrices, which can be efficiently implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, which seek to maximize the likelihood of"
D18-1292,D16-1073,0,0.15638,"ends a chart-based Bayesian PCFG induction model (Johnson et al., 2007b) to include depth bounding, which allows both bounded and unbounded PCFGs to be induced from unannotated text. Experiments reported in this paper confirm that 2721 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2721–2731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner parse of the sentence For parts the plant built to fail"
D18-1292,U11-1006,0,0.0253849,"fficient inside score calculation The complexity of the inside algorithm is cubic on the length of the sentence because it has to iterate over all start points i, all end points j and all split points k of a span. For a dense PCFG with a large number of states, the explicit looping is undesirable, especially when it can be formulated as matrix multiplication. The split point loop is therefore replaced with a matrix multiplication in order to take advantage of highly optimized GPU linear algebra packages like cuBLAS and cuSPARSE, whereas previous work explores how to parse efficiently on GPUs (Johnson, 2011; Canny et al., 2013; Hall et al., 2014). Inside likelihoods are propagated using a copy V0 of the inside likelihood tensor V with the first and second indices reversed: V0[ j,i,c] = V[i, j,c] (17) This reversal allows the sum over split points k ∈ {i+1, ..., j−1} to be calculated as a product of contiguous matrices, which can be efficiently implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, w"
D18-1292,N07-1018,0,0.109479,"bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depthbounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and without bounding.1 Results show that depth-bounding is indeed significantly effective in limiting the search space of the inducer and thereby increasing the accuracy of the resulting parsing model. Moreover, parsing results on English, Chinese and German show that this bounded model with a new inference technique is able to produce parse trees more accurately than or competitively with state-ofthe-art constituency-based grammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical s"
D18-1292,Q13-1006,0,0.0152799,"4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner parse of the sentence For parts the plant built to fail was awful. depth-bounding does empirically have the effect of significantly limiting the search space of the inducer. Analyses of this model also show that the posterior samples are indicative of implicit depth limits in the data. This work also shows for the first time that it is possible to induce an accurate unbounded PCFG from raw text with no strong linguistic constrai"
D18-1292,P11-1108,0,0.881268,"Missing"
D18-1292,P80-1024,0,0.635866,"formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983). Left-corner parsers require only minimal stack memory to process left-branching and right-branching structures, but require an extra stack element to process each center embedding in a structure. For example, a left-corner parser must add a stack element for each of the first three words in the sentence, For parts the plant built to fail was awful, shown in Figure 1. These kinds of depth bounds in sentence processing have been used to explain the relative difficulty of center-embedded sentences compared to more right-branching paraphrases like It was awful for the plant"
D18-1292,N18-1084,0,0.0431878,"Missing"
D18-1292,P92-1017,0,0.784554,"the search space of the inducer and thereby increasing the accuracy of the resulting parsing model. Moreover, parsing results on English, Chinese and German show that this bounded model with a new inference technique is able to produce parse trees more accurately than or competitively with state-ofthe-art constituency-based grammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical structures for strings of words. Using context-free grammars (CFGs) to define these structures, previous attempts at either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion dep"
D18-1292,P02-1017,0,0.338333,"are the effects of depth-bounding more directly, this work extends a chart-based Bayesian PCFG induction model (Johnson et al., 2007b) to include depth bounding, which allows both bounded and unbounded PCFGs to be induced from unannotated text. Experiments reported in this paper confirm that 2721 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2721–2731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner"
D18-1292,P04-1061,0,0.427954,"-art constituency-based grammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical structures for strings of words. Using context-free grammars (CFGs) to define these structures, previous attempts at either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnso"
D18-1292,J93-2004,0,0.0617007,"to set the hyperparameters of the model, this evaluation also uses it to explore interactions among parsing accuracy, model fit, depth limit and category domain. The first set of experiments explores various settings of D in the hope of acquiring a better picture of how depth-bounding affects the inducer. The second set of experiments uses the value of D tuned in the first experiments, and does PIoC on different sets of samples to examine the effect it has on parse quality. Optimal parameter values from these first two experiments are then applied in experiments on English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) data to show how the model performs compared with competing systems. Each run in evaluation uses one sample of parse trees from the posterior samples after convergence. Preliminary experiments show that the samples after convergence are very similar within a run and their parsing accuracies differ very little. This evaluation follows Seginer (2007) by running unlabeled PARSEVAL on parse trees collected from each run. Punctuation is retained in the raw text in induction, and removed in evaluation, a"
D18-1292,D15-1160,0,0.0227263,"tly implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, which seek to maximize the likelihood of data marginalizing out the latent trees, does not yield good performance. Because trees are the main target for evaluation, it may be preferable to find the most probable tree structures given the marginal posterior of tree structures compared to finding the most probable grammar. Some recent work (McClosky and Charniak, 2015; Keith et al., 2018) explores how to use marginal distributions of tree structures from supervised parsers to create more accurate parse trees. Based on these arguments, this model performs maximum a posteriori (MAP) inference on constituents (PIoC) using approximate conditional posteriors of spans to create final parses for evaluation. Formally, let σ?i, j be an MAP unlabeled span of words in a sentence from a corpus σ, with start point i and end point j, and σi,k , σk, j its possible children. This algorithm iteratively looks for the best pair of children σ?i,k , σ?k, j according to the pos"
D18-1292,D10-1120,0,0.0140445,"ussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner parse of the sentence For parts the plant built to fail was awful. depth-bounding does empirically have the effect of significantly limiting the search space of the inducer. Analyses of this model also show that the posterior samples are indicative of implicit depth limits in the data. This work also shows for the first time that it is possible to induce an accurate unbounded PCFG from raw text with"
D18-1292,D16-1004,0,0.428047,"of Linguistics The Ohio State University jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Abstract Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depthbounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and with"
D18-1292,J10-1001,1,0.785735,"hnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983). Left-corner parsers require only minimal stack memory to process left-branching and right-branching structures, but require an extra stack element to process each center embedding in a structure. For example, a left-corner parser must add a stack element for each of the first three words in the sentence, Fo"
D18-1292,P07-1049,0,0.785405,"rammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical structures for strings of words. Using context-free grammars (CFGs) to define these structures, previous attempts at either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983)."
D18-1292,C16-1092,1,0.730574,"State University jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Abstract Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depthbounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and without bounding.1 Resul"
D18-1292,xia-etal-2000-developing,0,0.0352094,"also uses it to explore interactions among parsing accuracy, model fit, depth limit and category domain. The first set of experiments explores various settings of D in the hope of acquiring a better picture of how depth-bounding affects the inducer. The second set of experiments uses the value of D tuned in the first experiments, and does PIoC on different sets of samples to examine the effect it has on parse quality. Optimal parameter values from these first two experiments are then applied in experiments on English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) data to show how the model performs compared with competing systems. Each run in evaluation uses one sample of parse trees from the posterior samples after convergence. Preliminary experiments show that the samples after convergence are very similar within a run and their parsing accuracies differ very little. This evaluation follows Seginer (2007) by running unlabeled PARSEVAL on parse trees collected from each run. Punctuation is retained in the raw text in induction, and removed in evaluation, also following Seginer (2007). Analysis of model behavi"
J09-3001,P85-1008,0,0.523898,"can be deﬁned, denoting sets or properties of individuals: E , T  (for example, writable), relations over individual pairs: E , E , T  (for example, contains), or ﬁrst-order functors over sets: E , T , E , T  (for example, a comparative adjective like larger). 3.1.2 Ontological Promiscuity. First-order or higher models (in which functors can take sets as arguments) can be mapped to equivalent zero-order models (with functors deﬁned only on entities). This is generally motivated by a desire to allow sets of individuals to be described in much the same way as individuals themselves (Hobbs 1985). Entities in a zero-order model M can be deﬁned from individuals in a higherorder model M∗ by mapping or reifying each set S = {ι1 , ι2 , . . . } in P (EM∗ ) (or each set of sets in P (P (EM∗ )), etc.) as an entity eS in a new domain EM .1 Relations l interpreted as zero-order functors in M can be deﬁned directly from relations l∗ interpreted as higher-order functors (over sets) in M∗ by mapping each instance of S1 , S2  in l∗ M∗ : P (EM∗ ) × P (EM∗ ) to a corresponding instance of eS1 , eS2  in lM : EM × EM . Set subsumption in M∗ can then be deﬁned on entities made from reiﬁed sets"
J09-3001,J03-1003,0,0.00941772,"Missing"
J09-3001,P01-1061,1,0.902639,"is deﬁned dynamically in this framework, in terms of transitions over time from less constrained referents to more constrained referents. Because it is deﬁned dynamically, interpretation in this framework can incorporate dependencies on referential context—for example, constraining interpretations to a presumed set of entities, or a presumed setting—which may be ﬁxed prior to recognition, or dynamically hypothesized earlier in the recognition process. This contrasts with other recent systems which interpret constituents only given ﬁxed inter-utterance contexts or explicit syntactic arguments (Schuler 2001; DeVault and Stone 2003; Gorniak and Roy 2004; Aist et al. 2007). Moreover, because it is deﬁned dynamically, in terms of transitions, this context-dependent interpretation framework can be directly integrated into a Viterbi decoding search, like ordinary state transitions in a Hidden Markov Model. The result is a single uniﬁed referential semantic probability model which brings several kinds of referential semantic context to bear in speech decoding, and performs accurate recognition in real time on large domains in the absence of example domain-speciﬁc training sentences. The remainder of t"
J09-3001,C08-1099,1,0.895386,"Missing"
J09-3001,J93-2004,0,\N,Missing
J09-3001,J10-1001,1,\N,Missing
J09-3001,P08-2027,1,\N,Missing
J09-3001,peters-peters-2000-treatment,0,\N,Missing
J10-1001,P81-1022,0,0.777392,"Missing"
J10-1001,N01-1021,0,0.672621,"Missing"
J10-1001,W04-0305,0,0.63428,"orage of constituent structure with that of ambiguous alternative analyses, but the memory demands of systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for using self-organizing maps as a uniﬁed memory resource are encouraging (Mayberry and Miikkulainen 2003), but are still limited to parsing relatively short travel planning queries with limited syntactic complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up constituents, and use connectionist models for probability estimation over these hypotheses (Henderson 2004) typically achieve better performance in practice. 4 Schuler et al. Parsing Using Human-Like Memory Constraints Previous memory-based explanations of problematic sentences (explaining garden path effects as exceeding a bound of four complete but unattached constituents, or explaining center embedding difﬁculties as exceeding a bound of seven active or awaited constituents) have been shown to underestimate human sentence processing capacity when equally complex but unproblematic sentences were examined. The hypothesis advanced in this article, that human sentence processing uses general-purpose"
J10-1001,P98-1101,0,0.704694,"Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg. Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for publication: 27 May 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 too austere to process the rich tree-like phrase structure commonly invoked to explain word-order regularities in natural language. This article aims to show that they are not. The article describes a comprehension model, based on a right-corner transform—a reversible tree transform related to the left-corner transform of Johnson (1998a)—that associates familiar phrase structure trees with the contents of a memory store of three to four partially completed constituents over time. Coverage results on the large syntactically annotated Penn Treebank corpus show a vast majority of naturally occurring sentences can be recognized using a memory store containing a maximum of only three incomplete constituents, and nearly all sentences can be recognized using four, consistent with estimates of human short-term memory capacity. This transform reduces memory usage in incremental (left to right) processing by transforming right-branch"
J10-1001,P03-1054,0,0.283415,"nal (Marcus, Santorini, and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped to a time-aligned bounded memory store as described in Section 4 to determine the amount of memory each sentence would require. 5.1 Binary Branching Structure In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the corpus is subjected to another pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of right-corner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. First, ordinary phrases and clauses are decomposed into head projections, each consisting of one subordinate head projection and one argument or modiﬁer, for example: A0 A0 ... VB:α1 ⇒ ... NP:α2 VB ... ... VB:α1 NP:α2 The selection of head constituents is done using rewrite rules similar to the MagermanBlack head rules (Magerman 1995)"
J10-1001,P95-1037,0,0.267893,"Missing"
J10-1001,J93-2004,0,0.0416121,"Missing"
J10-1001,C92-1032,0,0.338745,"r the right-corner nor left-corner parsing strategy by itself creates this ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as that of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), and thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in short-term memory (Resnik 1992). 22 Schuler et al. Parsing Using Human-Like Memory Constraints than four stack elements were excluded from training, in order to avoid generating inconsistent model probabilities (e.g., from expansions that could not be re-composed within the bounded memory store). Most likely sequences of HHMM stack conﬁgurations are evaluated by reversing the binarization, right-corner, and time-series mapping transforms described in Sections 4 and 5. But some of the binarization rewrites cannot be completely reversed, because they cannot be unambiguously matched to output trees. Automatically derived lexic"
J10-1001,J01-2004,0,0.643551,"lete constituent can be stored in each short-term memory element. Experimental results described in Section 5 suggest that a vast majority of English sentences can be recognized within these human-like memory bounds, even with this strict condition on chunking. If parsing can be performed in bounded memory under such strict conditions, it can reasonably be assumed to operate at least as well under more permissive circumstances, where some amount of syntactically-unrelated referential chunking is allowed. Several existing incremental systems are organized around a left-corner parsing strategy (Roark 2001; Henderson 2004). But these systems generally keep large numbers of constituents open for modiﬁer attachment in each hypothesis. This allows modiﬁers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have ﬁxed syntactic (and semantic) internal structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psycholinguistically attractive as simplifying assumptions. The HHMM model described in this article upholds"
J10-1001,N09-1039,1,0.8841,"Missing"
J10-1001,J09-3001,1,0.799662,"Missing"
J10-1001,1985.tmi-1.17,0,0.468414,"Missing"
J10-1001,A00-2018,0,\N,Missing
J10-1001,J98-4004,0,\N,Missing
J10-1001,J03-4003,0,\N,Missing
J10-1001,C98-1098,0,\N,Missing
J10-1001,J13-4008,0,\N,Missing
L18-1715,C10-1011,0,0.0337345,"reliminary understanding of how difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a heuristic extraction of nonlocal dependencies from gold Stanford dependencies and evaluated the results against our annotations. Heuristically, we mapped ‘nsubj’ in Stanford dependencies into ‘1’ dependencies in our annotations, ‘dobj’ into ‘2’ dependencies and ‘iobj’ into a ‘3’ dependencies. For relative clauses, Chinese Stanford dependencies have a dependency labeled ‘rcmod’ between the head noun and the main verb of a relative clause, reg"
L18-1715,W09-2307,0,0.038612,"more frequent in more colloquial text. 3. Experiments We first examined the proportion of each test set that can be recovered from Stanford dependencies converted from gold Treebank trees to explore the possibility to evaluate nonlocal dependency recovery on an ideal parser producing Stanford dependencies. Then we evaluated the performance of several automatic parsers on the task of recovering nonlocal dependencies to have a preliminary understanding of how difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a heuristic extraction"
L18-1715,A00-2018,0,0.654042,"ser producing Stanford dependencies. Then we evaluated the performance of several automatic parsers on the task of recovering nonlocal dependencies to have a preliminary understanding of how difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a heuristic extraction of nonlocal dependencies from gold Stanford dependencies and evaluated the results against our annotations. Heuristically, we mapped ‘nsubj’ in Stanford dependencies into ‘1’ dependencies in our annotations, ‘dobj’ into ‘2’ dependencies and ‘iobj’ into a ‘3’ dependen"
L18-1715,W15-3304,1,0.815391,"perior performance in recovering nonlocal dependencies in English (Rimell et al., 2009; Nguyen et al., 2012). In this research we focus on various nonlocal dependency types in Mandarin Chinese, a language that makes heavy use of nonlocal dependencies (Kummerfeld et al., 2013). We make full use of the trace categories annotated in the Penn Chinese Treebank (Xue et al., 2005) to generate test sets for eight nonlocal dependency constructions. We evaluate the nonlocal dependency recovery performance of parsers trained on generalized categorial grammar annotations (Bach, 1981; Nguyen et al., 2012; Duan and Schuler, 2015) with the annotated test sets. We hope these test sets can make it easier to automatically evaluate nonlocal dependency recovery in Mandarin Chinese. 2. 2.1. Nonlocal Dependency Data Sets The Constructions In a nonlocal dependency, a constituent seems to be moved from its canonical position in the predicate-argument pattern, while it still needs to be interpreted in the position from which it is moved. For this research we examined the trace categories annotated in Penn Chinese Treebank and included those constructions where either both the trace and the filler were clearly annotated or they c"
L18-1715,P03-1054,0,0.125143,"ank trees to explore the possibility to evaluate nonlocal dependency recovery on an ideal parser producing Stanford dependencies. Then we evaluated the performance of several automatic parsers on the task of recovering nonlocal dependencies to have a preliminary understanding of how difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a heuristic extraction of nonlocal dependencies from gold Stanford dependencies and evaluated the results against our annotations. Heuristically, we mapped ‘nsubj’ in Stanford dependencies into ‘1’ dependen"
L18-1715,P13-2018,0,0.0170489,", few state-of-the-art constituent parsers make use of these annotations to make predictions of the nonlocal dependencies. Categorial grammar annotations of the Treebank are advocated partly for their welldefined representations of filler-gap phenomena in natural languages. Parsers trained on categorial grammar annotations are found to produce superior performance in recovering nonlocal dependencies in English (Rimell et al., 2009; Nguyen et al., 2012). In this research we focus on various nonlocal dependency types in Mandarin Chinese, a language that makes heavy use of nonlocal dependencies (Kummerfeld et al., 2013). We make full use of the trace categories annotated in the Penn Chinese Treebank (Xue et al., 2005) to generate test sets for eight nonlocal dependency constructions. We evaluate the nonlocal dependency recovery performance of parsers trained on generalized categorial grammar annotations (Bach, 1981; Nguyen et al., 2012; Duan and Schuler, 2015) with the annotated test sets. We hope these test sets can make it easier to automatically evaluate nonlocal dependency recovery in Mandarin Chinese. 2. 2.1. Nonlocal Dependency Data Sets The Constructions In a nonlocal dependency, a constituent seems t"
L18-1715,E06-1011,0,0.0880655,"ow difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a heuristic extraction of nonlocal dependencies from gold Stanford dependencies and evaluated the results against our annotations. Heuristically, we mapped ‘nsubj’ in Stanford dependencies into ‘1’ dependencies in our annotations, ‘dobj’ into ‘2’ dependencies and ‘iobj’ into a ‘3’ dependencies. For relative clauses, Chinese Stanford dependencies have a dependency labeled ‘rcmod’ between the head noun and the main verb of a relative clause, regardless of the type of the relative clause."
L18-1715,de-marneffe-etal-2006-generating,0,0.107141,"us construction, can be more frequent in more colloquial text. 3. Experiments We first examined the proportion of each test set that can be recovered from Stanford dependencies converted from gold Treebank trees to explore the possibility to evaluate nonlocal dependency recovery on an ideal parser producing Stanford dependencies. Then we evaluated the performance of several automatic parsers on the task of recovering nonlocal dependencies to have a preliminary understanding of how difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a"
L18-1715,C12-1130,1,0.862081,"Missing"
L18-1715,nivre-etal-2006-maltparser,0,0.0847875,"al dependencies to have a preliminary understanding of how difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a heuristic extraction of nonlocal dependencies from gold Stanford dependencies and evaluated the results against our annotations. Heuristically, we mapped ‘nsubj’ in Stanford dependencies into ‘1’ dependencies in our annotations, ‘dobj’ into ‘2’ dependencies and ‘iobj’ into a ‘3’ dependencies. For relative clauses, Chinese Stanford dependencies have a dependency labeled ‘rcmod’ between the head noun and the main verb of a"
L18-1715,N07-1051,0,0.164358,"e nonlocal dependency recovery on an ideal parser producing Stanford dependencies. Then we evaluated the performance of several automatic parsers on the task of recovering nonlocal dependencies to have a preliminary understanding of how difficult it is to recover each of these nonlocal dependencies. 3.1. Stanford dependencies Stanford dependencies (Marneffe et al., 2006; Chang et al., 2009) are widely used in many dependency parsing evaluations and can be easily obtained from Treebank annotations. Constituent parsers, such as the Stanford parser (Klein and Manning, 2003), the Berkeley parser (Petrov and Klein, 2007) and the Brown parser (Charniak, 2000), can all be trained on Treebank annotations to yield Stanford dependencies. Dependency parsers, such as MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010) and MSTParser (MacDonal and Pereira, 2006), can be trained directly on Stanford dependencies to predict Stanford dependencies. Therefore we implemented a heuristic extraction of nonlocal dependencies from gold Stanford dependencies and evaluated the results against our annotations. Heuristically, we mapped ‘nsubj’ in Stanford dependencies into ‘1’ dependencies in our annotations, ‘dobj’ into ‘2’ depen"
L18-1715,D09-1085,0,0.0240058,"understanding the predicateargument structure of a sentence. Making full use of the trace annotations in the Penn Chinese Treebank (Xue et al., 2005), this research contributes several test sets of Chinese nonlocal dependencies which occur in different grammatical constructions. These datasets can be used by an automatic dependency parser to evaluate its performance on nonlocal dependency resolution in various syntactic constructions in Chinese. Keywords: nonlocal dependency, parsing, Mandarin Chinese 1. Introduction Recovering unbounded dependencies is challenging in English, as reported by Rimell et al. (2009). However, it serves as an important test of an automatic parser which cannot be easily accomplished by shallow language techniques. In spite of the low frequency of some nonlocal dependency constructions, correctly resolving these dependencies is crucial in understanding the underlying predicate-argument structure of a sentence. Although trace categories are annotated in the Penn Treebank (Xue et al., 2005), few state-of-the-art constituent parsers make use of these annotations to make predictions of the nonlocal dependencies. Categorial grammar annotations of the Treebank are advocated partl"
L18-1715,C10-1122,0,0.0350516,"ependencies on the side of the large training set. DEC IP NP-SBJ VP -NONEVA *T*-1 xin new A GCG parser To examine the performance of recovering nonlocal dependencies from an automatic parser, we experimented on parsers trained by the Berkeley latent variable PCFG trainer (Petrov and Klein, 2007) on generalized categorial grammar annotations (Bach, 1981; Nguyen et al., 2012; Duan and Schuler, 2015) of the Penn Chinese Treebank. We experimented with two different training sets, a small training set (gcg-s) and a large training set (gcg-l). The small training set is the same training set used in Tse and Curran (2010) which contains 15,957 sentences. The large training set includes all the sentences from the Penn Chinese Treebank 8 except those sentences used in nonlocal dependency test sets. The large training set contains 50,635 sentences. We trained two parsers with these two training sets and parsed the sentences in the nonlocal dependency test sets with these two parsers. Then we extracted the dependencies out of the parse outputs and evaluated the results against the dependencies annotated in the eight test sets. tese features CP WHNP-1 de Figure 2: Treebank annotations for xin de tese ‘new features’"
N09-1039,P97-1003,0,0.121336,"unbounded-stack CKY parser using the original grammar. Unlike the tree-based transforms described previously, the model-based transform described in this paper does not introduce additional context from corpus data beyond that contained in the original probabilistic grammar, making it possible to present a fair comparison between bounded- and unbounded-stack versions of the same model. Since this transform takes a probabilistic grammar as input, it can also easily accommodate horizontal and vertical Markovisation (annotating grammar symbols with parent and sibling categories) as described by Collins (1997) and subsequently. The remainder of this paper is organized as follows: Section 2 describes related approaches to parsing with stack bounds; Section 3 describes an existing bounded-stack parsing framework using a rightcorner transform defined over individual trees; Section 4 describes a redefinition of this transform to ap344 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 344–352, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ply to entire probabilistic grammars, cast as infinite sets of generable trees;"
N09-1039,W04-0305,0,0.221502,"much stricter estimates of human shortterm memory bounds (Cowan, 2001) than assumed by Abney and Johnson. 1 The lack of support for some of these available scope analyses may not necessarily be problematic for the present model. The complexity of interpreting nested raised quantifiers may place them beyond the capability of human interactive incremental interpretation, but not beyond the capability of post-hoc interpretation (understood after the listener has had time to think about it). 345 Several existing incremental systems are organized around a left-corner parsing strategy (Roark, 2001; Henderson, 2004). But these systems generally keep large numbers of constituents open for modifier attachment in each hypothesis. This allows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and semantic) structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psycholinguistically attractive as simplifying assumptions. The HHMM model examined in this paper upholds both the fixed-element and"
N09-1039,P98-1101,0,0.50454,"ults that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack CKY parser using the original grammar. 1 Introduction Statistical parsing models have recently been proposed that employ a bounded stack in time-series (left-to-right) recognition, in order to directly and tractably incorporate incremental phenomena such as (co-)reference or disfluency into parsing decisions (Schuler et al., 2008; Miller and Schuler, 2008). These models make use of a right-corner tree transform, based on the left-corner transform described by Johnson (1998), and are supported by corpus results suggesting that most sentences (in English, at least) can be parsed using a very small stack bound of three to four elements (Schuler et al., 2008). This raises an interesting question: if most sentences can be recognized with only three or four elements of stack memory, is the standard cubic-time CKY chart-parsing algorithm, which implicitly assumes an unbounded stack, wasting probability mass on trees whose complexity is beyond human recognition or generation capacity? This paper presents parsing accuracy results using transformed and untransformed versi"
N09-1039,C08-1072,1,0.868136,"fair comparison between bounded-stack and unbounded PCFG parsing using a common underlying model; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack CKY parser using the original grammar. 1 Introduction Statistical parsing models have recently been proposed that employ a bounded stack in time-series (left-to-right) recognition, in order to directly and tractably incorporate incremental phenomena such as (co-)reference or disfluency into parsing decisions (Schuler et al., 2008; Miller and Schuler, 2008). These models make use of a right-corner tree transform, based on the left-corner transform described by Johnson (1998), and are supported by corpus results suggesting that most sentences (in English, at least) can be parsed using a very small stack bound of three to four elements (Schuler et al., 2008). This raises an interesting question: if most sentences can be recognized with only three or four elements of stack memory, is the standard cubic-time CKY chart-parsing algorithm, which implicitly assumes an unbounded stack, wasting probability mass on trees whose complexity is beyond human re"
N09-1039,J01-2004,0,0.476461,"erate within much stricter estimates of human shortterm memory bounds (Cowan, 2001) than assumed by Abney and Johnson. 1 The lack of support for some of these available scope analyses may not necessarily be problematic for the present model. The complexity of interpreting nested raised quantifiers may place them beyond the capability of human interactive incremental interpretation, but not beyond the capability of post-hoc interpretation (understood after the listener has had time to think about it). 345 Several existing incremental systems are organized around a left-corner parsing strategy (Roark, 2001; Henderson, 2004). But these systems generally keep large numbers of constituents open for modifier attachment in each hypothesis. This allows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and semantic) structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psycholinguistically attractive as simplifying assumptions. The HHMM model examined in this paper upholds both the"
N09-1039,C08-1099,1,0.916888,"in order to ensure a fair comparison between bounded-stack and unbounded PCFG parsing using a common underlying model; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack CKY parser using the original grammar. 1 Introduction Statistical parsing models have recently been proposed that employ a bounded stack in time-series (left-to-right) recognition, in order to directly and tractably incorporate incremental phenomena such as (co-)reference or disfluency into parsing decisions (Schuler et al., 2008; Miller and Schuler, 2008). These models make use of a right-corner tree transform, based on the left-corner transform described by Johnson (1998), and are supported by corpus results suggesting that most sentences (in English, at least) can be parsed using a very small stack bound of three to four elements (Schuler et al., 2008). This raises an interesting question: if most sentences can be recognized with only three or four elements of stack memory, is the standard cubic-time CKY chart-parsing algorithm, which implicitly assumes an unbounded stack, wasting probability mass on trees whose co"
N09-1039,C98-1098,0,\N,Missing
N13-1010,W12-1706,0,0.357983,"re the position in the sentence that initiated the go-past region (SENTPOS) and the number of characters in the initiating word (NRCHAR). The difficulty of integrating a word may be seen in whether the immediately following word was fixated (NEXTISFIX), and similarly if the immediately previous word was fixated (PREVISFIX) the current word probably need not be fixated for as long. Finally, unigram (LOGPROB) and bigram probabilities are included. The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB). Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998)"
N13-1010,N01-1021,0,0.944168,"illips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing. 1 Introduction Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008). These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form). Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations. Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (G"
N13-1010,J93-2004,0,0.0434391,"if the immediately previous word was fixated (PREVISFIX) the current word probably need not be fixated for as long. Finally, unigram (LOGPROB) and bigram probabilities are included. The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB). Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003). This amounted to an n-gram training corpus of roughly 87 million words. These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998). Finally, total surprisal (SURP) was included to account for frequency effects in the baseline. The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were cal"
N13-1010,N07-1051,0,0.0290652,"Figure 2. Since two binary structural decisions (F and L) must be made in order to generate each word, there are four possible structures that may be generated (see Table 1). The F+L– transition initiates a new level of embedding at word xt and so requires the superordinate state to be encoded for later retrieval (e.g. on observing the in Figure 2). The F–L+ transition completes the deepest level of embedding and therefore requires the recall of the current superordinate connected component state with which the 5 The model has been shown to achieve an F-score of 87.8, within .2 points of the Petrov and Klein (2007) parser, which obtains an F-score of 88.0 on the same task. Because the sequence model is defined over binary-branching phrase structure, both parsers were evaluated on binary-branching phrase structure trees to provide a fair comparison. F–L– Cue Active Sign F+L– Initiate/Encode F–L+ Terminate/Integrate F+L+ Cue Awaited Sign Table 1: The hierarchical structure decisions and the operations they represent. F+L– initiates a new connected component, F–L+ integrates two disjoint connected components into a single connected component, and F–L– and F+L+ sequentially cue, respectively, a new active s"
N13-1010,P06-1055,0,0.0174,"ansition at time step t is defined in terms of (i) a probability φ of initiating a new connected component state with xt as its first observation, multiplied by (ii) the probability λ of terminating a connected component state with xt as its last observation, multiplied by (iii) the probabilities α and β of generating categories for active and awaited signs aqtn and bqtn in the resulting most subordinate 99 connected component state qtn . This kind of model can be defined directly on PCFG probabilities and trained to produce state-of-the-art accuracy by using the latent variable annotation of Petrov et al. (2006) (van Schijndel et al., in press).5 An example parse is shown in Figure 2. Since two binary structural decisions (F and L) must be made in order to generate each word, there are four possible structures that may be generated (see Table 1). The F+L– transition initiates a new level of embedding at word xt and so requires the superordinate state to be encoded for later retrieval (e.g. on observing the in Figure 2). The F–L+ transition completes the deepest level of embedding and therefore requires the recall of the current superordinate connected component state with which the 5 The model has be"
N13-1010,D09-1034,0,0.634649,"describes how these measures were used to predict reading time durations on the Dundee eye-tracking corpus. Sections 6 and 7 present results and discuss. 2 Frequency Measures 2.1 Surprisal One of the strongest predictors of processing complexity is surprisal (Hale, 2001). It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety 95 Proceedings of NAACL-HLT 2013, pages 95–105, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010). Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows: ! P P (s) s∈S(x1 ...xt ) surprisal (xt ) = − log2 P s∈S(x1 ...xt−1 ) P (s) (1) where S(x1 . . . xt ) is the set of syntactic trees whose leaves have x1 . . . xt as a prefix.1 In essence, surprisal measures how unexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be me"
N13-1010,J01-2004,0,0.0635772,"duction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011): ∆H(x1...t ) = max(0, H(x1...t−1 ) − H(x1...t )) (3) This measures the change in uncertainty about the discourse as each new word is processed. 3 Memory Measures 3.1 Dependency Locality In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument. Under the original formulation of DLT, there is a 1 The parser in this study uses a beam. However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation. 2 The incremental formulation used here was first proposed in Wu et al. (2010). 96 storage cost for each new referent introduced and an integration cost for each referent intervening in a dependency projection. This is a simplification made for ease of computation, and subsequent work has found DLT to be more accurate cross-linguistically if the intervening elements are structurally defined rather than defined in terms of referents (Kwon et al., 2010). That is, simply havin"
N13-1010,P10-1121,1,0.586611,"measures were used to predict reading time durations on the Dundee eye-tracking corpus. Sections 6 and 7 present results and discuss. 2 Frequency Measures 2.1 Surprisal One of the strongest predictors of processing complexity is surprisal (Hale, 2001). It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety 95 Proceedings of NAACL-HLT 2013, pages 95–105, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010). Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows: ! P P (s) s∈S(x1 ...xt ) surprisal (xt ) = − log2 P s∈S(x1 ...xt−1 ) P (s) (1) where S(x1 . . . xt ) is the set of syntactic trees whose leaves have x1 . . . xt as a prefix.1 In essence, surprisal measures how unexpected constructions are in a given context. What it does not provide is an explanation for why certain constructions would be less common and thus more surprising. 2.2 Entropy Reduction Processing difficulty can also be measured in terms of"
N13-1010,W12-1705,1,\N,Missing
N13-1010,J10-1001,1,\N,Missing
N15-1101,N09-1003,0,0.0557715,"and provides good representations for rare words, and CBOW would perform better and have higher accuracy for frequent words if trained on larger corpora. The purpose of this paper is not to compare the models, but to use the models to compare training corpora to see how different arrangement of information may impact the quality of the word embeddings. 3 Task Description To evaluate the effectiveness of full English Wikipedia and Simple English Wikipedia as training corpora for word embeddings, the word similarityrelatedness task described by Levy & Goldberg (2014) is used. As pointed out by Agirre et al (2009) and Levy & Goldberg (2014), relatedness may actually be measuring topical similarity and be better predicted by a bag-of-words model, and similarity may be measuring functional or syntactic 991 similarity and be better predicted by a contextwindow model. However, when the models are constant, the semantic information of the test words in the training corpora is crucial to allowing the model to build semantic representations for the words. It may be argued that when the corpus is explanatory, more semantic information about the target words is present; whereas when the corpus is non-explanator"
N15-1101,W10-0406,0,\N,Missing
N15-1101,D14-1162,0,\N,Missing
N15-1101,P14-2050,0,\N,Missing
N15-1101,W11-1601,0,\N,Missing
N15-1183,N06-1022,0,0.0289552,"hat the improvement in previous studies may have been due to latent n-gram information captured by cumulative PCFG surprisal. This finding is interesting because it suggests non-local hierarchic structure does not significantly influence reading times. The next section explores this hypothesis further by testing the fit of a hierarchic syntactic formalism whose strength lies in modeling long-distance dependencies. 5 Grammar Formalism Comparison So far, this study has tried to allay previous concerns that models of hierarchic syntax may just be accounting for the sparsity of n-gram statistics (Charniak et al., 2006; Frank and Bod, 2011). This section investigates whether a representation of hierarchic syntax that preserves long-distance dependencies can improve reading time predictions over a hierarchic representation based on the Penn Treebank which discards long-distance dependencies. This evaluation compares total PCFG surprisal as calculated by the original Penn Treebank grammar to total PCFG surprisal calculated by the Nguyen et al. (2012) Generalized Categorial Grammar (GCG). 5.1 GCG A GCG has a category set C, which consists of a set of primitive category types U , typically labeled Model Baselin"
N15-1183,D12-1033,0,0.0712355,"ing times. Instead, the present set of results support recent claims made by van Schijndel et al. (2014) that nonlocal subcategorization decisions are made early 1603 during processing and so would have a strong influence on the reading time measures used in the present study. Such decisions would have to be conditioned on hierarchic structural information not present in either PTB PCFG surprisal or the sequential structure models of Frank and Bod (2011). Further, predictability has been shown to affect word duration during speech production (Jurafsky et al., 2001; Aylett and Turk, 2006), and Demberg et al. (2012) found that hierarchic structure significantly improves over n-gram computations of predictability in that domain as well. Together, these findings suggest that hierarchic structure is not only a convenient descriptive tool for linguists, but that such structure is deeply rooted in the human language processor and is used during online language processing. Previous work has made a distinction between lexical surprisal, syntactic surprisal, and total surprisal (Demberg and Keller, 2008; Roark et al., 2009). Given a prefix derivation of the structure of the context, syntactic surprisal measures"
N15-1183,W12-1706,0,0.208984,"liam Schuler Department of Linguistics The Ohio State University {vanschm,schuler}@ling.osu.edu Abstract The present study builds on this finding by showing that cumulative n-gram probabilities significantly improve an n-gram baseline to better capture sequential frequency statistics. Further, this study shows that measures of hierarchic structural frequencies (as captured by PCFG surprisal) significantly improve reading time predictions over that improved sequential baseline. Previous work has debated whether humans make use of hierarchic syntax when processing language (Frank and Bod, 2011; Fossum and Levy, 2012). This paper uses an eye-tracking corpus to demonstrate that hierarchic syntax significantly improves reading time prediction over a strong n-gram baseline. This study shows that an interpolated 5-gram baseline can be made stronger by combining n-gram statistics over entire eye-tracking regions rather than simply using the last n-gram in each region, but basic hierarchic syntactic measures are still able to achieve significant improvements over this improved baseline. 1 Introduction In NLP, a concern exists that models of hierarchic syntax may be increasingly used exclusively to compensate for"
N15-1183,N01-1021,0,0.958342,"this improved baseline. 1 Introduction In NLP, a concern exists that models of hierarchic syntax may be increasingly used exclusively to compensate for n-gram sparsity (Lease et al., 2006). In the context of psycholinguistic modeling, Frank and Bod (2011) find that hierarchic measures of syntactic processing are not as good at predicting reading times as sequential part-of-speech-based models of processing.1 Fossum and Levy (2012) follow up on this finding and show that, when better n-gram information is present in the models, measures of hierarchic syntactic processing cost (PCFG surprisal; Hale, 2001; Levy, 2008) are as good at predicting reading times as the sequential models presented by Frank and Bod. 1 Frank and Bod (2011) find that hierarchic measures significantly improve the descriptive linguistic accuracy of models but that such measures are unable to improve upon a strong linear baseline when predicting reading times. First, this work defines a stronger n-gram baseline than that used in previous studies by replacing a bigram baseline computed from 101 million words with an interpolated 5-gram baseline computed over 2.96 billion words. Second, while previous work has used n-grams"
N15-1183,P13-2121,0,0.0570362,". 2 1 Cumu-Bigram: The red apple that the girl ate X : bigram targets X: bigram conditions Table 2: Influences on bigram factor predictions of reading times on girl following fixation on red. pos), word length (wlen), region length in words (rlen), whether the previous word was fixated (prevfix), and basic 5-gram log probability of the current word given the preceding context (5-gram). All independent predictors are centered and scaled before being added to each model. The 5-gram probabilities are interpolated 5-grams computed over the Gigaword 4.0 corpus (Graff and Cieri, 2003) using KenLM (Heafield et al., 2013). Gigaword 4.0 consists of around 2.96 billion words from around 4 million English newswire documents, which provides appropriate n-gram statistics since the Dundee corpus is also English news text. Each mixed effects model contains random intercepts for subject and word, and random by-subject slopes for all fixed effects. Since the following evaluations use ablative testing to determine whether a fixed effect significantly improves the fit of a model compared to a model without that fixed effect, all models in a given evaluation include random slopes for all fixed effects used in that evaluat"
N15-1183,J93-2004,0,0.0536489,"3). Even when this improved baseline is combined with a standard n-gram baseline, this paper demonstrates that PCFG surprisal is a significant predictor of reading times (Section 4). This paper also applies region accumulation to total surprisal and finds that it is not significantly better than non-accumulated total surprisal. In fact, cumulative surprisal is shown not to be a significant predictor of reading times at all when a cumulative ngram factor is included in the baseline. Finally, this paper compares two different models of hierarchic syntax: the Penn Treebank (PTB) representation (Marcus et al., 1993) and the psycholinguisticallymotivated Nguyen et al. (2012) Generalized Categorial Grammar (GCG). Each model of syntax is shown to provide orthogonal improvements to reading time predictions (Section 5). 1597 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1597–1605, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Factors Bigram Cumu-Bigram Durations w6 Rw5 P(w4|w3 ) P(w6|w5 ) P(w4|w3 ) P(w6|w5 )·P(w5|w4 ) w4 Rw4 Table 1: Bigram factors and their predictions of reading times in example eye-trackin"
N15-1183,C12-1130,1,0.929602,"Missing"
N15-1183,P06-1055,0,0.473257,"tions in this study. 4 Hierarchic Syntax Predictors This section tests the main hypothesis of this study: that hierarchic syntactic processing is a significant contributor to reading times. For the purposes of this evaluation, total PCFG surprisal (Hale, 2001; Levy, 2008; Roark et al., 2009) will be used as a measure of hierarchic syntactic processing. Specifically, PCFG surprisal will be calculated using the van Schijndel et al. (2013a) incremental parser trained on sections 02-21 of the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) using 5 iterations of split-merge (Petrov et al., 2006) and a beam width of 5000. ences, by-subject random slopes for both predictors of interest are included in the baseline. This practice is repeated throughout this study. 5 Twice the log-likelihood difference of two nested models can be approximated by a χ2 distribution with degrees of freedom equal to the difference in degrees of freedom of the models in question. The probability of obtaining a given log-likelihood difference D between the two models is therefore analogous to P(2 · D) under the corresponding χ2 distribution. Factors surp cumusurp Durations w4 w6 Rw4 Rw5 −log P(w4|T3 ) −log P(w"
N15-1183,D09-1034,0,0.505404,"ood ratio testing.5 Table 3 shows that both n-gram factors significantly improve the fit of the model and the final line shows that each factor provides a significant orthogonal improvement. Both n-gram factors will therefore be included as fixed effects and as by-subject random slopes in the baselines of the remaining evaluations in this study. 4 Hierarchic Syntax Predictors This section tests the main hypothesis of this study: that hierarchic syntactic processing is a significant contributor to reading times. For the purposes of this evaluation, total PCFG surprisal (Hale, 2001; Levy, 2008; Roark et al., 2009) will be used as a measure of hierarchic syntactic processing. Specifically, PCFG surprisal will be calculated using the van Schijndel et al. (2013a) incremental parser trained on sections 02-21 of the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) using 5 iterations of split-merge (Petrov et al., 2006) and a beam width of 5000. ences, by-subject random slopes for both predictors of interest are included in the baseline. This practice is repeated throughout this study. 5 Twice the log-likelihood difference of two nested models can be approximated by a χ2 distribution wi"
N15-1183,N13-1010,1,0.886645,"Missing"
N15-1183,W13-2605,1,0.8985,"Missing"
P00-1057,E91-1005,0,0.0110272,"nd l , call the set of nodes dominated by one node but not strictly dominated by the other the site-segment hh ; l i.  Removing a site-segment must not deprive a tree of its foot node. That is, no site-segment hh ; l i may contain a foot node unless l is itself the foot node.  If two tree sets adjoin into the same tree, the two site-segments must be simultaneously removable. That is, the two sitesegments must be disjoint, or one must contain the other. Because of the rst restriction, we depict tree sets with the components connected by a dominance link (dotted line), in the manner of (Becker et al., 1991). As written, the above rules only allow tree-local adjunction; we can generalize them to allow set-local adjunction by treating this dominance link like an ordinary arc. But this would increase the weak generative capacity of the system. For present purposes it is sucient just to allow one type of set-local adjunction: adjoin the upper tree to the upper foot, and the lower tree to the lower root (see Figure 5). This does not increase the weak generative capacity, as will be shown in Section 2.3. Observe that the set-local TAG given in Figure 5 obeys the above restrictions. 2.2 2LTAG For the"
P00-1057,W00-2008,1,0.8006,"ed later. Of course, in our case the stack"" has at most one element. The Pop rule does the reverse: every completed elementary tree set must contain a site-segment, and the Pop rule places it back where the site-segment came from, emptying the stack."" The Pop-push rule performs setlocal adjunction: a completed elementary tree set is placed between the two trees of yet another elementary tree set, and the stack"" is unchanged. Pop-push is computationally the most expensive rule; since it involves six indices and three di erent elementary trees, its running time is O(n6 G3 ). It was noted in (Chiang et al., 2000) that for synchronous RF-2LTAG, parse forests could not be transferred in time O(n6 ). This fact turns out to be connected to several properties of RF-TAG (Rogers, 1994).3 3 Thanks to Anoop Sarkar for pointing out the rst The CKY-style parser for regular form TAG described in (Rogers, 1994) essentially keeps track of adjunctions using stacks, and the regular form constraint ensures that the stack depth is bounded. The only kinds of adjunction that can occur to arbitrary depth are root and foot adjunction, which are treated similarly to substitution and do not a ect the stacks. The reader will"
P00-1057,P99-1011,1,0.850223,"node has Gorn address , then its ith child has Gorn address 2 Figure 6: Adjoining into by removing .  An auxiliary tree may adjoin anywhere.  When a tree is adjoined at a node ,  is rewritten as , and the foot of inherits the label of . The tree set of hG; G0 i, T (hG; G0 i), is f G [T (G0)], where f G is the yield function of G and T (G0 ) is the tree set of G0. Thus, the elementary trees of G0 are combined to form a derived tree, which is then interpreted as a derivation tree for G, which gives instructions for combining elementary trees of G into the nal derived tree. It was shown in Dras (1999) that when the meta-level grammar is in the regular form of Rogers (1994) the formalism is weakly equivalent to TAG. 2.3 Reducing restricted R-MCTAG to RF-2LTAG Consider the case of a multicomponent tree set f 1 ; 2 g adjoining into an initial tree (Figure 6). Recall that we de ned a sitesegment of a pair of adjunction sites to be all the nodes which are dominated by the upper site but not the lower site. Imagine that the site-segment is excised from , and that 1 and 2 are fused into a single elementary tree. Now we can simulate the multi-component adjunction by ordinary adjunction: adjoin the"
P00-1057,P95-1021,0,0.3527,"of tree-local. seem : sleep sleep : S does S likely S .. .. . . VP John VP seem VP seem VP likely VP to sleep S likely : Figure 4: SL-MCTAG generable derivation Unfortunately, unrestricted set-local multicomponent TAGs not only have more derivational generative capacity than TAGs, but they also have more weak generative capacity: SL-MCTAGs can generate the quadruple copy language wwww, for example, which does not correspond to any known linguistic phenomenon. Other formalisms aiming to model dependency correctly similarly expand weak generative capacity, notably D-tree Substitution Grammar (Rambow et al., 1995), and consequently end up with much greater parsing complexity. The work in this paper follows another Figure 5: Set-local adjunction. line of research which has focused on squeezing as much strong generative capacity as possible out of weakly TAG-equivalent formalisms. Tree-local multicomponent TAG (Weir, 1988), nondirectional composition (Joshi and Vijay-Shanker, 1999), and segmented adjunction (Kulick, 2000) are examples of this approach, wherein the constraint on weak generative capacity naturally limits the expressivity of these systems. We discuss the relation of the formalism of this pa"
P00-1057,P94-1022,0,0.333071,": Adjoining into by removing .  An auxiliary tree may adjoin anywhere.  When a tree is adjoined at a node ,  is rewritten as , and the foot of inherits the label of . The tree set of hG; G0 i, T (hG; G0 i), is f G [T (G0)], where f G is the yield function of G and T (G0 ) is the tree set of G0. Thus, the elementary trees of G0 are combined to form a derived tree, which is then interpreted as a derivation tree for G, which gives instructions for combining elementary trees of G into the nal derived tree. It was shown in Dras (1999) that when the meta-level grammar is in the regular form of Rogers (1994) the formalism is weakly equivalent to TAG. 2.3 Reducing restricted R-MCTAG to RF-2LTAG Consider the case of a multicomponent tree set f 1 ; 2 g adjoining into an initial tree (Figure 6). Recall that we de ned a sitesegment of a pair of adjunction sites to be all the nodes which are dominated by the upper site but not the lower site. Imagine that the site-segment is excised from , and that 1 and 2 are fused into a single elementary tree. Now we can simulate the multi-component adjunction by ordinary adjunction: adjoin the fused 1 and 2 into what is left of ; then replace by adjoining it betwee"
P00-1057,1985.tmi-1.17,0,0.0253703,"erence between the two is in the kinds of languages that they are able to describe: DSG is both less and more restrictive than R-MCTAG. DSG can generate the language count-k for some arbitrary k (that is, fa1 n a2 n : : : ak n g), which makes it extremely powerful, whereas R-MCTAG can only generate count-4. However, DSG cannot generate the copy language (that is, fww j w 2 g with  some terminal alphabet), whereas R-MCTAG can; this may be problematic for a formalism modeling natural language, given the key role of the copy language in demonstrating that natural language is not context-free (Shieber, 1985). RMCTAG is thus a more constrained relaxation of the notion of immediate dominance in favor of non-immediate dominance than is the case for DSG. Another formalism of particular interest here is the Segmented Adjoining Grammar of (Kulick, 2000). This generalization of TAG is characterized by an extension of the adjoining operation, motivated by evidence in scrambling, clitic climbing and subject-to-subject raising. Most interestingly, this extension to TAG, proposed on empirical grounds, is de ned by a composition operation with constrained non-immediate dominance links that looks quite simila"
P00-1057,W00-2015,0,\N,Missing
P01-1061,P96-1016,0,0.212841,"g to incorporate a certain amount of quantifier scope   ambiguity in order to allow a complete evaluation of all subderivations in a shared forest before making any disambiguation decisions in syntax.3 Various synchronous formalisms have been introduced for associating syntactic representations with logical functions in isomorphic or locally non-isomorphic derivations, including Categorial Grammars (CGs) (Wood, 1993), Synchronous Tree Adjoining Grammars (TAGs) (Joshi, 1985; Shieber and Schabes, 1990; Shieber, 1994), and Synchronous Description Tree Grammars (DTGs) (Rambow et al., 1995; Rambow and Satta, 1996). Most of these formalisms can be extended to define semantic associations over entire shared forests, rather than merely over individual parse trees, in a straightforward manner, preserving the ambiguity of the syntactic forest without exceeding its polynomial size, or the polynomial time complexity of creating or traversing it. Since one of the goals of this architecture is to use the system’s representation of its environment to resolve ambiguity in its instructions, a space-efficient shared forest of logical functions will not be enough. The system must also be able to efficiently calculat"
P01-1061,P95-1021,0,0.021885,"ends structure sharing to incorporate a certain amount of quantifier scope   ambiguity in order to allow a complete evaluation of all subderivations in a shared forest before making any disambiguation decisions in syntax.3 Various synchronous formalisms have been introduced for associating syntactic representations with logical functions in isomorphic or locally non-isomorphic derivations, including Categorial Grammars (CGs) (Wood, 1993), Synchronous Tree Adjoining Grammars (TAGs) (Joshi, 1985; Shieber and Schabes, 1990; Shieber, 1994), and Synchronous Description Tree Grammars (DTGs) (Rambow et al., 1995; Rambow and Satta, 1996). Most of these formalisms can be extended to define semantic associations over entire shared forests, rather than merely over individual parse trees, in a straightforward manner, preserving the ambiguity of the syntactic forest without exceeding its polynomial size, or the polynomial time complexity of creating or traversing it. Since one of the goals of this architecture is to use the system’s representation of its environment to resolve ambiguity in its instructions, a space-efficient shared forest of logical functions will not be enough. The system must also be abl"
P01-1061,C90-3045,0,0.251365,"ave been at least partially resolved by other means); whereas the approach described here extends structure sharing to incorporate a certain amount of quantifier scope   ambiguity in order to allow a complete evaluation of all subderivations in a shared forest before making any disambiguation decisions in syntax.3 Various synchronous formalisms have been introduced for associating syntactic representations with logical functions in isomorphic or locally non-isomorphic derivations, including Categorial Grammars (CGs) (Wood, 1993), Synchronous Tree Adjoining Grammars (TAGs) (Joshi, 1985; Shieber and Schabes, 1990; Shieber, 1994), and Synchronous Description Tree Grammars (DTGs) (Rambow et al., 1995; Rambow and Satta, 1996). Most of these formalisms can be extended to define semantic associations over entire shared forests, rather than merely over individual parse trees, in a straightforward manner, preserving the ambiguity of the syntactic forest without exceeding its polynomial size, or the polynomial time complexity of creating or traversing it. Since one of the goals of this architecture is to use the system’s representation of its environment to resolve ambiguity in its instructions, a space-effic"
P01-1061,H89-1033,0,0.0141974,"tenance instructions, the parser is run on a large subset of the Xtag English grammar (XTAG Research Group, 1998), which has been annotated with lexical semantic classes (Kipper et al., 2000) associated with the objects, states, and processes in the maintenance simulation. Since the grammar has several thousand lexical entries, the parser is exposed to considerable lexical and structural ambiguity as a matter of course. The environment-based disambiguation architecture described in this paper has much in common with very early environment-based approaches, such as those described by Winograd (Winograd, 1972), in that it uses the actual entities in an environment database to resolve ambiguity in the input. This research explores two extensions to the basic approach however: 1. It incorporates ideas from type theory to represent a broad range of linguistic phenomena in a manner for which their extensions or potential referents in the environment are welldefined in every case. This is elaborated in Section 2. 2. It adapts the concept of structure sharing, taken from the study of parsing, not only to translate the many possible interpretations of ambiguous sentences into shared logical expressions, b"
P01-1061,P89-1018,0,0.385896,"cognition algorithm (such as CKY (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) or Earley (Earley, 1970)) can be interpreted as a polynomial sized andor graph with space complexity equal to the time complexity of recognition, whose disjunctive nodes represent possible constituents in the analysis, and whose conjunctive nodes represent binary applications of rules in the grammar. This is called a shared forest of parse trees, because it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses (Tomita, 1985; Billot and Lang, 1989), and can be constructed and traversed in time of the same comfor context free grammars). plexity (e.g. For example, the two parse trees for the noun phrase ‘button on handle beside adapter’ shown in Figure 1 can be merged into the single shared forest in Figure 2 without any loss of information. These shared syntactic structures can further be associated with compositional semantic functions that correspond to the syntactic elements in the forest, to create a shared forest of trees each representing a complete expression in some logical form. This extended sharing is similar to the ‘packing’"
P01-1061,P81-1022,0,0.448223,"t change their membership over time.            3 Sharing referents across interpretations Any method for using the environment to guide the interpretation of natural language sentences requires a tractable representation of the many possible interpretations of each input. The representation described here is based on the polynomial-sized chart produced by any dynamic programming recognition algorithm. A record of the derivation paths in any dynamic programming recognition algorithm (such as CKY (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) or Earley (Earley, 1970)) can be interpreted as a polynomial sized andor graph with space complexity equal to the time complexity of recognition, whose disjunctive nodes represent possible constituents in the analysis, and whose conjunctive nodes represent binary applications of rules in the grammar. This is called a shared forest of parse trees, because it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses (Tomita, 1985; Billot and Lang, 1989), and can be constructed and traversed in time of the same comfor context free grammars). p"
P01-1061,W90-0102,0,\N,Missing
P03-1067,E91-1005,0,0.0176749,"PP ! P NP x1 ::: xn=2 : $1(x1 ::: xn ) ^ $2(x1 ) fhh1 ; g1 i; hh2 ; b1 ig NP ! girl x1 : Girl (x1 ) fg1 ; g2 ; g3 g P ! with x1 x2 : With (x2 ; x1 ) fhh1 ; g1 i; hh2 ; b1 ig NP ! hat x1 : Hat (x1 ) fh1 ; h2 ; h3 ; h4 g PP ! PP x2 ::: xn=2 : Qx1 : $1(x1 ::: xn ) fg1 g PP ! P NP x1 ::: xn=2 : $1(x1 ::: xn ) ^ $2(x1 ) fhc1 ; g1 ig P ! behind x1 x2 : Behind (x2 ; x1 ) fhc1 ; g1 ig NP ! counter x1 : Counter (x1 ) fc1 ; c2 g Figure 5: Shared forest for `the girl with the hat behind the counter.&apos; tensions of tree-adjoining grammar (Joshi, 1985), namely multi-component tree adjoining grammar (Becker et al., 1991) and description tree substitution grammar (Rambow et al., 1995), and indeed represents something of a combination of the two: 1. Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2. Like multi-component TAGs, but unlike description tree substitution grammars, it allows the speci cation of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (which is used for merging quanti ers from different elementary trees). The"
P03-1067,P89-1018,0,0.00991581,"regular-form grammar has a regular path set means that only a nite number of states will be required to keep track of this promised, unrecognized structure in a bottom-up traversal, so the parser will have the usual O(n3 ) complexity (times a constant equal to the nite number of possible unrecognized structures). Moreover, since the parser can recognize any string derivable by such a grammar, it can create a shared forest representation of every possible analysis of a given input by annotating every possible application of parse rules that could be used in the derivation of each constituent (Billot and Lang, 1989). This polynomial-sized shared forest representation can then be interpreted determine which constituents denote entities and relations in the world model, in order to allow model-theoretic semantic information to guide disambiguation decisions in parsing. Finally, the regular form restriction also has the important e ect of ensuring that the number of unrecognized quanti er nodes at any step in a bottomup analysis { and therefore the number of free variables in any word or phrase constituent of a parse { is also bounded by some constant, which limits the size of any constituent&apos;s denotation t"
P03-1067,P94-1016,0,0.0376584,"re isomorphic as well, hence the reduction of synchronous tree pairs to semanticallyannotated syntax trees. This isomorphism restriction on derived trees reduces the number of quanti er scoping con gurations that can be assigned to any given input (most of which are unlikely to be used in a practical application), but its relative parsimony allows syntactically ambiguous inputs to be semantically interpreted in a shared forest representation in worst-case polynomial time. The interleaving of semantic evaluation and parsing for the purpose of disambiguation also has much in common with that of Dowding et al. (1994), except that in this case, constituents are not only semantically type-checked, but are also fully interpreted each time they are proposed. There are also commonalities between the underspeci ed semantic representation of structurallyambiguous elementary tree constituents in a shared forest and the underspeci ed semantic representation of (e.g. quanti er) scope ambiguity described by Reyle (1993).3 3 Evaluation The contribution of this model-theoretic semantic information toward disambiguation was evaluated on a set of directions to animated agents collected in a controlled but spatially comp"
P03-1067,P96-1008,0,0.627925,"se interface would require (which also must be precisely described), introducing a great deal of ambiguity into input processing. This paper therefore explores the use of a statistical model of language conditioned on the meanings or denotations of input utterances in the context of an interface&apos;s underlying application environment or world model. This use of model-theoretic interpretation represents an important extension to the `semantic grammars&apos; used in existing statistical spoken language interfaces, which rely on co-occurrences among lexically-determined semantic classes and slot llers (Miller et al., 1996), in that the probability of an analysis is now also conditioned on the existence of denoted entities and relations in the world model. The advantage of the interpretation-based disambiguation advanced here is that the probability of generating, for example, the noun phrase `the lemon next to the safe&apos; can be more reliably estimated from the frequency with which noun phrases have non-empty denotations { given the fact that `the lemon next to the safe&apos; does indeed denote something in the world model { than it can from the relatively sparse co-occurrences of frame labels such as lemon and next-t"
P03-1067,P95-1021,0,0.0139286,"h2 ; b1 ig NP ! girl x1 : Girl (x1 ) fg1 ; g2 ; g3 g P ! with x1 x2 : With (x2 ; x1 ) fhh1 ; g1 i; hh2 ; b1 ig NP ! hat x1 : Hat (x1 ) fh1 ; h2 ; h3 ; h4 g PP ! PP x2 ::: xn=2 : Qx1 : $1(x1 ::: xn ) fg1 g PP ! P NP x1 ::: xn=2 : $1(x1 ::: xn ) ^ $2(x1 ) fhc1 ; g1 ig P ! behind x1 x2 : Behind (x2 ; x1 ) fhc1 ; g1 ig NP ! counter x1 : Counter (x1 ) fc1 ; c2 g Figure 5: Shared forest for `the girl with the hat behind the counter.&apos; tensions of tree-adjoining grammar (Joshi, 1985), namely multi-component tree adjoining grammar (Becker et al., 1991) and description tree substitution grammar (Rambow et al., 1995), and indeed represents something of a combination of the two: 1. Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2. Like multi-component TAGs, but unlike description tree substitution grammars, it allows the speci cation of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (which is used for merging quanti ers from different elementary trees). The use of lambda calculus functions to de ne decomposable meanings"
P03-1067,P94-1022,0,0.0191429,"ion is that such systems allow the application of well-studied restrictions to limit their recursive capacity to generate structural descriptions (in this case, to limit the unbounded overlapping of quanti er-variable dependencies that can produce unlimited numbers of free variables at certain steps in a derivation), without limiting the multi-level structure of their elementary trees, used here for capturing the well-formedness constraint that a predicate be dominated by its variables&apos; quanti ers. One such restriction, based on the regular form restriction de ned for tree adjoining grammars (Rogers, 1994), prohibits a grammar from allowing any cycle of elementary trees, each intervening inside a spine (a path connecting the insertion sites of any argument) of the next. This restriction is de ned below: De nition 2.1 Let a spine in an elementary tree be the path of nodes (or object-level rule applications) connecting all insertion site addresses of the same argument. De nition 2.2 A grammar G is in regular form if a directed acyclic graph hV; E i can be drawn with vertices vH ; vA 2 V corresponding to partitioned elementary trees of G (partitioned as described above), and directed edges hvH ; v"
P08-2027,P06-1021,0,0.301278,"of Shriberg (1994). A speech repair consists of a reparandum, an interruption point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mistake can begin. The alteration contains the ∗ This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors. Recent advances in recognizing spontaneous speech with repairs (Hale et al., 2006; Johnson and Charniak, 2004) have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above. One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error. For instance, in the repair . . . a flight to Boston, uh, I mean, to Denver on Friday . . . , there is an exact match of the word ‘to’ between reparandum and repair, and a part of speech match between the words ‘Boston’ an"
P08-2027,P04-1005,0,0.228797,". A speech repair consists of a reparandum, an interruption point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mistake can begin. The alteration contains the ∗ This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors. Recent advances in recognizing spontaneous speech with repairs (Hale et al., 2006; Johnson and Charniak, 2004) have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above. One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error. For instance, in the repair . . . a flight to Boston, uh, I mean, to Denver on Friday . . . , there is an exact match of the word ‘to’ between reparandum and repair, and a part of speech match between the words ‘Boston’ and ‘Denver’. Another sort of s"
P08-2027,P98-1101,0,0.293772,"o be built up using fluent speech rules up until the moment of interruption, at which point a special repair rule may be applied. This property will be examined further in section 2.3, following a technical description of the representation scheme. 2.1 Binary branching structure In order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of rightcorner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. Trees containing speech repairs are reduced in arity by merging repair structure lower in the tree, when possible. As seen in the left tree below, 1 repair structure is annotated in a flat manner, which can lead to high-arity rules which are sparsely represented in the data set, and thus difficult to l"
P08-2027,P03-1054,0,0.0318216,"fluent speech rules up until the moment of interruption, at which point a special repair rule may be applied. This property will be examined further in section 2.3, following a technical description of the representation scheme. 2.1 Binary branching structure In order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of rightcorner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. Trees containing speech repairs are reduced in arity by merging repair structure lower in the tree, when possible. As seen in the left tree below, 1 repair structure is annotated in a flat manner, which can lead to high-arity rules which are sparsely represented in the data set, and thus difficult to learn. This problem can be mitig"
P08-2027,J98-4004,0,\N,Missing
P08-2027,C98-1098,0,\N,Missing
P09-2070,P06-1021,0,0.246879,"hown that syntactic cues can be used to increase accuracy of detection of reparanda, which can increase overall parsing accuracy. The first source of structure used to recognize repair is what Levelt (1983) called the “Well-formedness Rule.” This rule essentially states that a speech repair acts like a conjunction; that is, the reparandum and the alteration must be of the same syntactic category. Of course, the reparandum is often unfinished, so the Well-formedness Rule allows for the reparandum category to be inferred. This source of structure has been used by two related approaches, that of Hale et al. (2006) and Miller (2009). Hale and colleagues exploit this structure by adding contextual information to the standard reparandum label “EDITED”. In their terminology, daughter annotation takes the (possibly unfinished) constituent label of the reparandum and appends it to the EDITED label. This ∗ This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1 Ferreira et al. use the term ‘revisions’. 277 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277–280, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP S allo"
P09-2070,P03-1054,0,0.0177724,"ndia Figure 1: Section of interest of a standard phrase structure tree containing speech repair with unfinished noun phrase (NP). PP PP/PP IN PP/NP 3 Model Description 3.1 Right-corner transform This work first uses a right-corner transform, which turns right-branching structure into leftbranching structure, using category labels that use a “slash” notation α/γ to represent an incomplete constituent of type α “looking for” a constituent of type γ in order to complete itself. This transform first requires that trees be binarized. This binarization is done in a similar way to Johnson (1998) and Klein and Manning (2003). Rewrite rules for the right-corner transform are as follows, first flattening right-branching structure:2 A1 α1 α2 A3 ⇒ A1 /A2 α1 a3 A1 α1 A3 α2 a3 ... ⇒ A1 /A2 α1 α2 A2 /A3 ... α2 then replacing it with left-branching structure: A1 A1 /A2 :α1 A2 /A3 α2 A1 α3 . . . ⇒ A1 /A3 A1 / A2 :α1 α3 PP/PP IN NP/NN NN EDITEDPP as DT westerner EDITEDPP/NP-UNF NP-UNF IN DT as a india in a an unfinished constituent in the Switchboard corpus is to append the -UNF label to the lowest unfinished constituent (see Figure 1). Since one goal of this work is separation of linguistic knowledge from language process"
P09-2070,N09-1074,1,0.831455,"s can be used to increase accuracy of detection of reparanda, which can increase overall parsing accuracy. The first source of structure used to recognize repair is what Levelt (1983) called the “Well-formedness Rule.” This rule essentially states that a speech repair acts like a conjunction; that is, the reparandum and the alteration must be of the same syntactic category. Of course, the reparandum is often unfinished, so the Well-formedness Rule allows for the reparandum category to be inferred. This source of structure has been used by two related approaches, that of Hale et al. (2006) and Miller (2009). Hale and colleagues exploit this structure by adding contextual information to the standard reparandum label “EDITED”. In their terminology, daughter annotation takes the (possibly unfinished) constituent label of the reparandum and appends it to the EDITED label. This ∗ This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors . 1 Ferreira et al. use the term ‘revisions’. 277 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277–280, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP S allows a learned proba"
P09-2070,J98-4004,0,\N,Missing
P10-1121,P08-2002,0,0.189542,"ts syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). ◦ schuler@ling.ohio-state.edu A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore,"
P10-1121,C00-1017,0,0.499844,"(i.e., from syntax, semantics, discourse) are likely necessary for a full accounting of linguistic complexity, so current computational models (with some exceptions) narrow the scope to syntactic or lexical complexity. Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189–1198, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguis"
P10-1121,D09-1034,1,0.878887,"n for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). ◦ schuler@ling.ohio-state.edu A parser-derived complexity metric such as surprisal can only be as good (empirically) as the model of language from which it derives (Frank, 2009). Ideally, a psychologically-plausible language model would produce a surprisal that would correlate better with linguistic complexity. Therefore, the specification of how to encode a syntactic language model is of utmost importance to the quality of the metric. However, it is difficult to quantify linguistic complexity and reading difficulty. The two commonly-used empirical quantifications of reading difficulty are e"
P10-1121,J01-2004,0,0.445223,"1; 2006), assuming that q1..t ∈ Dt , i.e., that the syntactic states we are considering form derivations Dt , or partial trees, consistent with the observed words. We will see that this is the case for our parser in Sections 2.2–2.4. Entropy is a measure of uncertainty, defined as H(x) = −P(x) log2 P(x). Now, the entropy Ht of a t-word string o1..t in an HMM can be written: X Ht = P(q1..t o1..t ) log2 P(q1..t o1..t ) (4) This mitigates the problems of large state spaces (e.g., that of all possible grammatical derivations). Since beams have been shown to perform well (Brants and Crocker, 2000; Roark, 2001; Boston et al., 2008b), complexity metrics in this paper are calculated on a beam rather than over all (unbounded) possible derivations Dt . The equations above, then, will replace the assumption q1..t ∈ Dt with qt ∈ Bt . 2.2 Hierarchical Hidden Markov Models Hidden states q can have internal structure; in Hierarchical HMMs (Fine et al., 1998; Murphy and Paskin, 2001), this internal structure will be used to represent syntax trees and looks like several HMMs stacked on top of each other. As such, qt is factored into sequences of depth-specific variables — one for each of D levels in the HMM h"
P10-1121,C08-1099,1,0.925043,"erstood that center-embedding incurs significant syntactic processing costs (Miller and Chomsky, 1963; Gibson, 1998). Thus, we would expect for the usage of the center-embedding memory store in an HHMM parser to correlate with reading times (and therefore linguistic complexity). The HHMM parser processes syntactic constructs using a bounded number of store states, defined to represent short-term memory elements; additional states are utilized whenever centerembedded syntactic structures are present. Similar models such as Crocker and Brants (2000) implicitly allow an infinite memory size, but Schuler et al. (2008; 2010) showed that a right-corner HHMM parser can parse most sentences in English with 4 or fewer center-embedded-depth levels. This behavior is similar to the hypothesized size of a human short-term memory store (Cowan, 2001). A positive result in predicting reading times will lend additional validity to the claim that the HHMM parser’s bounded memory corresponds to bounded memory in human sentence processing. The rest of this paper is organized as follows: Section 2 defines the language model of the HHMM parser, including definitions of the three complexity metrics. The methodology for eval"
P10-1121,J10-1001,1,0.831011,"ng times; this paper uses reading times to find the predictiveness of several parser-derived complexity metrics. Various factors (i.e., from syntax, semantics, discourse) are likely necessary for a full accounting of linguistic complexity, so current computational models (with some exceptions) narrow the scope to syntactic or lexical complexity. Three complexity metrics will be calculated in a Hierarchical Hidden Markov Model (HHMM) parser that recognizes trees in right-corner form (the left-right dual of left-corner form). This type of parser performs competitively on standard parsing tasks (Schuler et al., 2010); also, it reflects plausible accounts of human language processing as incremental (Tanenhaus et al., 1995; Brants and Crocker, 2000), as considering hypotheses probabilistically in parallel (Dahan and Gaskell, 2007), as bounding memory usage to short-term memory limits (Cowan, 2001), and as requiring more memory storage for center-embedding structures than for right- or left-branching ones (Chomsky and Miller, 1963; Gibson, 1998). Also, unlike most other parsers, this parser preserves the arceager/arc-standard ambiguity of Abney and John1189 Proceedings of the 48th Annual Meeting of the Assoc"
P10-1121,N01-1021,0,0.760221,"r HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution. 1 Introduction Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal has received a lot of attention in recent literature due to nice mathematical properties (Levy, 2008) and predictive ability on eye-tracking movements (Demberg and Keller, 2008; Boston et al., 2008a). Many other complexity metrics have been suggested as mutually contributing to reading difficulty; for example, entropy reduction (Hale, 2006), bigram probabilities (McDonald and Shillcock, 2003), and split-syntactic/lexical versions of other metrics (Roark et al., 2009). ◦ schuler"
P10-1121,N09-1039,1,0.91056,"the complexity metrics is described in Section 3, with actual results in Section 4. Further discussion on results, and comparisons to other work, are in Section 5. 2 Parsing Model This section describes an incremental parser in which surprisal and entropy reduction are simple calculations (Section 2.1). The parser uses a Hierarchical Hidden Markov Model (Section 2.2) and recognizes trees in a right-corner form (Section 2.3 and 2.4). The new complexity metric, embedding difference (Section 2.5), is a natural consequence of this HHMM definition. The model is equivalent to previous HHMM parsers (Schuler, 2009), but reorganized into 5 cases to clarify the right-corner structure of the parsed sentences. 2.1 Surprisal and Entropy in HMMs Hidden Markov Models (HMMs) probabilistically connect sequences of observed states ot and hidden states qt at corresponding time steps t. In parsing, observed states are words; hidden states can be a conglomerate state of linguistic information, here taken to be syntactic. The HMM is an incremental, time-series structure, so one of its by-products is the prefix probability, which will be used to calculate surprisal. This is the probability that that words o1..t have b"
P11-1063,W07-0702,0,0.0512813,"ains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree s"
P11-1063,D07-1090,0,0.0202207,"n top-down or bottom-up parsing. We provided a rigorous formal definition of incremental syntactic languages models, and detailed what steps are necessary to incorporate such LMs into phrase-based decoding. We integrated an incremental syntactic language model into Moses. The translation quality significantly improved on a constrained task, and the perplexity improvements suggest that interpolating between n-gram and syntactic LMs may hold promise on larger data sets. The use of very large n-gram language models is typically a key ingredient in the best-performing machine translation systems (Brants et al., 2007). Our n-gram model trained only on WSJ is admittedly small. Our future work seeks to incorporate largescale n-gram language models in conjunction with incremental syntactic language models. The added decoding time cost of our syntactic language model is very high. By increasing the beam size and distortion limit of the baseline system, future work may examine whether a baseline system with comparable runtimes can achieve comparable translation quality. A more efficient implementation of the HHMM parser would speed decoding and make more extensive and conclusive translation experiments possible"
P11-1063,J90-2002,0,0.613811,"Missing"
P11-1063,2003.mtsummit-papers.6,0,0.536633,"approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast t"
P11-1063,P98-1035,0,0.45749,"and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing"
P11-1063,P08-1009,0,0.00775866,"on with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. 3 Parser as Syntactic Language Model in Phrase-Based Translation Parsing is the task of selecting the representation τˆ (typically a tree) that best models the structure of ... ... ÀËÂÃÄÅÆ ÀËÂÍÄÅÆ hsi president president Friday τ˜13 τ˜23 ÊÁÂÃÄÅÆ ÊËÂÃÄÅÆ ÊËÌÃÄÅÆ hsi that that president τ˜12 τ˜22 Obama met τ˜32 ÀÁÂÃÄÅÆ ÊÁÂÃÄÅÆ ÊËÂÃÄÅÆ ÊËÌÃÄÅÆ hsi hsi the the president president meets τ˜0 τ˜"
P11-1063,P05-1033,0,0.0800322,"l refer to incremental processing which proceeds from the beginning of a sentence as left-to-right. 620 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics statistical MT that can run in linear-time (§4) • Integration with Moses (§5) along with empirical results for perplexity and significant translation score improvement on a constrained UrduEnglish task (§6) 2 Related Work Neither phrase-based (Koehn et al., 2003) nor hierarchical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examin"
P11-1063,P10-1146,0,0.0152744,"porated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language"
P11-1063,P05-1063,0,0.182058,"ree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translat"
P11-1063,W06-1628,0,0.0338022,"Missing"
P11-1063,D09-1076,0,0.0101563,"extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we inc"
P11-1063,D07-1079,0,0.0114627,"arch has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translat"
P11-1063,P05-1067,0,0.0218033,"e-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to la"
P11-1063,P81-1022,0,0.795817,"set of trees under consideration to τ˜t , that subset of analyses or partial analyses that remains after any pruning is performed. An incremental syntactic language model can then be defined by a probability mass function (Equation 5) and a transition function δ (Equation 6). The role of δ is explained in §3.3 below. Any parser which implements these two functions can serve as a syntactic language model. Incremental syntactic language model An incremental parser processes each token of input sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After 622 P(e1 ...et ) ≈ P(˜ τ t) = X P(e1 ...et |τ )P(τ ) (5) τ ∈˜ τt δ(et , τ˜t−1 ) → τ˜t (6) 3.2 Decoding in phrase-based translation S Given a source language input sentence f , a trained source-to-target translation model, and a target language model, the task of translation is to find the maximally probable translation e ˆ using a linear combination of j feature functions h weighted according to tuned parameters λ (Och and Ney, 2002). X e ˆ = argmax exp( λj hj (e, f )) e DT VP NN VP The president VB PP NP meet"
P11-1063,P03-2041,0,0.0165951,"2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree"
P11-1063,P09-1087,0,0.0804127,"yntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in conc"
P11-1063,N04-1035,0,0.0604707,"ps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work"
P11-1063,P06-1121,0,0.0188307,"may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using synta"
P11-1063,N10-1127,0,0.0204658,"a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. 3 Parser as Syntactic Language Model in Phrase-Based Translation Parsing is the task of selecting the representation τˆ (typically a tree) that best models the structure of ... ... ÀËÂÃÄÅÆ ÀËÂÍÄÅÆ hsi president president Friday τ˜13 τ˜23 ÊÁÂÃÄÅÆ ÊË"
P11-1063,P03-1011,0,0.0126599,"s contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information t"
P11-1063,N04-1014,0,0.00964682,"e of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-b"
P11-1063,P07-1037,0,0.0905615,"Missing"
P11-1063,W04-0305,0,0.0296463,"cal choice, but still often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal"
P11-1063,D10-1027,0,0.0260813,"to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarc"
P11-1063,P10-1110,0,0.0398287,"grammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal definition of an incremental parser for 1 Whil"
P11-1063,2006.amta-papers.8,0,0.0280808,"any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollman"
P11-1063,N03-1017,0,0.0181292,"for 1 While not all languages are written left-to-right, we will refer to incremental processing which proceeds from the beginning of a sentence as left-to-right. 620 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics statistical MT that can run in linear-time (§4) • Integration with Moses (§5) along with empirical results for perplexity and significant translation score improvement on a constrained UrduEnglish task (§6) 2 Related Work Neither phrase-based (Koehn et al., 2003) nor hierarchical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to imp"
P11-1063,P07-2045,1,0.00717479,"-gram WSJ 4-gram WSJ 5-gram WSJ HHMM Interpolated WSJ 5-gram + HHMM In-domain WSJ 23 ppl 1973.57 349.18 262.04 244.12 232.08 384.66 Out-of-domain ur-en dev ppl 3581.72 1312.61 1264.47 1261.37 1261.90 529.41 209.13 225.48 258.35 312.28 222.39 123.10 174.88 321.05 Giga 5-gram Interp. Giga 5-gr + WSJ HHMM Interp. Giga 5-gr + WSJ 5-gram Figure 7: Average per-word perplexity values. HHMM was run with beam size of 2000. Bold indicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics. Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). 6 Results As an initial measure to compare language models, average per-word perplexity, ppl, reports how surprised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens. ppl = b −logb P(e1 ...eT ) T (25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when tr"
P11-1063,J10-4005,0,0.00767483,"l, the task of translation is to find the maximally probable translation e ˆ using a linear combination of j feature functions h weighted according to tuned parameters λ (Och and Ney, 2002). X e ˆ = argmax exp( λj hj (e, f )) e DT VP NN VP The president VB PP NP meets DT NN IN NP on Friday the board (7) Figure 2: Sample binarized phrase structure tree. j Phrase-based translation constructs a set of translation options — hypothesized translations for contiguous portions of the source sentence — from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n-gram language model history is also maintained at each node in the translation lattice. The search space is further trimmed with hypothesis recombination, which collapses lattice nodes that share a common coverage vector and n-gram state. 3.3 NP S S/NP NP S/PP IN Friday S/VP VP NP VP/NN NP/NN NN DT president The on NN VP/NP DT board VB the meets Figure 3: Sample binarized phrase structure tree after application of right-corner transform. Incorporati"
P11-1063,P06-1077,0,0.011446,"hrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation gramma"
P11-1063,P07-1089,0,0.0174076,"ation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonte"
P11-1063,P09-1063,0,0.0305554,"Missing"
P11-1063,J93-2004,0,0.037355,"all in italics. Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). 6 Results As an initial measure to compare language models, average per-word perplexity, ppl, reports how surprised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens. ppl = b −logb P(e1 ...eT ) T (25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when trained on the same WSJ data; the best perplexity results for in-domain and out-of-domain test sets4 are found by interpolating 4 In-domain is WSJ Section 23. Out-of-domain are the English reference translations of the dev section , set aside in (Baker et al., 2009) for parameter tuning, of the NIST Open MT 2008 Urdu-English task. Sentence length 10 20 30 40 Moses 0.21 0.53 0.85 1.13 +HHMM beam=50 533 1193 1746 2095 +HHMM beam=2000 1143 2562 3749 4588 Moses LM(s) n-gram only HHMM + n-gram Figure 8: Mean"
P11-1063,P04-1083,0,0.00963529,") which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine tr"
P11-1063,D08-1022,0,0.0146806,"g translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substanti"
P11-1063,P08-1023,0,0.0238383,"hether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can d"
P11-1063,2006.amta-papers.15,0,0.0202505,"ove translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a stan"
P11-1063,P02-1038,0,0.127444,"he beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After 622 P(e1 ...et ) ≈ P(˜ τ t) = X P(e1 ...et |τ )P(τ ) (5) τ ∈˜ τt δ(et , τ˜t−1 ) → τ˜t (6) 3.2 Decoding in phrase-based translation S Given a source language input sentence f , a trained source-to-target translation model, and a target language model, the task of translation is to find the maximally probable translation e ˆ using a linear combination of j feature functions h weighted according to tuned parameters λ (Och and Ney, 2002). X e ˆ = argmax exp( λj hj (e, f )) e DT VP NN VP The president VB PP NP meets DT NN IN NP on Friday the board (7) Figure 2: Sample binarized phrase structure tree. j Phrase-based translation constructs a set of translation options — hypothesized translations for contiguous portions of the source sentence — from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n-gram language model history is"
P11-1063,2008.amta-papers.16,0,0.0493922,"f n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-ba"
P11-1063,P05-1034,0,0.0411963,"els: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621"
P11-1063,J01-2004,0,0.258771,"terms of lexical choice, but still often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation"
P11-1063,J10-1001,1,0.917078,"till often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal definition of an incre"
P11-1063,N09-1039,1,0.931192,"f rt = r⊥ : PθS-T-W,d (st |st−1 rt ) (14) def PθR,d (rtd |rtd+1 sdt−1 sd−1 t−1 ) =  if crtd+1 6= xt : Jrtd = r⊥ K (15) if crtd+1 = xt : PθR-R,d (rtd |sdt−1 sd−1 t−1 ) These HHMM right-corner parsing operations are then defined in terms of branch- and depth-specific PCFG probabilities θG-R,d and θG-L,d : 3 3 (13) where r⊥ is a null state resulting from the failure of an incomplete constituent to complete, and constants are defined for the edge conditions of s0t and rtD+1 . Figure 5 illustrates this model in action. These pushdown automaton operations are then refined for right-corner parsing (Schuler, 2009), distinguishing active transitions (model θS-T-A,d , in which an incomplete constituent is completed, but not reduced, and then immediately expanded to a Model probabilities are also defined in terms of leftprogeny probability distribution EθG-RL∗,d which is itself defined in terms of PCFG probabilities: X 0 def EθG-RL∗,d (cη → cη0 ...) = PθG-R,d (cη → cη0 cη1 ) (16) cη1 k def EθG-RL∗,d (cη → cη0k 0 ...) = X k−1 EθG-RL∗,d (cη → cη0k ...) cη0k · X PθG-L,d (cη0k → cη0k 0 cη0k 1 ) ∗ def + def EθG-RL∗,d (cη → cηι ...) = ∞ X k EθG-RL∗,d (cη → cηι ...) (18) k=0 ∗ EθG-RL∗,d (cη → cηι ...) = EθG-RL∗,"
P11-1063,P08-1066,0,0.0549959,"tituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 200"
P11-1063,C90-3045,0,0.410186,"improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009)."
P11-1063,W04-3312,0,0.0139479,"nd such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translat"
P11-1063,J97-3002,0,0.0534401,"tructure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in"
P11-1063,P01-1067,0,0.07917,"chniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic"
P11-1063,2007.mtsummit-papers.71,0,0.0231704,"ty. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic)"
P11-1063,W06-3119,0,0.0318199,"., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1"
P11-1063,C90-3001,0,\N,Missing
P11-1063,C00-2092,0,\N,Missing
P11-1063,C04-1015,0,\N,Missing
P11-1063,C98-1035,0,\N,Missing
P11-1063,N04-1021,0,\N,Missing
P11-1063,W90-0102,0,\N,Missing
P11-1117,E09-1018,0,0.452536,"mention from a combination of global entity properties and local attentional state. Ng (2008) did similar work using the same unsupervised generative model, but relaxed head generation as head-index generation, enforced agreement constraints at the global level, and assigned salience only to pronouns. Another unsupervised generative model was recently presented to tackle only pronoun anaphora Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169–1178, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics resolution (Charniak and Elsner, 2009). The expectation-maximization algorithm (EM) was applied to learn parameters automatically from the parsed version of the North American News Corpus (McClosky et al., 2008). This model generates a pronoun’s person, number and gender features along with the governor of the pronoun and the syntactic relation between the pronoun and the governor. This inference process allows the system to keep track of multiple hypotheses through time, including multiple different possible histories of the discourse. Haghighi and Klein (2010) improved their nonparametric model by sharing lexical statistics at t"
P11-1117,P05-2004,0,0.0293126,"te signifies that there is a new mention in the present time step. In this event, a new mention will be added to the entity set, as represented by its set of feature values and position in the coreference table. The old state indicates that there is a mention in the present time state and that this mention refers back to some antecedent mention. In such a case, the list of entities in the buffer will be reordered deterministically, moving the currently mentioned entity to the top of the list. Notice that opt is defined to depend on opt−1 and post−1 . This is sometimes called a switching FHMM (Duh, 2005). This dependency can be useful, for example, if opt−1 is new, in which case opt has a higher probability of being none or old. If post−1 is a verb or preposition, opt has more probability of being old or new. One may wonder why opt generates post , and not the other way around. This model only roughly models the process of (new and old) entity generation, and either direction of causality might be consistent with a model of human entity generation, but this direction of causality is chosen to represent the effect of semantics (referents) generating syntax (POS tags). In addition, this is a jo"
P11-1117,P07-1107,0,0.0284702,"other, there may be conflicting coreferences. Graph-partitioning methods attempt to reconcile pairwise scores into a final coherent clustering, but they are combinatorially harder to work with in discriminative approaches. One line of research aiming at overcoming the limitation of pairwise models is to learn a mentionranking model to rank preceding mentions for a given anaphor (Denis and Baldridge, 2007) This approach results in more coherent coreference chains. Recent years have also seen the revival of interest in generative models in both machine learning and natural language processing. Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. In contrast to pairwise models, this fully generative model produces each mention from a combination of global entity properties and local attentional state. Ng (2008) did similar work using the same unsupervised generative model, but relaxed head generation as head-index generation, enforced agreement constraints at the global level, and assigned salience only to pronouns. Another unsupervised generative model was recently presented to tackle only pronoun anaphora Proceedings of the 49th Annual Meeting of the"
P11-1117,D09-1120,0,0.0142373,"and other-pronoun. We then define a variable loct whose value is how far back in the list of antecedents the current hypothesis must have gone to arrive at the current value of it . If we have the syntax annotations or parsed trees, then, the part of speech model can be defined when opt is old as Pbinding (post |loct , sloct ). For example, if post ∈ ref lexive, P(post |loct , sloct ) where loct has smaller values (implying closer mentions to post ) and sloct = subject should have higher values since reflexive pronouns always refer back to subjects within its governing domains. This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al., 2006) in which syntactic roles are annotated. We finally switched to the ACE corpus for the purpose of comparison with other work. In the ACE corpus, no syntactic roles are annotated. We did use the Stanford parser to extract syntactic roles from the ACE corpus. But the result is largely affected by the parsing accuracy. Again, for a fair comparison, we extract similar features to Denis and Baldridge (2007), which is the model we mainly compare with. They approximate syntactic contexts with POS tags surrounding the pronou"
P11-1117,N10-1061,0,0.0980984,"24, 2011. 2011 Association for Computational Linguistics resolution (Charniak and Elsner, 2009). The expectation-maximization algorithm (EM) was applied to learn parameters automatically from the parsed version of the North American News Corpus (McClosky et al., 2008). This model generates a pronoun’s person, number and gender features along with the governor of the pronoun and the syntactic relation between the pronoun and the governor. This inference process allows the system to keep track of multiple hypotheses through time, including multiple different possible histories of the discourse. Haghighi and Klein (2010) improved their nonparametric model by sharing lexical statistics at the level of abstract entity types. Consequently, their model substantially reduces semantic compatibility errors. They report the best results to date on the complete end-to-end coreference task. Further, this model functions in an online setting at mention level. Namely, the system identifies mentions from a parse tree and resolves resolution with a left-to-right sequential beam search. This is similar to Luo (2005) where a Bell tree is used to score and store the searching path. In this paper, we present a supervised prono"
P11-1117,hasler-etal-2006-nps,0,0.0153502,"e list of antecedents the current hypothesis must have gone to arrive at the current value of it . If we have the syntax annotations or parsed trees, then, the part of speech model can be defined when opt is old as Pbinding (post |loct , sloct ). For example, if post ∈ ref lexive, P(post |loct , sloct ) where loct has smaller values (implying closer mentions to post ) and sloct = subject should have higher values since reflexive pronouns always refer back to subjects within its governing domains. This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al., 2006) in which syntactic roles are annotated. We finally switched to the ACE corpus for the purpose of comparison with other work. In the ACE corpus, no syntactic roles are annotated. We did use the Stanford parser to extract syntactic roles from the ACE corpus. But the result is largely affected by the parsing accuracy. Again, for a fair comparison, we extract similar features to Denis and Baldridge (2007), which is the model we mainly compare with. They approximate syntactic contexts with POS tags surrounding the pronoun. Inspired by this idea, we successfully represent binding features with POS"
P11-1117,P03-1054,0,0.0105214,"ning, the distribution of its part of speech, gender, number and entity type will be unknown. In this case, a special unknown words model is used. The part of speech of unknown words P(post |wt = unkword) is estimated using a decision tree model. This decision tree is built by splitting letters in words from the end of the word backward to its beginning. A P OS tag is assigned to the word after comparisons between the morphological features of words trained from the corpus and the strings concatenated from the tree leaves are made. This method is about as accurate as the approach described by Klein and Manning (2003). Next, a similar model is set up for estimating P(nt |wt = unkword). Most English words have regular plural forms, and even irregular words have their patterns. Therefore, the morphological features of English words can often be used to determine whether a word is singular or plural. Gender is irregular in English, so model-based predictions are problematic. Instead, we follow Bergsma and Lin (2005) to get the distribution of gender from their gender/number data and then predict the gender for unknown words. 4 Evaluation and Discussion 4.1 Experimental Setup In this research, we used the ACE"
P11-1117,H05-1004,0,0.040718,"ple hypotheses through time, including multiple different possible histories of the discourse. Haghighi and Klein (2010) improved their nonparametric model by sharing lexical statistics at the level of abstract entity types. Consequently, their model substantially reduces semantic compatibility errors. They report the best results to date on the complete end-to-end coreference task. Further, this model functions in an online setting at mention level. Namely, the system identifies mentions from a parse tree and resolves resolution with a left-to-right sequential beam search. This is similar to Luo (2005) where a Bell tree is used to score and store the searching path. In this paper, we present a supervised pronoun resolution system based on Factorial Hidden Markov Models (FHMMs). This system is motivated by human processing concerns, by operating incrementally and maintaining a limited short term memory for holding recently mentioned referents. According to Clark and Sengul (1979), anaphoric definite NPs are much faster retrieved if the antecedent of a pronoun is in immediately previous sentence. Therefore, a limited short term memory should be good enough for resolving the majority of pronou"
P11-1117,P00-1023,0,0.0503629,"erns, by operating incrementally and maintaining a limited short term memory for holding recently mentioned referents. According to Clark and Sengul (1979), anaphoric definite NPs are much faster retrieved if the antecedent of a pronoun is in immediately previous sentence. Therefore, a limited short term memory should be good enough for resolving the majority of pronouns. In order to construct an operable model, we also measured the average distance between pronouns and their antecedents as discussed in next sections and used distances as important salience features in the model. Second, like Morton (2000), the current system essentially uses prior information as a discourse model with a time-series manner, using a dynamic programming inference algorithm. Third, the FHMM described here is an integrated system, in contrast with (Haghighi and Klein, 2010). The model generates part of speech tags as simple structural information, as well as related semantic information at each time step or word-by-word step. 1170 While the framework described here can be extended to deeper structural information, POS tags alone are valuable as they can be used to incorporate the binding features (described below)."
P11-1117,D08-1067,0,0.0270022,"he limitation of pairwise models is to learn a mentionranking model to rank preceding mentions for a given anaphor (Denis and Baldridge, 2007) This approach results in more coherent coreference chains. Recent years have also seen the revival of interest in generative models in both machine learning and natural language processing. Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. In contrast to pairwise models, this fully generative model produces each mention from a combination of global entity properties and local attentional state. Ng (2008) did similar work using the same unsupervised generative model, but relaxed head generation as head-index generation, enforced agreement constraints at the global level, and assigned salience only to pronouns. Another unsupervised generative model was recently presented to tackle only pronoun anaphora Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169–1178, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics resolution (Charniak and Elsner, 2009). The expectation-maximization algorithm (EM) was applied to learn p"
P11-1117,qiu-etal-2004-public,0,0.0342988,"document refer to the same entity. Adopting terminology used in the Automatic Context Extraction (ACE) program (NIST, 2003), these expressions are called mentions. Each mention is a reference to some entity in the domain of discourse. Mentions usually fall into three categories – proper mentions (proper names), nominal mentions (descriptions), and pronominal mentions (pronouns). There is a great deal of related work on this subject, so the descriptions of other systems below are those which are most related or which the current model has drawn insight from. Pairwise models (Yang et al., 2004; Qiu et al., 2004) and graph-partitioning methods (McCallum 1169 William Schuler The Ohio State University Columbus, Ohio schuler@ling.osu.edu and Wellner, 2003) decompose the task into a collection of pairwise or mention set coreference decisions. Decisions for each pair or each group of mentions are based on probabilities of features extracted by discriminative learning models. The aforementioned approaches have proven to be fruitful; however, there are some notable problems. Pairwise modeling may fail to produce coherent partitions. That is, if we link results of pairwise decisions to each other, there may b"
P11-1117,P04-1017,0,0.0232196,"c expressions in a document refer to the same entity. Adopting terminology used in the Automatic Context Extraction (ACE) program (NIST, 2003), these expressions are called mentions. Each mention is a reference to some entity in the domain of discourse. Mentions usually fall into three categories – proper mentions (proper names), nominal mentions (descriptions), and pronominal mentions (pronouns). There is a great deal of related work on this subject, so the descriptions of other systems below are those which are most related or which the current model has drawn insight from. Pairwise models (Yang et al., 2004; Qiu et al., 2004) and graph-partitioning methods (McCallum 1169 William Schuler The Ohio State University Columbus, Ohio schuler@ling.osu.edu and Wellner, 2003) decompose the task into a collection of pairwise or mention set coreference decisions. Decisions for each pair or each group of mentions are based on probabilities of features extracted by discriminative learning models. The aforementioned approaches have proven to be fruitful; however, there are some notable problems. Pairwise modeling may fail to produce coherent partitions. That is, if we link results of pairwise decisions to each"
P19-1234,P15-1135,0,0.0254808,"shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate d"
P19-1234,W06-2912,0,0.348131,"Missing"
P19-1234,K18-2005,0,0.0188753,"rger corpora. Because the inside algorithm is quadratic on the length of the sentences, the batch size for training gets quadratically smaller from 400 to 1 as sentences get longer. We use the Adam optimizer (Kingma and Ba, 2015), initialized with learning rates 0.1 for d and N, and 0.001 for L and parameters in g−1 . Means and standard deviations of evaluation metrics are reported in tables with 10 runs of the proposed system. We use ELMo embeddings (Peters et al., 2018) with 1024 dimensions from averaging representations from two BiLSTM layers and the word encoder in ELMo for all languages (Che et al., 2018).4 These embeddings are each trained with 20 million words from Wikipedia and Common Crawl. We initialize d and N with multinomials drawn from a Dirichlet distribution with 0.2 as the concentration parameter, following PCFG induction work with Bayesian models (Jin et al., 2018b). We assign the same diagonal variance matrix to all latent Gaussian distributions, calculated empirically from embeddings from 5000 randomly sampled sentences. M is initialized with the empirical mean of the same sampled embeddings, but with random Gaussian noise added to each row. The parameters of the normalizing flo"
P19-1234,D10-1056,0,0.030481,"unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Reze"
P19-1234,P99-1065,0,0.474841,"Missing"
P19-1234,W18-5452,0,0.256397,"ping constituents that have a single incoming and no outgoing dependency arc. For example, constituents like noun phrases that are kept in conversion may only have one incoming arc from the main verb, and no outgoing arc to any modifier. Each dataset has 15,000 sentences randomly sampled from the dependency treebank (if the treebank has enough sentences), or is augmented with sentences randomly sampled from Wikipedia (if the treebank has fewer sentences). Finally, unlabeled parsing experiments on the three constituency treebanks are reported, one following Jin et al. (2018a) and one following Htut et al. (2018). The hyperparameters of the model for all experiments are tuned on the Brown Corpus portion of the Penn Treebank. We set the number of categories C to 30, the categorical distance constraint strength λ1 to be 0.0001, and the drifting penalty 3 WSJ20test is the second half of WSJ20. λ2 to be 10. Function g−1 is set to have 4 coupling layers with q(i) being a feed-forward network with one hidden layer for both NICE and Real NVP, following He et al. (2018). We train the system until the marginal likelihood over the whole training set starts to oscillate, around 10,000 batches for smaller corpora"
P19-1234,D16-1073,0,0.133786,") induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved performance on POS ind"
P19-1234,D18-1292,1,0.337993,"oys context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information1 . Linguistically motivated similarity penalty and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers. 1 Introduction Unsupervised PCFG inducers (Jin et al., 2018b) automatically bracket sentences into nested spans, and label these spans with consistent, linguistically relevant syntactic categories, which may be useful in downstream applications or linguistic research on under-resourced languages. Their success also provides evidence for learnability of grammar in absence of strong linguistic universals (MacWhinney and Bates, 1993; Plunkett and Wood, 2004; Bannard et al., 2009). However, current PCFG induction models, using word tokens 1 The code can be found at https://github.com/ lifengjin/acl_flow as input, are unable to incorporate semantics and mo"
P19-1234,N07-1018,0,0.307333,"y-motivated regularization terms help the flow-based model perform even better. Most notably, the similarity performance helps the flow models greatly by restricting the freedom that the flow models have to change the context embeddings, indicating that the information in context embeddings is valuable for induction. The Real NVP model produces higher data likelihood but its performance is lower than other NICE-based models, indicating that the volume-preserving property of NICE is important for preventing overfitting. 6 Related work Earlier work on PCFG induction (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012) shows that directly inducing PCFGs from raw text is difficult. Recent work (Shain et al., 2016; Jin et al., 2018b,a) shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures"
P19-1234,P02-1017,0,0.582533,"normalizing flow g−1 are initialized from a uniform distribution with 0 mean and √ a standard deviation of 1/D. For labeled constituency evaluation, we compare against the state-of-the-art PCFG induction system DIMI (D2K15: depth bounded at 2 and 15 categories; Jin et al., 2018a) which takes word tokens as input and produces labeled trees.5 For unlabeled constituency evaluation, results from other unsupervised systems are used for comparison, including CCL (Seginer, 2007), UPPARSE (Ponvert et al., 2011), PRPN (Shen et al., 2018), as well as systems which use gold part-of-speech tags: DMV+CCM (Klein and Manning, 2002) and UML-DOP (Bod, 2006). 4 https://github.com/HIT-SCIR/ ELMoForManyLangs. 5 The DB-PCFG system (Jin et al., 2018b) is formally equivalent to the DIMI system. 2445 Model WSJ20test µ(σ) max WSJ µ(σ) CTB20 max DIMI 23.0(6.5) 34.1 this work 22.8(6.0) 24.0 22.2(3.8) 27.0 µ(σ) CTB max µ(σ) NEGRA20 max 15.4(4.4) 20.7 19.7(1.9) 24.0 13.8(3.4) 20.2 µ(σ) max NEGRA µ(σ) max 13.6(1.6) 17.5 26.2(2.8) 30.4 24.5(2.7) 29.1 Table 1: Recall-V-Measure scores for labeled grammar induction models trained on the listed treebanks with punctuation. For all tables, µ (σ) means the mean (standard deviation) of the rep"
P19-1234,P04-1061,0,0.553182,"induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex"
P19-1234,P09-1011,0,0.0412885,"tion terms help the flow-based model perform even better. Most notably, the similarity performance helps the flow models greatly by restricting the freedom that the flow models have to change the context embeddings, indicating that the information in context embeddings is valuable for induction. The Real NVP model produces higher data likelihood but its performance is lower than other NICE-based models, indicating that the volume-preserving property of NICE is important for preventing overfitting. 6 Related work Earlier work on PCFG induction (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012) shows that directly inducing PCFGs from raw text is difficult. Recent work (Shain et al., 2016; Jin et al., 2018b,a) shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised pa"
P19-1234,N15-1144,0,0.026577,"Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved performance on POS induction and dependency induction by incorporating normalizing flows into baseline models (Klein and Manning, 2004; Lin et al., 2015). 7 Conclusion This work proposes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information. Linguistically motivated similarity penalty and categorical distance constraints are also imposed on the inducer as regularization. Labeled and unlabeled evaluation shows that the PCFG induction model with normalizing flow and context embeddings produces grammars with state-of-the-art accuracy on a variety of different languages. Results show consistent and meaningfu"
P19-1234,J93-2004,0,0.0678061,"xη to penalize the output drifting too far from the input 2444 embedding. The final objective to maximize is: |σ| L(σ) = X 1 X > log P(σi ) + λ1 kδ> d M − δe Mk2 |σ |i=0 d,e X −1 − λ2 kg (xη ) − xη k2 , (10) η∈σi where σ is a minibatch of sentences, a, b, c, d, e are all category labels, λ1 and λ2 are the weights for the two regularization terms and k . . . kn is the n-norm. 5 Experiments We report results of labeled parsing evaluation and unlabeled parsing evaluation against existing grammar induction and unsupervised parsing models. We evaluate our models on full English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) constituency treebanks and the 20-or-fewer-word subsets for labeled parsing performance.3 For unlabeled parsing evaluation, we first report results on a set of languages with complex morphology chosen prior to evaluation. This set includes Czech and Russian, which are fusional languages, Korean and Uyghur, which are agglutinative languages, and Finnish, which has elements of both types. Dependency trees from the Universal Dependency Treebank (Nivre et al., 2016) of these languages are converted int"
P19-1234,D14-1162,0,0.0858082,"nsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved performance on POS induction and dependency induction by incorporating normalizing flows into baseline models (Klein and Manning, 2004; Lin et al., 2015). 7 Con"
P19-1234,N18-1202,0,0.324098,"linois.edu Abstract Unsupervised PCFG inducers hypothesize sets of compact context-free rules as explanations for sentences. These models not only provide tools for low-resource languages, but also play an important role in modeling language acquisition (Bannard et al., 2009; Abend et al., 2017). However, current PCFG induction models, using word tokens as input, are unable to incorporate semantics and morphology into induction, and may encounter issues of sparse vocabulary when facing morphologically rich languages. This paper describes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information1 . Linguistically motivated similarity penalty and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers. 1 Introduction Unsupervised PCFG inducers (Jin et al., 2018b) automatically bracket sen"
P19-1234,P11-1108,0,0.426482,"Missing"
P19-1234,D07-1043,0,0.0269832,"13.8(3.4) 20.2 µ(σ) max NEGRA µ(σ) max 13.6(1.6) 17.5 26.2(2.8) 30.4 24.5(2.7) 29.1 Table 1: Recall-V-Measure scores for labeled grammar induction models trained on the listed treebanks with punctuation. For all tables, µ (σ) means the mean (standard deviation) of the reported scores. 5.1 Labeled parsing evaluation Metric: Labeled trees induced by DIMI (Jin et al., 2018a) and the flow-based system are evaluated on six different datasets. In this evaluation, predicted labels of induced constituents that are in gold trees are compared against gold labels of these constituents6 using V-Measure (Rosenberg and Hirschberg, 2007). Recall of the induced trees is used to weight these V-Measure scores. The final Recall-V-Measure (RVM) score is computed as the product of these two measures. RVM can be maximized when gold constituents are included in induced trees and their clustering is consistent with gold annotation. RVM is equal to unlabeled recall when the matching constituents have the same clustering of labels as the gold annotation. Results: Left- and right-branching baselines are constructed by assigning 21 random labels7 to constituents in purely left- and right-branching trees. However, both branching baselines"
P19-1234,P07-1049,0,0.874694,"itialized with the empirical mean of the same sampled embeddings, but with random Gaussian noise added to each row. The parameters of the normalizing flow g−1 are initialized from a uniform distribution with 0 mean and √ a standard deviation of 1/D. For labeled constituency evaluation, we compare against the state-of-the-art PCFG induction system DIMI (D2K15: depth bounded at 2 and 15 categories; Jin et al., 2018a) which takes word tokens as input and produces labeled trees.5 For unlabeled constituency evaluation, results from other unsupervised systems are used for comparison, including CCL (Seginer, 2007), UPPARSE (Ponvert et al., 2011), PRPN (Shen et al., 2018), as well as systems which use gold part-of-speech tags: DMV+CCM (Klein and Manning, 2002) and UML-DOP (Bod, 2006). 4 https://github.com/HIT-SCIR/ ELMoForManyLangs. 5 The DB-PCFG system (Jin et al., 2018b) is formally equivalent to the DIMI system. 2445 Model WSJ20test µ(σ) max WSJ µ(σ) CTB20 max DIMI 23.0(6.5) 34.1 this work 22.8(6.0) 24.0 22.2(3.8) 27.0 µ(σ) CTB max µ(σ) NEGRA20 max 15.4(4.4) 20.7 19.7(1.9) 24.0 13.8(3.4) 20.2 µ(σ) max NEGRA µ(σ) max 13.6(1.6) 17.5 26.2(2.8) 30.4 24.5(2.7) 29.1 Table 1: Recall-V-Measure scores for lab"
P19-1234,C16-1092,1,0.881982,"he flow models greatly by restricting the freedom that the flow models have to change the context embeddings, indicating that the information in context embeddings is valuable for induction. The Real NVP model produces higher data likelihood but its performance is lower than other NICE-based models, indicating that the volume-preserving property of NICE is important for preventing overfitting. 6 Related work Earlier work on PCFG induction (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012) shows that directly inducing PCFGs from raw text is difficult. Recent work (Shain et al., 2016; Jin et al., 2018b,a) shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation"
P19-1234,W16-5907,0,0.058664,"onvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved p"
P19-1234,xia-etal-2000-developing,0,0.0637969,"2444 embedding. The final objective to maximize is: |σ| L(σ) = X 1 X > log P(σi ) + λ1 kδ> d M − δe Mk2 |σ |i=0 d,e X −1 − λ2 kg (xη ) − xη k2 , (10) η∈σi where σ is a minibatch of sentences, a, b, c, d, e are all category labels, λ1 and λ2 are the weights for the two regularization terms and k . . . kn is the n-norm. 5 Experiments We report results of labeled parsing evaluation and unlabeled parsing evaluation against existing grammar induction and unsupervised parsing models. We evaluate our models on full English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) constituency treebanks and the 20-or-fewer-word subsets for labeled parsing performance.3 For unlabeled parsing evaluation, we first report results on a set of languages with complex morphology chosen prior to evaluation. This set includes Czech and Russian, which are fusional languages, Korean and Uyghur, which are agglutinative languages, and Finnish, which has elements of both types. Dependency trees from the Universal Dependency Treebank (Nivre et al., 2016) of these languages are converted into constituency trees (Collins et al., 1999) by keeping"
P19-1235,P99-1065,0,0.291393,"Missing"
P19-1235,N12-1069,0,0.0253326,"his weak correlation points to the fact that the maximization of data likelihood at convergence may be non-optimal for model selection, and this non-optimality indicates other conRelated work Induction of PCFGs has previously been considered a difficult problem (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012). Earlier work attributed the lack of success for induction to a lack of correlation between parsing accuracy and data likelihood (Johnson et al., 2007), or to the likelihood function or the posterior being filled with weak local optima (Liang et al., 2009; Gimpel and Smith, 2012). Later work has shown that it is possible to induce PCFGs with useful labels from words alone (Shain et al., 2016; Jin et al., 2018b,a). Induction models of constituency grammars or trees usually use data likelihood as both the objective and the model selection criterion (Seginer, 2007; Johnson et al., 2007; Ponvert et al., 2011; Shen et al., 2018), but the weak correlation between data likelihood and parsing accuracy hints at the non-optimality of this practice (Smith, 2006; Headden et al., 2009; Jin 2453 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
P19-1235,D18-1292,1,0.668144,"model selection provides a significant accuracy boost. Further evidence shows VAS to be a better candidate than data likelihood for predicting word order typology classification. Analyses show that VAS seems to separate content words from function words in natural language grammars, and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory. 1 2 Introduction Unsupervised grammar induction models learn to produce hierarchical structures for strings of words. Previous work (Seginer, 2007; Ponvert et al., 2011; Shain et al., 2016; Jin et al., 2018b) show that using data likelihood as both the objective for optimization and the criterion for model selection, either implicitly (in the case of Bayesian models) or explicitly (in the case of EM), gives good results on grammar induction. However, it is also known that data likelihood is only weakly correlated with parsing accuracy, especially at convergence (Smith, 2006; Johnson et al., 2007; Jin et al., 2018a). This weak correlation points to the fact that the maximization of data likelihood at convergence may be non-optimal for model selection, and this non-optimality indicates other conRe"
P19-1235,D16-1004,0,0.122904,"of the evaluation metrics used in the generative linguistics tradition is the complexity of a grammar (Chomsky, 1965). Often the number of rules is used as a proxy measurement of how complex a proposed grammatical analysis is against some other reference grammatical analysis. According to this theory, fewer unique rules present in the Viterbi parses would indicate higher grammar quality. Average stack depth Embedding depth is a known limiting factor to human sentence processing (Chomsky and Miller, 1963; Wu, 2010; Rajkumar et al., 2016), and is shown to benefit unsupervised grammar induction (Noji and Johnson, 2016; Jin et al., 2018b). It is also evaluated in this work as a predictor of parsing accuracy, defined as the expected number of stack elements per sentence in a left-corner parser for the Viterbi parses. Theories such as that of Chomsky and Miller (1963) predict it to correlate negatively with parsing accuracy. Zipf likelihood ratio The distribution of words in a corpus is known to follow Zipf’s law (Zipf, 1935), in which the frequency of a word is inversely proportional to its frequency rank. Counts of syntactic rules in annotated corpora also follow this law (Yang, 2013). 2454 Motivated by thi"
P19-1235,P11-1108,0,0.387246,"Missing"
P19-1235,P07-1049,0,0.754684,"ihood, and that using VAS instead of data likelihood for model selection provides a significant accuracy boost. Further evidence shows VAS to be a better candidate than data likelihood for predicting word order typology classification. Analyses show that VAS seems to separate content words from function words in natural language grammars, and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory. 1 2 Introduction Unsupervised grammar induction models learn to produce hierarchical structures for strings of words. Previous work (Seginer, 2007; Ponvert et al., 2011; Shain et al., 2016; Jin et al., 2018b) show that using data likelihood as both the objective for optimization and the criterion for model selection, either implicitly (in the case of Bayesian models) or explicitly (in the case of EM), gives good results on grammar induction. However, it is also known that data likelihood is only weakly correlated with parsing accuracy, especially at convergence (Smith, 2006; Johnson et al., 2007; Jin et al., 2018a). This weak correlation points to the fact that the maximization of data likelihood at convergence may be non-optimal for mo"
P19-1235,N07-1018,0,0.700516,"inguistic theory. 1 2 Introduction Unsupervised grammar induction models learn to produce hierarchical structures for strings of words. Previous work (Seginer, 2007; Ponvert et al., 2011; Shain et al., 2016; Jin et al., 2018b) show that using data likelihood as both the objective for optimization and the criterion for model selection, either implicitly (in the case of Bayesian models) or explicitly (in the case of EM), gives good results on grammar induction. However, it is also known that data likelihood is only weakly correlated with parsing accuracy, especially at convergence (Smith, 2006; Johnson et al., 2007; Jin et al., 2018a). This weak correlation points to the fact that the maximization of data likelihood at convergence may be non-optimal for model selection, and this non-optimality indicates other conRelated work Induction of PCFGs has previously been considered a difficult problem (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012). Earlier work attributed the lack of success for induction to a lack of correlation between parsing accuracy and data likelihood (Johnson et al., 2007), or to the likelihood function or the posterior being filled with weak local optim"
P19-1235,C16-1092,1,0.633215,"data likelihood for model selection provides a significant accuracy boost. Further evidence shows VAS to be a better candidate than data likelihood for predicting word order typology classification. Analyses show that VAS seems to separate content words from function words in natural language grammars, and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory. 1 2 Introduction Unsupervised grammar induction models learn to produce hierarchical structures for strings of words. Previous work (Seginer, 2007; Ponvert et al., 2011; Shain et al., 2016; Jin et al., 2018b) show that using data likelihood as both the objective for optimization and the criterion for model selection, either implicitly (in the case of Bayesian models) or explicitly (in the case of EM), gives good results on grammar induction. However, it is also known that data likelihood is only weakly correlated with parsing accuracy, especially at convergence (Smith, 2006; Johnson et al., 2007; Jin et al., 2018a). This weak correlation points to the fact that the maximization of data likelihood at convergence may be non-optimal for model selection, and this non-optimality ind"
P19-1235,W04-3241,0,0.348427,"ng, 2017). Dryer (1992) argues that grammars with certain constituent ordering should produce trees with consistent branching tendencies, which is in contrast to theories that attribute constituent ordering to processing (Hawkins, 1994; Gibson, 1998). Rajkumar et al. (2016) and Jin et al. (2018b) show that grammars should generally control the maximal allowed stack depth. Yang (2013) observes that rules in a natural language grammar follow Zipf’s law, just like words. Grammars may also contribute to the observation that the likelihood of each sentence tends to decrease as a monologue goes on (Keller, 2004; Levy and Jaeger, 2007). are non-phrasal lexical categories, from ‘object patterners,’ which are phrasal categories. It predicts that VO languages tend towards rightbranching structures and OV languages tend towards left-branching structures. Let |cright → a b| be the number of right children of a parent expanding into two non-terminal categories in all parse trees, and |c∗ → a b |be the total number of nodes that expand into two non-terminal categories, then 3 Rule complexity Predictors Motivated by these constraints, six accuracy predictors — data likelihood, right-branching score, rule com"
P19-1235,P09-1011,0,0.0153968,"in the case of Bayesian models) or explicitly (in the case of EM), gives good results on grammar induction. However, it is also known that data likelihood is only weakly correlated with parsing accuracy, especially at convergence (Smith, 2006; Johnson et al., 2007; Jin et al., 2018a). This weak correlation points to the fact that the maximization of data likelihood at convergence may be non-optimal for model selection, and this non-optimality indicates other conRelated work Induction of PCFGs has previously been considered a difficult problem (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012). Earlier work attributed the lack of success for induction to a lack of correlation between parsing accuracy and data likelihood (Johnson et al., 2007), or to the likelihood function or the posterior being filled with weak local optima (Liang et al., 2009; Gimpel and Smith, 2012). Later work has shown that it is possible to induce PCFGs with useful labels from words alone (Shain et al., 2016; Jin et al., 2018b,a). Induction models of constituency grammars or trees usually use data likelihood as both the objective and the model selection criterion (Seginer, 2007; Johnson et al., 200"
P19-1235,J93-2004,0,0.0711045,"VAS = −  (3)  N i=1  |σi | N j=1 |σ j | where N is the number of sentences in the corpus, and σi is the i-th sentence. Because sentences in larger corpora contain different numbers of function words, VAS is predicted to be high when the distinction between predicted function words and predicted content words in the induced grammar aligns with human judgments, indicating that VAS should be positively correlated with parsing accuracy. 4 Dataset The grammar accuracy predictors described above are evaluated on multiple languages using corpora annotated with constituents (Xia et al., 2000; Marcus et al., 1993; Alastair et al., 2018) and corpora annotated with dependencies (Nivre et al., 2016) which are converted to constituents (Collins et al., 1999). An example is shown in Figure 1. These evaluations use corpora with at least 2,000 annotated sentences, excluding all sentences with nonprojective dependency graphs. Each induction run uses approximately 15,000 sentences randomly sampled from each language corpus. Languages with fewer than 15,000 annotated sentences are augmented with sentences sampled from Wikipedia (Zeman et al., 2017). Evaluations initially screen predictors on a development parti"
P19-1235,P10-1121,1,0.693878,"2007). BDT predicts that different word orders favor different branching directions. One of the evaluation metrics used in the generative linguistics tradition is the complexity of a grammar (Chomsky, 1965). Often the number of rules is used as a proxy measurement of how complex a proposed grammatical analysis is against some other reference grammatical analysis. According to this theory, fewer unique rules present in the Viterbi parses would indicate higher grammar quality. Average stack depth Embedding depth is a known limiting factor to human sentence processing (Chomsky and Miller, 1963; Wu, 2010; Rajkumar et al., 2016), and is shown to benefit unsupervised grammar induction (Noji and Johnson, 2016; Jin et al., 2018b). It is also evaluated in this work as a predictor of parsing accuracy, defined as the expected number of stack elements per sentence in a left-corner parser for the Viterbi parses. Theories such as that of Chomsky and Miller (1963) predict it to correlate negatively with parsing accuracy. Zipf likelihood ratio The distribution of words in a corpus is known to follow Zipf’s law (Zipf, 1935), in which the frequency of a word is inversely proportional to its frequency rank."
P19-1235,xia-etal-2000-developing,0,0.0510939,"X log P(σ j )  VAS = −  (3)  N i=1  |σi | N j=1 |σ j | where N is the number of sentences in the corpus, and σi is the i-th sentence. Because sentences in larger corpora contain different numbers of function words, VAS is predicted to be high when the distinction between predicted function words and predicted content words in the induced grammar aligns with human judgments, indicating that VAS should be positively correlated with parsing accuracy. 4 Dataset The grammar accuracy predictors described above are evaluated on multiple languages using corpora annotated with constituents (Xia et al., 2000; Marcus et al., 1993; Alastair et al., 2018) and corpora annotated with dependencies (Nivre et al., 2016) which are converted to constituents (Collins et al., 1999). An example is shown in Figure 1. These evaluations use corpora with at least 2,000 annotated sentences, excluding all sentences with nonprojective dependency graphs. Each induction run uses approximately 15,000 sentences randomly sampled from each language corpus. Languages with fewer than 15,000 annotated sentences are augmented with sentences sampled from Wikipedia (Zeman et al., 2017). Evaluations initially screen predictors o"
P98-2192,P95-1023,0,0.0172198,"syntactic constructions in natural language that can be handled by unrestricted TAGs. In particular, we describe an algorithm for parsing a strict subclass of TAG in O(nS), and a t t e m p t to show that this subclass retains enough generative power to make it useful in the general case. 1 Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n6), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than O(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound o"
P98-2192,P94-1022,0,0.546924,"twofold: • We define a strict subclass of TAG where adjunction of so-called wrapping trees at the spine is restricted to take place at no more than one distinct node. We show that in this case the parsing problem for TAG can be solved in worst case time O(n5). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 1176 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G = ( N , ~ , I ,"
P98-2192,J94-2002,1,0.873309,"or parsing a strict subclass of TAG in O(nS), and a t t e m p t to show that this subclass retains enough generative power to make it useful in the general case. 1 Introduction Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n6), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995), these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than O(n 6) are unlikely to have small hidden constants. A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which restrictions on TAGs are needed in order to"
P98-2192,J94-1004,0,0.0460098,". (1) where X n is any projection of category X, y,nax is the maximal projection of Y, and the order of the constituents is variable. 3 A c o m p l e m e n t auxiliary tree, on the other hand, introduces a lexical head t h a t subcategorizes for the tree's foot node and assigns it a thematic role. The structure of a complement auxiliary tree may be •described as: Xrnax _+ ... yO . . . Xrna~ . . . , (2) where X r n a ~ is the maximal projection Of some category X, and y 0 is the lexical projection 2The same linguistic distinction is used in the conception of 'modifier' and 'predicative' trees (Schabes and Shieber, 1994), but Schabes and Shieber give the trees special properties in the calculation of derivation structures, which we do not. 3The CFG-like notation is taken directly from (Kroch, 1989), where it is used to specify labels at the root and frontier nodes of a tree without placing constraints on the internal structure. 3. The foot node of an athematic auxiliary tree is dominated only by the root, with no intervening nodes, so it falls outside of the maximal projection of the head. 4. The foot node of a complement auxiliary tree is dominated by the maximal projection of the head, which may also domina"
P98-2192,J95-4002,0,0.627165,"ntribution of this paper is twofold: • We define a strict subclass of TAG where adjunction of so-called wrapping trees at the spine is restricted to take place at no more than one distinct node. We show that in this case the parsing problem for TAG can be solved in worst case time O(n5). • We provide evidence that subclass still captures the of TAG analyses that have proposed for the syntax of several other languages. the proposed vast majority been currently English and of Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994). Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above. 1176 2 Overview We introduce here the subclass of TAG that we investigate in this paper, and briefly compare it with other proposals in the literature. A TAG is a tuple G"
P98-2192,P93-1017,0,\N,Missing
P99-1012,W98-0107,0,0.368913,"transfer cannot be handled in a synchronous TAG that has an isomorphism restriction on derivation trees• On the other hand, we do not wish to return to the original non-local formulation of synchronous TAG (Shieber and Schabes, 1990) because the non-local inheritance of links on the derived tree is difficult to implement, and because the non-local formulation can recognize languages beyond the generative power of TAG. Rambow, Wier and Vijay-Shanker themselves introduce D-Tree G r a m m a r (Rambow et al., 1995) and Candito and Kahane introduce the D T G variant Graph Adjunction G r a m m a r (Candito and Kahane, 1998b) in order to solve this problem using a derivation process that mirrors composition more directly, but both involve potentially significantly greater recognition complexity t h a n TAG. 2See (Schabes and Shieber, 1994) for definitions of modifier and predicative auxiliaries. 89 VP Vo S VP is Vo Vo supposed S VP Vo Vo VP* S pressuposto I Vo I to VP S* que VP S Vo be VP Vo able Vo VP Vo I to VP* VP NP$ Vo capaz I VP NP.I. VP I I VP Vo S Vo Vo t VP* i fly voar de Figure 2: Synchronous tree pairs for &quot;supposed to be able to fly&quot; proceed from the b o t t o m up, the attachment of predicates to ar"
P99-1012,P96-1016,0,0.550382,"Missing"
P99-1012,P95-1021,0,0.072169,"(VP) Overview fln:press•(S) l , o. I fin--1:vai(VP) fln:supp.(VP) This sort of non-local non-isomorphic transfer cannot be handled in a synchronous TAG that has an isomorphism restriction on derivation trees• On the other hand, we do not wish to return to the original non-local formulation of synchronous TAG (Shieber and Schabes, 1990) because the non-local inheritance of links on the derived tree is difficult to implement, and because the non-local formulation can recognize languages beyond the generative power of TAG. Rambow, Wier and Vijay-Shanker themselves introduce D-Tree G r a m m a r (Rambow et al., 1995) and Candito and Kahane introduce the D T G variant Graph Adjunction G r a m m a r (Candito and Kahane, 1998b) in order to solve this problem using a derivation process that mirrors composition more directly, but both involve potentially significantly greater recognition complexity t h a n TAG. 2See (Schabes and Shieber, 1994) for definitions of modifier and predicative auxiliaries. 89 VP Vo S VP is Vo Vo supposed S VP Vo Vo VP* S pressuposto I Vo I to VP S* que VP S Vo be VP Vo able Vo VP Vo I to VP* VP NP$ Vo capaz I VP NP.I. VP I I VP Vo S Vo Vo t VP* i fly voar de Figure 2: Synchronous tre"
P99-1012,J94-1004,0,0.291205,"r rules in Figure 2, we will be forced into an ill-formed derivation: 1 Introduction The primary goal of this paper is to solve the problem of preserving semantic dependencies in Isomorphic Synchronous Tree Adjoining Grammar (ISTAG) (Shieber, 1994; Shieber and Schabes, 1990), a variant of Tree Adjoining Grammar (Joshi, 1985) in which source and target elementary trees are assembled into isomorphic derivations. The problem, first described in Rambow, Wier and Vijay-Shanker (Rainbow et al., 1995), stems from the fact that the TAG derivation structure - even using a flat adjunction of modifiers (Schabes and Shieber, 1994) - deviates from the appropriate dependency :voar I ;31 :~-capaz-de (VP) I /~2 :~-pressuposto-que (S ?) because the raising construction is-supposedto translates to a bridge construction dpressuposto-que and cannot adjoin anywhere in the tree for ~-capaz-de (the translation of beable-to) because there is no S-labeled adjunction site. The correct target derivation: a:voar *The author would like to thank Karin Kipper, Aravind Joshi, Martha Palmer, Norm Badler, and the anonymous reviewers for their valuable comments. This work was partially supported by NSF Grant SBP~8920230 and ARO Grant DAAH040"
P99-1012,W98-0137,1,0.851487,"osed-to,0,be-able-to}. T h e n we adjoin ~31:be-able-to at node VP of a:fly, which produces the dependency (be-able-to,0,fly). The resulting dependencies are represented graphiCally in the dependency structure below: ¢0 :supposed-to I ¢] :be-able-to(0) I • For each tree selected by ¢, set the predicate variable of each anchor item to ¢. ¢2:fly(0) This example is relatively straightforward, simply reversing the direction of adjunction dependencies as described in (Candito and Kahane, 1998a), but this algorithm can transduce 3See (Joshi and Vijay-Shanker, 1999) for a complete description. 4See (Schuler, 1998) for a discussion of statistically filtering TAG forests using semantic dependencies. 91 the correct isomorphic dependency structure for the Portuguese derivation as well, similar to the distributed derivation tree in Candito and Kahane&apos;s example 5b, &quot;Paul claims Mary seems to adore hot dogs,&quot; (Rambow et al., 1995), where there is no edge corresponding to the dependency between the raising and bridge verbs: As Candito and Kahane point out, this derivation tree does not match the dependency structure of the sentence as described in Meaning Text Theory (Mel&apos;cuk, 1988), because there is no edge i"
P99-1012,C90-3045,0,0.201007,"stract Rambow, Wier and Vijay-Shanker (Rainbow et al., 1995) point out the differences between TAG derivation structures and semantic or predicateargument dependencies, and Joshi and VijayShanker (Joshi and Vijay-Shanker, 1999) describe a monotonic compositional semantics based on attachment order that represents the desired dependencies of a derivation without underspecifying predicate-argument relationships at any stage. In this paper, we apply the Joshi and Vijay-Shanker conception of compositional semantics to the problem of preserving semantic dependencies in Synchronous TAG translation (Shieber and Schabes, 1990; Abeill~ et al., 1990). In particular, we describe an algorithm to obtain the semantic dependencies on a TAG parse forest and construct a target derivation forest with isomorphic or locally non-isomorphic dependencies in O(n 7) time. (1) X is supposed to be able to fly. using the trees in Figure 1, we get the following derivation:l a:fly I 131:be-able-to(VP) I j32:is-supposed-to(VP) with the auxiliary is-supposed-to adjoining at the VP to predicate over be-able-to and the auxiliary be-able-to adjoining at the VP to predicate over fly. If we then try to assemble an isomorphic tree in a languag"
P99-1012,E93-1045,0,0.0647788,"ranslation, we will need to obtain this structure from a source derivation and then construct a target derivation with an isomorphic structure. The first algorithm we present obtains semantic dependencies for derivations by keeping track of an additional field in each chart item during parsing, corresponding to the predicate variable from Section 2. Other t h a n the additional field, the algorithm remains essentially the same as the parsing algorithm described in (Schabes and Shieber, 1994), so it can be applied as a transducer during recognition, or as a post-process on a derivation forest (Vijay-Shanker and Weir, 1993). Once the desired dependencies are obtained, the forest may be filtered to select a single most-preferred tree using statistics or rule-based selectional restrictions on those dependencies. 4 For calculating dependencies, we define a function arg(~) to return the argument position associated with a substitution site or foot node ~? in elementary tree V. Let a dependency be defined as a labeled arc (¢, l, ~b), from predicate ¢ to predicate ¢ with label I. Since the number of possible values for the additional predicate variable field is bounded by n, where n is the number of lexical items in t"
P99-1012,W98-0106,0,\N,Missing
P99-1012,C90-3001,0,\N,Missing
P99-1012,W90-0102,0,\N,Missing
Q18-1016,N10-1083,0,0.138572,"Missing"
Q18-1016,W06-2912,0,0.167751,"ion outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be ext"
Q18-1016,W12-1913,0,0.296332,"uction (Johnson et al., 2007; Liang et al., 2009), in which subtypes of categories like noun phrases and verb phrases are induced on a given tree structure. The model described in this paper is given only words and not only induces categories for constituents but also tree structures. There are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar indu"
Q18-1016,W01-0713,0,0.17576,"ith those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models. 1 Introduction Grammar acquisition or grammar induction (Carroll and Charniak, 1992) has been of interest to linguists and cognitive scientists for decades. This task is interesting because a well-performing acquisition model can serve as a good baseline for examining factors of grounding (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), or as a piece of evidence (Clark, 2001; Zuidema, 2003) about the Distributional Hypothesis (Harris, 1954) against the poverty of the stimulus (Chomsky, 1965). Unfortunately, previous attempts at inducing unbounded context-free grammars (Johnson et al., 2007; Liang et al., 2009) converged to weak modes of a very multimodal distribution of grammars. There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). Ponvert et al. (2011) and Shain et al. (2016) in particular"
Q18-1016,P99-1065,0,0.418249,"Missing"
Q18-1016,D17-1176,0,0.0601686,"al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human speakers. Unlike most of these approaches, the model described in this paper uses cognitively motivated"
Q18-1016,N09-1012,0,0.0322381,"2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human spe"
Q18-1016,D16-1073,0,0.0850205,"UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human speakers. Unlike most of these approaches, the model described in this paper uses cog"
Q18-1016,P02-1017,0,0.741086,"are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that include"
Q18-1016,P04-1061,0,0.436796,"ree structure. The model described in this paper is given only words and not only induces categories for constituents but also tree structures. There are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an oppo"
Q18-1016,D10-1119,0,0.0369459,"d speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models. 1 Introduction Grammar acquisition or grammar induction (Carroll and Charniak, 1992) has been of interest to linguists and cognitive scientists for decades. This task is interesting because a well-performing acquisition model can serve as a good baseline for examining factors of grounding (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), or as a piece of evidence (Clark, 2001; Zuidema, 2003) about the Distributional Hypothesis (Harris, 1954) against the poverty of the stimulus (Chomsky, 1965). Unfortunately, previous attempts at inducing unbounded context-free grammars (Johnson et al., 2007; Liang et al., 2009) converged to weak modes of a very multimodal distribution of grammars. There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). Ponvert et al. (201"
Q18-1016,E12-1024,0,0.0244365,"bounds on left-corner configurations of dependency grammars (Noji and Johnson, 2016), but the use of a dependency grammar makes the system impractical for addressing questions of how category types such as noun phrases may be learned. Unlike these, the model described in this paper induces a PCFG directly and then bounds it with a model-to-model transform, which yields a smaller space of learnable parameters and directly models the acquisition of category types as labels. Some induction models learn semantic grammars from text annotated with semantic predicates (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2012). There is evidence humans use semantic bootstrapping during grammar acquisition (Naigles, 1990), but these models typically rely on a set of pre-defined universals, such as combinators (Steedman, 2000), which simplify the induction task. In order to help address the question of whether such universals are indeed necessary for grammar induction, the model described in this paper does not assume any strong universals except independently motivated limits on working memory. 3 Background Like Noji and Johnson (2016) and Shain et al. (2016), the model described in this paper defines bounding depth"
Q18-1016,J93-2004,0,0.0649123,"luation The DB-PCFG model described in Section 4 is evaluated first on synthetic data to determine whether it can reliably learn a recursive grammar from data with a known optimum solution, and to determine the hyper-parameter value for β for doing so. Two experiments on natural data are then carried out. First, the model is run on natural data from the Adam 12 Again, d¯ = maxd {adt−1 , ⊥}. and Eve parts of the CHILDES corpus (Macwhinney, 1992) to compare with other grammar induction systems on a human-like acquisition task. Then data from the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) is used for further comparison in a domain for which competing systems are optimized. The competing systems include UPPARSE (Ponvert et al., 2011)13 , CCL (Seginer, 2007a)14 , BMMM+DMV with undirected dependency features (Christodoulopoulos et al., 2012)15 and UHHMM (Shain et al., 2016).16 For the natural language datasets, the variously parametrized DB-PCFG systems17 are first validated on a development set, and the optimal system is then run until convergence with the chosen hyperparameters on the test set. In development experiments, the log-likelihood of the dataset plateaus usually after"
Q18-1016,D16-1004,0,0.512782,"uistics The Ohio State University jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu Abstract There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). This work extends this depthbounding approach to probabilistic contextfree grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depthbounding. Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by o"
Q18-1016,C16-1003,0,0.0153129,"ammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human speakers. Unlike most of these approaches, the model described in this paper uses cognitively motivated bounds on the depth of human recursive processing to constrain its search of possible trees for input sentences. Some previous work uses depth bounds in the form of sequence models (Ponvert et al., 2011; Shain et al"
Q18-1016,P11-1108,0,0.417115,"Missing"
Q18-1016,C92-1032,0,0.0489003,"ersals, such as combinators (Steedman, 2000), which simplify the induction task. In order to help address the question of whether such universals are indeed necessary for grammar induction, the model described in this paper does not assume any strong universals except independently motivated limits on working memory. 3 Background Like Noji and Johnson (2016) and Shain et al. (2016), the model described in this paper defines bounding depth in terms of memory elements required in a left-corner parse. A left-corner parser (Rosenkrantz and Lewis, 1970; JohnsonLaird, 1983; Abney and Johnson, 1991; Resnik, 1992) uses a stack of memory elements to store derivation fragments during incremental processing. Each derivation fragment represents a disjoint connected component of phrase structure a/b consisting of a top sign a lacking a bottom sign b yet to come. For example, Figure 1 shows the derivation fragments in a traversal of a phrase structure tree for the sentence The cart the horse the man bought pulled broke. Immediately before processing the word man, the traversal has recognized three fragments of tree structure: two from category NP to category RC (covering the cart and the horse) and one from"
Q18-1016,J10-1001,1,0.87507,"Missing"
Q18-1016,D14-1141,0,0.0469766,"Missing"
Q18-1016,P07-1049,0,0.935012,"describes a Bayesian Dirichlet model of depth-bounded probabilistic context-free grammar (PCFG) induction. Bayesian Dirichlet models have been applied to the related area of latent variable PCFG induction (Johnson et al., 2007; Liang et al., 2009), in which subtypes of categories like noun phrases and verb phrases are induced on a given tree structure. The model described in this paper is given only words and not only induces categories for constituents but also tree structures. There are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headd"
Q18-1016,C16-1092,1,0.0661038,"niversity jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu Abstract There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). This work extends this depthbounding approach to probabilistic contextfree grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depthbounding. Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition mode"
S14-1018,P02-1041,0,0.0422107,"r, 2013). But accounting for syntax and semantics in this way must be done carefully in order to preserve linguistically important distinctions. For example, positing spurious local dependencies in filler-gap constructions can lead to missed integrations of dependency structure in incremental processing, resulting in weaker model fitting (van Schijndel et al., 2013). Similar care may be necessary in cases of dependencies arising from anaphoric coreference or quantifier scope. Unfortunately, most existing theories of compositional semantics (Montague, 1973; Barwise and Cooper, 1981; Bos, 1996; Baldridge and Kruijff, 2002; Koller, 2004; Copestake et al., 2005) are defined at the computational level (Marr, 1982), employing beta reduction over complete or underspecified lambda calculus expressions as a precise description of the language processing task to be modeled, not at the algorithmic level, as a model of human language processing itself. The structured expressions these theories generate are not intended to represent re-usable referential states of the sort that could be modeled in current theories of associative memory. As such, it should not be surprising that structural adaptations of lambda calculus e"
S14-1018,de-marneffe-etal-2006-generating,0,0.0546274,"Missing"
S14-1018,W09-3714,0,0.0251209,"ndencies with all possible labels in increasingly longer paths from one or more constant vector states (e.g. vectors for predicate constants). This graph matching does not necessarily preclude the introduction of monotonicity constraints from matched quantifiers. For example, More than two perl scripts work, can entail More than two scripts work, using a subgraph in the first argument, but Fewer than two scripts work, can entail Fewer than two perl scripts work, using a supergraph in the first argument. This consideration is similar to those observed in representations based on natural logic (MacCartney and Manning, 2009) which also uses low-level matching to perform some kinds of inference, but representations based on natural logic typically exclude other forms of inference, whereas the present model does not. This matching also assumes properties of nuclear scope variables are inherited from associated restrictor variables, e.g. through a set of dependencies from nuclear scope sets to restrictor sets not shown in the figure. This assumption will be revisited in Section 3. begins with a space and contains two numbers shown in Figure 1b does not contain the graphical representation of the sentence Every line"
S14-1018,W11-1108,0,0.0905235,"the tree, any number of conjuncts can impinge upon a common outscoping continuation, so there is no longer any need for explicit conjunction nodes. The representation is also attractive in that it locally distinguishes queries about, say, the cardinality of the set of numbers in each document line (Set s0N d0N s0L ) from queries about the cardinality of the set of numbers in general (Set s0N d0N s0⊥ ) which is crucial for successful inference by pattern matching. Finally, connected sets of continuation dependencies form natural ‘scope graphs’ for use in graph-based disambiguation algorithms (Manshadi and Allen, 2011; Manshadi et al., 2013), which will be used to evaluate this representation in Section 6. 3 Mapping to Lambda Calculus It is important for this representation not only to have attractive graphical subsumption properties, but also to be sufficiently expressive to define corresponding expressions in lambda calculus. When continuation dependencies are filled in, the resulting dependency structure can be transa) Every 0 p L 1 2 λ 1 eL 1 dL 2 s0L sL 0 Two λ d0L Every eN 0 p L 1 λ 1 0 λ 1 1 dL 2 s0L 0 λ dN λ 0 eS 2 s0C sC 1 0 d0N λ 2 1 0 pS 1 1 λ eC 1 0 e0C 0 λ 1 Some 2 2 1 dS 0 λ 0 p B 1 2 s0S sS"
S14-1018,C12-1130,1,0.569809,"Missing"
S14-1018,W14-2003,1,0.828322,"from GCG derivations as described in Section 4 are scopally underspecified. Scope disambiguations must then in the sentence Every line contains two numbers, a continuation dependency may be stored from the nuclear scope set associated with this word to the nuclear scope set of the subject every line, forming an in-situ interpretation with some amount of activation (see Figure 4), and with some (probably smaller) amount of activation, a continuation dependency may be stored from the nuclear scope set of this subject to the nuclear scope set of this word, forming an inverted interpretation. See Schuler (2014) for a model of how sentence processing in associative memory might incrementally store dependencies like these as cued associations. the dependency graph, then begins connecting it, selecting the highest-ranked referent of that partition that is not yet attached and designating it as the new highest-scoping referent in that partition, attaching it as the context of the previously highest-scoping referent in that partition if one exists. This proceeds until: 1. the algorithm reaches a restrictor or nuclear scope referent with a sibling (superset or subset) nuclear scope or restrictor referent"
S14-1018,P11-2025,0,0.141934,"to its sibling with a continuation dependency from the nuclear scope referent to the restrictor referent and merges the two siblings’ partitions. In this manner, all set referents in the dependency graph are eventually assembled into a single tree of continuation dependencies. 6 Evaluation This paper defines a graphical semantic representation with desirable properties for storing sentence meanings as cued associations in associative memory. In order to determine whether this representation of continuation dependencies is reliably learnable, the set of test sentences from the QuanText corpus (Manshadi et al., 2011) was automatically annotated with these continuation dependencies and evaluated against the associated set of gold-standard quantifier scopes. The sentences in this corpus were collected as descriptions of text editing tasks using unix tools like sed and awk, collected from online tutorials and from graduate students asked to write and describe example scripts. Gold-standard scoping relations in this corpus are specified over bracketed sequences of words in each sentence. For example, the sentence Print every line that starts with a number might be annotated: Print [1 every line] that starts w"
S14-1018,N13-1010,1,0.840504,"Missing"
S14-1018,W13-2605,1,0.894892,"Missing"
S14-1018,P13-1007,0,\N,Missing
S14-1018,P85-1008,0,\N,Missing
S14-1018,C69-7001,0,\N,Missing
S14-1018,C69-6902,0,\N,Missing
S16-1188,W15-3304,1,0.86508,"Missing"
S16-1188,C12-1130,1,0.864058,"Missing"
S16-1188,N07-1051,0,0.0363354,"Missing"
S16-1188,P15-1150,0,0.109712,"Missing"
S16-1188,W03-1730,0,0.0635272,"semantic dependencies as it improves the scores on TEXT by about 4 percent, and the predicate-argument features further improve the F score by another 2 percent on TEXT, and 1 percent on NEWS. The submitted systems use all three feature categories to train and test. 3.4 Preprocessing and postprocessing There are a few preprocessing and postprocessing steps that are adopted in the system to generate better results. Word Segmentation: The datasets provided in the task come pre-segmented, but the training data used by word2vec need to be segmented first to produce embeddings. We use NLPIR 2015 (Zhang et al., 2003) for segmentation with all words in training and development datasets put into the user dictionary for segmentation. This guarantees that there is no out-of-vocabulary word in development, which also increases parser performance on development data. OOV Replacement: The OOV words are processed in the following way. Suppose there is a large dataset D1 for training word embeddings. The vocabulary of D1 is the set V1 . Suppose there is a new dataset D2 which has the vocabulary set V2 . The OOV words from D2 are Voov = V2 − V1 . We first train an embedding model W1 for D1 and W2 for D2 , and then"
S16-1189,W15-3304,1,0.836695,"parse data problem faced by any heavily lexicalized annotations. Parsers trained with English GCG annotations have been shown to have state-of-the-art parsing performance and better long-distance dependency recovery (Rimell et al., 2009; Nguyen et al., 2012). Parsers trained with Chinese GCG annotations have been shown to achieve better parsing accuracy than the parser trained with Chinese Combinatory Categorial Grammar, CCG, (Steedman, 2000; Steedman, 2012) annotations (Tse and Curran, 2010; Tse and Curran, 2012) for those trees which both grammar annotations assign the same tree structures (Duan and Schuler, 2015). The current experiment is our first experiment with dependency relations generated from the Chinese GCG annotations. We evaluate them against the manually annotated semantic dependencies in the current SemEval task. Since the purpose of the system is to verify the semantic dependencies generated by the Chinese GCG parser are reasonable, we adopt a minimalist machine learning scheme for this system to accomplish the evaluation. We first train the Berkeley parser (Petrov and Klein, 2007) with GCG annotations converted from the currently 1218 Proceedings of SemEval-2016, pages 1218–1224, c San"
S16-1189,C12-1130,1,0.895459,"Missing"
S16-1189,N07-1051,0,0.0850996,"Missing"
S16-1189,D09-1085,0,0.0550004,"Missing"
S16-1189,C10-1122,0,0.0275633,"tionality of its syntactic derivations and elegant analysis of filler-gap phenomenon, and on the other hand, mitigates the sparse data problem faced by any heavily lexicalized annotations. Parsers trained with English GCG annotations have been shown to have state-of-the-art parsing performance and better long-distance dependency recovery (Rimell et al., 2009; Nguyen et al., 2012). Parsers trained with Chinese GCG annotations have been shown to achieve better parsing accuracy than the parser trained with Chinese Combinatory Categorial Grammar, CCG, (Steedman, 2000; Steedman, 2012) annotations (Tse and Curran, 2010; Tse and Curran, 2012) for those trees which both grammar annotations assign the same tree structures (Duan and Schuler, 2015). The current experiment is our first experiment with dependency relations generated from the Chinese GCG annotations. We evaluate them against the manually annotated semantic dependencies in the current SemEval task. Since the purpose of the system is to verify the semantic dependencies generated by the Chinese GCG parser are reasonable, we adopt a minimalist machine learning scheme for this system to accomplish the evaluation. We first train the Berkeley parser (Petr"
S16-1189,N12-1030,0,0.0133751,"ctic derivations and elegant analysis of filler-gap phenomenon, and on the other hand, mitigates the sparse data problem faced by any heavily lexicalized annotations. Parsers trained with English GCG annotations have been shown to have state-of-the-art parsing performance and better long-distance dependency recovery (Rimell et al., 2009; Nguyen et al., 2012). Parsers trained with Chinese GCG annotations have been shown to achieve better parsing accuracy than the parser trained with Chinese Combinatory Categorial Grammar, CCG, (Steedman, 2000; Steedman, 2012) annotations (Tse and Curran, 2010; Tse and Curran, 2012) for those trees which both grammar annotations assign the same tree structures (Duan and Schuler, 2015). The current experiment is our first experiment with dependency relations generated from the Chinese GCG annotations. We evaluate them against the manually annotated semantic dependencies in the current SemEval task. Since the purpose of the system is to verify the semantic dependencies generated by the Chinese GCG parser are reasonable, we adopt a minimalist machine learning scheme for this system to accomplish the evaluation. We first train the Berkeley parser (Petrov and Klein, 2007) wit"
W00-2008,P99-1011,1,0.572462,"Missing"
W00-2008,P94-1022,0,0.528315,"Missing"
W00-2008,E93-1045,0,0.08135,"Missing"
W00-2008,P99-1012,1,\N,Missing
W00-2008,P96-1016,0,\N,Missing
W00-2021,W98-0104,1,0.872805,"Missing"
W00-2021,W98-0106,0,0.0508368,"Missing"
W00-2021,P98-1046,1,0.88436,"Missing"
W00-2021,J88-2003,0,0.280656,"Missing"
W00-2021,C98-1046,1,\N,Missing
W10-2004,P08-2002,0,0.218491,"mes at a price. The HHMM parser obtains good coverage within human-like memory bounds only by pursuing an ‘optionally arc-eager’ parsing strategy, nondeterministically guessing which constituents can be kept open for attachment (occupying an active memory element), or closed for attachment (freeing a memory element for subsequent constituents). Although empirically determining the number of parallel competing hypotheses used in human sentence processing is difficult, previous results in computational models have shown that human-like behavior can be elicited at very low levels of parallelism (Boston et al., 2008b; Brants and Crocker, 2000), suggesting that large numbers of active hypotheses are not needed. Previously, the HHMM parser has only been evaluated on large beam widths, leaving this aspect of its psycholinguistic plausibility untested. In this paper, the performance of an HHMM parser will be evaluated in two experiments that Hierarchical Hidden Markov Model (HHMM) parsers have been proposed as psycholinguistic models due to their broad coverage within human-like working memory limits (Schuler et al., 2008) and ability to model human reading time behavior according to various complexity metri"
W10-2004,C00-1017,0,0.0259076,"Missing"
W10-2004,N01-1021,0,0.0570401,"parsing difficulty in computational models with delays in reading time in human subjects. vary the amount of parallelism allowed during parsing, measuring the degree to which this degrades the system’s accuracy. In addition, the evaluation will compare the HHMM parser to an off-the-shelf probabilistic CKY parser to evaluate the actual run time performance at various beam widths. This serves two purposes, evaluating one aspect of the plausibility of this parsing framework as a psycholinguistic model, and evaluating its potential utility as a tool for operating on unsegmented text or speech. 2 Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t − 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. (2008a) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser. Roark et al. (2009) dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and s"
W10-2004,N09-1039,1,0.849015,"Figure 2: Mapping of schematized right-corner tree into HHMM memory elements. the most likely sequence, deterministically mapping that sequence back to a right-corner tree, and reversing the right-corner transform to produce an ordinary phrase structure tree. Unfortunately exact inference is not tractable with this model and dataset. The state space is too large to manage for both space and time reasons, and thus approximate inference is carried out, through the use of a beam search. At each time step, only the top N most probable hypothesized states are maintained. Experiments described in (Schuler, 2009) suggest that there does not seem to be much lost in going from exact inference using the CKY algorithm to a beam search with a relatively large width. However, the opposite experiment, examining the effect of going from a relatively wide beam to a very narrow beam has not been thoroughly studied in this parsing architecture. 4 Optionally Arc-eager Parsing The right-corner transform described in Section 3.1 saves memory because it transforms any right-expanding sequence with left-child subtrees into a left-expanding sequence of incomplete constituents, with the same sequence of subtrees as rig"
W10-2004,W04-0305,0,0.0151136,"Figure 3(a) a left-child category NP/NP at time t=4 is composed with a noun new of category NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in a new parent category NP/NNP at time t=5 replacing the left child category NP/NP in the topmost d=1 memory element. 3 It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates this ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as that of Henderson (2004) adopt a arc-standard strategy, essentially always choosing analysis (b) above, and thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in short-term memory (Resnik, 1992). 31 t=4 t=5 t=6 t=7 ) P( ? ) )/N (city m. P N (de NP P /NP /NN − NP NNP NP /NP /N − NP NNP /NP − − NP − − city k yor new for ong and dem str city k yor new for word − d=3 t=3 P d=2 − − − − − and dem − − − d=1 t=2 /P NP b) t=1 t=7 − t=6 − t=5 /NN NP t=4 ) m. (de NP P /NN − NP P /NN"
W10-2004,P10-1121,1,0.543334,"parallel, is not limitlessly so, as evidenced by the existence of garden path sentences (Bever, 1970; Lewis, 2000). If this were not the case, garden-path sentences would not cause problems, as reaching the disambiguating word would simply result in a change in the favored hypothesis. In fact, garden path sentences typically cannot be understood on a first pass and must be reread, indicating that the correct analysis is attainable and yet not present in the set of parallel hypotheses of the first pass. While parsers meeting these three criteria can claim to not violate any psycholinguistic conWu et al. (2010) evaluate the same Hierarchical Hidden Markov Model parser used in this work in terms of its ability to reproduce human-like results for various complexity metrics, including some of those mentioned above, and introduce a new metric called embedding difference. This metric is based on the idea of embedding depth, which is the number of elements in the memory store required to hold a given hypothesis. Using more memory elements corresponds to center embedding in phrase structure trees, and presumably correlates to some degree with complexity. Average embedding for a time step is computed by com"
W10-2004,P03-1054,0,0.0230366,"Missing"
W10-2004,J93-2004,0,0.0335225,"Missing"
W10-2004,C92-1032,0,0.217257,"right-corner nor left-corner parsing strategy by itself creates this ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as that of Henderson (2004) adopt a arc-standard strategy, essentially always choosing analysis (b) above, and thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in short-term memory (Resnik, 1992). 31 t=4 t=5 t=6 t=7 ) P( ? ) )/N (city m. P N (de NP P /NP /NN − NP NNP NP /NP /N − NP NNP /NP − − NP − − city k yor new for ong and dem str city k yor new for word − d=3 t=3 P d=2 − − − − − and dem − − − d=1 t=2 /P NP b) t=1 t=7 − t=6 − t=5 /NN NP t=4 ) m. (de NP P /NN − NP P /NN − NP /NP − NP P /P NP − ong str word − d=3 t=3 − d=2 − d=1 t=2 /NN NP a) t=1 Figure 3: Alternative analyses of ‘strong demand for new york city ...’: a) using in-element composition, compatible with ‘strong demand for new york city is ...’ (in which the demand is for the city); and b) using cross-element (or delayed"
W10-2004,D09-1034,0,0.0221975,"ty of this parsing framework as a psycholinguistic model, and evaluating its potential utility as a tool for operating on unsegmented text or speech. 2 Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t − 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. (2008a) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser. Roark et al. (2009) dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and showed that these new metrics are strong predictors of reading difficulty. Related Work There are several criteria a parser must meet in order to be plausible as a psycholinguistic model of the human sentence-processing mechanism (HSPM). Incremental operation is perhaps the most obvious. The HSPM is able to process sentences incrementally, meaning that at each point in time of processing input, it has some hypothesis of the interpretation of that input,"
W10-2004,C08-1099,1,0.929205,"iversity of Minnesota, Twin Cities and The Ohio State University schuler@ling.ohio-state.edu Abstract models raises the question of how humans can accurately parse linguistic input without access to this same global optimization. This question creates a niche in computational research for models that are able to parse accurately while adhering as closely as possible to human-like psycholinguistic constraints. Recent work on incremental parsers includes work on Hierarchical Hidden Markov Model (HHMM) parsers that operate in linear time by maintaining a bounded store of incomplete constituents (Schuler et al., 2008). Despite this seeming limitation, corpus studies have shown that through the use of grammar transforms, this parser is able to cover nearly all sentences contained in the Penn Treebank (Marcus et al., 1993) using a small number of unconnected memory elements. But this bounded-memory parsing comes at a price. The HHMM parser obtains good coverage within human-like memory bounds only by pursuing an ‘optionally arc-eager’ parsing strategy, nondeterministically guessing which constituents can be kept open for attachment (occupying an active memory element), or closed for attachment (freeing a mem"
W10-2004,J10-1001,1,0.816767,"vidence about how humans perceive these sentences word by word. However, as will be described below, the HHMM has advantages over the CHMM from a psycholinguistic modeling perspective. The HHMM uses a limited memory store containing only four elements which is consistent with many estimates of human short term memory limits (Cowan, 2001; Miller, 1956). In addition to modeling memory limits, the limited store acts as a fixed-depth stack that ensures linear asymptotic parsing time, and a grammar transform allows for wide coverage of speech and newspaper corpora within that limited memory store (Schuler et al., 2010). 3 In order to parse with an HHMM, phrase structure trees need to be mapped to a hierarchical sequence of states of nested HMMs. Since Murphy and Paskin showed that the run time complexity of the HHMM is exponential on the depth of the nested HMMs, it is important to minimize the depth of the model for optimal performance. In order to do this, a tree transformation known as a right-corner transform is applied to the phrase structure trees comprising the training data, to transform right-expanding sequences of complete constituents into left-expanding sequences of incomplete constituents Aη /A"
W10-4401,J93-2004,0,0.0330423,"ammar, using a right-corner transform to minimize memory usage in incremental processing. This sequence model characterization is attractive as a cognitive model because it does not posit any internal representation of complex phrasal structure beyond the pair of categories in each incomplete constituent resulting from the application of a right-corner transform; and because these incomplete constituents represent contiguous connected chunks of phrase structure, in line with characterizations of chunking in working memory (Miller, 1956). Experiments on large phrasestructure annotated corpora (Marcus et al., 1993) show this model could process the vast majority of sentences in a typical newspaper using only three or four store elements (Schuler et al., 2008; Schuler et al., 2010), in line with recent estimates of human short-term working memory capacity (Cowan, 2001). This derivation can be applied to efﬁciently inFigure 4 shows a sample store sequence corresponding to a parse of a noun phrase modiﬁed by an object relative clause. Here, the afﬁx ‘-xNP’ is used to identify a phrase containing an extracted NP constituent, and the category label ‘GCnpS’ is used to identify the maximal projection of a gapp"
W10-4401,C08-1099,1,0.840277,"cognitive model because it does not posit any internal representation of complex phrasal structure beyond the pair of categories in each incomplete constituent resulting from the application of a right-corner transform; and because these incomplete constituents represent contiguous connected chunks of phrase structure, in line with characterizations of chunking in working memory (Miller, 1956). Experiments on large phrasestructure annotated corpora (Marcus et al., 1993) show this model could process the vast majority of sentences in a typical newspaper using only three or four store elements (Schuler et al., 2008; Schuler et al., 2010), in line with recent estimates of human short-term working memory capacity (Cowan, 2001). This derivation can be applied to efﬁciently inFigure 4 shows a sample store sequence corresponding to a parse of a noun phrase modiﬁed by an object relative clause. Here, the afﬁx ‘-xNP’ is used to identify a phrase containing an extracted NP constituent, and the category label ‘GCnpS’ is used to identify the maximal projection of a gapped clause. If the GC constituent (and only this constituent) is associated with the referent or dependency information of the ﬁller constituent (t"
W10-4401,J10-1001,1,0.837974,"e it does not posit any internal representation of complex phrasal structure beyond the pair of categories in each incomplete constituent resulting from the application of a right-corner transform; and because these incomplete constituents represent contiguous connected chunks of phrase structure, in line with characterizations of chunking in working memory (Miller, 1956). Experiments on large phrasestructure annotated corpora (Marcus et al., 1993) show this model could process the vast majority of sentences in a typical newspaper using only three or four store elements (Schuler et al., 2008; Schuler et al., 2010), in line with recent estimates of human short-term working memory capacity (Cowan, 2001). This derivation can be applied to efﬁciently inFigure 4 shows a sample store sequence corresponding to a parse of a noun phrase modiﬁed by an object relative clause. Here, the afﬁx ‘-xNP’ is used to identify a phrase containing an extracted NP constituent, and the category label ‘GCnpS’ is used to identify the maximal projection of a gapped clause. If the GC constituent (and only this constituent) is associated with the referent or dependency information of the ﬁller constituent (the bike in this example"
W10-4401,N09-1039,1,0.928746,"are of constituent categories cηι anywhere in the left progeny of a right child of category cη : represented graphically: cη cη cη0 cη1 x ¯η0 x ¯η1 ⇒ cη /cη1 x ¯η1 cη0 (14) EθG-RL∗,d (cη → cη0 ...) = ∑ PθG-R,d (cη → cη0 cη1 ) 0 x ¯η0 cη1 Essentially this transformation turns rightexpanding sequences of constituents (with subscript addresses ending in ‘1’) into left-expanding sequences of incomplete constituents, which can be composed together as they are recognized incrementally from left to right. The decompositions of Equations 8–13, taken together, are a model-based right-corner transform (Schuler, 2009). This transformation can then be deﬁned on depthspeciﬁc rules, allowing the sequence model to keep track of a bounded amount of center-embedded phrase structure. PθIns(G),d (¯ xη ∣ c η ) = ∑ (19) EθG-RL∗,d (cη → cη0k 0 ...) = ∑ EθG-RL∗,d (cη → cη0k ...) k−1 k cη0k ⋅ ∑ PθG-L,d (cη0k → cη0k 0 cη0k 1 ) (20) cη0k 1 ∞ EθG-RL∗,d (cη → cηι ...) = ∑ EθG-RL∗,d (cη → cηι ...) ∗ k k=0 (21) EθG-RL∗,d (cη → cηι ...) = EθG-RL∗,d (cη → cηι ...) + ∗ − EθG-RL∗,d (cη → cηι ...) 0 PθIC(G),d (¯ xη /¯ xηι , cηι ∣ cη ) (22) ι∈1+ ,¯ xηι ,cηι ⋅ PθIns(G),d (¯ xηι ∣ cηι ) (15) In practice the inﬁnite sum is estimated"
W10-4401,1985.tmi-1.17,0,0.0772355,"e the model is ultimately deﬁned through transitions on entire working memory stores, it is also possible to relax the independence assumptions from the original PCFG model, and introduce additional dependencies across store elements that do not correspond to context-free dependencies. These additional dependencies might be used approximate dependencies of mildly context-sensitive grammar formalisms like tree adjoining grammars (Joshi, 1985), e.g. to model long-distance dependencies in ﬁller-gap constructions (Kroch and Joshi, 1986), or crossed and nested dependencies in languages like Dutch (Shieber, 1985). and as a sequence of store states in Figure 3, corresponding to the output of Viterbi most likely sequence estimator. This estimation runs in linear time on the length of the input, so the parser can be run continuously on unsegmented or unpunctuated input. An ordinary phrase structure tree can be obtained by applying the transforms from Section 3, in reverse, to the right-corner recursive phrase structure tree represented in the sequence of store states. 5 Discussion This paper has presented a derivation of a factored probabilistic sequence model from a probabilistic context-free grammar, u"
W11-0131,P97-1003,0,0.825133,"hat is necessarily interactive (between syntax and semantics); vector composition and parsing are then twin lenses by which the process may be viewed. Thus, unlike previous models, a unique phrasal vectorial semantic representation is composed during decoding. Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing. The structured vectorial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed: lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006; Gesmundo et al., 2009)). Because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (Klein and Manning, 2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracy — it will only show parsing gains where semantic information does not overlap with distributional syntactic information."
W11-0131,D08-1094,0,0.0852325,"Missing"
W11-0131,W05-0602,0,0.0291485,"fine the way in which the syntax and semantics interact during structured vectorial semantic composition. SVS will specify this interface such that the composition of semantic vectors is probabilistically consistent and subsumes parsing under various frameworks. Parsing has at times added semantic annotations that unwittingly carry some semantic value: headwords (Collins, 1997) are one-word concepts that subsume the words below them; latent annotations (Matsuzaki et al., 2005) are clustered concepts that touch on both syntactic and semantic information at a node. Of course, other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics. In this light, semantic concepts (vector indices i) and relation labels (matrix arguments l) may also be seen as annotations on grammar trees. Let us introduce notation to make the connection with parsing and syntax explicit. This paper will denote syntactic categories as c and string yields as x. The location of these variables in phrase structure will be identified using subscripts that describe the path from the root to the constituent.1 Paths consist of left and/or right branches (indicated by ‘0’s and ‘1’s, respectively, as in Figure 1a). Variables"
W11-0131,W09-1205,0,0.0135201,"ch the process may be viewed. Thus, unlike previous models, a unique phrasal vectorial semantic representation is composed during decoding. Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing. The structured vectorial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed: lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006; Gesmundo et al., 2009)). Because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (Klein and Manning, 2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracy — it will only show parsing gains where semantic information does not overlap with distributional syntactic information. Evaluations show that interactively considering semantic information with syntax has the predicted positive impact on parsing accu"
W11-0131,P03-1054,0,0.0463695,". Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing. The structured vectorial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed: lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006; Gesmundo et al., 2009)). Because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (Klein and Manning, 2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracy — it will only show parsing gains where semantic information does not overlap with distributional syntactic information. Evaluations show that interactively considering semantic information with syntax has the predicted positive impact on parsing accuracy over syntax alone; it also lowers per-word perplexity. The remainder of this paper is organized as follows: Section 2 describes SVS as"
W11-0131,P08-1068,0,0.0151674,"tical syntactic parse. Since semantic information is used in syntactic disambiguation (MacDonald et al., 1994), we would expect practical improvements in parsing accuracy by accounting for the interactive interpretation process. Others have incorporated syntactic information with vector-space semantics, challenging the bagof-words assumption. Syntax and semantics may be jointly generated with Bayesian methods (Griffiths et al., 2005); syntactic structure may be coupled to the basis elements of a semantic space (Pad´o and Lapata, 2007); clustered semantics may be used as a pre-processing step (Koo et al., 2008); or, semantics may be learned in some defined syntactic context (Lin, 1998). These techniques are interactive, but their semantic models are not syntactically compositional (Frege, 1892). SVS is a generative model of sentences that uses a variant of the last strategy to incorporate syntax at preterminal tree nodes, but is inherently compositional. Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p = f (u, v, R, K) 295 (1) where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed ve"
W11-0131,P95-1037,0,0.5267,"consider frequencies of terms that occur in similar semantic relations (e.g., head lI D or modifier lM OD ). Reducing the dimensionality of terms in a term–context matrix will result in relationally-clustered concepts. From a parsing perspective, this amounts to latent annotations (Matsuzaki et al., 2005) in l-context. 299 Let us re-notate the headword-lexicalized version of SVS (the example in Section 2.2.1) using h for headword semantics, and reserve i for relationally-clustered concepts. Treebank trees can be deterministically annotated with headwords h and relations l by using head rules (Magerman, 1995). The 5 SVS models θM , θL , θP-Vit(G) , πGǫ , and πG can thus be obtained by counting instances and normalizing. Empirical probabilities of this kind are denoted with a tilde, whereas estimated models have a hat. Concepts i in a distributed semantic representation, however, cannot be found from annotated trees (see example concepts in Figure 2). Therefore, we use Expectation Maximization (EM) in a variant of the inside-outside algorithm (Baker, 1979) to learn distributed-concept behavior. In the M-step, the datainformed result of the E-step is used to update the estimates of θM , θL , and θH"
W11-0131,P05-1010,0,0.334166,"ition and parsing are then twin lenses by which the process may be viewed. Thus, unlike previous models, a unique phrasal vectorial semantic representation is composed during decoding. Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing. The structured vectorial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed: lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006; Gesmundo et al., 2009)). Because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (Klein and Manning, 2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracy — it will only show parsing gains where semantic information does not overlap with distributional syntactic information. Evaluations show that interactively considering semantic information with syntax has"
W11-0131,P08-1028,0,0.435846,"y be jointly generated with Bayesian methods (Griffiths et al., 2005); syntactic structure may be coupled to the basis elements of a semantic space (Pad´o and Lapata, 2007); clustered semantics may be used as a pre-processing step (Koo et al., 2008); or, semantics may be learned in some defined syntactic context (Lin, 1998). These techniques are interactive, but their semantic models are not syntactically compositional (Frege, 1892). SVS is a generative model of sentences that uses a variant of the last strategy to incorporate syntax at preterminal tree nodes, but is inherently compositional. Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p = f (u, v, R, K) 295 (1) where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor). In this initial work of theirs, they leave out any notion of syntactic context, focusing on additive and multiplicative vector composition (with some variations): Add: p[i] = u[i] + v[i] Mult: p[i] = u[i] ⋅ v[i] (2) Since the structured vectorial semantics proposed here may be viewed within this framework, our discussion will begin from their definition in"
W11-0131,D09-1045,0,0.0863769,"Missing"
W11-0131,J07-2002,0,0.116571,"Missing"
W11-0131,P06-1055,0,0.0113409,"en twin lenses by which the process may be viewed. Thus, unlike previous models, a unique phrasal vectorial semantic representation is composed during decoding. Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have chosen to report results on the well-understood dual problem of parsing. The structured vectorial semantic framework subsumes variants of several common parsing algorithms, two of which will be discussed: lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006; Gesmundo et al., 2009)). Because previous work has shown that linguistically-motivated syntactic state-splitting already improves parses (Klein and Manning, 2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracy — it will only show parsing gains where semantic information does not overlap with distributional syntactic information. Evaluations show that interactively considering semantic information with syntax has the predicted positiv"
W11-0131,P10-1093,0,0.052542,"rs are to the resulting vector (e.g., Lγ×α defines the the relevance of eα to eγ ), with the intuition that two 1D vectors are under consideration and require a 2D matrix to relate them. 1 is a vector of ones — this takes a diagonal matrix and returns a column vector corresponding to the diagonal elements. Of note in this definition of f (⋅) is the presence of matrices that operate on distributed semantic vectors. While it is widely understood that matrices can represent transformations, relatively few have used matrices to represent the distributed, dynamic nature of meaning composition (see Rudolph and Giesbrecht (2010) for a counterexample). 2.2 Syntax–Semantics Interface This section aims to more thoroughly define the way in which the syntax and semantics interact during structured vectorial semantic composition. SVS will specify this interface such that the composition of semantic vectors is probabilistically consistent and subsumes parsing under various frameworks. Parsing has at times added semantic annotations that unwittingly carry some semantic value: headwords (Collins, 1997) are one-word concepts that subsume the words below them; latent annotations (Matsuzaki et al., 2005) are clustered concepts t"
W11-0806,T75-2013,0,0.719095,"perset of the functor-argument decompositions that string-rewriting systems can produce. 1 Introduction Multi-word expressions (MWEs), whose structure and meaning cannot be derived from their component words as they occur independently, account for a large portion of the language used in day-to-day interactions. Indeed, the relatively low frequency of comparable single-word paraphrases for elementary spatial relations like ‘in front of’ (compare to ‘before’) or ‘next to’ (compare to ‘beside’) suggest a fundamentality of expressions, as opposed to words, as a basic unit of meaning in language (Becker, 1975; Fillmore, 2003). Other examples of MWEs are idioms such as ‘kick the bucket’ or ‘spill the beans’, which have figurative meanings as expressions that sometimes even allow modification (‘spill some of the beans’) and variation in sentence forms (‘which beans Models have been proposed for MWEs based on string-rewriting systems such as HPSG (Sag et al., 2002), which model compositionality as string adjacency of a functor and an argument substring. This string-rewriting model of compositionality essentially treats each projection of a head word as a functor, each capable of combining with an arg"
W11-0806,P94-1022,0,0.12677,"he local non-compositionality of larger multi-word expressions like ‘threw X to the lions’ (see Figure 5), because only downward branches with multiple non3 Recognition and parsing of feature-based grammars, and of tree-rewriting systems whose elementary trees contain multiple foot nodes, are both exponential in the worst case. However, both types of grammars are amenable to regular-from restrictions which prohibit recursive adjunction at internal (nonroot, non-foot) tree nodes, and thereby constrain recognition and parsing complexity to cubic time for most kinds of natural language grammars (Rogers, 1994). 29 S NP↓ VP VP threw⋄ PP NP↓ to⋄ the⋄ NP lions⋄ Figure 5: Elementary structure for MWE idiom ‘threw . . . to the lions,’ allowing modification to both VP, PP and NP sub-constituents (e.g. ‘threw your friends today right to the proverbial lions). argument children can produce the multi-level subtrees containing the word ‘threw’ and the word ‘lions’ in the same elementary unit. 4 Conclusion This paper has shown that tree-rewriting systems are able to produce a superset of the functorargument decompositions that can be produced by string-rewriting systems such as categorial grammars and feature"
W11-0806,W04-0411,0,0.0704373,"Missing"
W12-1705,N01-1021,0,0.532816,"plement continuous dimensions of recurrent hidden units. This paper presents a refinement of a factored probabilistic sequence model of comprehension (Schuler, 2009) in the direction of a recurrent neural network model and presents some observed efficiencies due to this refinement. This paper will adopt an incremental probabilistic context-free grammar (PCFG) parser (Schuler, 2009) that uses a right-corner variant of the leftcorner parsing strategy (Aho and Ullman, 1972) coupled with strict memory bounds, as a model of human-like parsing. Syntax can readily be approximated using simple PCFGs (Hale, 2001; Levy, 2008; Demberg and Keller, 2008), which can be easily tuned (Petrov and Klein, 2007). This paper will show that this representation can be streamlined to exploit the fact that a right-corner parse guarantees at most one expansion and at most one reduction can take place after each word is seen (see Section 2.2). The primary finding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser (Schuler, 2009) yielding a simpler structure that more closely resembles con"
W12-1705,J93-2004,0,0.0411298,"tes an indicator probability which is 1 if φ and 0 otherwise. 2.2 Right-Corner Parsing each subtree into a left-expanding hierarchy ensures at most a single expansion (push) will occur at any given observation. That is, each new observation will be the leftmost leaf of a right-expanding subtree. Additionally, by reducing multiply rightbranching subtrees to single rightward branches, the transform also ensures that at most a single reduction (pop) will take place at any given observation. Schuler et al. (2010) show near complete coverage of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) can be achieved with a right-corner incremental parsing strategy using no more than four incomplete contituents (deferred processes), in line with recent estimates of human working memory capacity (Cowan, 2001). Section 3 will show that, in addition to being desirable for bounded working memory restrictions, the single expansion/reduction guarantee reduces the search space between words to only two decision points — whether to expand and whether to reduce. This allows rapid processing of each candidate parse within a sequence modelling framework. 2.3 Model Formulation Parsers such as that of"
W12-1705,N07-1051,0,0.504364,"refinement of a factored probabilistic sequence model of comprehension (Schuler, 2009) in the direction of a recurrent neural network model and presents some observed efficiencies due to this refinement. This paper will adopt an incremental probabilistic context-free grammar (PCFG) parser (Schuler, 2009) that uses a right-corner variant of the leftcorner parsing strategy (Aho and Ullman, 1972) coupled with strict memory bounds, as a model of human-like parsing. Syntax can readily be approximated using simple PCFGs (Hale, 2001; Levy, 2008; Demberg and Keller, 2008), which can be easily tuned (Petrov and Klein, 2007). This paper will show that this representation can be streamlined to exploit the fact that a right-corner parse guarantees at most one expansion and at most one reduction can take place after each word is seen (see Section 2.2). The primary finding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser (Schuler, 2009) yielding a simpler structure that more closely resembles connectionist models such as TRACE (McClelland and Elman, 1986), Shortlist (Norris, 1994; Norr"
W12-1705,P06-1055,0,0.0961921,"ime where n is the length of the input. Further, by its incremental nature, this parser is able to run continuously on a stream of input, which allows any other processes dependent on the input (such as discourse integration) to run in parallel regardless of the length of the input. 5 Computational Benefit Due to the decreased number of decisions required by this simplified model, it is substantially faster than previous similar models. To test this speed increase, the simplified model was compared with that of Schuler (2009). Both parsers used a grammar that had undergone 5 iterations of the Petrov et al. (2006) split-merge-smooth algorithm as found to be optimal by Petrov and Klein (2007), and both used a beam-width of 500 elements. Sections 02-21 of the Wall Street Journal Treebank were used in training the grammar induction for both parsers according to Petrov et al. (2006), and Section 23 was used for evaluation. No tuning was done as part of the transform to a sequence model. Speed results can be seen in Table 1. While the speed is not state-of-the-art in the field of parsing at large, it does break new ground for factored sequence model parsers. To test the accuracy of this parser, it was compa"
W12-1705,J01-2004,0,0.228508,"d Klein (2007), and both used a beam-width of 500 elements. Sections 02-21 of the Wall Street Journal Treebank were used in training the grammar induction for both parsers according to Petrov et al. (2006), and Section 23 was used for evaluation. No tuning was done as part of the transform to a sequence model. Speed results can be seen in Table 1. While the speed is not state-of-the-art in the field of parsing at large, it does break new ground for factored sequence model parsers. To test the accuracy of this parser, it was compared using varying beam-widths to the Petrov and Klein (2007) and Roark (2001) parsers. With the exception of the Roark (2001) parser, all parsers used 5 iterations of the Petrov et al. (2006) split57 System Schuler 2009 Current Model Sec/Sent 74 12 Table 1: Speed comparison with an unfactored probabilistic sequence model using a beam-width of 500 elements System Roark 2001 Current Model (500) Current Model (2000) Current Model (5000) Petrov Klein (Binary) Petrov Klein (+Unary) P 86.6 86.6 87.8 87.8 88.1 88.3 R 86.5 87.3 87.8 87.8 87.8 88.6 F 86.5 87.0 87.8 87.8 88.0 88.5 Table 2: Accuracy comparison with state-of-the-art models. Numbers in parentheses are number of par"
W12-1705,J10-1001,1,0.885298,"s the right child of cη . The set of syntactic categories in the grammar is denoted by C. Finally, JφK denotes an indicator probability which is 1 if φ and 0 otherwise. 2.2 Right-Corner Parsing each subtree into a left-expanding hierarchy ensures at most a single expansion (push) will occur at any given observation. That is, each new observation will be the leftmost leaf of a right-expanding subtree. Additionally, by reducing multiply rightbranching subtrees to single rightward branches, the transform also ensures that at most a single reduction (pop) will take place at any given observation. Schuler et al. (2010) show near complete coverage of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) can be achieved with a right-corner incremental parsing strategy using no more than four incomplete contituents (deferred processes), in line with recent estimates of human working memory capacity (Cowan, 2001). Section 3 will show that, in addition to being desirable for bounded working memory restrictions, the single expansion/reduction guarantee reduces the search space between words to only two decision points — whether to expand and whether to reduce. This allows rapid processing of"
W12-1705,N09-1039,1,0.102682,"has often been modelled using recurrent neural networks (Elman, 1991; Mayberry and Miikkulainen, 2003), which utilize a hidden context with a severely bounded representational capacity (a fixed number of continuous units or dimensions), similar to models of activationbased memory in the prefrontal cortex (Botvinick, 51 2007), with the interesting possibility that the distributed behavior of neural columns (Horton and Adams, 2005) may directly implement continuous dimensions of recurrent hidden units. This paper presents a refinement of a factored probabilistic sequence model of comprehension (Schuler, 2009) in the direction of a recurrent neural network model and presents some observed efficiencies due to this refinement. This paper will adopt an incremental probabilistic context-free grammar (PCFG) parser (Schuler, 2009) that uses a right-corner variant of the leftcorner parsing strategy (Aho and Ullman, 1972) coupled with strict memory bounds, as a model of human-like parsing. Syntax can readily be approximated using simple PCFGs (Hale, 2001; Levy, 2008; Demberg and Keller, 2008), which can be easily tuned (Petrov and Klein, 2007). This paper will show that this representation can be streamlin"
W13-2605,J93-2004,0,0.0474536,"ration to a +N operation (to integrate the connected component containing the gap with that containing the filler). In order to verify that the modifications to the transform correctly reallocate probability mass for gap operations, the goodness of fit to reading times of a model using this modified transform is compared against the publicly-available baseline model from van Schijndel and Schuler (2013), which uses the original Schuler (2009) transform.5 To ensure a valid comparison, both parsers are trained on a GCG-reannotated version of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) before being fit to reading times using linear mixed-effects models (Baayen et al., 2008).6 This evaluation focuses on the processing that can be done up to a given point in a sentence. In human subjects, this processing includes both immediate lexical access and regressions that aid in the integration of new information, so the reading times of interest in this evaluation are logtransformed go-past durations.7 The first and last word of each line in the Dundee corpus, words not observed at least 5 times in the WSJ training corpus, and fixations after long saccades (>4 words) are omitted from"
W13-2605,C12-1130,1,0.792358,"Missing"
W13-2605,P06-1055,0,0.048731,"who officials say stole millions using these rules is shown in Figure 2. The semantic expression produced by this derivation consists of a conjunction of terms defining the edges in the graph shown in Figure 1b. This GCG formulation captures many of the insights of the HPSG-like context-free filler-gap notation used by Hale (2001) or Lewis and Vasishth (2005): inference rules with adjacent premises can be cast as context-free grammars and weighted using probabilities, which allow experiments to calculate frequency measures for syntactic constructions. Applying a latent variable PCFG trainer (Petrov et al., 2006) to this formulation was shown to yield state-of-the-art accuracy for recovery of unbounded dependencies (Nguyen et al., 2012). Moreover, the functor-argument dependencies in a GCG define deep syntactic dependency graphs for all derivations, which can be used in incremental parsing to calculate connected components for memory-based measures. (Mh) These rules use composition functions fIM and fFM for initial and final modifiers, which define dependency edges numbered ‘1’ from referents of modifier functors i to referents of modificands j: def fIM = λg h j ∃i (1 i)= j ∧ (g i) ∧ (h j) def fFM = λ"
W13-2605,N01-1021,0,0.853722,"l based on deep syntactic dependencies. Results on eye-tracking data indicate that completing deep syntactic embeddings yields significantly more facilitation than completing surface embeddings. 1 William Schuler The Ohio State University schuler@ling.osu.edu Introduction Self-paced reading and eye-tracking experiments have often been used to support theories about inhibitory effects of working memory operations in sentence processing (Just and Carpenter, 1992; Gibson, 2000; Lewis and Vasishth, 2005), but it is possible that many of these effects can be explained by frequency (Jurafsky, 1996; Hale, 2001; Karlsson, 2007). Experiments on large naturalistic text corpora (Demberg and Keller, 2008; Wu et al., 2010; van Schijndel and Schuler, 2013) have shown significant memory effects at the ends of center embeddings when frequency measures have been included as separate factors, but these memory effects have been facilitatory rather than inhibitory. Some of the memory-based measures that produce these facilitatory effects (Wu et al., 2010; van Schijndel and Schuler, 2013) are defined in terms of initiation and integration of connected components of syntactic structure,1 with the presumption ∗ *T"
W13-2605,N09-1039,1,0.931757,"active sign of a previous connected component containing a filler without completing the current connected component. The four +F, –F, +L, and –L operations are therefore combined with applications of unary rules Ga–c for hypothesizing referents as fillers for gaps (providing f ′ 5 Evaluation The F, L, and N productions defined in the previous section can be made probabilistic by first computing a probabilistic context-free grammar (PCFG) from a tree-annotated corpus, then transforming that PCFG model into a model of probabilities over incremental parsing operations using a grammar transform (Schuler, 2009). This allows the intermediate PCFG to be optimized using an existing PCFG-based latent variable trainer 42 ∃i0 (.. :T/T {i0 } i0 ) the +Fa,–La,–N ∃i0 i2 (.. :T/T {i0 } i0 ) ∧ (.. :N/N-aD {i2 } i2 ) person –Fa,–La,–N ∃i0 i2 (.. :T/T {i0 } i0 ) ∧ (.. :N/V-rN {i2 } i2 ) who +Fa,+Lc,–N ∃i0 i2 i3 (.. :T/T {i0 } i0 ) ∧ (.. :N/V-gN {i3 } i2 ) officials +Fa,–La,–N ∃i0 i2 i3 i5 (.. :T/T {i0 } i0 ) ∧ (.. :N/V-gN {i3 } i2 ) ∧ (.. :V-gN/V-aN-gN {i5 } i5 ) say +Fb,+La,+N ∃i0 i2 i6 (.. :T/T {i0 } i0 ) ∧ (.. :N/V-aN {i6 } i2 ) stole +Fa,+La,–N ∃i0 i2 i7 (.. :T/T {i0 } i0 ) ∧ (.. :N/N {i7 } i2 ) millions –Fa"
W13-2605,N13-1010,1,0.837737,"Missing"
W13-2605,W12-1705,1,0.788155,"Missing"
W13-2605,P10-1121,1,0.889861,"actic embeddings yields significantly more facilitation than completing surface embeddings. 1 William Schuler The Ohio State University schuler@ling.osu.edu Introduction Self-paced reading and eye-tracking experiments have often been used to support theories about inhibitory effects of working memory operations in sentence processing (Just and Carpenter, 1992; Gibson, 2000; Lewis and Vasishth, 2005), but it is possible that many of these effects can be explained by frequency (Jurafsky, 1996; Hale, 2001; Karlsson, 2007). Experiments on large naturalistic text corpora (Demberg and Keller, 2008; Wu et al., 2010; van Schijndel and Schuler, 2013) have shown significant memory effects at the ends of center embeddings when frequency measures have been included as separate factors, but these memory effects have been facilitatory rather than inhibitory. Some of the memory-based measures that produce these facilitatory effects (Wu et al., 2010; van Schijndel and Schuler, 2013) are defined in terms of initiation and integration of connected components of syntactic structure,1 with the presumption ∗ *Thanks to Micha Elsner and three anonymous reviewers for their feedback. This work was funded by an Ohio Stat"
W14-2003,N01-1021,0,0.628906,"sing each word. This property greatly simplifies a vectorial implementation because it allows these single grammar rule applications to be superposed in cases of attachment ambiguity. Predictions of the vectorial model described in this paper are then calculated on a simple centerembedded sentence processing task, producing a lower completion accuracy for center-embedded sentences than for right-branching sentences with the same number of words. As noted by Levy and Gibson (2013), this kind of memory effect is not easily explained by existing information-theoretic models of frequency effects (Hale, 2001; Levy, 2008). The model described in this paper also provides an explanation for the apparent reality of linguistic objects like categories, grammar rules, discourse referents and dependency relations, as cognitive states in activation-based memory (in the case of categories and discourse referents), or cued associations in weight-based memory (in the case of grammar rules, and dependency relations), without having to posit complex machinery specific to language processing. In this sense, unlike existing chart-based parsers or connectionist models based on recurrent neural networks, this mode"
W14-2003,N07-1051,0,0.0881393,"0 ... ; a0 → a00 b00 0 00 a/b a /b matrix multiplication:1 b00 (+J) (–J) Figure 2: Fork and join operations from the van Schijndel et al. (2013a) left-corner parser formulation. During the fork phase, word x either completes an existing incomplete category a, or forks into a new complete category a0 . During the join phase, complete category a00 becomes a left child of a grammar rule application, then either joins onto a superordinate incomplete category a/b or remains disjoint. probabilistic version of this incremental parser can reproduce the results of a state-of-the-art chartbased parser (Petrov and Klein, 2007). 4 T/T the +F T/T, D –J T/T, NP/N wind –F T/T, NP –J T/T, S/VP shook +F T/T, S/VP, V +J T/T, S/NP the +F T/T, S/NP, D +J T/T, S/N mud +F T/T, S/N, N –J T/T, S/N, N/N room –F T/T, S/N, N +J T/T, S/N door –F T/T, S +J T/T Vectorial Parsing This left corner parser can be implemented in a vectorial model of working memory using vectors as activation-based memory and matrices as weight-based memory. Following Anderson et al. (1977) and others, vectors v in activation-based memory are cued from other vectors u through weight-based memory matrices M using ordinary v=Mu (1) This representation has be"
W14-2003,N13-1010,1,0.888324,"Missing"
W14-2003,W13-2605,1,0.863818,"Missing"
W14-2003,J10-1001,1,\N,Missing
W14-2003,W12-1706,0,\N,Missing
W15-0611,Q13-1005,0,0.0143705,"l patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical students were asked to complete a focused history of pre"
W15-0611,P05-1074,0,0.115102,"Missing"
W15-0611,P14-1133,0,0.0129923,"The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical students were asked to complete a focused history of present illness of a patient"
W15-0611,P09-1053,0,0.0458946,"Missing"
W15-0611,D08-1069,0,0.0752837,"Missing"
W15-0611,W11-2107,0,0.0343506,"Missing"
W15-0611,C04-1051,0,0.0143944,"Missing"
W15-0611,E12-1048,0,0.0229144,"nce, allowing them to “suspend disbelief” and behave as if the virtual patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of t"
W15-0611,W13-1738,0,0.0236171,"uspend disbelief” and behave as if the virtual patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical student"
W15-0611,D13-1090,0,0.0291762,"Missing"
W15-0611,N12-1019,0,0.0727964,"Missing"
W15-0611,P02-1040,0,0.0915643,"Missing"
W15-0611,W03-1209,0,0.289668,"Missing"
W15-0611,U06-1019,0,0.0515201,"Missing"
W15-0611,Q14-1034,0,0.0577806,"Missing"
W15-1109,J80-3005,0,0.679391,"Missing"
W15-1109,N01-1021,0,0.358614,"uency band, which suggests that many modeling claims could be tested on this sort of data even without controlling for frequency effects. 1 Introduction Current accounts of sentence processing (e.g., Gibson, 2000; Lewis and Vasishth, 2005) usually involve working memory: parts of sentences are stored while unrelated material is processed, then retrieved when they can be integrated. But evidence for the role of memory in sentence processing usually comes in the form of latency measurements in self-paced reading or eye-tracking data, in which frequency effects are a powerful potential confound (Hale, 2001; Levy, 2008; Demberg and Keller, 2008; ∗ Formerly of the Dept. of Machine Learning, Carnegie Mellon University Experiments described in this paper therefore attempt to find a clearer measure of variable memory usage in sentence processing, independent of frequency influences. In particular, this paper focuses on the coherence of oscillatory neural activity between anterior and posterior areas of left cortex. Areas including the left inferior frontal gyrus and the posterior left temporal cortex have been implicated in language use, especially passive listening tasks (Hagoort and Indefrey, 2014"
W15-1109,J93-2004,0,0.0501919,"h from a left branch in a syntactic binarybranching tree.5 Experiments described in this paper estimate syntactic memory load when processing a particular word of a sentence as the center-embedding depth of that word, which is the number of incomplete categories maintained while processing that word using a left-corner parser (Aho and Ullman, 1972; Johnson-Laird, 1983; Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994). To obtain an accurate estimate of center-embedding depth, this study uses the van Schijndel et al. (2013) leftcorner PCFG parser trained on the Penn Treebank (Marcus et al., 1993) reannotated into a Nguyen et al. (2012) generalized categorial grammar (GCG),6 which makes PCFG probabilities sensitive to fillergap propagation. This parser achieves a linguistic accuracy comparable to the Petrov and Klein (2007) parser, and the PCFG surprisal estimates it outputs using this grammar provide a state-of-the-art fit to psycholinguistic measures like self-paced reading times and eye-tracking fixation durations (van Schijndel and Schuler, 2015). The experiments described in Section 6 run this parser on transcripts of the Heart of Darkness dataset described in Section 3, calculati"
W15-1109,C12-1130,1,0.906332,"Missing"
W15-1109,N07-1051,0,0.017485,"he number of incomplete categories maintained while processing that word using a left-corner parser (Aho and Ullman, 1972; Johnson-Laird, 1983; Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994). To obtain an accurate estimate of center-embedding depth, this study uses the van Schijndel et al. (2013) leftcorner PCFG parser trained on the Penn Treebank (Marcus et al., 1993) reannotated into a Nguyen et al. (2012) generalized categorial grammar (GCG),6 which makes PCFG probabilities sensitive to fillergap propagation. This parser achieves a linguistic accuracy comparable to the Petrov and Klein (2007) parser, and the PCFG surprisal estimates it outputs using this grammar provide a state-of-the-art fit to psycholinguistic measures like self-paced reading times and eye-tracking fixation durations (van Schijndel and Schuler, 2015). The experiments described in Section 6 run this parser on transcripts of the Heart of Darkness dataset described in Section 3, calculating centerembedding depth for each word epoch based on its position in the best output parse. This parser is also used to calculate PCFG surprisal as a potentially confounding predictor. 5 In fact, there are conditions where a post-"
W15-1109,C92-1032,0,0.541658,"mbedding depth on the left. example) that must be retained in working memory until the dependency can be completed (Gibson, 2000). The load should increase every time there is a right branch from a left branch in a syntactic binarybranching tree.5 Experiments described in this paper estimate syntactic memory load when processing a particular word of a sentence as the center-embedding depth of that word, which is the number of incomplete categories maintained while processing that word using a left-corner parser (Aho and Ullman, 1972; Johnson-Laird, 1983; Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994). To obtain an accurate estimate of center-embedding depth, this study uses the van Schijndel et al. (2013) leftcorner PCFG parser trained on the Penn Treebank (Marcus et al., 1993) reannotated into a Nguyen et al. (2012) generalized categorial grammar (GCG),6 which makes PCFG probabilities sensitive to fillergap propagation. This parser achieves a linguistic accuracy comparable to the Petrov and Klein (2007) parser, and the PCFG surprisal estimates it outputs using this grammar provide a state-of-the-art fit to psycholinguistic measures like self-paced reading times and eye-tr"
W15-1109,D09-1034,0,0.0896517,"ported in this paper show that this does indeed seem to be the case. Exploratory analyses with the development partition of a dataset of MEG recordings of subjects listening to narrative text revealed a strong effect for memory load on alphaband coherence between an anterior and posterior pair of left-hemisphere sensors. Follow-on validation with a larger test partition confirmed the significance of this effect. Moreover, these effects could not be explained by frequency or sentence position predictors, unlike effects on self-paced reading and eye-tracking latencies (Demberg and Keller, 2008; Roark et al., 2009; Wu et al., 2010). The remainder of this paper is organized as follows: Section 2 provides a brief introduction to magnetoencephalography, Section 3 describes the MEG dataset used in these experiments, Section 4 describes the oscillatory coherence measure used to evaluate phase-aligned activation, Section 5 describes the center-embedding depth predictor, Section 6 describes the regression experiments and their results, and Section 7 discusses implications of these results for some open debates about hierarchic sentence processing. 2 MEG Background Magnetoencephalography (MEG), like electroenc"
W15-1109,N15-1183,1,0.883095,"Missing"
W15-1109,D14-1030,0,0.0724659,"ion means that it is not as sensitive to deep sources. And correspondingly, any magnetic coherence between two sensors can be more reliably traced to coherence between the two corresponding regions of the brain, whereas the poor spatial resolution of EEG means that coherence between sensors does not necessarily reflect coherence between the corresponding regions of the brain. 3 Data Collection This study makes use of a naturalistic audio-book listening task during MEG recording. This design allows us to examine language processing in a more ecologically realistic manner (Brennan et al., 2012; Wehbe et al., 2014a; Wehbe et al., 2014b), as both the participant experience (reading/listening to a story for enjoyment) and author’s aim are authentic language acts. Participants were asked to sit still in an upright position with their eyes closed, while they listened to an 80-minute excerpt of an English-language novel. The listening task was split into 8 sections of approximately 10 minutes each, and participants had the opportunity to rest between them. The text used was the second chapter of the novel Heart of Darkness by Joseph Conrad, containing 628 sentences and 12,342 word tokens. The plain-text and"
W15-1109,H94-1097,0,0.465132,"lorme and Makeig, 2003). The signal time-courses and component scalp-maps were visually inspected for eye-movement and line-noise components, but none were identified. The parallel audio recording channel was used to identify the precise sample points at which each of the 8 audio runs began and ended (these varied as participants chose to take breaks of different lengths). The eight excerpts were then spliced together to form a continuous set of MEG signals corresponding exactly to the complete audio-book time-course. This allowed us to use speech recognition forced alignment methods (MS HTK; Woodland et al., 1994) to precisely locate the onset and offset times of each auditory word. These automatically derived onset and offset times were subsequently validated by hand. 4 Coherence There are a variety of measures available that reflect the connectivity between two brain regions. This study makes use of ‘spectral coherence,’ which is sensitive both to power/energy increases registered by the relevant sensors and to the degree of phase 81 synchronization observed by those sensors. Spectral coherence is computed with the following formula: E[Sxy ] coherence(x, y) = p E[Sxx ] · E[Syy ] (1) where x and y are"
W15-1109,P10-1121,1,0.910947,"show that this does indeed seem to be the case. Exploratory analyses with the development partition of a dataset of MEG recordings of subjects listening to narrative text revealed a strong effect for memory load on alphaband coherence between an anterior and posterior pair of left-hemisphere sensors. Follow-on validation with a larger test partition confirmed the significance of this effect. Moreover, these effects could not be explained by frequency or sentence position predictors, unlike effects on self-paced reading and eye-tracking latencies (Demberg and Keller, 2008; Roark et al., 2009; Wu et al., 2010). The remainder of this paper is organized as follows: Section 2 provides a brief introduction to magnetoencephalography, Section 3 describes the MEG dataset used in these experiments, Section 4 describes the oscillatory coherence measure used to evaluate phase-aligned activation, Section 5 describes the center-embedding depth predictor, Section 6 describes the regression experiments and their results, and Section 7 discusses implications of these results for some open debates about hierarchic sentence processing. 2 MEG Background Magnetoencephalography (MEG), like electroencephalography (EEG)"
W15-3304,P13-2018,0,0.152368,"oded constraints are expressed using dependency functions,2 labeled with dependency types or argument position numbers: f0 , f1 , f2 , etc. For example, a constraint function g may consist of a single ‘0’-labeled dependency to a constant ‘people’: Introduction Categorial grammar annotations are attractive because they have a transparent syntactic-semantic interface and provide a natural account of traces (Rimell et al., 2009; Nguyen et al., 2012). This is especially important in parsing Chinese, which generates 1.5 times as many traces as English and makes heavy use of unbounded dependencies (Kummerfeld et al., 2013). Unfortunately, the accuracy of parsers trained on existing categorial grammar reannotations (Chinese CCGbank; Tse and Curran, 2010) of the Penn Chinese Treebank (Xue et al., 2005) is much lower than that of parsers trained on the original Treebank (Tse and Curran, 2012). This may be because previous 1 Nguyen et al (2012) notate the ‘//’ and ‘\’ operators of Bach (1981) as -g and -h, mnemonic for ‘gap’ and ‘heavy shift’. 2 Dependencies shown here can be interpreted as a shorthand for distributed representations of sentence meanings compatible with cognitive computational neuroscientific mode"
W15-3304,C12-1130,1,0.835346,"Missing"
W15-3304,N07-1051,0,0.594283,"Missing"
W15-3304,P06-1055,0,0.193586,"Missing"
W15-3304,D09-1085,0,0.412522,"Missing"
W15-3304,S14-1018,1,0.839362,"curacy of parsers trained on existing categorial grammar reannotations (Chinese CCGbank; Tse and Curran, 2010) of the Penn Chinese Treebank (Xue et al., 2005) is much lower than that of parsers trained on the original Treebank (Tse and Curran, 2012). This may be because previous 1 Nguyen et al (2012) notate the ‘//’ and ‘\’ operators of Bach (1981) as -g and -h, mnemonic for ‘gap’ and ‘heavy shift’. 2 Dependencies shown here can be interpreted as a shorthand for distributed representations of sentence meanings compatible with cognitive computational neuroscientific models of episodic memory (Schuler and Wheeler, 2014). 25 Proceedings of the Grammar Engineering Across Frameworks (GEAF) Workshop, 53rd Annual Meeting of the ACL and 7th IJCNLP, pages 25–32, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics λ x (f0 x)=people. 3 The first composition rule Aa stipulates that when a predicate h of category pϕ1..n-1 -ac takes a preceding argument g of category c as its n-th argument, the syntactic dependency that g is h’s n-th argument is added. The second composition rule Ab is an argument composition rule taking a succeeding argument. Chinese Syntax in GCG Chinese is typically an"
W15-3304,C10-1122,0,0.14231,"o that defined for English by Nguyen et al (2012), which uses a larger set of language-specific inference rules and a substantially smaller category set. Experimental results show a statistically significant gain in parsing accuracy from this moderately lexicalized grammar over parsing with a strongly lexicalized CCG. Categorial grammars are attractive because they have a clear account of unbounded dependencies. This accounting is especially important in Mandarin Chinese which makes extensive usage of unbounded dependencies. However, parsers trained on existing categorial grammar annotations (Tse and Curran, 2010) extracted from the Penn Chinese Treebank (Xue et al., 2005) are not as accurate as those trained on the original treebank, possibly because enforcing a small set of inference rules in these grammars leads to large sets of categories, which cause sparse data problems. This work reannotates the Penn Chinese Treebank into a generalized categorial grammar which uses a larger rule set and a substantially smaller category set while retaining the capacity to model unbounded dependencies. Experimental results show a statistically significant improvement in parsing accuracy with this categorial gramma"
W15-3304,N12-1030,0,0.228107,"mmar annotations are attractive because they have a transparent syntactic-semantic interface and provide a natural account of traces (Rimell et al., 2009; Nguyen et al., 2012). This is especially important in parsing Chinese, which generates 1.5 times as many traces as English and makes heavy use of unbounded dependencies (Kummerfeld et al., 2013). Unfortunately, the accuracy of parsers trained on existing categorial grammar reannotations (Chinese CCGbank; Tse and Curran, 2010) of the Penn Chinese Treebank (Xue et al., 2005) is much lower than that of parsers trained on the original Treebank (Tse and Curran, 2012). This may be because previous 1 Nguyen et al (2012) notate the ‘//’ and ‘\’ operators of Bach (1981) as -g and -h, mnemonic for ‘gap’ and ‘heavy shift’. 2 Dependencies shown here can be interpreted as a shorthand for distributed representations of sentence meanings compatible with cognitive computational neuroscientific models of episodic memory (Schuler and Wheeler, 2014). 25 Proceedings of the Grammar Engineering Across Frameworks (GEAF) Workshop, 53rd Annual Meeting of the ACL and 7th IJCNLP, pages 25–32, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics λ"
W16-4104,P13-2121,0,0.0391619,"Missing"
W16-4104,N15-1183,1,0.901506,"Missing"
W16-4106,P13-2121,0,0.0433064,"Treebank style representations, which are distributed with the Natural Stories corpus. The GCG framework was chosen because it contains an implicit representation of syntactic dependencies and because it can be used to calculate incremental representations of the memory store of a left-corner parser. This allowed us to compute all predictors under consideration from source trees. To control for memory-independent information theoretic effects, for each word in the corpus we also computed 5-gram forward probabilities from the Gigaword 4.0 corpus (Graff and Cieri, 2003) using the KenLM toolkit (Heafield et al., 2013) and PCFG surprisal using the van Schijndel et al. (2013a) parser. It is an open question as to when during processing the effects in question will occur. For example, while readers may slow down when they encounter the final word of a center-embedding region, it is also possible that they would not slow down until the following word, when the need for integration is confirmed. In addition, self-paced reading (SPR) data are known to sometimes produce later effects (Kaiser, 2014; Jegerski, 2014). We therefore calculate variants of each of these 20 predictors in four spillover positions, yieldin"
W16-4106,C12-1130,1,0.913047,"Missing"
W16-4106,C92-1032,0,0.570119,"s increases to 13 for DLT-V because the finite verb caught, which intervenes in both dependencies c and d, is upweighted from 1 to 2, along with fled itself. Because the cost of supervisors and co-workers, which also intervenes in both c and d, decreases from 2 to 1 for DLT-C, the DLT-C integration cost of fled is reduced by 2 (from 10 to 8). DLT-M ignores the preceding modifier dependency d, resulting in an integration cost of 4 (dependency a) + 1 (discourse cost of fled) = 5. 3.2 Left-corner parsing Many sentence processing models (Johnson-Laird, 1983; Abney and Johnson, 1991; Gibson, 1991; Resnik, 1992; Stabler, 1994; Lewis and Vasishth, 2005) are defined in terms of left-corner parsing operations (Aho and Ullman, 1972; van Schijndel et al., 2013a), which assemble local dependencies between signs using a minimal store of incomplete derivation fragments. Left-corner parsers account for sequences of words x1 . . . xT as stacked-up derivation fragments a/b, each consisting of a top sign a 52 lacking a bottom sign b yet to come. When a left-corner parser consumes a word, it makes decisions to fork off and/or join up these derivation fragments. When the current word xt satisfies the missing bott"
W16-4106,J10-1001,1,0.836512,"he Bachrach et al. (2009) corpus to investigate the correlation between changes in embedding depth and reading times and found a positive effect on latency.2 3 Background This work explores two related models of the relationship between memory and sentence processing: (1) the Dependency Locality Theory, in which memory is predicted to be used to construct syntactic dependencies to words in the preceding context with a cost proportional to the length of the dependency (or dependencies) being constructed, and (2) left-corner theories of sentence processing, such as Lewis and Vasishth (2005) and Schuler et al. (2010), in which certain parser operations require disjoint incomplete signs (referring to discourse referents) to be recalled from working memory. We outline these broader frameworks, along with a number of possible implementations of each, in the remainder of this section. 3.1 Dependency Locality Theory The Dependency Locality Theory (DLT; Gibson, 2000) predicts a cost for integrating a word into an incomplete parse proportional to the number of discourse referents that intervene in any syntactic dependencies the word shares with words in its preceding context. For simplicity, Gibson (2000) implem"
W16-4106,N13-1010,1,0.895529,"Missing"
W16-4106,W13-2605,1,0.870552,"Missing"
W16-4106,P10-1121,1,0.933699,"g model of sentence processing. Our results indicate that memory access during sentence processing does take time, but suggest that stimuli requiring many memory access events may be necessary in order to observe the effect. 1 Introduction Any incremental model of sentence processing where an abstract meaning representation is built up word-by-word must involve storage and retrieval of information about previously encountered material from some memory store. The retrieval operations have been hypothesized to be associated with increased processing time (Gibson, 2000; Lewis and Vasishth, 2005; Wu et al., 2010), and this prediction has been borne out in experiments using constructed stimuli (Gibson, 2000; Grodner and Gibson, 2005; Boston et al., 2011; von der Malsburg et al., 2015). However, memory-based latency effects have been null or even negative in broad-coverage reading time experiments using naturally-occurring text data that included baseline controls for n-gram and probabilistic phrase-structure grammar (PCFG) surprisal (Demberg and Keller, 2008; van Schijndel et al., 2013b). The failure of experimental latency effects to generalize to naturally-occurring data raises doubts about their exi"
W18-0101,E12-1041,0,0.013425,"chner law (Fechner, 1966), which maintains that perception of stimuli increase additively as stimulus strength increases multiplicatively. Stevens’ power law (Stevens, 1957) expresses a similar relationship. For word frequencies, which exhibit a Zipfian curve, the log of the probability essentially converts the frequencies to a linear perception curve, allowing easier differentiation of the relative rarity of words that occur exponentially more or less frequently. Ngram Surprisal controls for conditional word frequency, given preceding words as context, and is a commonly used baseline effect (Monsalve et al., 2012; van Schijndel and Schuler, 2015). 5gram probability is calculated as the linear combination of most likely n-grams up to 5 words long, S(wi ) = − log P (wi |wi−n ...wi−1 ) (1) To control for the effect of surprisal due to syntactic context, the current work estimates the probability of syntactic tree structure at each given word (Shain et al., 2016; van Schijndel and Schuler, 2015). Syntactic context is defined as the linear combination of all previous syntactic rule productions up to the current word. Probabilistic Context-Free Grammar (PCFG) Surprisal follows that used by van Schijndel and"
W18-0101,C12-1130,1,0.901751,"Missing"
W18-0101,W16-4106,1,0.870721,"Missing"
W18-0101,N01-1021,0,0.133656,"y smoothing allows the full sequence to be estimated as an interpolation of shorter n-grams. Following Shain et al. (2016), this work uses 5-gram probabilities from the Gigaword 4.0 corpus (Graff and Cieri, 2003) using the KenLM toolkit (Heafield et al., 2013): In order to isolate new effects, it is necessary to statistically control for known effects. These experiments use word length, n-gram surprisal, syntactic surprisal, and story position. Word length is a baseline predictor measured as the number of characters in each word. Longer words are predictive of longer reading times. Surprisal (Hale, 2001) is the log of the inverse frequency, which increases as the frequency decreases. The log transform makes surprisal a more linear measure of exponential changes in stimulus. The linearity of surprisal is desirable not only because it allows LMER fitting, but because it corresponds with the Weber-Fechner law (Fechner, 1966), which maintains that perception of stimuli increase additively as stimulus strength increases multiplicatively. Stevens’ power law (Stevens, 1957) expresses a similar relationship. For word frequencies, which exhibit a Zipfian curve, the log of the probability essentially c"
W18-0101,P13-2121,0,0.0335736,"Missing"
W18-0101,N15-1183,1,0.86666,"Missing"
W98-0137,P97-1003,0,0.0416942,"oiting Semantic Dependencies in Parsing William Schuler Computer and Information Science Dept. University of Pennsylvania Philadelphia, PA 19103 schuler©linc.cis.upenn.edu Abstract In this paper we describe a semantic dependency model for estimating probabilities in a stochastic TAG parser (Resnik, 1992) (Schabes, 1992), and we compare it with the syntactic dependency model inherent in a TAG derivation using the flat treatment of modifiers described in (Schabes and Shieber, 1994). 1 Introduction The use of syntactic dependencies to estimate parser probabilities is not uncommon (Eisner, 1996) (Collins, 1997) (Charniak, 1997). Typically, a maximum probability parse is estimated from bigram statistics of lexical items that participate in head-modifier or head-complement dependencies with other lexical items. These dependencies can be characterized as ( head, label, modifier ) triples and ( head, label, complement ) triples - or as labeled directed arcs in a graph - which have the property that each lexical item may participate as a modifier or a complement in no more than one dependency. Using a TAG derivation tree (Joshi, 1987) with a flat treatment of modifiers (Schabes and Shieber, 1994), it is"
W98-0137,C92-2065,0,0.0746612,"ifiers (Schabes and Shieber, 1994), it is possible to capture the long distance dependencies of wh-extractions and relative clauses as adjacent arcs in a dependency structure, making them available for probability estimates withiu the parser as well. In this case, the head-complement dependencies for a sentence correspond to a set S of substitution triples (/, rJ, a) (where tree a substitutes into tree &apos;Y at note address ?J), and the head-modifier dependencies correspond to a set A of adjunction triples (/, 1}; ß) (where tree ß adjoins into tree &apos;Y at node address 7J), in a probabilistic TAG (Resnik, 1992).1 Although the TAG-based syntactic dependency rnodel has the necessary domain of locality (in terms of adjacent arcs on the derivation tree) to accurately guide a statistical parser, it is still susceptible to sparse data effects, in part because it does not generalize attachment statistics across syntactic transformations. An adjective used as a declarative predicate, for example, could not draw on attachment statistics for the same adjective used as a modifier, or as a predicate in a relative clause, and vice versa, because each transformation uses a different syntactic dependency structure"
W98-0137,C92-2066,0,0.0513686,"Missing"
W98-0137,C88-2121,0,0.0429265,"ic model.The triples in the semantic dependency set 1) for the sentences, 11The damaged handle is attached to the drawer,&quot; and &quot;The handle attached to the drawer is damaged,&quot; are represented as arcs in Figure 2. Formally, we augment the syntactic dependency sets S and A with a semantic dependency set V of ( predicate, label, argument ) triples defined as follows: i handle attach to ~ door The handle auached to the door is damaged. Figure 2: Semantic dependencies 2 Parsing Parsing proceeds in three passes of O(n 6 ) complexity. First, the chart is filled in from the bottom up, as described in (Schabes et al., 1988), and the input is recognized or rejected. The parser then constructs a shared forest (VijayShanker and Weir, 1993) top-down from the elements in the chart, ignoring those items on bottom-up dead ends. Finally, the parser proceeds with the more expensive Operations of feature unification and probability estimation on the reduced set of nodes in the shared forest. The chart consists of a set of items that each specify a node address 77 in an elementary tree a, a top (T) or bottom (.l) marker denoting the • For every substitution (head-complement) phase of operation on the node, and four indices"
W98-0137,E93-1045,0,\N,Missing
zhao-etal-2000-machine,C90-3001,0,\N,Missing
zhao-etal-2000-machine,C90-3045,0,\N,Missing
zhao-etal-2000-machine,W90-0102,0,\N,Missing
