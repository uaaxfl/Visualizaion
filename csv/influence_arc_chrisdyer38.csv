2012.amta-papers.4,2010.amta-papers.16,0,0.0900591,"Missing"
2012.amta-papers.4,W09-0432,0,0.169229,"tems are often used for information assimilation, which allows users to make sense of information written in various languages they do not speak. This use case is particularly important for translation web services, such as Google Translate and Microsoft Bing Translator, which seek to make more of the web accessible to more users. A crucial challenge facing such systems is that they must translate documents from a variety of different domains, but it has been observed that the performance of statistical systems can suffer substantially when testing conditions deviate from training conditions (Bertoldi and Federico, 2009). However, it is not always possible or costeffective to collect training data for all of the desired application domains. In fact, training data tends to be collected opportunistically from any available sources rather than from curated sources that match the distribution of test data (Koehn and Schroeder, 2007). As a result, inputs from each application domain may frequently be very different from the training data. With such domain mismatches being commonplace, this paper looks at a way of adapting the behavior of a translation system based on the domain of the input documents, which can be"
2012.amta-papers.4,W06-1615,0,0.056453,"edze et al. (2007) report their experiments in a shared task for domain adaptation of dependency parsing in which they explored adding features more likely to transfer across domains and removing features less likely to transfer. Machine learning: Domain adaptation has also been well-studied from a more general machine learning perspective. Daum´e (2006) points out that “the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution” and goes on to formuvvlate the M EGA (Maximum Entropy Genre Adaptation) model. Blitzer et al. (2006; Blitzer (2007) describe structural correspondence learning, which automatically discovers features that behave similarly in both a source and target domain. Ben-David et al. (2010) provide formal bounds on source and target error for various discriminative learning criteria. Huang and Yates (2012) propose a domain adaptation method of representation learning, which automatically discovers feature more likely to generalize across domains. By using posterior regularization to bias the process of representation learning, they observe improvements on a part of speech tagging task and a named ent"
2012.amta-papers.4,D08-1024,0,0.0610337,"due to sentence-level LMs being impractical due to their large size, which results from the large space of target translations. 3 guage model probability of each partial hypothesis. In the case of the cdec decoder, which was used to implement this work, the modification involved only a few lines of code. 2.2 Pairwise Ranking Optimization As the number of domains increases, feature augmentation can result in much larger feature sets to be optimized. While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008) has been shown to be effective over larger feature sets, as an on-line margin learning algorithm, it is more difficult to regularize – this will become important in Section 2.4. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner. PRO works by sampling pairs of hypotheses from the decoder’s k-best list and then providing these pairs as a binary training examples to a standard binary classifier to obtain a new set of weights for the linear model. In our case, the binary classifier is trained using L-BFGS. This procedure of sampling training pairs and then optimi"
2012.amta-papers.4,J07-2003,0,0.0779102,"regularization as partitioning the feature set into 2 groups (domain-agnostic and domain-specific) and then applying a `2 regularizer with weight γ to the domainspecific group. This preserves the convexity of the objective function and makes it easy to incorporate into the gradient-based updates of PRO. With this in mind, the gradient is: ∇w Rcomplexity = γ |h| X 2wi (2) i=0 iff SPECIFIC(hi ) We apply this penalty to the domain-specific features in addition to the default `2 regularizer. 3 Experimental Setup Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first wordaligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. Such aligned subphrases are used to generalize their parent phrases by being substituted as a single nonterminal symbol [X]. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrase-based and chart-based paradigms including Hiero and syntactic systems. Our decision to use Hiero was primarily motivated by the cdec decoder’s API being most amenable to im"
2012.amta-papers.4,W07-0722,0,0.0306277,"ature weights to aggregate the component models. Unlike the work in this paper, the optimization procedure for training these final feature weights cannot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et al. (2007) use source monolingual data in various ways to improve target domain performance, focusing on corpus filtering techniques such that more releva"
2012.amta-papers.4,P07-1033,0,0.338681,"Missing"
2012.amta-papers.4,P10-4002,1,0.822415,"se translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. Such aligned subphrases are used to generalize their parent phrases by being substituted as a single nonterminal symbol [X]. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrase-based and chart-based paradigms including Hiero and syntactic systems. Our decision to use Hiero was primarily motivated by the cdec decoder’s API being most amenable to implenting these techniques. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011), though analysis indicates that the parameters converged much earlier. The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization (as described in Section 2.4) is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix arr"
2012.amta-papers.4,W07-0717,0,0.0722863,"on adapting the translation model and language model. Koehn and Schroeder (2007) explore several techniques for domain adaptation in SMT including multiple translation models (via multiple factored decoding paths), interpolated langauge models, and multiple language models. Xu et al. (2007) build a general domain translation system and then construct domain-specific language models and tune domain-specific feature weights to aggregate the component models. Unlike the work in this paper, the optimization procedure for training these final feature weights cannot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to ac"
2012.amta-papers.4,D10-1044,0,0.0681968,"nguage models. Xu et al. (2007) build a general domain translation system and then construct domain-specific language models and tune domain-specific feature weights to aggregate the component models. Unlike the work in this paper, the optimization procedure for training these final feature weights cannot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et"
2012.amta-papers.4,W12-3134,0,0.0128464,"eline learner. PRO works by sampling pairs of hypotheses from the decoder’s k-best list and then providing these pairs as a binary training examples to a standard binary classifier to obtain a new set of weights for the linear model. In our case, the binary classifier is trained using L-BFGS. This procedure of sampling training pairs and then optimizing the pairwise rankings is repeated for a specified number of iterations. It has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). 2.3 Impact on Time and Space Requirements In this section, we describe the minimal impact that this technique has on development and runtime time and space requirements. During system development, no additional time nor space is required for building additional translation models or language models since we construct only one per language pair. This also holds at runtime, which can be important if multiple deployed translation systems are competing for CPU and RAM resources on a shared server. The main burden introduced by feature augmentation is the larger number of features itself. As disc"
2012.amta-papers.4,D12-1120,0,0.020982,"a more general machine learning perspective. Daum´e (2006) points out that “the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution” and goes on to formuvvlate the M EGA (Maximum Entropy Genre Adaptation) model. Blitzer et al. (2006; Blitzer (2007) describe structural correspondence learning, which automatically discovers features that behave similarly in both a source and target domain. Ben-David et al. (2010) provide formal bounds on source and target error for various discriminative learning criteria. Huang and Yates (2012) propose a domain adaptation method of representation learning, which automatically discovers feature more likely to generalize across domains. By using posterior regularization to bias the process of representation learning, they observe improvements on a part of speech tagging task and a named entity recognition task. Domain adaptation has also been framed as a semi-supervised learning problem in which unlabelled in-domain data is used to augment out-of-domain labelled data (Daum´e III et al., 2010). Perhaps most similar to this work is Daum´e (2007), which proposes a method for feature augm"
2012.amta-papers.4,2007.mtsummit-papers.34,0,0.0262638,"ot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et al. (2007) use source monolingual data in various ways to improve target domain performance, focusing on corpus filtering techniques such that more relevant data is included in the models. Bertoldi and Federico (2009) saw large improvements by using automatic translations of monolingual in-domain data to"
2012.amta-papers.4,W07-0733,0,0.493994,"e to more users. A crucial challenge facing such systems is that they must translate documents from a variety of different domains, but it has been observed that the performance of statistical systems can suffer substantially when testing conditions deviate from training conditions (Bertoldi and Federico, 2009). However, it is not always possible or costeffective to collect training data for all of the desired application domains. In fact, training data tends to be collected opportunistically from any available sources rather than from curated sources that match the distribution of test data (Koehn and Schroeder, 2007). As a result, inputs from each application domain may frequently be very different from the training data. With such domain mismatches being commonplace, this paper looks at a way of adapting the behavior of a translation system based on the domain of the input documents, which can be matched during both tuning and test time. One of the key trade offs in designing a statistical model is the balance between bias and variance. In domain adaptation, we would like to bias each domain’s model toward the distribution of that specific domain, yet we also desire models with low variance. Since each d"
2012.amta-papers.4,D07-1104,0,0.0202934,"syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011), though analysis indicates that the parameters converged much earlier. The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization (as described in Section 2.4) is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008a; Lopez, 2007; Lopez, 2008b), which is distributed with cdec: • log Pcoherent (e|f ): The coherent phrase-tophrase translation probability (Lopez, 2008b, p. 103). The phrasal probability of each English SCFG antecedent (e.g. “el [X] gato”) given a particular foreign SCFG antecedent “the [X] cat” combined with coherence, the ratio of successful source extractions to the number of attempted extractions • log Plex (e|f ), log Plex (f |e): The lexical alignment probabilities within each translation rule, as computed by a maximum likelihood estimation over the Viterbi alignments Sentences Translations NIST Trai"
2012.amta-papers.4,C08-1064,0,0.0186954,"that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011), though analysis indicates that the parameters converged much earlier. The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization (as described in Section 2.4) is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008a; Lopez, 2007; Lopez, 2008b), which is distributed with cdec: • log Pcoherent (e|f ): The coherent phrase-tophrase translation probability (Lopez, 2008b, p. 103). The phrasal probability of each English SCFG antecedent (e.g. “el [X] gato”) given a particular foreign SCFG antecedent “the [X] cat” combined with coherence, the ratio of successful source extractions to the number of attempted extractions • log Plex (e|f ), log Plex (f |e): The lexical alignment probabilities within each translation rule, as computed by a maximum likelihood estimation over the Viterbi alignments Sentences Translat"
2012.amta-papers.4,P02-1040,0,0.0867868,"t the 0.01 level according to approximate randomization over 5 optimizer replications. Test (All Domains) Baseline Domain Augmented 46.5 47.5 (+1.0) Figure 6: Results of feature augmentation experiments on the 1M sentence Czech→English data set as measured by the BLEU metric. The domain augmented system has 49 features. Both the Meteor and TER evaluation metrics also showed improvements. Results are significant at the 0.01 level according to approximate randomization over 3 optimizer replications. Evaluation: We quantify increases in translation quality using automatic metrics including BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by measuring statistical significance according to approximate randomization (Clark et al., 2010).7 Evaluation is performed on tokenized lowercased references. 4 Results and Analysis We show the results of using feature augmentation for domain adaptation to an Arabic→English system in Figure 5. There, we see an overall improvement of only 0.2 - 0.3 BLEU. Interestingly, we see an improvement of up to 0.7 BLEU in the weblog domain while we see no improvement in the newswire do7 MultEval 0.4.2 is available at github.com/jhclark/ multev"
2012.amta-papers.4,P07-1004,0,0.0504022,"al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et al. (2007) use source monolingual data in various ways to improve target domain performance, focusing on corpus filtering techniques such that more relevant data is included in the models. Bertoldi and Federico (2009) saw large improvements by using automatic translations of monolingual in-domain data to augment the training data of their original system. Domain identification: Closely related to domain adaptation is domain identification. Banerjee et al. (2010) focus on the problem of determining the domain of the input data so that the appropriate domain-specific translation system can be used. EBMT:"
2012.amta-papers.4,P11-2031,1,\N,Missing
2012.amta-papers.4,N10-1031,1,\N,Missing
2012.amta-papers.4,D07-1112,0,\N,Missing
2012.amta-papers.4,bojar-etal-2012-joy,0,\N,Missing
2012.amta-papers.4,D11-1125,0,\N,Missing
2020.acl-main.231,P08-1090,0,0.103392,"Missing"
2020.acl-main.231,W02-1002,0,\N,Missing
2020.acl-main.231,P08-1100,0,\N,Missing
2020.findings-emnlp.106,N19-1423,0,0.140809,"er labeled dataset (Figure 1). 1183 Unsupervised Pre-training with CPC Semi-supervised Speech Recognition T H E _ D O G _ I S _ …. CTC …. ( 100 Hz ) …. : Causal conv …. ( 16kHz / 160 = 100 Hz ) : Causal strided conv …. ( 16kHz ) Pre-trained parameters are ﬁxed. Figure 1: Left, unsupervised representation learning with forward contrastive predictive coding. The learned representations are fixed and used as inputs to a speech recognition model (Right). 3.1 Unsupervised learning with bi-directional CPC Following the success of bidirectional models in representation learning (Peters et al., 2018; Devlin et al., 2019), we extend the original CPC method explained above with bidirectional context networks. The encoder function genc is shared for both directions, but there are two autoregresfwd bwd ) which read encoded sive models (gar and gar observations (z) from the forward and backward contexts, respectively. The forward and backward bwd are learned with context representations cfwd t , ct separate InfoNCE losses. When they are used for downstream tasks, a concatenation of two reprebwd sentations ct = [cfwd t ; ct ] is used. A similar technique has been used in image representation learning where represen"
2020.findings-emnlp.106,L16-1611,0,0.0209422,"h recognition systems on that task (Kuchaiev et al., 2018). For the other datasets, transcriptions are lowercased 1185 and unpronounced symbols (e.g., punctuation, silence markers) are removed. We also remove utterances containing numbers as they are transcribed inconsistently across and within datasets. Transcribed multilingual speech In order to evaluate the transferability of the representations, we use speech recognition datasets in 4 African languages collected by the ALFFA project,3 Amharic (Tachbelie et al., 2014), Fongbe (A. A Laleye et al., 2016), Swahili (Gelas et al., 2012), Wolof (Gauthier et al., 2016), for evaluation. These languages have unique phonological properties (e.g. height harmony) and phonetic inventories, making them a good contrast to English. These African languages are low-resource, each with 20 hours or less of transcribed speech. We also use 21 phonetically diverse languages from OpenSLR.4 See Appendix A for more detail. 4.2 Unsupervised Representation Learning We train the model described above (§3.1) using the datasets described in the previous section (§4.1). Similarly to Schneider et al. (2019)), audio signals are randomly cropped with a window size 149,600 observations"
2020.findings-emnlp.106,N18-1202,0,0.0239965,"a different or smaller labeled dataset (Figure 1). 1183 Unsupervised Pre-training with CPC Semi-supervised Speech Recognition T H E _ D O G _ I S _ …. CTC …. ( 100 Hz ) …. : Causal conv …. ( 16kHz / 160 = 100 Hz ) : Causal strided conv …. ( 16kHz ) Pre-trained parameters are ﬁxed. Figure 1: Left, unsupervised representation learning with forward contrastive predictive coding. The learned representations are fixed and used as inputs to a speech recognition model (Right). 3.1 Unsupervised learning with bi-directional CPC Following the success of bidirectional models in representation learning (Peters et al., 2018; Devlin et al., 2019), we extend the original CPC method explained above with bidirectional context networks. The encoder function genc is shared for both directions, but there are two autoregresfwd bwd ) which read encoded sive models (gar and gar observations (z) from the forward and backward contexts, respectively. The forward and backward bwd are learned with context representations cfwd t , ct separate InfoNCE losses. When they are used for downstream tasks, a concatenation of two reprebwd sentations ct = [cfwd t ; ct ] is used. A similar technique has been used in image representation l"
2020.findings-emnlp.106,H92-1073,0,0.63971,"English Datasets. tions. Finally, we include the audio (again ignoring transcriptions) from the standard training splits of the evaluation datasets below. This collection spans a range of recording conditions, noise levels, speaking styles, and languages and amounts to about 8000 hours of audio. Transcribed read English For evaluation, we look at the performance of our representations on a variety of standard English recognition tasks, as well as their ability to be trained on one and tested on another. For read English, we use LibriSpeech (Panayotov et al., 2015) and the Wall Street Journal (Paul and Baker, 1992). Transcribed spoken English To explore more extreme domain shifts, we additionally used conversational speech and public speaking datasets. We used Switchboard (Godfrey et al., 1992), a standard conversational speech recognition dataset consisting of two-sided telephone conversations (test only). Since the data was recorded more than 10 years ago and at a lower sampling rate than the other corpora, it presents a noisy and challenging recognition problem. Finally, we also use the Tedlium-3 (Hernandez et al., 2018) corpus, a large spoken English dataset containing 450 hours of speech extracted"
2020.findings-emnlp.106,W18-2507,0,\N,Missing
2020.tacl-1.23,D18-1045,0,0.339892,"age model X is trained on dataset Y. A bigger language model improves the doc-reranker but does not help the sent-reranker. Architecture Data PPL transformer-XL transformer-XL NIST sent NIST + GW sent 83.3 96.5 LSTM transformer-XL transformer-XL NIST doc NIST doc NIST + GW doc 71.6 43.8 43.4 Table 3: Perplexity per word of language models on NIST dev set. GW refers to Gigaword. Figure 3: Effect of n-best list. about the reliability of using BLEU at assessing cross-sentential consistency (Voita et al., 2019b). To compare the effectiveness of leveraging monolingual data between backtranslation (Edunov et al., 2018; Sennrich et al., 2016a) and our model, we train the document transformer (Zhang et al., 2018) using additional synthetic parallel documents generated by backtranslation (q ′ ). For fair comparison we use the same monolingual data for both models. As shown in Table 1, although both techniques improve translation, backtranslation is less effective than our model. Because we have a new model q ′ , we can use it as a proposal model for our doc-reranker—effectively using the monolingual data twice. We find that this improves results even further, indicating that the effect of both approaches is a"
2020.tacl-1.23,P19-1019,0,0.0128348,"ed must be carefully selected; the ratio of backtranslated data and original data must be balanced carefully. While techniques for doing this are fairly well established for single sentence models, no such established techniques exist for documents. More generally, strategies for using monolingual data in nueral MT systems is an active research area (G¨ulc¸ehre et al., 2015; Cheng et al., 2016, inter alia). Backtranslation (Edunov et al., 2018; Sennrich et al., 2016a), originally invented for semi-supervised MT, has been used as a standard approach for unsupervised MT (Lample et al., 2018a,b; Artetxe et al., 2019, 2018). Noisy channel decompositions have been a standard approach in statistical machine translation (Brown et al., 1993; Koehn et al., 2007) and recently have been applied to neural models (Yu et al., 2017; Yee et al., 2019; A Appendix Ng et al., 2019). Unlike prior work, we adopt noisy channel models for document-level MT. While the model from Yu et al. (2017) could be used on documents by concatenating their sentences to form a single long sequence, this would not let us use the conditional sentence independence assumptions that gives our model the flexibility to use just parallel sentenc"
2020.tacl-1.23,J93-1004,0,0.777583,"Description We define x = (x1 , x2 , . . . , xI ) as the source document with I sentences, and similarly, y = (y1 , y 2 , . . . , y J ) as the target document with J sentences. During the (human) translation process, translators may split or recombine sentences, but we will assume that I = J .1 Let xi = (xi1 , xi2 , . . . , xiM ) represent the ith sentence in the document, consisting of M words; likewise y i = i (y1i , y2i , . . . , yN ) denote the ith sentence in the target document, containing N words. 1 Size mismatches are addressed by merging sentences using sentence alignment algorithms (Gale and Church, 1993). The translation of a document x is determined ˆ , where p(ˆ y |x) is by finding the document y optimal. ˆ = arg max p(y |x). y (1) y Instead of modeling the probability p(y |x) directly, we factorize it using Bayes’ rule: ˆ = arg max y y p( x |y ) × p( y ) p(x) = arg max p(x |y ) × y |{z } channel model p( y ) |{z} . (2) language model We further assume that sentences are independently translated, and that the sentences are generated by a left-to-right factorization according to the chain rule. Therefore, we have ˆ ≈ arg max y y |x| Y Figure 1: Graphical model showing the factorization of ou"
2020.tacl-1.23,D18-1549,0,0.0430969,"Missing"
2020.tacl-1.23,N18-1118,0,0.420146,"lled a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities challenging. More subtly, documents are considerably more diverse than sentences, and models must be carefully biased so as not to pick up spurious correlations. Our Bayes’ rule decomposition (§2) permits several innovations that enable us to solve these problems. Rather than directly modeling the conditional distribution, we re"
2020.tacl-1.23,J93-2003,0,0.192577,"Missing"
2020.tacl-1.23,P18-1118,0,0.0611387,"guage understanding, and semi-supervised machine translation. Recent studies (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia) 355 have shown that exploiting document-level context improves translation performance, and in particular improves lexical consistency and coherence of the translated text. Existing work in the area of context-aware NMT typically adapts the MT system to take additional context as input, either a few previous sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Werlen et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for exploiting monolingual documentlevel data have been explored in two recent studies (Voita"
2020.tacl-1.23,P16-1185,0,0.0212721,"that 1) the language model is portable across domain and language pairs; 2) our model involves straightforward training procedures. Specifically, for backtranslation to succeed, monolingual data that will be back-translated must be carefully selected; the ratio of backtranslated data and original data must be balanced carefully. While techniques for doing this are fairly well established for single sentence models, no such established techniques exist for documents. More generally, strategies for using monolingual data in nueral MT systems is an active research area (G¨ulc¸ehre et al., 2015; Cheng et al., 2016, inter alia). Backtranslation (Edunov et al., 2018; Sennrich et al., 2016a), originally invented for semi-supervised MT, has been used as a standard approach for unsupervised MT (Lample et al., 2018a,b; Artetxe et al., 2019, 2018). Noisy channel decompositions have been a standard approach in statistical machine translation (Brown et al., 1993; Koehn et al., 2007) and recently have been applied to neural models (Yu et al., 2017; Yee et al., 2019; A Appendix Ng et al., 2019). Unlike prior work, we adopt noisy channel models for document-level MT. While the model from Yu et al. (2017) could be"
2020.tacl-1.23,N19-1213,0,0.100327,"Missing"
2020.tacl-1.23,W19-5321,0,0.0352241,"2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for exploiting monolingual documentlevel data have been explored in two recent studies (Voita et al., 2019a; Junczys-Dowmunt, 2019). Both use backtranslation (Edunov et al., 2018; Sennrich et al., 2016a) to create synthetic parallel documents as additional training data. In contrast, we train a large-scale language model and use it to refine the consistency between sentences under a noisy channel framework. Advantages of our model over back-translation are that 1) the language model is portable across domain and language pairs; 2) our model involves straightforward training procedures. Specifically, for backtranslation to succeed, monolingual data that will be back-translated must be carefully selected; the ratio of backt"
2020.tacl-1.23,D16-1139,0,0.0241688,"y 2.5 BLEU. 353 The two best systems submitted to the WMT19 Chinese–English translation task are Microsoft Research Asia’s system (Xia et al., 2019) and Baidu’s system (Sun et al., 2019), both of which use multiple techniques to improve upon the transformer big model. Here, we mainly compare our results with those from Xia et al. (2019) because we use the same evaluation metric SacreBLEU (Post, 2018) and the same validation and test sets. Using extra parallel training data and the techniques of masked sequence-to-sequence pretraining (Song et al., 2019), sequence-level knowledge distillation (Kim and Rush, 2016), and backtranslation (Edunov et al., 2018), the best model from Xia et al. (2019) achieves 30.8, 30.9, and 39.3 on newstest2017, newstest2018, and newstest2019, respectively. Although our best results are lower than this, it is notable that our model achieves comparable results to their model, which was trained on 56M sentences of parallel data—over two times more training data than we use. However, our method is orthogonal to these works and can be combined with other techniques to make further improvement. 5 Analysis In this section, we present the quantitative and qualitative analysis of o"
2020.tacl-1.23,N19-1423,0,0.142685,"oss-sentence context in the language model, and it outperforms existing document translation approaches. 1 Introduction There have been many recent demonstrations that neural language models based on transformers (Vaswani et al., 2017; Dai et al., 2019) are capable of learning to generate remarkably coherent documents with few (Zellers et al., 2019) or no (Radford et al., 2019) conditioning variables. Despite this apparent generation ability, in practical applications, unconditional language models are most often used to provide representations for natural language understanding applications (Devlin et al., 2019; Yang et al., 2019; Peters 346 Transactions of the Association for Computational Linguistics, vol. 8, pp. 346–360, 2020. https://doi.org/10.1162/tacl a 00319 Action Editor: David Chiang. Submission batch: 12/2019; Revision batch: 2/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. conditional distribution to learning two different distributions: a language model p(y ), which provides unconditional estimates of the output (in this paper, documents); and p(x |y ), which provides the probability of translating a candidate output y in"
2020.tacl-1.23,P07-2045,1,0.0265342,"log q (y i |x)+ log pLM (y i |y <i )+ λ2 log pTM (xi |y i ) + λ3 |y i |+ O(x, y <i−1 , y i−1 ), (4) where |y |denotes the number of tokens in the sentence y , and where the base case O(x, y <0 , y 0 ) = 0. Note that Eq. 4 is a generalization of Eq. 3 in log space—if we set λ1 = λ3 = 0 and λ2 = 1 and take the log of Equation 3 the two objectives are equivalent. The extra factors—the proposal probability and the length of the output—provide improvements (e.g., by calibrating the expected length of the output), and can be incorporated at no cost in the model; they are widely used in prior work (Koehn et al., 2007; Yu et al., 2017; Yee 2 Our proposal model can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As"
2020.tacl-1.23,N18-1202,0,0.0527799,"Missing"
2020.tacl-1.23,W18-6319,0,0.0719972,"s, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the standard parallel training data, and compare our model with a strong transformer baseline, as well as related models from prior work."
2020.tacl-1.23,P16-1009,0,0.666657,"l can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the"
2020.tacl-1.23,P16-1162,0,0.850069,"l can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the"
2020.tacl-1.23,N19-1313,0,0.376484,"bles, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our model: We simply optimize the channel mod"
2020.tacl-1.23,2006.amta-papers.25,0,0.0900797,"Missing"
2020.tacl-1.23,W19-5333,0,0.267637,"onal task: machine translation. The application of Bayes’ rule to transform the translation modeling problem p(y |x), where y is the target language, and x is the source language, has a long tradition and was the dominant paradigm in speech and language processing for many years (Brown et al., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities chall"
2020.tacl-1.23,W19-5341,0,0.0121626,"spensable for the doc-reranker, indicating that Bayes’ rule provides reliable estimates of translation probabilities. Table 5 presents the results of our model together with baselines on the WMT19 Chinese– English translation task. We find that the results follow the same pattern as those on NIST: A better language model leads to better translation results and overall the reranker outperforms the transformer-big by approximately 2.5 BLEU. 353 The two best systems submitted to the WMT19 Chinese–English translation task are Microsoft Research Asia’s system (Xia et al., 2019) and Baidu’s system (Sun et al., 2019), both of which use multiple techniques to improve upon the transformer big model. Here, we mainly compare our results with those from Xia et al. (2019) because we use the same evaluation metric SacreBLEU (Post, 2018) and the same validation and test sets. Using extra parallel training data and the techniques of masked sequence-to-sequence pretraining (Song et al., 2019), sequence-level knowledge distillation (Kim and Rush, 2016), and backtranslation (Edunov et al., 2018), the best model from Xia et al. (2019) achieves 30.8, 30.9, and 39.3 on newstest2017, newstest2018, and newstest2019, respe"
2020.tacl-1.23,W17-4811,0,0.298707,"., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities challenging. More subtly, documents are considerably more diverse than sentences, and models must be carefully biased so as not to pick up spurious correlations. Our Bayes’ rule decomposition (§2) permits several innovations that enable us to solve these problems. Rather than directly modeling the conditiona"
2020.tacl-1.23,Q18-1029,0,0.395626,"h closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of documents, the task of translating Chinese into English poses some particularly acute challenges. As Chinese makes fewer inflectional distinctions than English does, and the relevant clues for predicting, for example, what tense an English verb should be in, or whether an English noun should have singular or plural morphology, may be spread throughout a document, it is crucial that extra-sentential context is used. Our experiments (§4) explor"
2020.tacl-1.23,D19-1571,0,0.48966,"improve a conditional task: machine translation. The application of Bayes’ rule to transform the translation modeling problem p(y |x), where y is the target language, and x is the source language, has a long tradition and was the dominant paradigm in speech and language processing for many years (Brown et al., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation p"
2020.tacl-1.23,D19-1081,0,0.749257,"i ’s. By conditioning on the child variables, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our"
2020.tacl-1.23,P19-1116,0,0.456022,"i ’s. By conditioning on the child variables, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our"
2020.tacl-1.23,D16-1138,1,0.769514,"r reverse translation model treats sentences independently. The search is guided by a proposal distribution that provides candidate continuations of a document prefix, and these are reranked according to the posterior distribution. In particular, we compare two proposal models: one based on estimates of independent sentence translations (Vaswani et al., 2017) and one that conditions on the source document context (Zhang et al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of d"
2020.tacl-1.23,P18-1117,0,0.0741973,"arch: context-aware neural machine translation, large-scale language models for language understanding, and semi-supervised machine translation. Recent studies (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia) 355 have shown that exploiting document-level context improves translation performance, and in particular improves lexical consistency and coherence of the translated text. Existing work in the area of context-aware NMT typically adapts the MT system to take additional context as input, either a few previous sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Werlen et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for ex"
2020.tacl-1.23,D17-1301,0,0.528329,"e-translation-evaluation. 5 http://www.statmt.org/wmt19/translationtask.html. 350 For WMT19, we use the transformer as both the channel and proposal model. The hyperparameters for training the transformer is the same as transformer big (Vaswani et al., 2017), namely, 1,024 hidden size, 4,096 filter size, 16 attention heads, and 6 layers. The model is trained on 8 GPUs with batch size of 4,096. The setup for the language model is the same as that of NIST except that the training data is the English side of the parallel training data and Gigaword. Method Model Proposal MT06 MT03 MT04 MT05 MT08 (Wang et al., 2017) (Kuang et al., 2017) (Zhang et al., 2018) RNNsearch Transformer + cache Doc-transformer – – – 37.76 48.14 49.69 – 48.05 50.21 – 47.91 49.73 36.89 48.53 49.46 27.57 38.38 39.69 Baseline Sent-transformer Doc-transformer (q ) Backtranslation (q ′ ) Sent-reranker – – – q 47.72 49.79 50.77 51.33 47.21 49.29 51.80 52.23 49.08 50.17 51.61 52.36 46.86 48.99 51.81 51.63 40.18 41.70 42.47 43.63 Doc-reranker Doc-reranker q q′ 51.99 53.63 52.77 54.51 52.84 54.23 51.84 54.86 44.17 45.17 This work Table 1: Comparison with prior work on NIST Chinese–English translation task. The evaluation metric is tokeniz"
2020.tacl-1.23,D18-1325,0,0.330585,"t al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of documents, the task of translating Chinese into English poses some particularly acute challenges. As Chinese makes fewer inflectional distinctions than English does, and the relevant clues for predicting, for example, what tense an English verb should be in, or whether an English noun should have singular or plural morphology, may be spread throughout a document, it is crucial that extra-sentential context is used. Our experi"
2020.tacl-1.23,K19-1074,0,0.0859984,"Missing"
2020.tacl-1.23,D18-1049,0,0.0540894,"uch harder decoding problem (§3). To address this problem, we propose a new beam-search algorithm, exploiting the fact that our document language model operates left-to-right, and our reverse translation model treats sentences independently. The search is guided by a proposal distribution that provides candidate continuations of a document prefix, and these are reranked according to the posterior distribution. In particular, we compare two proposal models: one based on estimates of independent sentence translations (Vaswani et al., 2017) and one that conditions on the source document context (Zhang et al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al"
2020.tacl-1.50,P16-1231,0,0.0149245,"This non-incremental procedure is justified, however, because we aim to design the most informative teacher distributions for the nonincremental BERT student, which also has access to bidirectional context. i∈M (x) w ∈Σ i log pθ (˜ xi = w|c(x1 ), · · · , c(xk )) , 9 780 We use the same setup as Preliminary Experiments. where t˜φ,ω (w|x<i , x>i ) is our approximation of tφ (w|x<i , x>i ), as defined in Eqs. 5 and 6. teacher that we distill into BERT also uses phrasestructure trees, this setup is related to self-training (Yarowsky, 1995; Charniak, 1997; Zhou and Li, 2005; McClosky et al., 2006; Andor et al., 2016, inter alia). Interpolation. The RNNG teacher is an expert on syntax, although in practice it is only feasible to train it on a much smaller dataset. Hence, we not only want the BERT student to learn from the RNNG’s syntactic expertise, but also from the rich common-sense and semantics knowledge contained in large text corpora by virtue of predicting the true identity of the masked token xi ,10 as done in the standard BERT setup. We thus interpolate the KD loss and the original BERT masked LM objective: Phrase-structure Parsing - OOD. Still in the context of phrase-structure parsing, we evalu"
2020.tacl-1.50,N19-1112,0,0.367367,"tillation Pretraining for Bidirectional Encoders Adhiguna Kuncoro∗♠♦ Lingpeng Kong∗♠ Daniel Fried∗♣ Dani Yogatama♠ Laura Rimell♠ Chris Dyer♠ Phil Blunsom♠♦ ♠ DeepMind, London, UK Department of Computer Science, University of Oxford, UK ♣ Computer Science Division, University of California, Berkeley, CA, USA {akuncoro,lingpenk,dyogatama,laurarimell,cdyer,pblunsom}@google.com dfried@cs.berkeley.edu ♦ Abstract dels have also been shown to perform remarkably well at syntactic grammaticality judgment tasks (Goldberg, 2019), and encode substantial amounts of syntax in their learned representations (Liu et al., 2019a; Tenney et al., 2019a,b; Hewitt and Manning, 2019; Jawahar et al., 2019). Intriguingly, success on these syntactic tasks has been achieved by Transformer architectures (Vaswani et al., 2017) that lack explicit notions of hierarchical syntactic structures. Based on such evidence, it would be tempting to conclude that data scale alone is all we need to learn the syntax of natural language. Nevertheless, recent findings that systematically compare the syntactic competence of models trained at varying data scales suggest that model inductive biases are in fact more important than data scale for"
2020.tacl-1.50,E17-1117,1,0.89748,"Missing"
2020.tacl-1.50,P19-1334,0,0.0254666,"our UG-KD model outperforms the baseline on CoLA, but performs slightly worse on the other GLUE tasks in aggregate, leading to a slightly lower overall test set accuracy (80.0 for the UG-KD as opposed to 80.3 for the No-KD baseline). The improvement on the syntax-sensitive CoLA provides additional evidence—beyond the improvement on the syntactic tasks (Table 2)— that our approach indeed yields improved syntactic competence. We conjecture that these improvements do not transfer to the other GLUE tasks because they rely more on lexical and semantic properties, and less on syntactic competence (McCoy et al., 2019). We defer a more thorough investigation of how much syntactic competence is necessary for solving most of the GLUE tasks to future work, but make two remarks. First, the findings on GLUE are consistent with the hypothesis that our approach yields improved structural competence, albeit at the expense of a slightly less rich meaning representation, which we attribute to the smaller dataset used to train the RNNG teacher. Second, 785 human-level natural language understanding includes the ability to predict structured outputs, for example, to decipher ‘‘who did what to whom’’ (SRL). Succeeding i"
2020.tacl-1.50,W12-4501,0,0.0208281,"Missing"
2020.tacl-1.50,P16-1162,0,0.00712367,"composition function,1 which recursively combines smaller units into larger ones. RNNGs attempt to maximize the probability of correct action sequences relative to each gold tree.2 1 Not all syntactic LMs have hierarchical biases; Choe and Charniak (2016) modeled strings and phrase structures sequentially with LSTMs. This model can be understood as a special case of RNNGs without the composition function. 2 Unsupervised RNNGs (Kim et al., 2019) exist, although they perform worse on measures of syntactic competence. Extension to Subwords. Here we extend the RNNG to operate over subword units (Sennrich et al., 2016) to enable compatibility with the BERT student. As each word can be split into an arbitrary-length sequence of subwords, we preprocess the phrase-structure trees to include an additional nonterminal symbol that represents a word sequence, as illustrated by the example ‘‘(S (NP (WORD the) (WORD d ##og)) (VP (WORD ba ##rk ##s)))’’, where tokens prefixed by ‘‘##’’ are subword units.3 Figure 1: An example of the masked LM task, where [MASK] = chase, and window is an attractor (red). We suppress phrase-structure annotations and corruptions on the context tokens for clarity. 3 Approach We begin with"
2020.tacl-1.50,D18-1412,1,0.829908,"ammatical judgment; our work represents a step towards answering this question. Substantial progress has recently been made in improving the performance of BERT and other masked LMs (Lan et al., 2020; Liu et al., 2019b; Raffel et al., 2019; Sun et al., 2020, inter alia). Our structure distillation technique is orthogonal, and can be applied for these approaches. Lastly, our findings on the benefits of syntactic knowledge for structured prediction tasks that are not explicitly syntactic in nature, such as SRL and coreference resolution, are consistent with those of prior work (He et al., 2017; Swayamdipta et al., 2018; He et al., 2018; Strubell et al., 2018, inter alia). 6 Conclusion the BERT student is a bidirectional model that estimates the conditional probabilities of masked words in context, we propose to distill an efficient yet surprisingly effective approximation of the RNNG’s posterior estimate for generating each word conditional on its bidirectional context. Our findings suggest that syntactic inductive biases are beneficial for a diverse range of structured prediction tasks, including for tasks that are not explicitly syntactic in nature. In addition, these biases are particularly helpful for i"
2020.tacl-1.50,N19-1334,0,0.102096,"Missing"
2020.tacl-1.50,P95-1026,0,0.377119,"eparate discriminative parser, which has access to yet unseen words x>i . This non-incremental procedure is justified, however, because we aim to design the most informative teacher distributions for the nonincremental BERT student, which also has access to bidirectional context. i∈M (x) w ∈Σ i log pθ (˜ xi = w|c(x1 ), · · · , c(xk )) , 9 780 We use the same setup as Preliminary Experiments. where t˜φ,ω (w|x<i , x>i ) is our approximation of tφ (w|x<i , x>i ), as defined in Eqs. 5 and 6. teacher that we distill into BERT also uses phrasestructure trees, this setup is related to self-training (Yarowsky, 1995; Charniak, 1997; Zhou and Li, 2005; McClosky et al., 2006; Andor et al., 2016, inter alia). Interpolation. The RNNG teacher is an expert on syntax, although in practice it is only feasible to train it on a much smaller dataset. Hence, we not only want the BERT student to learn from the RNNG’s syntactic expertise, but also from the rich common-sense and semantics knowledge contained in large text corpora by virtue of predicting the true identity of the masked token xi ,10 as done in the standard BERT setup. We thus interpolate the KD loss and the original BERT masked LM objective: Phrase-struc"
2020.tacl-1.50,P19-1452,0,0.0849005,"ntation learners that work well at scale still benefit from explicit syntactic biases? And where exactly would such syntactic biases be helpful in different language understanding tasks? Here we work towards answering these questions by devising a new pretraining strategy that injects syntactic biases into a BERT (Devlin et al., 2019) learner that works well at scale. We hypothesize that this approach can improve the competence of BERT on various tasks, which provides evidence for the benefits of syntactic biases in large-scale models. Our approach is based on the prior work of Kuncoro et al. (2019), who devised an effective knowledge distillation (KD; Bucilˇa et al., 2006; Hinton et al., 2015) procedure for improving the syntactic competence of scalable LMs that lack explicit syntactic biases. More concretely, their KD procedure utilized the predictions of an explicitly hierarchical (albeit hard to scale) syntactic LM, recurrent neural network grammars Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open questi"
2020.tacl-1.50,P19-1230,0,0.0386635,", and the English Web Treebank (Petrov and McDonald, 2012). Following Fried et al. (2019), we test the PTB-trained parser on the test splits11 of these OOD treebanks without any retraining, to simulate the case where no in-domain labeled data are available. We use the same codebase as above. 1 Xh θˆB-KD = arg min αℓKD (x; θ ) + (1 − α) |D | θ x∈D i X − log pθ (xi |c(x1 ), · · · , c(xk )) , Dependency Parsing - PTB. Our third task is PTB dependency parsing with Stanford Dependencies (De Marneffe and Manning, 2008) v3.3.0. We use the BERT-augmented joint phrasestructure and dependency parser of Zhou and Zhao (2019), which is inspired by head-driven phrase-structure grammar (HPSG; Pollard and Sag, 1994). i∈M (x) (7) omitting the next-sentence prediction for brevity. We henceforth set α = 0.5 unless stated otherwise. 4 Experiments Semantic Role Labeling. Our fourth evaluation task is span-based (SRL) on the English CoNLL 2012 (OntoNotes) dataset (Pradhan et al., 2013). We apply our approach on top of the BERTaugmented model of Shi and Lin (2019), as implemented on AllenNLP (Gardner et al., 2018). Here we outline the evaluation setup, present our results, and discuss the implications of our findings. 4.1 E"
2020.wmt-1.36,P19-1425,0,0.021074,"ranslation quality as shown by existing work (Sun et al., 2019; Ng et al., 2019). After training the proposal models with the mix of real and synthetic parallel data, we fine-tuned the models with CWMT and a subset of newstest2017 and newstest2018 which were not used for validation. 4.4 Improving Uncertainty Estimation To improve the robustness of noisy channel reranking, we explore two approaches for improving uncertainty estimation of the seq2seq scoring models. 4.4.1 Adversarially Trained Proposal Models To simulate different wordings and noises in source and candidate sentences, we follow Cheng et al. (2019) to train the models on noisy adversarial inputs and targets. We use bidirectional languagemodels to provide the noisy candidates and select the candidates with highest loss (i.e., adversarial source-target inputs). During the training, we optimize the original loss with clean source-target pairs, the language model losses for source and target sides, and the adversarial loss using adversarial source-target inputs. In the final scoring, we use an ensemble of eight adversarially trained models with few differences from Cheng et al. (2019): (a) We explore training with and without the language m"
2020.wmt-1.36,P19-1285,0,0.0283336,"dunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) and optimized with distillation and fine-tuning with in-domain data (§5). During development, we observed weaknesses in our system’s translations for long sentences, largely due to premature truncations. We developed several techniques to mitigate this issue such as sentence segmentation (breaking sentences into logical complete segments) and training specialized models with synthetically constructed long sequences to generate additional proposals for our reranker (§A). Experiments show that the aforementioned techniques are very effective: our system outperforms the Transformer baseline by 9"
2020.wmt-1.36,D18-1045,0,0.17332,"and optimized independently while at inference time, when we reason over the posterior distribution of translations given the source document, conditional dependencies between translations are induced by the language model prior. The core of our document-level translation architecture is the noisy channel reranker. It requires proposal, channel, and language models, each of which is optimized separately using different techniques and approaches. For the proposal and channel models we use Transformer models (Vaswani et al., 2017) (§4.1) with data augmentation (§4.2), such as back translation (Edunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) a"
2020.wmt-1.36,D11-1125,0,0.118354,"Missing"
2020.wmt-1.36,D16-1139,0,0.184286,"t inference time, when we reason over the posterior distribution of translations given the source document, conditional dependencies between translations are induced by the language model prior. The core of our document-level translation architecture is the noisy channel reranker. It requires proposal, channel, and language models, each of which is optimized separately using different techniques and approaches. For the proposal and channel models we use Transformer models (Vaswani et al., 2017) (§4.1) with data augmentation (§4.2), such as back translation (Edunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) and optimized with distillation and"
2020.wmt-1.36,P18-1007,0,0.0261476,"BLEU, such that: a sentence were replaced with the capitalized variant that occurred most frequently in other positions of the English monolingual training data. Thus, in the previous sentence the initial token would have been words rather than Words. (1 − META)4 = TER× (1 − BLEU)× Subword units To encode text into sub-word units, we used the sentencepiece tool (Kudo and Richardson, 2018). For seq2seq models (i.e., the channel model and proposal models), we trained the segmentation model on the first 10 million sentences of the parallel training corpus,3 using joint source and target unigram (Kudo, 2018) subword segmentation algorithm with a target vocabulary of 32K tokens and minimum character coverage of 0.9995, which resulted in 32,768 word pieces.4 For the language model, we used the English side alone with the same vocabulary size and a character coverage of 1.0. (1 − METEOR)× (1 − q0.1 (BLEU)). (4) When several configurations of hyperparameters achieve values of META very close to the maximum (within 0.02), we pick the one maximizing BLEU and/or minimizing the L2 norm of the λs, considered as a vector. This corresponds to an intuitive prior towards giving more weight to the language mod"
2021.acl-long.104,P18-1118,0,0.100429,"). While noteworthy progress has been made at modeling monolingual documents (Brown et al., 2020), progress on document translation has been less remarkable, and continues to be hampered by the limited quantities of parallel document data relative to the massive quantities of monolingual document data. One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018; Haffari and Maruf, 2018). This two-stage training approach provides an inductive bias that encourages the learner to explain translation decisions preferentially in terms of the current sentence being translated, but these can be modulated at the margins by using document context. However, a weakness of this approach is that the conditional dependence of a translation on its surrounding context given the source sentence is weak, and learning good context representations purely on the basis of scarce parallel document data is challenging. A recent strategy for making better use of document context in translation is to"
2021.acl-long.104,N18-1118,0,0.0197288,".74 29.62 29.60 29.78 29.89 Table 7: Results from using a three staged training approach. When there is large disparity between the amount of sentence pair data and document data, this method enables training new attention blocks with the maximum amount of available data given their input restrictions. Scherrer, 2017). Follow-up work adds additional context layers to the neural sequence-to-sequence models in order to have a better encoding of the context information (Zhang et al., 2018; Miculicich et al., 2018, inter alia). They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al., 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has als"
2021.acl-long.104,2020.wmt-1.71,0,0.0613691,"Missing"
2021.acl-long.104,P18-1008,0,0.0218183,"crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context. Our best multicontext model consistently outperforms the best existing context-aware transformers. 1 Introduction Generating an adequate translation for a sentence often requires understanding the context in which the sentence occurs (and in which its translation will occur). Although single-sentence translation models demonstrate remarkable performance (Chen et al., 2018; Vaswani et al., 2017; Bahdanau et al., 2015), extra-sentential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation. A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context (Zhang et al., 2018; Miculicich et al., 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly condition"
2021.acl-long.104,N19-1213,0,0.0243866,"models. Voita et al. (2019) train a post-editing model from monolingual documents to post-edit sentence-level translations into document-level translations. Yu et al. (2020b,a) uses Bayes’ rule to combine a monolingual document language model probability with sentence translation probabilities. Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020). They have also been used to improve text generation tasks, such as sentence-level machine translation (Song et al., 2019; Edunov et al., 2019; Zhu et al., 2020) and summarization (Zhang et al., 2019, 2020; Dong et al., 2019), and repurposing unconditional language generation (Ziegler et al., 2019; de Oliveira and Rodrigo, 2019). Our work is closely related to that from Zhu et al. (2020), where pretrained largescale language models are applied to documentlevel machine translation tasks. We advance this line of reasoning by designing an architectu"
2021.acl-long.104,N19-1423,0,0.128909,"ion block which learns to perform the adaptation. The independence between the models means that different input data can be provided to each, which enables extra information during the translation process. In this work, we leverage this technique to: (1) enhance a sentence-level model with additional source embeddings; (2) convert a sentence-level model to a document-level model by providing contextual embeddings. Like BERTfused (Zhu et al., 2020), we use pretrained masked language models to generate the external embeddings. 2.1 Pre-Trained Models We use two kinds of pretrained models: BERT (Devlin et al., 2019) and PEGASUS (Zhang et al., 2020). Although similar in architecture, we conjecture that these models will capture different signals on account of their different training objectives. BERT is trained with a masked word objective and a two sentence similarity classification task. During training, it is provided with two sentences that may or may not be adjacent, with some of their words masked or corrupted. BERT predicts the correct words and determining if the two sentences form a contiguous sequence. Intuitively, BERT provides rich word-in-context embeddings. In terms of machine translation, i"
2021.acl-long.104,N19-1409,0,0.0201494,"o combine a monolingual document language model probability with sentence translation probabilities. Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020). They have also been used to improve text generation tasks, such as sentence-level machine translation (Song et al., 2019; Edunov et al., 2019; Zhu et al., 2020) and summarization (Zhang et al., 2019, 2020; Dong et al., 2019), and repurposing unconditional language generation (Ziegler et al., 2019; de Oliveira and Rodrigo, 2019). Our work is closely related to that from Zhu et al. (2020), where pretrained largescale language models are applied to documentlevel machine translation tasks. We advance this line of reasoning by designing an architecture that uses composition to incorporate multiple pretrained models at once. It also enables conditioning on different inputs to the same pretrained model, enabling us to circumvent BERT’s tw"
2021.acl-long.104,D18-1045,0,0.02476,"l., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has also been work on leveraging monolingual documents to improve document-level machine translation. Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation (Sennrich et al., 2016; Edunov et al., 2018) and uses the combination of the original and the synthetic parallel documents to train the document translation models. Voita et al. (2019) train a post-editing model from monolingual documents to post-edit sentence-level translations into document-level translations. Yu et al. (2020b,a) uses Bayes’ rule to combine a monolingual document language model probability with sentence translation probabilities. Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of"
2021.acl-long.104,D19-6503,0,0.0360126,"Missing"
2021.acl-long.104,P18-1007,0,0.0607903,"Missing"
2021.acl-long.104,D18-2012,0,0.0512731,"Missing"
2021.acl-long.104,2020.acl-main.322,0,0.0314922,"ST validation set with respect to the number of contextualized sentences used for training. We can see that it requires an example pool size over 300K before these models outperform the baseline. We conjecture that sufficient contextualized sentence pairs are crucial for document-level models to achieve good performance, which would explain why these models don’t perform well on the IWSLT’14 and WMT’14 datasets. Further, this pattern of results helps shed light on the inconsistent findings in the literature regarding the effectiveness of document context models. A few works (Kim et al., 2019; Li et al., 2020; Lopes et al., 2020) have found that the benefit provided by many document context models can be explained away by factors other than contextual conditioning. We can now see from Figure 2 that these experiments were done in the low data regime. The randomly initialized context model needs around 600K Document Data Augmentation We further validate our hypothesis about the importance of sufficient contextualized data by experimenting with document data augmentation, this time drawing data from different domains. We augment the IWSLT dataset with news commentary v15, an additional 345K document"
2021.acl-long.104,2020.eamt-1.24,0,0.0131744,"document context contribute to performance. Adding additional parameters also helps but we only see this when going from one to two blocks. Parameter control experiments are shown in light grey. this section, we investigate to what extent the quantities of parallel documents affect the performance of our models. To do so, we retrain enhanced models with subsets of the NIST training dataset. It is important to note that the underlying sentence transformer model was not retrained in these experiments meaning that these experiments simulate adding document context to a strong baseline as done in Lopes et al. (2020). Figure 2 shows the BLEU scores of different models on the NIST validation set with respect to the number of contextualized sentences used for training. We can see that it requires an example pool size over 300K before these models outperform the baseline. We conjecture that sufficient contextualized sentence pairs are crucial for document-level models to achieve good performance, which would explain why these models don’t perform well on the IWSLT’14 and WMT’14 datasets. Further, this pattern of results helps shed light on the inconsistent findings in the literature regarding the effectivene"
2021.acl-long.104,N19-1313,0,0.075815,"target side document context using Bayes’ rule (Yu et al., 2020b). While noteworthy progress has been made at modeling monolingual documents (Brown et al., 2020), progress on document translation has been less remarkable, and continues to be hampered by the limited quantities of parallel document data relative to the massive quantities of monolingual document data. One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018; Haffari and Maruf, 2018). This two-stage training approach provides an inductive bias that encourages the learner to explain translation decisions preferentially in terms of the current sentence being translated, but these can be modulated at the margins by using document context. However, a weakness of this approach is that the conditional dependence of a translation on its surrounding context given the source sentence is weak, and learning good context representations purely on the basis of scarce parallel document data is challenging. A recent"
2021.acl-long.104,D18-1325,0,0.0941945,"translation will occur). Although single-sentence translation models demonstrate remarkable performance (Chen et al., 2018; Vaswani et al., 2017; Bahdanau et al., 2015), extra-sentential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation. A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context (Zhang et al., 2018; Miculicich et al., 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al., 2020b). While noteworthy progress has been made at modeling monolingual documents (Brown et al., 2020), progress on document translation has been less remarkable, and continues to be hampered by the limited quantities of parallel document data relative to the massive quantities of monolingual document data. One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level se"
2021.acl-long.104,N18-1202,0,0.0320115,"s the combination of the original and the synthetic parallel documents to train the document translation models. Voita et al. (2019) train a post-editing model from monolingual documents to post-edit sentence-level translations into document-level translations. Yu et al. (2020b,a) uses Bayes’ rule to combine a monolingual document language model probability with sentence translation probabilities. Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020). They have also been used to improve text generation tasks, such as sentence-level machine translation (Song et al., 2019; Edunov et al., 2019; Zhu et al., 2020) and summarization (Zhang et al., 2019, 2020; Dong et al., 2019), and repurposing unconditional language generation (Ziegler et al., 2019; de Oliveira and Rodrigo, 2019). Our work is closely related to that from Zhu et al. (2020), where pretrained largescale language models are ap"
2021.acl-long.104,W18-6319,0,0.0122834,"not include a masked word auxiliary objective. We use the public PEGASUS large6 on the English side of WMT’14, for everything else, we use our models. See Appendix B for batch size and compute details. 3.3 Evaluation To reduce the variance of our results and help with reproducibility, we use checkpoint averaging. We select the ten contiguous checkpoints with the highest average validation BLEU. We do this at two critical points: (1) with the transformer models used to bootstrap enhanced models; (2) before calculating the validation and test BLEU scores we report. We use the sacreBLEU script (Post, 2018)7 on our denormalized output to calculate BLEU. 4 https://github.com/google-research/ bert 5 https://www.tensorflow.org/datasets/ catalog/c4#c4multilingual 6 https://github.com/google-research/ pegasus 7 https://github.com/mjpost/sacreBLEU Results In this section, we present our main results and explore the importance of each component in the multi-context model. Additionally, we investigate the performance impact of document-level parallel data scarcity, the value of source-side versus targetside context, and the importance of target context quality. Table 2 compares our Multi-source and Mult"
2021.acl-long.104,2020.tacl-1.23,1,0.761773,"entential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation. A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context (Zhang et al., 2018; Miculicich et al., 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al., 2020b). While noteworthy progress has been made at modeling monolingual documents (Brown et al., 2020), progress on document translation has been less remarkable, and continues to be hampered by the limited quantities of parallel document data relative to the massive quantities of monolingual document data. One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training (Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018;"
2021.acl-long.104,K19-1074,0,0.0570413,"Missing"
2021.acl-long.104,D18-1049,0,0.342034,"s (and in which its translation will occur). Although single-sentence translation models demonstrate remarkable performance (Chen et al., 2018; Vaswani et al., 2017; Bahdanau et al., 2015), extra-sentential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation. A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context (Zhang et al., 2018; Miculicich et al., 2018), operating jointly on multiple sentences at once (Junczys-Dowmunt, 2019), or indirectly conditioning on target side document context using Bayes’ rule (Yu et al., 2020b). While noteworthy progress has been made at modeling monolingual documents (Brown et al., 2020), progress on document translation has been less remarkable, and continues to be hampered by the limited quantities of parallel document data relative to the massive quantities of monolingual document data. One recurring strategy for dealing with this data scarcity—and which is the basis for this work—is to"
2021.acl-long.104,P16-1009,0,0.0384676,"t al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has also been work on leveraging monolingual documents to improve document-level machine translation. Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation (Sennrich et al., 2016; Edunov et al., 2018) and uses the combination of the original and the synthetic parallel documents to train the document translation models. Voita et al. (2019) train a post-editing model from monolingual documents to post-edit sentence-level translations into document-level translations. Yu et al. (2020b,a) uses Bayes’ rule to combine a monolingual document language model probability with sentence translation probabilities. Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art resu"
2021.acl-long.104,W17-4811,0,0.041113,"Missing"
2021.acl-long.104,Q18-1029,0,0.0245218,"When there is large disparity between the amount of sentence pair data and document data, this method enables training new attention blocks with the maximum amount of available data given their input restrictions. Scherrer, 2017). Follow-up work adds additional context layers to the neural sequence-to-sequence models in order to have a better encoding of the context information (Zhang et al., 2018; Miculicich et al., 2018, inter alia). They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al., 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has also been work on leveraging monolingual documents to improve document-level machine trans"
2021.acl-long.104,D19-1081,0,0.0118778,"r to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has also been work on leveraging monolingual documents to improve document-level machine translation. Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation (Sennrich et al., 2016; Edunov et al., 2018) and uses the combination of the original and the synthetic parallel documents to train the document translation models. Voita et al. (2019) train a post-editing model from monolingual documents to post-edit sentence-level translations into document-level translations. Yu et al. (2020b,a) uses Bayes’ rule to combine a monolingual document language model probability with sentence translation probabilities. Finally, large-scale representation learning with language modeling has achieved success in improving systems in language understanding, leading to state-of-the-art results on a wide range of tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Yang et al., 2019; Chronopoulou et al., 2019; L"
2021.acl-long.104,P18-1117,0,0.0186187,"ntion blocks with the maximum amount of available data given their input restrictions. Scherrer, 2017). Follow-up work adds additional context layers to the neural sequence-to-sequence models in order to have a better encoding of the context information (Zhang et al., 2018; Miculicich et al., 2018, inter alia). They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al., 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has also been work on leveraging monolingual documents to improve document-level machine translation. Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation (Sennrich et al., 2016; Edunov e"
2021.acl-long.104,D17-1301,0,0.0251273,"his method enables training new attention blocks with the maximum amount of available data given their input restrictions. Scherrer, 2017). Follow-up work adds additional context layers to the neural sequence-to-sequence models in order to have a better encoding of the context information (Zhang et al., 2018; Miculicich et al., 2018, inter alia). They vary in terms of whether to incorporate the source-side context (Bawden et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or target-side context (Tu et al., 2018), and whether to condition on a few adjacent sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has also been work on leveraging monolingual documents to improve document-level machine translation. Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtransla"
2021.findings-acl.25,J93-2003,0,0.18939,"Missing"
2021.findings-acl.25,W03-0318,0,0.189343,"ly ignore those rare cases at little cost in terms of cross-entropy, they have an out-sized impact on BLEU, and therefore the RL learner is sensitive to them. Finally, it is important to note that while RLS EG - MENT improves BLEU at a corpus level, there exist cases where individual translation examples (Appendix C.3) are worse because of inappropriate segmentation. 5 Related Work The segmentation of long texts and sentences into segments suitable for translation has been a recurring topic in machine translation research (Tien and Minh, 2019; Pouget-Abadie et al., 2014; Goh and Sumita, 2011; Doi and Sumita, 2003); however, we are the first to apply reinforcement learning to solve the problem. A related problem to the segmentation problem occurs in automated simultaneous interpretation, where the system must produce translations as quickly as possible, but it is necessary to wait until sufficient context has been received before an accurate translation can be produced. Grissom II et al. (2014) used an RL approach, targeting a reward that balances translation quality with translation latency. Chinese comma disambiguation has likewise been studied. However, without exception these have sought to predict"
2021.findings-acl.25,J93-1004,0,0.850371,"ther be able to cope with potentially long, multisentence inputs (i.e., translating any text that falls between unambiguous sentence-ending punctuation) or, alternatively, they must be able to determine which comma occurrences terminate complete sentences that can be translated independently Being able to directly accommodate long, multisentence inputs has clear appeal. However, in practice, the training data available for translation models is dominated by the relatively short (sub)sentence pairs that are preferentially recovered by standard approaches to sentence alignment (Tiedemann, 2011; Gale and Church, 1993), and as a result of the natural distribution of sentence lengths. Unfortunately, generalisation from training on short sequences to testing on long sequences continues to be an unsolved problem even in otherwise well-performing translation models (Lake and Baroni, 2018; Koehn and Knowles, 2017). Rather than addressing the length generalisation problem directly, in this paper we side-step it by learning to make decisions about segmentation so as to maximise the performance of an unreliable machine translation system that operates optimally only on shorter segments of input Chinese text. While"
2021.findings-acl.25,D14-1140,0,0.0791713,"Missing"
2021.findings-acl.25,W04-1101,0,0.164803,"blem occurs in automated simultaneous interpretation, where the system must produce translations as quickly as possible, but it is necessary to wait until sufficient context has been received before an accurate translation can be produced. Grissom II et al. (2014) used an RL approach, targeting a reward that balances translation quality with translation latency. Chinese comma disambiguation has likewise been studied. However, without exception these have sought to predict normative notions of what constitutes a complete clause or elementary discourse unit (Xu and Li, 2013; Xue and Yang, 2011; Jin et al., 2004), on the basis of syntactic annotations in the Chinese Treebank (Xue et al., 2005). In contrast, our solution is directly targeted at developing a segmentation strategy that results in a good downstream translation, rather than conforming to any single normative notion of what constitutes a complete sentence. 6 level and by 3 BLEU on a sub-corpus comprising only source sentences longer than 60 words. Acknowledgements We would like to thank Laurent Sartran, Wojciech Stokowiec, Lei Yu and Wang Ling for helpful discussions and guidance during different stages of this work in terms of experimentat"
2021.findings-acl.25,W17-3204,0,0.0554023,"eing able to directly accommodate long, multisentence inputs has clear appeal. However, in practice, the training data available for translation models is dominated by the relatively short (sub)sentence pairs that are preferentially recovered by standard approaches to sentence alignment (Tiedemann, 2011; Gale and Church, 1993), and as a result of the natural distribution of sentence lengths. Unfortunately, generalisation from training on short sequences to testing on long sequences continues to be an unsolved problem even in otherwise well-performing translation models (Lake and Baroni, 2018; Koehn and Knowles, 2017). Rather than addressing the length generalisation problem directly, in this paper we side-step it by learning to make decisions about segmentation so as to maximise the performance of an unreliable machine translation system that operates optimally only on shorter segments of input Chinese text. While numerous text segmentation techniques designed to improve machine translation have been proposed over the years (§5), these have typically 293 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 293–302 August 1–6, 2021. ©2021 Association for Computational Linguisti"
2021.findings-acl.25,N03-1017,0,0.0347505,"ried out, becoming a tripartite cooperation between China, Africa and the United Nations. Figure 1: An example taken from the WMT2020 test set that shows a single source Chinese segment is translated into two separate English sentences. The highlighted comma separates the two corresponding complete sentences in the Chinese text, whereas the other two commas are sentence-internal boundaries. Introduction and which do not. Machine translation systems typically operate on sentence-like units, where sentences are translated independently of each other (Vaswani et al., 2017; Bahdanau et al., 2015; Koehn et al., 2003; Brown et al., 1993), in some cases with additional conditioning on a representation of adjacent sentences to improve coherence (Miculicich et al., 2018; Zhang et al., 2018). While many pairs of languages use similar orthographic conventions to designate sentence boundaries, English and Chinese diverge considerably: complete sentences in Chinese may be terminated either unambiguously with a full stop (。) or ambiguously with a comma. See Figure 1 for an example. This divergence poses a challenge for Chinese– English translation systems since they must either be able to cope with potentially lo"
2021.findings-acl.25,P11-2111,0,0.0942605,"es the example-level BLEU score. This benchmark is the upper limit of any segmentation policy, given a translation model. It is quite expensive to compute since it requires decoding all possible segmentations of every sequence. • O RACLE S UP – Oracle segmentation decisions from the training corpus are used to setup a supervised binary classification task with an architecture similar to RLS EGMENT’s policy network. • C OMMAC LASS – Using the syntactic patterns from the Penn Chinese Treebank 6.0, this system builds a comma classifier to disambiguate terminal and non-terminal commas similar to (Xue and Yang, 2011). This uses a transformer encoder followed by a positional feed-forward network to classify every comma. Model full BLEU BP ≥ 60 BLEU BP N O S PLIT (Baseline) [2000] A LL S PLIT [6233] O RACLE S UP [3107] C OMMAC LASS [2569] H EURISTIC [2071] RLS EGMENT (Ours) [2228] O RACLE [3513] 31.89 29.45 31.57 31.82 32.02 32.21 36.36 25.73 27.88 27.66 26.93 27.05 29.03 34.71 89.42 93.18 90.89 89.98 89.59 91.34 96.19 78.17 94.43 88.02 81.16 80.80 88.09 93.45 Table 1: BLEU and brevity penalty scores, both on the corpus and long sentences only (source length ≥ 60 words) on the test dataset of WMT20 Chinese–"
2021.findings-acl.25,2020.emnlp-main.170,0,0.0112695,"In both the cases, we also report the brevity penalty (BP), a component of BLEU to show the impact on the overall length of the translation. We see that our proposed segmentation policy, RLS EGMENT improves both the BLEU scores and brevity penalties as compared to the baseline translation case N O S PLIT. Specifically, the RL model improves BLEU scores on long sentences by 3+ BLEU points and BP on those sentences by about 9+ points. This shows that our model, via smart segmentation, suffers less because of premature truncation of long translations as compared to the baseline—a common problem (Meister et al., 2020; Koehn and Knowles, 2017). While segmentation of long sentences at appropriate punctuations helps performance, segmentation at all punctuations is expected to hurt performance as it is highly likely to produce extremely small segments which lose a lot of necessary source context when individually translated. This is demonstrated by poorer BLEU score of the A LL S PLIT baseline, even though it achieves good BP scores both on the corpus and long translations. Compared to supervised baselines trained on syntactic data such as C OMMAC LASS and H EURISTIC, our model performs competitively on both"
2021.findings-acl.25,D18-1325,0,0.0188218,"shows a single source Chinese segment is translated into two separate English sentences. The highlighted comma separates the two corresponding complete sentences in the Chinese text, whereas the other two commas are sentence-internal boundaries. Introduction and which do not. Machine translation systems typically operate on sentence-like units, where sentences are translated independently of each other (Vaswani et al., 2017; Bahdanau et al., 2015; Koehn et al., 2003; Brown et al., 1993), in some cases with additional conditioning on a representation of adjacent sentences to improve coherence (Miculicich et al., 2018; Zhang et al., 2018). While many pairs of languages use similar orthographic conventions to designate sentence boundaries, English and Chinese diverge considerably: complete sentences in Chinese may be terminated either unambiguously with a full stop (。) or ambiguously with a comma. See Figure 1 for an example. This divergence poses a challenge for Chinese– English translation systems since they must either be able to cope with potentially long, multisentence inputs (i.e., translating any text that falls between unambiguous sentence-ending punctuation) or, alternatively, they must be able to"
2021.findings-acl.25,W18-6319,0,0.0211153,"Missing"
2021.findings-acl.25,W14-4009,0,0.0617262,"Missing"
2021.findings-acl.25,D18-1397,0,0.027388,"punctuation on which an action still needs to be taken), un-split punctuation and split punctuation. At each timestep t, an action is taken at the next immediate undecided punctuation from 294 the left and the state update involves appropriately updating the punctuation marker (to unsplit or split) corresponding to that position. The episode is considered terminal when there are no unattended punctuation markers in the sentence. • For our reward, we use rt = ∗ ∗ BLEU(τ (st+1 ), y ) − BLEU(τ (st ), y ), the marginal difference added to BLEU score based on the current action decision similar to Wu et al. (2018). τ represents the translation of the source inputs constructed from the state definition, where we split the sentence according to the punctuation markers in st , translate each segment independently and recombine the best translation of each segment. An action of no split yields 0 rewards as segments remain identical and splitting would produce positive or negative rewards depending on the improvement/degradation to the quality of the overall sentence translation. Marginal BLEU sums to the sentence level BLEU obtained on the full sequence of translations, but provides a denser reward signal"
2021.naacl-main.223,D15-1075,0,0.0604614,"t al., 2013) and a corpus acceptability task using the CoLA dataset (Warstadt et al., 2019; Wang et al., 2018). The sentiment analysis task contains 9.6k sentences labelled with a positive or negative sentiment, while the acceptability task contains 8.5k sentences labelled with an acceptability judgement about whether or not it is a grammatically correct English sentence. Entailment and Question Pair Classification: this task requires a model to encode two sentences and output a classification based on the relation between them. We evaluate on a textual entailment task using the SNLI dataset (Bowman et al., 2015a) and a question pair classification task using the QQP dataset (Wang et al., 2018). SNLI contains 550k sentence pairs and requires models to encode two different sentences, a premise and a hypothesis, and predict one of three relations between them: an entailment, a contradiction or a neutral relation. The QQP task contains 364k pairs and requires models to encode two different text inputs, a question and an alternate question composed of different words, and to predict whether or not the two questions correspond to the same answer. Document Classification: this task requires models to encod"
2021.naacl-main.223,N19-1352,0,0.0636176,"ing to obtain a smaller vocabulary, “out-of-vocabulary” (OOV) words are replaced with an unknown word token &lt;UNK>. This reduction in vocabulary size has many advantages. Models with reduced vocabulary are more easily interpretable and achieve increased transparency (Adadi and Berrada, 2018; Samek et al., 2019), require less memory, can be 1 Introduction used in resource constrained settings, and are less Most state-of-the-art NLP methods use neural net- prone to overfitting (Sennrich et al., 2015; Shi and works that require a pre-defined vocabulary to vec- Knight, 2017; L’Hostis et al., 2016; Chen et al., 2019). However, reducing the vocabulary size with torise and encode text. In large text datasets, the a heuristic such as frequency is often not optimal. vocabulary size can grow to hundreds of thousands For example, Figure 1 shows the top ranked words of words, and having an embedding space over the entire vocabulary results in models that are expen- according to frequency (blue), that are largely unimsive in memory and compute, and hard to interpret. portant for the sentiment task at hand. We consider the vocabulary selection problem: Many of the words in the vocabulary are not given a target voc"
2021.naacl-main.223,P15-1144,1,0.793926,"Following the expensive selection step, we now have the benefit of a smaller model which is more interpretable and explainable, has a reduced memory consumption and potentially less prone to overfitting. We have proposed several ways to mitigate the compute load of selecting the vocabulary: applying a heuristic pre-filtering step and using logisitic regression models rather than the full model while estimating power indices. 6 Related Work Model compression: Using the full vocabulary to train models limits the applicability of models in memory-constrained or computationconstrained scenarios (Faruqui et al., 2015; Yogatama et al., 2015). Earlier work discusses methods for compressing model size. These yield models that are less expensive in memory and compute, and that are also more easily interpretable. Model compression methods include matrix compression methods such as sparsification of weights in a matrix (Wen et al., 2016), Bayesian inference for compression (Molchanov et al., 2017; Neklyudov et al., 2017), feature selection methods such as ANOVA (Girden, 1992), precision reduction methods (Han et al., 2015; Hubara et al., 2017) and approximations of the weight matrix (Tjandra et al., 2017; Le et"
2021.naacl-main.223,D18-2012,0,0.0270432,"e interpretability in our case stems from having few features, clearly highlighting the most impactful features in the dataset. Vocabulary selection methods and subword and character level embeddings: earlier work examined selecting a vocabulary for an NLP task. Some alternatives drop out words (Chen et al., 2019), whereas character-level methods that attempt to represent the input text at the level of individual characters (Kim et al., 2015; Lee et al., 2017; Ling et al., 2015) while subword methods attempt to tokenize words into parts of words in a more efficient way (Sennrich et al., 2015; Kudo and Richardson, 2018). Character level embedding methods decompose words to allow each individual character to have its own embedding. This reduces the vocabulary size to the number of characters, much smaller than the number of words in the full vocabulary. However, this is not applicable for some character-free languages (e.g. Chinese, Japanese, Korean). Also, such methods have reduced performance, and typically use larger embedding sizes than word embedding models to obtain reasonable quality (Zhang et al., 2015; Kim et al., 2015). We proposed a vocabulary selection method for In contrast, subword embeddings ha"
2021.naacl-main.223,Q17-1026,0,0.0275034,"it filters our vocabulary words, and can thus operate with any NLP architecture (i.e. the method is agnostic to the model architecture used). Further, the interpretability in our case stems from having few features, clearly highlighting the most impactful features in the dataset. Vocabulary selection methods and subword and character level embeddings: earlier work examined selecting a vocabulary for an NLP task. Some alternatives drop out words (Chen et al., 2019), whereas character-level methods that attempt to represent the input text at the level of individual characters (Kim et al., 2015; Lee et al., 2017; Ling et al., 2015) while subword methods attempt to tokenize words into parts of words in a more efficient way (Sennrich et al., 2015; Kudo and Richardson, 2018). Character level embedding methods decompose words to allow each individual character to have its own embedding. This reduces the vocabulary size to the number of characters, much smaller than the number of words in the full vocabulary. However, this is not applicable for some character-free languages (e.g. Chinese, Japanese, Korean). Also, such methods have reduced performance, and typically use larger embedding sizes than word emb"
2021.naacl-main.223,P17-2091,0,0.0606102,"Missing"
2021.naacl-main.223,D13-1170,0,0.0125851,"nstituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model’s performance. We apply power indices from cooperative game theory, including the Shapley value and Banzhaf index, that measure the relative importance of individual team members in accomplishing a joint task. We approximately compute these indices to identify the most influential words. Figure 1: An example sentence from SST-2 (Socher et al., 2013), as well as the distribution of heuristic values based on vocabulary selection algorithms. Frequency and TF-IDF weight stopwords (right) higher whereas a game-theoretic Shapley-based approach tends to value task-specific words (left) more. Our empirical evaluation examines multiple NLP tasks, including sentence and document classification, question answering and textual entailment. We compare to baselines that select words based on frequency, TF-IDF and regression coefficients under L1 regularization, and show that this game-theoretic vocabulary selection outperforms all baselines on a range"
2021.naacl-main.223,W18-5446,0,0.0481879,"Missing"
2021.naacl-main.223,Q19-1040,0,0.012096,"Rank words in V based on Shapley estimates πw Return top k words in ranking 3 Evaluation We evaluate our algorithm on multiple tasks, contrasting its performance with common baselines. 2791 3.1 Datasets and Tasks We consider three different task structures. Single Sentence Classification: the task requires a model to encode the words of a given sentence and output a classification based on properties of sentences (for e.g., sentiment or acceptability). We evaluate on a sentiment-analysis task using the SST-2 dataset (Socher et al., 2013) and a corpus acceptability task using the CoLA dataset (Warstadt et al., 2019; Wang et al., 2018). The sentiment analysis task contains 9.6k sentences labelled with a positive or negative sentiment, while the acceptability task contains 8.5k sentences labelled with an acceptability judgement about whether or not it is a grammatically correct English sentence. Entailment and Question Pair Classification: this task requires a model to encode two sentences and output a classification based on the relation between them. We evaluate on a textual entailment task using the SNLI dataset (Bowman et al., 2015a) and a question pair classification task using the QQP dataset (Wang"
bhatia-etal-2014-unified,J98-2001,0,\N,Missing
bhatia-etal-2014-unified,C88-1044,0,\N,Missing
bhatia-etal-2014-unified,W13-2234,1,\N,Missing
C12-1022,N03-2002,0,0.0898836,"Missing"
C12-1022,P11-1087,1,0.836269,"position step amounts to interpolated back-off. Baroni and Matiasek (2002) proposed basic models of German compounds for use in predictive text input, exploiting the same link between right-headedness and context as we have, although their focus was restricted to compounds with two components. In terms of Bayesian modelling, the PYP has been found to be very useful in a variety of tasks, including word segmentation, speech recognition, domain adaption and unsupervised PoS tagging (Goldwater et al., 2006; Mochihashi et al., 2009; Huang and Renals, 2007; Neubig et al., 2010; Wood and Teh, 2009; Blunsom and Cohn, 2011). In all cases its power-law scaling and ease of extensibility via the base distribution allowed the formulation of interesting models that achieved competitive results. 7 Conclusion We have demonstrated how an existing hierarchical Bayesian model can be used to build an n-gram language model that is informed by intuitions about the specific linguistic phenomenon of closed-form compounds. While our focus was on compounds, we argue that this approach can be useful for other phenomena, such as rich morphology more generally, where data sparsity creates smoothing problems for n-gram language mode"
C12-1022,E12-3008,1,0.185309,"utch and Afrikaans), compounds are written as single orthographic units. NLP systems that rely on whitespace to demarcate their elementary modelling units, e.g. the “grams” in n-gram models, are thus prone to suffer from sparse data effects that can be attributed to compounds specifically. An account of compounds in terms of their components therefore holds the potential of improving the performance of such systems. Examples of compounds • A basic noun-noun compound: Auto + Unfall = Autounfall (car crash) 1 Preliminary work on the approach we follow in this paper was previously reported on by Botha (2012). Here, we expand on the scale and depth of the empirical evaluation and investigate an additional inference technique. 342 • Linking elements can appear between components Küche + Tisch = Küchentisch (kitchen table) • Components can undergo stemming Schule + Hof = Schulhof (schoolyard) • Compounding is recursive (Geburt + Tag) + Kind = Geburtstag + Kind = Geburtstagskind (birthday boy/girl) • Compounding extends beyond noun components Zwei-Euro-Münze (two Euro coin) Fahrzeug (vehicle) A compound is said to consist of a head component and one or more modifier components, with optional linking"
C12-1022,N09-1046,1,0.934021,"Missing"
C12-1022,P10-4002,1,0.797671,"n constrained to this vocabulary. Our test corpus for the monolingual task is the union of all the WMT11 development data for German (news-test2008,9,10, 7065 sentences). For translation experiments, the preprocessed English-German bitext was filtered to exclude sentences longer than 50 tokens, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as a basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the linear translation models were tuned towards BLEU using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the development set news-test2008 (2051 sentences) was used, while final BLEU scores are measured on the official test set newstest2011 (3003 sentences, 171460 tokens), without detokenising or recasing hypotheses. Compound segmentation For this evaluation, we used an a priori segmentation of compounds into parts to build our models. This means we assume a single, fixed analysis of a compound regardless of the context it occurs in, which is necessitated by the fact that our probabilistic model does not specify a step for choosing an analysis. To co"
C12-1022,N06-2013,0,0.0197616,"to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in the case of German compounds, the split-off modifiers would take precedence in a split-off head’s n-gra"
C12-1022,W08-0318,0,0.358285,"ted conditioned on some history of preceding feature values. This allows one to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in the case of German com"
C12-1022,E03-1076,0,0.107741,"feature value is generated conditioned on some history of preceding feature values. This allows one to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in th"
C12-1022,N06-1014,0,0.0371505,"known” token if they do not appear in the target-side of the bitext (see below). The motivation is that the hypotheses to be scored against the language model during decoding are by definition constrained to this vocabulary. Our test corpus for the monolingual task is the union of all the WMT11 development data for German (news-test2008,9,10, 7065 sentences). For translation experiments, the preprocessed English-German bitext was filtered to exclude sentences longer than 50 tokens, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as a basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the linear translation models were tuned towards BLEU using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the development set news-test2008 (2051 sentences) was used, while final BLEU scores are measured on the official test set newstest2011 (3003 sentences, 171460 tokens), without detokenising or recasing hypotheses. Compound segmentation For this evaluation, we used an a priori segmentation of compounds into parts to build our models. This means we assume a singl"
C12-1022,P09-1012,0,0.0190265,"robust since it does retain the original surface form of the word – recall that the decomposition step amounts to interpolated back-off. Baroni and Matiasek (2002) proposed basic models of German compounds for use in predictive text input, exploiting the same link between right-headedness and context as we have, although their focus was restricted to compounds with two components. In terms of Bayesian modelling, the PYP has been found to be very useful in a variety of tasks, including word segmentation, speech recognition, domain adaption and unsupervised PoS tagging (Goldwater et al., 2006; Mochihashi et al., 2009; Huang and Renals, 2007; Neubig et al., 2010; Wood and Teh, 2009; Blunsom and Cohn, 2011). In all cases its power-law scaling and ease of extensibility via the base distribution allowed the formulation of interesting models that achieved competitive results. 7 Conclusion We have demonstrated how an existing hierarchical Bayesian model can be used to build an n-gram language model that is informed by intuitions about the specific linguistic phenomenon of closed-form compounds. While our focus was on compounds, we argue that this approach can be useful for other phenomena, such as rich morpholo"
C12-1022,P03-1021,0,0.0361389,"corpus for the monolingual task is the union of all the WMT11 development data for German (news-test2008,9,10, 7065 sentences). For translation experiments, the preprocessed English-German bitext was filtered to exclude sentences longer than 50 tokens, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as a basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the linear translation models were tuned towards BLEU using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the development set news-test2008 (2051 sentences) was used, while final BLEU scores are measured on the official test set newstest2011 (3003 sentences, 171460 tokens), without detokenising or recasing hypotheses. Compound segmentation For this evaluation, we used an a priori segmentation of compounds into parts to build our models. This means we assume a single, fixed analysis of a compound regardless of the context it occurs in, which is necessitated by the fact that our probabilistic model does not specify a step for choosing an analysis. To construct a segmentation dictionary,"
C12-1022,E09-3008,0,0.149328,"ome history of preceding feature values. This allows one to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in the case of German compounds, the sp"
C12-1022,P06-1124,0,0.633892,"hat the family of hierarchical Pitman-Yor language models is an attractive vehicle through which to address the problem, and demonstrate the approach by proposing a model for German compounds. In our empirical evaluation the model outperforms a modified Kneser-Ney n-gram model in test set perplexity. When used as part of a translation system, the proposed language model matches the baseline BLEU score for English→German while improving the precision with which compounds are output. We find that an approximate inference technique inspired by the Bayesian interpretation of Kneser-Ney smoothing (Teh, 2006) offers a way to drastically reduce model training time with negligible impact on translation quality. TITLE AND ABSTRACT IN AFRIKAANS Bayes-modellering van saamgestelde woorde in Duits Hierdie werk neem uitdagings rondom die uitbreiding van n-gramtaalmodelle volgens voorafgaande linguistieke intuïsie onder die loep. Ons voer aan dat die familie van hiërargiese Pitman-Yor taalmodelle ’n wenslike stuk gereedskap is om hierdie probleem mee aan te pak en formuleer ’n model van Duitse saamgestelde woorde om die benadering te demonstreer. Met behulp van ’n empiriese evaluering bevind ons dat die mo"
C12-1022,J07-2003,0,\N,Missing
C14-1100,bhatia-etal-2014-unified,1,0.727454,"es or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in Chinese (a language without articles), where the existential construction can be used to express indefinite subjects and the ba- construction can be used to express definite direct objects (Chen, 2004). Aside from this variation in the form of (in)definite NPs within and across languages, there is also variability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites expressing these functions. We refer to these as communicative functions of definiteness, following Bhatia et al. (2014). Croft (2003, pp. 6–7) shows that even when two languages have access to the same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French translations (both languages use definite as well as indefinite articles) such as: (1) He showed extreme care. (unmarked) Il montra un soin extrême. (indef.) (2) I love artichokes and asparagus. (unmarked) J’aime les artichauts et les asperges. (def.) (3) His brother became a soldier. (indef.) Son frère est devenu"
C14-1100,D13-1174,1,0.849382,"ness. After preprocessing the text with a dependency parser and coreference resolver, which is described in §6.1, we extract several kinds of percepts for each NP. 4.2.1 Basic Words of interest. These are the head within the NP, all of its dependents, and its governor (external to the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing the dependency path upward from the head. For each of these words, we have separate percepts capturing: the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a 3 Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection. As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are excluded from regularization. 5 See Theorem 1.2 in Breiman (2001) for details. 4 1063 binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we have additional features specific to the first and the last one. Moreover, to better capture tense, aspect and modality, we collect the attached verb’s auxiliaries. We also make note of the negative particle (with"
C14-1100,P05-1066,0,0.0178084,"rom some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000),"
C14-1100,C88-1044,0,0.577504,"to predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listen"
C14-1100,2007.mtsummit-papers.29,0,0.0509283,"ic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identifica"
C14-1100,T87-1035,0,0.537601,"aced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they are spoken. In co"
C14-1100,C10-1068,0,0.0128025,"se, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that re"
C14-1100,P06-1077,0,0.0109354,"matical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While defini"
C14-1100,W00-0708,0,0.0373875,"(Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distingu"
C14-1100,C02-1139,0,0.0446422,"dded or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share"
C14-1100,C00-2162,0,0.0170846,"s made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of"
C14-1100,W12-3807,1,0.89797,"Missing"
C14-1100,N13-1071,0,0.0612652,"Missing"
C14-1100,D10-1032,0,0.0256045,"gh we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in multiple communicative functions for each grammatical construction. Other attempts have also been made to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation, Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by prepositions. 8 Conclusion We have presented a data-driven approach to modeling the relationship between universal communicative functions associated with (in)definiteness and their lexical/grammatical realization in a particular language. Our feature-rich classifiers can give insights into this relationship as well as predict communicative functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear classifier com"
C14-1100,P10-1005,0,0.108554,"tediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling o"
C14-1100,N10-1018,0,0.0157261,"(b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which"
C14-1100,P13-1045,0,0.0784612,"Missing"
C14-1100,W13-2234,1,0.798697,"s input to (or jointly with) the coreference task. Applications such as information extraction and dialogue processing could be expected to benefit not only from coreference information, but also from some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of"
C14-1100,P02-1039,0,0.00839924,"to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme predictio"
C14-1100,2007.iwslt-1.3,0,0.0123911,"rmation (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly"
C16-1018,P16-1231,0,0.0336952,"d each candidate parse at the current position. We can then calculate a probability distribution over the candidates given both surface context and the previous parses in a manner similar to the process in Section 2.2. p(yt = a|x, y1 , y2 , ..., yt−1 ) = softmax(Rxt × ht ) (15) When decoding, we use a greedy approach to select the parse to add to the previous parse LSTM. Thus, our objective is to predict the output sequence yˆ = argmaxy˜∈YX 2.4 T Y p(˜ yt |x, y˜1 , y˜2 , ..., y˜t−1 ) (16) i=1 Conditional Random Field Joint Decoding Locally normalized models suffer from the label bias problem (Andor et al., 2016), meaning that they have little to no ability to revise previous decisions. Conditional Random Fields (CRFs) have been shown to be effective at modeling sequences in tasks like part of speech tagging and named entity recognition (Lample et al., 2016). In this approach, we attempt to find the best sequence of parses that takes the entire sentence into account. The CRF model is built on top of our full-context surface model, using the same process for embedding the parses and the surface context. Rather than taking the softmax over the combined representation of the surface context vector and th"
C16-1018,P05-1071,0,0.162478,"sentence are not accusative. Sentence and translation Evi bulabildiniz mi? – Did you find the house? Evi gerc¸ekten g¨uzelmis¸. – His/Her house is really beautiful Analysis of evi ev+Noun+3sg +Pnon+Acc ev+Noun+3sg +P3sg+Nom Table 2: Possible interpretations for “evi” based on context The problem of disambiguating over the candidate morphological parses generated by a morphological analyzer has been tackled in many languages and with different strategies. These systems primarily rely on methods for capturing the structure of a target word and its candidate tag sequences (Yuret and T¨ure, 2006; Habash and Rambow, 2005; Daybelge and Cicekli, 2007; Daoud, 2009) and/or the surrounding context of a target word (Hakkani-T¨ur et al., 2002; Smith et al., 2005; Sak et al., 2008; Lee et al., 2011) to choose the best candidate analysis. Despite the breadth of work on this problem, there is little work on disambiguation using neural network models. Based on previous work, it is not clear whether neural models are able to disambiguate only using surface forms or how context plays a role in a neural disambugation model. Models that disambiguate jointly over tokens incorporate more information about the surrounding cont"
C16-1018,W07-1709,0,0.123917,"Missing"
C16-1018,N16-1030,1,0.0537472,"1 ) = softmax(Rxt × ht ) (15) When decoding, we use a greedy approach to select the parse to add to the previous parse LSTM. Thus, our objective is to predict the output sequence yˆ = argmaxy˜∈YX 2.4 T Y p(˜ yt |x, y˜1 , y˜2 , ..., y˜t−1 ) (16) i=1 Conditional Random Field Joint Decoding Locally normalized models suffer from the label bias problem (Andor et al., 2016), meaning that they have little to no ability to revise previous decisions. Conditional Random Fields (CRFs) have been shown to be effective at modeling sequences in tasks like part of speech tagging and named entity recognition (Lample et al., 2016). In this approach, we attempt to find the best sequence of parses that takes the entire sentence into account. The CRF model is built on top of our full-context surface model, using the same process for embedding the parses and the surface context. Rather than taking the softmax over the combined representation of the surface context vector and the parse matrix, we use the product directly as a vector ut of emission scores between each word xt and its possible parses. ut = Rxt × ht (17) To model the transition scores between the j-th parse of word at position p and the i-th analysis of the pr"
C16-1018,P11-1089,0,0.0218893,"ev+Noun+3sg +Pnon+Acc ev+Noun+3sg +P3sg+Nom Table 2: Possible interpretations for “evi” based on context The problem of disambiguating over the candidate morphological parses generated by a morphological analyzer has been tackled in many languages and with different strategies. These systems primarily rely on methods for capturing the structure of a target word and its candidate tag sequences (Yuret and T¨ure, 2006; Habash and Rambow, 2005; Daybelge and Cicekli, 2007; Daoud, 2009) and/or the surrounding context of a target word (Hakkani-T¨ur et al., 2002; Smith et al., 2005; Sak et al., 2008; Lee et al., 2011) to choose the best candidate analysis. Despite the breadth of work on this problem, there is little work on disambiguation using neural network models. Based on previous work, it is not clear whether neural models are able to disambiguate only using surface forms or how context plays a role in a neural disambugation model. Models that disambiguate jointly over tokens incorporate more information about the surrounding context of a word, but models that make decisions at the word level are often simpler to train. In this paper, we present a language-agnostic LSTM-based approach for morphologica"
C16-1018,W96-0207,0,0.769018,"Missing"
C16-1018,H05-1060,0,0.0298769,"e is really beautiful Analysis of evi ev+Noun+3sg +Pnon+Acc ev+Noun+3sg +P3sg+Nom Table 2: Possible interpretations for “evi” based on context The problem of disambiguating over the candidate morphological parses generated by a morphological analyzer has been tackled in many languages and with different strategies. These systems primarily rely on methods for capturing the structure of a target word and its candidate tag sequences (Yuret and T¨ure, 2006; Habash and Rambow, 2005; Daybelge and Cicekli, 2007; Daoud, 2009) and/or the surrounding context of a target word (Hakkani-T¨ur et al., 2002; Smith et al., 2005; Sak et al., 2008; Lee et al., 2011) to choose the best candidate analysis. Despite the breadth of work on this problem, there is little work on disambiguation using neural network models. Based on previous work, it is not clear whether neural models are able to disambiguate only using surface forms or how context plays a role in a neural disambugation model. Models that disambiguate jointly over tokens incorporate more information about the surrounding context of a word, but models that make decisions at the word level are often simpler to train. In this paper, we present a language-agnostic"
C16-1018,N06-1042,0,0.790915,"Missing"
C16-1095,J92-4003,0,0.0814094,"Missing"
C16-1095,J93-1003,0,0.0211791,"gorithms, smoothed via additive (dirichlet) smoothing. Our pilot experiments showed that this resulted in better performance when compared to the performance with random initializations. The runtime (O(token ∗ iterations ∗ C 2 )) is higher than that of Brown algorithm (O(types ∗ C 2 + tokens)) because we estimate all the distributions of the probabilistic HMM using dynamic programming (Rabiner and Juang, 1986). Hence, subsequent models used interpolated stochastic batch updates (Liang and Klein, 2009) instead of batch updates so that the convergence is faster. We used a likelihood ratio test (Dunning, 1993), which is a form of hypothesis testing that decides whether the second word in the bigram is unusually associated with the first word of the bigram or not, to determine an initial list of possible NE collocations. For both Sorani and Tajik, this measure results in desirable LR score graphs for bigrams with a distinct “elbow” for both the languages. However, determining the threshold, below which a collocation should not be considered genuine for the purposes of further steps, was done manually by a human linguist looking at IPA representations of the collocations. This is a judgment that need"
C16-1095,P13-2054,0,0.0895686,"ence strategy in greater detail in Littell et al. (2016). For Tajik, we constructed a more traditional gazetteer using Tajik’s relatively extensive Wikipedia. Since Wikipedia titles are linked between Wikipedias in different languages, we had parallel English and Tajik titles; we filtered the English titles by heuristics including capitalization, and used the corresponding Tajik titles as the gazetteer entries. 3.2 Unsupervised morphology induction The Western Iranian languages are morphologically rich, with Sorani in particular having a high degree of morphological complexity (Walther, 2011; Esmaili and Salavati, 2013). In such languages, the presence of certain morphemes is strongly correlated with certain grammatical functions being present, which could be informative for the problem of identifying and discriminating named entities.1 While unsupervised induction of morphological grammars is a long-standing problem, the inferred morphological analyses typically diverge from conventional linguistic analyses rather substantially. We addressed this shortcoming by using feedback from human linguists using a modification of the interactive learning paradigm proposed by Hu et al. (2011). While superficially rela"
C16-1095,P11-1026,0,0.0114994,"Walther, 2011; Esmaili and Salavati, 2013). In such languages, the presence of certain morphemes is strongly correlated with certain grammatical functions being present, which could be informative for the problem of identifying and discriminating named entities.1 While unsupervised induction of morphological grammars is a long-standing problem, the inferred morphological analyses typically diverge from conventional linguistic analyses rather substantially. We addressed this shortcoming by using feedback from human linguists using a modification of the interactive learning paradigm proposed by Hu et al. (2011). While superficially related to active learning (Settles, 1 While Western Iranian languages also utilize prefixes and root modification, we concentrated here on suffixes alone; this simplifies the model and concentrates on those morphological alternations we believe more likely to be relevant to NER. 1000 Higher quality affixes will probably be higher in the list, but please go through the entire list. Examples of analyses containing each affix are listed for informational purposes. It is important to remember that you are not judging these anaylses, i.e., whether the affix in question is act"
C16-1095,N09-1069,0,0.0106249,"e expected to be identified as collocations. We warm-started with the distributions obtained by the Brown cluster algorithms, smoothed via additive (dirichlet) smoothing. Our pilot experiments showed that this resulted in better performance when compared to the performance with random initializations. The runtime (O(token ∗ iterations ∗ C 2 )) is higher than that of Brown algorithm (O(types ∗ C 2 + tokens)) because we estimate all the distributions of the probabilistic HMM using dynamic programming (Rabiner and Juang, 1986). Hence, subsequent models used interpolated stochastic batch updates (Liang and Klein, 2009) instead of batch updates so that the convergence is faster. We used a likelihood ratio test (Dunning, 1993), which is a form of hypothesis testing that decides whether the second word in the bigram is unusually associated with the first word of the bigram or not, to determine an initial list of possible NE collocations. For both Sorani and Tajik, this measure results in desirable LR score graphs for bigrams with a distinct “elbow” for both the languages. However, determining the threshold, below which a collocation should not be considered genuine for the purposes of further steps, was done m"
C16-1095,W03-0430,0,0.0445735,"of related languages; we describe this “IPAization” process in more detail in Mortensen et al. (2016) and Littell et al. (2016). 3 Named Entity Recognition For the NER task, we focused on identifying mentions of persons (PER), locations (LOC), and organizations (ORG) in the textual data. Our core system is a CRF based system with L1-regularization, where x is the input sequence and output y is the appropriate tag sequence, and no features look beyond a history of length 1. 999 f (y |x) = |x| ∑ f (x, yi , yi−1 ). (1) i=1 Based on previous work in the area (Tjong Kim Sang and De Meulder, 2003; McCallum and Li, 2003; Sha and Pereira, 2003), we begin with a standard set of features commonly used for training NER systems: • • • • • • • • • • • Current token and Current tag Previous token and Current tag Next token and Current tag Current+previous token and current tag Current+next token and current tag First five features but with previous tag First five features conjoined with both current and previous tags Contains foreign script characters Indicator features for tokens containing digits Features about capitalization information Prefix features However, given the paucity of training data, these features"
C16-1095,C16-1328,1,0.762765,"t languages and scripts could be more directly compared, and so that judgments about the data could be made rapidly by linguists without native proficiency in the Perso-Arabic and Cyrillic writing systems, we produced representations of the Sorani, Kurmanji, and Tajik data in the International Phonetic Alphabet (IPA). To disambiguate ambiguous Sorani forms, we used a conditional random field (CRF) (Lafferty et al., 2001) that utilized a combination of human judgments, universal phonetic features, and language models of related languages; we describe this “IPAization” process in more detail in Mortensen et al. (2016) and Littell et al. (2016). 3 Named Entity Recognition For the NER task, we focused on identifying mentions of persons (PER), locations (LOC), and organizations (ORG) in the textual data. Our core system is a CRF based system with L1-regularization, where x is the input sequence and output y is the appropriate tag sequence, and no features look beyond a history of length 1. 999 f (y |x) = |x| ∑ f (x, yi , yi−1 ). (1) i=1 Based on previous work in the area (Tjong Kim Sang and De Meulder, 2003; McCallum and Li, 2003; Sha and Pereira, 2003), we begin with a standard set of features commonly used"
C16-1095,N03-1028,0,0.00672678,"we describe this “IPAization” process in more detail in Mortensen et al. (2016) and Littell et al. (2016). 3 Named Entity Recognition For the NER task, we focused on identifying mentions of persons (PER), locations (LOC), and organizations (ORG) in the textual data. Our core system is a CRF based system with L1-regularization, where x is the input sequence and output y is the appropriate tag sequence, and no features look beyond a history of length 1. 999 f (y |x) = |x| ∑ f (x, yi , yi−1 ). (1) i=1 Based on previous work in the area (Tjong Kim Sang and De Meulder, 2003; McCallum and Li, 2003; Sha and Pereira, 2003), we begin with a standard set of features commonly used for training NER systems: • • • • • • • • • • • Current token and Current tag Previous token and Current tag Next token and Current tag Current+previous token and current tag Current+next token and current tag First five features but with previous tag First five features conjoined with both current and previous tags Contains foreign script characters Indicator features for tokens containing digits Features about capitalization information Prefix features However, given the paucity of training data, these features are numerous and sparse,"
C16-1095,W03-0419,0,0.0542807,"Missing"
C16-1328,D16-1153,1,0.877003,"ning data for a task like this. 9 The consonant/vowel feature should have been largely equivalent to the feature [±syllabic]. 3480 Features Accuracy CER Basic Basic+phon. Basic+Kurmanji Basic+Kurmanji+phon. Basic+Tajik Basic+Tajik+phon. All features 0.635 0.669 0.701 0.721 0.661 0.664 0.721 0.237 0.234 0.223 0.221 0.231 0.228 0.221 Table 3: IPA prediction for Sorani, trained on 244 tokens, tested on 402 tokens 4.2 NER with Phonologically-Aware Neural Models We subsequently experimented with PanPhon—and its sister package, Epitran—in performing NER with a character-based LSTM-CRF architecture (Bharadwaj et al., 2016). We made this architecture phonologically-aware by substituting phonological feature vectors from PanPhon for characters. We used the resulting features in a series of NER experiments in both monolingual and transfer scenarios. As a baseline, we employed a character based LSTM-CRF NER system with features from pre-trained word vectors. In a series of monolingual experiment using CoNLL 2002 data from Spanish (see Tab. 4, adapted from (Bharadwaj et al., 2016)) it was found that substituting PanPhon and phonological attention features for orthographic and orthographic attention features (in a mo"
C16-1328,N07-2004,0,0.022074,"ographies” are ambiguous writing systems that lose segmental information present in the speech stream. In the linguistic subfield of phonology, features are often represented in square brackets with the name on the right and the value presented as +, −, or ± (indicating that the feature is binary but that the value is not known). 5 Note that alternate feature systems use the feature [±front] to account for this contrast. 4 3476 and innate feature inventory (Mielke, 2008). Likewise, in the speech sciences and speech technology, phonological features have been the subject of widespread inquiry (Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) use"
C16-1328,P10-4002,1,0.793303,"aphs) in the script. 2. Human linguists identify the possible IPA representations of each orthographic unit, using knowledge of the language and the writing system. 3. The system generates all possible hypotheses for a subset of tokens of the language using the mapping developed in the previous step. 4. Human linguists generate training data using a grammar and lexicon (Thackston, 2006) by picking one or more valid pronunciations (or, if unknown, one or more likely hypotheses) for a selection of tokens. 5. Character-level chain conditional random field (CRF) is trained (Lafferty et al., 2001; Dyer et al., 2010) on resulting data. 3479 The hypothesis space within which the CRF operates was determined by the symbol-level IPA map developed in the first step of our work-flow. It is important to note that we allowed many-to-many mapping between orthographic input character sequences and IPA output character sequences in the sense that a single input character can be mapped to multiple IPA symbols and an input multigraph (consisting of multiple orthographic characters) can be mapped to a single IPA symbol. PanPhon feature vectors were used to create one set of features that were consumed by the CRF. This"
C16-1328,J96-4003,0,0.0356167,"e [±front] to account for this contrast. 4 3476 and innate feature inventory (Mielke, 2008). Likewise, in the speech sciences and speech technology, phonological features have been the subject of widespread inquiry (Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but w"
C16-1328,W06-1107,0,0.0408455,"ogical rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but with features weighted by the winnow algorithm rather than by hand; they achieved comparable results without engineered weights. In a related strain of research, Kondrak and Sherif (2006) explored phonological similarity measures based, in some cases, on a kind of phonological feature system (but with a multivalued place feature unlike the binary and ternary features integral to PanPhon). 3 PanPhon and Its Functionality PanPhon facilitates further experimentation with phonological (specifically, articulatory) features. Our goal in implementing PanPhon was not to implement a state-of-the-art feature system (from the standpoint of linguistic theory) but to develop a methodologically solid resource that would be useful for NLP researchers. Contemporary feature theories posit hier"
C16-1328,N16-1030,1,0.569259,"Missing"
C16-1328,L16-1529,1,0.912655,"o two classes of tasks: orthography-to-IPA character transduction and named entity recognition (NER). While the second set of experiments (on NER) were prompted by a need to test a particular class of model—phonologically aware LSTM-CRFs—the first set were motivated by the need to solve a particular problem: how best to convert Sorani Kurdish (a Northwestern Iranian language of Iraq and Iran) from orthographic to IPA representation. 4.1 Orthography-IPA Character Transduction As part of a NER system for the low-resource Sorani Kurdish language, we developed a Soraniorthography-to-IPA converter Littell et al. (2016). This was challenging because the Sorani orthography, like many Perso-Arabic scripts, badly underdetermines the equivalent phonetic representation. The following steps summarize the workflow behind building the Sorani-to-IPA converter: 1. Human linguists identify the orthographic units (i.e., characters and multigraphs) in the script. 2. Human linguists identify the possible IPA representations of each orthographic unit, using knowledge of the language and the writing system. 3. The system generates all possible hypotheses for a subset of tokens of the language using the mapping developed in"
C16-1328,N07-2030,0,0.0345232,"s writing systems that lose segmental information present in the speech stream. In the linguistic subfield of phonology, features are often represented in square brackets with the name on the right and the value presented as +, −, or ± (indicating that the feature is binary but that the value is not known). 5 Note that alternate feature systems use the feature [±front] to account for this contrast. 4 3476 and innate feature inventory (Mielke, 2008). Likewise, in the speech sciences and speech technology, phonological features have been the subject of widespread inquiry (Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted"
C16-1328,W06-1630,0,0.0104035,"Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but with features weighted by the winnow algorithm rather than by hand; they achieved comparable results without engineered weights. In a related strain of research, Kondrak and Sherif (2006) explored phonologica"
C16-1328,P07-1015,0,0.0272154,"more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but with features weighted by the winnow algorithm rather than by hand; they achieved comparable results without engineered weights. In a related strain of research, Kondrak and Sherif (2006) explored phonological similarity measures based, in some cases, on a kind of phonological feature system (but with a multivalued place feature unlike the binary and ternary features integral to PanPhon). 3 PanPhon and Its Functionality PanPhon facilitates further experimentation with phonological (specifically, articulatory) features. Our goal in implementing PanPh"
D10-1052,P09-1088,1,0.923747,"association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provides an approximation of the bilingual syntacti"
D10-1052,J93-2003,0,0.0769298,"state-of-the-art translation models has focused on long-distance reordering, but its counterpart in alignment models has remained focused on local reordering, typically modeling distortion based entirely on positional information. This leaves most alignment decisions to association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational comp"
D10-1052,N10-1015,0,0.0149489,"heless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009). However, UALIGN uses deep syntactic analysis and hand-crafted heuristics i"
D10-1052,P06-2014,0,0.0495046,"t the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provides an approximation of the bilingual syntactic divergences that drive the reordering. To our knowledge, however, this approach has been used mainly to constrain reordering possibilities, or to add to the generalization ability of association-based scores, not to directly model reordering in the context of alignment. 534 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534–544, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics In this paper, we introduce a"
D10-1052,P05-1033,0,0.0293107,"pair. Our training data consists of manually aligned corpora available from LDC (LDC2006E93 and LDC2008E57) and unaligned corpora, which include FBIS, ISI, HKNews and Xinhua. In total, the manually aligned corpora consist of more than 21 thousand sentence pairs, while the unaligned corpora consist of more than 710 thousand sentence pairs. The manually-aligned corpora are primarily used for training the reordering models and for discriminative training purposes. For translation experiments, we used cdec (Dyer et al., 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. We constructed the list of function words in English manually and in Chinese from (Howard, 2002). Punctuation marks were added to the list, resulting in 883 and 359 tokens in the Chinese and English lists, respectively. For the alignment experiments, we took the first 500 sentence pairs from the newswire genre of the manually-aligned corpora and used the first 250 sentences as the development set, with the remaining 250 as the test set. To ensure blind experimentation, we excluded these sentence pairs from the training of the features,"
D10-1052,P07-1003,0,0.0143765,"ation. This leaves most alignment decisions to association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provide"
D10-1052,J93-1003,0,0.0181782,"o predict alignments, we use a linear model of the following form: Aˆ = arg max A∈A(S,T ) θ · f (A, S, T ) (9) where A(S, T ) is the set of all possible alignments of a source sentence S and target sentence T , and f (A, S, T ) is a vector of feature functions on A, S, and T , and θ is a parameter vector. In addition to the six reordering models, our model employs several association-based scores that look at alignments in isolation. These features include: 1. Normalized log-likelihood ratio (LLR). This feature represents an association score, derived from statistical testing statistics. LLR (Dunning, 1993) has been widely used especially to measure lexical association. Since the values of LLR are unnormalized, we normalize them on a per-sentence basis, so that the normalized LLRs of, say, a particular source word to the target words in a particular sentence sum up to one. 2. Translation table from IBM model 4. This feature represents another association score, derived from a generative model, in particular the wordbased IBM model 4. The use of this feature is widespread in recent alignment models, since it provides a relatively accurate initial prediction. 3. Translation table from manually-ali"
D10-1052,P10-4002,1,0.818476,"xtrinsically on a large-scale translation task, focusing on ChineseEnglish as the language pair. Our training data consists of manually aligned corpora available from LDC (LDC2006E93 and LDC2008E57) and unaligned corpora, which include FBIS, ISI, HKNews and Xinhua. In total, the manually aligned corpora consist of more than 21 thousand sentence pairs, while the unaligned corpora consist of more than 710 thousand sentence pairs. The manually-aligned corpora are primarily used for training the reordering models and for discriminative training purposes. For translation experiments, we used cdec (Dyer et al., 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. We constructed the list of function words in English manually and in Chinese from (Howard, 2002). Punctuation marks were added to the list, resulting in 883 and 359 tokens in the Chinese and English lists, respectively. For the alignment experiments, we took the first 500 sentence pairs from the newswire genre of the manually-aligned corpora and used the first 250 sentences as the development set, with the remaining 250 as the test set. To ensure blind"
D10-1052,D07-1006,0,0.0725776,"mply the set of alignments {A(1) , A(2) , . . . , A(n) } encountered in the stochastic search. While MERT does not scale to large numbers of features, the scarcity of manually aligned training data also means that models with large numbers of sparse features would be difficult to learn discriminatively, so this limitation is somewhat inherent in the problem space. Additionally, MERT has several advantages that make it particularly useful for our task. First, we can optimize F-measure of the alignments directly, which has been shown to correlate with translation quality in a downstream system (Fraser and Marcu, 2007b). Second, we are optimizing the quality of the 1-best alignments under the model. Since translation pipelines typically use only a single word alignment, this criterion is appropriate. Finally, and very importantly for us, MERT requires only an approximation of the model’s hypothesis space to carry out optimization. Since we are using a stochastic search, this is crucial, since subsequent evaluations of the same sentence pair (even with the same weights) may result in a different candidate set. Although MERT is a non-probabilistic optimizer, we explore the alignment space stochastically. Thi"
D10-1052,J07-3002,0,0.233907,"mply the set of alignments {A(1) , A(2) , . . . , A(n) } encountered in the stochastic search. While MERT does not scale to large numbers of features, the scarcity of manually aligned training data also means that models with large numbers of sparse features would be difficult to learn discriminatively, so this limitation is somewhat inherent in the problem space. Additionally, MERT has several advantages that make it particularly useful for our task. First, we can optimize F-measure of the alignments directly, which has been shown to correlate with translation quality in a downstream system (Fraser and Marcu, 2007b). Second, we are optimizing the quality of the 1-best alignments under the model. Since translation pipelines typically use only a single word alignment, this criterion is appropriate. Finally, and very importantly for us, MERT requires only an approximation of the model’s hypothesis space to carry out optimization. Since we are using a stochastic search, this is crucial, since subsequent evaluations of the same sentence pair (even with the same weights) may result in a different candidate set. Although MERT is a non-probabilistic optimizer, we explore the alignment space stochastically. Thi"
D10-1052,P09-1104,0,0.0397026,"syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the"
D10-1052,D09-1024,0,0.0114907,"ation with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009). However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model. 9 Conclusions Languages exhibit regularities of word order that are preserved when projected to another language. We use the notion of function words to infer such regularities, resulting in several reordering models that are employed as features in a discriminative alignment model. In particular, our models predict the reordering of function words by looking at their dependencies with respect to their neighboring phrases, their neighboring function words, and the sentence boundaries. By capturing such lon"
D10-1052,N03-1017,0,0.0555603,"n) plus the 5 best alignments according to the log-likelihood ratio. 4 Using only the A LIGN operator, it is possible to explore the full alignment space; however, using all three operators increases mobility. 540 l&apos; (a) m&apos; m A′ ∈N (A(i) ) In addition to the current ‘active’ alignment configuration A(i) , the algorithm keeps track of the highest scoring alignment observed so far, Amax . After n steps, the algorithm returns Amax as its approximaˆ In the experiments reported below, we tion of A. initialized A(1) with the Model 4 alignments symmetrized by using the grow-diag-final-and heuristic (Koehn et al., 2003). l l&apos; (b) m&apos; m (c) m&apos; Figure 3: Illustrations for (a) A LIGN, (b) A LIGN E XCLU SIVE , and (c) S WAP operators, as applied to align the dotted, smaller circle (l, m) to (l, m′ ). The left hand side represents A(i) , while the right hand side represents a candidate for A(i+1) . The solid circles represent the new alignment points added to A(i+1) . 6 Discriminative Training To set the model parameters θ, we used the minimum error rate training (MERT) algorithm (Och, 2003) to maximize the F-measure of the 1-best alignment of the model on a development set consisting of sentence pairs with manual"
D10-1052,W02-1018,0,0.0129243,"U points. 8 Related Work The focus of our work is to strengthen the reordering component of alignment modeling. Although the de facto standard, the IBM models do not generalize well in practice: the IBM approach employs a series of reordering models based on the word’s position, but reordering depends on syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the"
D10-1052,P04-1066,0,0.0373192,"rk The focus of our work is to strengthen the reordering component of alignment modeling. Although the de facto standard, the IBM models do not generalize well in practice: the IBM approach employs a series of reordering models based on the word’s position, but reordering depends on syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model propose"
D10-1052,P06-1090,0,0.0218572,"e pair and h/si is the end-of-sentence marker. 3.1 Six (Sub-)Models To model o(Li,S→T ), o(Ri,S→T ), i.e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by Setiawan et al. (2007). Formally, this model takes the form of probability distribution Pori (o(Li,S→T ), o(Ri,S→T )|Yi,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro2 This heuristic is commonly used in learning phrase pairs from parallel text. The maximality ensures the uniqueness of L and R. 537 jections of the function word and the neighboring phrase are adjacent or separated by an intervening phrase. To model d(F Wi−1,S→T ), d(F Wi+1,S→T ), i.e. whether Li,S→T and Ri,S→T ext"
D10-1052,J04-4002,0,0.109325,"→ T denotes the projection direction from source to target. The subscript for the other direction is T → S. tic approach. For instance, to estimate the spans of Li,S→T , Ri,S→T , our reordering model assumes that any span to the left of Yi,S is a possible Li,S and any span to the right of Yi,S is a possible Ri,S , deciding which is most probable via features, rather than committing to particular spans (e.g. as defined by a monolingual text chunker or parser). We only enforce one criterion on Li,S→T and Ri,S→T : they have to be the maximal alignment blocks satisfying the consistent heuristic (Och and Ney, 2004) that end or start with Yi,S→T on the source S side respectively.2 To infer these phrases, we decompose Li,S→T into (o(Li,S→T ), d(F Wi−1,S→T ), b(hsi)); similarly, Ri,S→T into (o(Ri,S→T ),d(F Wi+1,S→T ), b(h/si) )). Taking the decomposition of Li,S→T as a case in point, here o(Li,S→T ) describes the reordering of the left neighbor Li,S→T with respect to the function word Yi,S→T , while d(F Wi−1,S→T ) and b(hsi)) probe the span of Li,S→T , i.e. whether it goes beyond the preceding function word phrase pairs F Wi−1,S→T and up to the beginning-ofsentence marker hsi respectively. The same definit"
D10-1052,P03-1021,0,0.234047,", our reordering model enumerates the function words on both source and target sides, modeling their reordering relative to their neighboring phrases, their neighboring function words, and the sentence boundaries. Because the frequency of function words is high, we find that by predicting the reordering of function words accurately, the reordering of the remaining words improves in accuracy as well. In total, we introduce six sub-models involving function words, and these serve as features in a log linear model. We train model weights discriminatively using Minimum Error Rate Training (MERT) (Och, 2003), optimizing F-measure. The parameters of our sub-models are estimated from manually-aligned corpora, leading the reordering model more directly toward reproducing human alignments, rather than maximizing the likelihood of unaligned training data. This use of manual data for parameter estimation is a reasonable choice because these models depend on a small, fixed number of lexical items that occur frequently in language, hence only small training corpora are required. In addition, the availability of manually-aligned corpora has been growing steadily. The remainder of the paper proceeds as fol"
D10-1052,N10-1014,0,0.147938,"ny useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provides an approximation of the bilingual syntactic divergences that drive the reordering. To our knowledge, however, this approach has been used mainly to constrain reordering possibilities, or to add to the generalization ability of association-based scores, not to directly model reordering in the context of alignment. 534 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534–544, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics In this paper, we introduce a new approach to impr"
D10-1052,P07-1090,1,0.846448,"of the left neighbor Li,S→T with respect to the function word Yi,S→T , while d(F Wi−1,S→T ) and b(hsi)) probe the span of Li,S→T , i.e. whether it goes beyond the preceding function word phrase pairs F Wi−1,S→T and up to the beginning-ofsentence marker hsi respectively. The same definition applies to the decomposition of Ri,S→T , where F Wi+1,S→T is the succeeding function word phrase pair and h/si is the end-of-sentence marker. 3.1 Six (Sub-)Models To model o(Li,S→T ), o(Ri,S→T ), i.e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by Setiawan et al. (2007). Formally, this model takes the form of probability distribution Pori (o(Li,S→T ), o(Ri,S→T )|Yi,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original"
D10-1052,P09-1037,1,0.868891,"RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro2 This heuristic is commonly used in learning phrase pairs from parallel text. The maximality ensures the uniqueness of L and R. 537 jections of the function word and the neighboring phrase are adjacent or separated by an intervening phrase. To model d(F Wi−1,S→T ), d(F Wi+1,S→T ), i.e. whether Li,S→T and Ri,S→T extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of Setiawan et al. (2009). Taking d(F Wi−1,S→T ) as a case in point, this model takes the form Pdom (d(F Wi−1,S→T )|Yi−1,S→T , Yi,S→T ), where d takes one of the following four dominance values: leftFirst, rightFirst, dontCare, or neither. We will detail the exact formulation of these values in the next subsection. However, to provide intuition, the value of either leftFirst or neither for d(F Wi−1,S→T ) would suggest that the span of Li,S→T doesn’t extend to Yi−1,S→T ; the further distinction between leftFirst and neither concerns with whether the span of Ri−1,S→T extends to F Wi,S→T . To model b(hsi), b(h/si), i.e."
D10-1052,C96-2141,0,0.875717,"nslation models has focused on long-distance reordering, but its counterpart in alignment models has remained focused on local reordering, typically modeling distortion based entirely on positional information. This leaves most alignment decisions to association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inferenc"
D10-1052,J97-3002,0,0.239328,"models based on the word’s position, but reordering depends on syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data"
D10-1052,P05-1059,0,0.0240985,"gnment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009). However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model. 9 Conclusions Languages exhibit regularities of word order that are preserved when projected to a"
D10-1052,P06-2122,0,0.0402909,"Missing"
D11-1055,J96-1002,0,0.00963011,"in this collection, complete data for its papers postdate the end of the last training period, so we chose to exclude it from our dataset. 3 Model Our forecasting approach is based on generalized linear models for regression and classification. The models are trained with an `2 -penalty, often called a “ridge” model (Hoerl and Kennard, 1970).3 For the NBER data, where (log) number of downloads is nearly a continuous measure, we use linear regression. For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a “maximum entropy” model (Berger et al., 1996) or a log-linear model. We briefly review the class of models. Then, we describe a time series model appropriate for time series data. 3.1 3.2 Generalized Linear Models Consider a model that predicts a response y given a vector input x = hx1 , . . . , xd i ∈ Rd . Our models are linear functions of x and parameterized by the vector β. Given a corpus of M document features, X, and responses Y , we estimate: ˆ = argmin R(β) + L(β, X, Y ) β β where there is a feature vector β c for each class c ∈ C. Under this interpretation, parameter estimation is maximum a posteriori inference for β, and R(β) i"
D11-1055,D08-1038,0,0.179494,"●● ●● ● ●● ● ●● ● ● ● ● ● ● ● ●● ● ●● ● ● ●● ● ● ●● ● ●● ● ● ●● ● ●●●● ● ● ● ● ● ● 1990 2000 cur frequently in conference session titles. On the right are term frequencies (with smoothing, since year-to-year frequencies are bumpy). Most terms decline over time. On the left, by contrast, are the weights learned by our time series model. They tell a very different story: for example, parsing has shown a definite increase in interest, while interest in grammars (e.g., formalisms) has declined somewhat. These trends have face validity, giving credence to our analysis; they also broadly agree with Hall et al. (2008). Authors The regression method also allows analysis of author influence, since we fit a coefficient for each of the authors in the ACL dataset. Figure 6(a) addresses the following question: do prolific authors get cited more often, even after accounting for the content of their papers?13 The effect is present but relatively small according to our model: the total number of papers co-authored by an author has a weak correlation to the author’s citation prediction coefficient (τ = 0.16). Next, does the model provide more information than the simple citation probability of an author? Figure 6(b)"
D11-1055,N10-1038,1,0.872504,"ices of train/test separation, and we elucidate these issues. In addition, we introduce a new regularization technique that leverages the intuition that the relationship between observable features and response should evolve smoothly over time. This regularizer allows the learner to rely more strongly on more recent evidence, while taking into account a long history of training data. Our time series-inspired regularizer is computationally efficient in learning and is a significant advance over earlier text-driven forecasting models that ignore the time variable altogether (Kogan et al., 2009; Joshi et al., 2010). We evaluate our approaches in two novel experimental settings: predicting downloads of economics articles and predicting citation of papers at ACL conferences. Our approaches substantially outperProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594–604, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics # docs. 2500 1500 # docs. 0 0 4 log(# downloads) 9 0 # citations 18 Figure 1: Left: the distribution of log download counts for papers in the NBER dataset one year after posting. Right: the distribution of wi"
D11-1055,N09-1031,1,0.710307,"the usual best practices of train/test separation, and we elucidate these issues. In addition, we introduce a new regularization technique that leverages the intuition that the relationship between observable features and response should evolve smoothly over time. This regularizer allows the learner to rely more strongly on more recent evidence, while taking into account a long history of training data. Our time series-inspired regularizer is computationally efficient in learning and is a significant advance over earlier text-driven forecasting models that ignore the time variable altogether (Kogan et al., 2009; Joshi et al., 2010). We evaluate our approaches in two novel experimental settings: predicting downloads of economics articles and predicting citation of papers at ACL conferences. Our approaches substantially outperProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594–604, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics # docs. 2500 1500 # docs. 0 0 4 log(# downloads) 9 0 # citations 18 Figure 1: Left: the distribution of log download counts for papers in the NBER dataset one year after posting. Right: t"
D11-1055,C08-1087,0,0.0401132,"ike authors, topical categories, and publication venues. 1 Introduction Written communication is an essential component of the complex social phenomenon of science. As such, natural language processing is well-positioned to provide tools for understanding the scientific process, by analyzing the textual artifacts (papers, proceedings, etc.) that it produces. This paper is about modeling collections of scientific documents to understand how their textual content relates to how a scientific community responds to them. While past work has often focused on citation structure (Borner et al., 2003; Qazvinian and Radev, 2008), our emphasis is on the text content, following Ramage et al. (2010) and Gerrish and Blei (2010). Instead of task-independent exploratory data analysis (e.g., topic modeling) or multi-document sum594 Noah A. Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu marization, we consider supervised models of the collective response of a scientific community to a published article. There are many measures of impact of a scientific paper; ours come from direct measurements of the number of downloads (from an established website where prominent eco"
D11-1055,W09-3607,0,0.114182,"Missing"
D12-1021,P11-1087,1,0.0339292,"guous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hΣ, ∆, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). Σ is a finite set of source language terminal symbols, ∆ is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewrite rules. A string pair is generated by star"
D12-1021,P09-1088,1,0.883446,"pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn s"
D12-1021,J92-1002,0,0.138042,"text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences— including discontiguous, many-to-many alignments—and produces competitive translation results. Further, inference is efficient and we present results on significantly larger corpora than prior work. 1 Phil Blunsom Dept. of Computer Science University of Oxford pblunsom@cs.ox.ac.uk Introduction In the twenty years since Brown et al. (1992) pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed. The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs). Such models are now widely used in translation and represent the state-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for tra"
D12-1021,J93-2003,0,0.0321846,"ed a rule template which indicates how large the rule is, whether each position contains a terminal or nonterminal symbol, and the reordering of the source nonterminals a. To conclude the process we must select the terminal types from the source and target 226 vocabularies. To do so, we use the following distribution: Pterminals (s, t) = PM 1← (s, t) + PM 1→ (s, t) 2 where PM 1← (s, t) (PM 1→ (s, t)) first generates the source (target) terminals from uniform draws from the vocabulary, then generates the string in the other language according to IBM M ODEL 1, marginalizing over the alignments (Brown et al., 1993). 4 Gibbs Sampler In this section we introduce a Gibbs sampler that enables us to perform posterior inference given a corpus of sentence pairs. Our innovation is to represent the synchronous derivation of a sentence pair in a hierarchical 4-dimensional binary alignment grid, with elements z[s,t,u,v] ∈ {0, 1}. The settings of the grid variables completely determine the SCFG rules in the current derivation. A setting of a binary variable z[s,t,u,v] = 1 represents a constituent linking the source span [s, t] and the target span [u, v] in the current derivation; variables with a value of 0 indicat"
D12-1021,W07-0403,0,0.313161,"us phrases. This had the dual benefits of biasing the model towards learning minimal translation units, and integrating out the parameters such that a much smaller set of statistics would suffice for inference with a Gibbs sampler. However this work fell short by not evaluating the model independently, instead only presenting results in which it was combined with a standard word-alignment initialisation, thus leaving open the question of its efficacy. The fact that flat phrasal models lack a structured approach to reordering has led many researchers to pursue SCFG induction instead (Wu, 1997; Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009). The asymptotic time complexity of the inside algorithm for even the simplest SCFG models is O(|s|3 |t|3 ), too high to be practical for most real translation data. A popular solution to this problem is to heuristically restrict inference to derivations which agree with an independent alignment model (Cherry and Lin, 2007; Zhang et al., 2008). However this may have the unintended effect of biasing the model back towards the initial alignments that they attempt to improve upon. More recently Neubig et al. (2011) reported a novel Bayesian model for phr"
D12-1021,J07-2003,0,0.563916,"duction In the twenty years since Brown et al. (1992) pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed. The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs). Such models are now widely used in translation and represent the state-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011"
D12-1021,D08-1033,0,0.639634,"Missing"
D12-1021,P10-4002,1,0.251798,"a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1 BASELINE I NITIALISATION W EAK M1 I NIT. S TRONG HMM I NIT. UR - EN MT 09 ZH - EN MT 03-08 E URO PARL 23.1 29.4 28.4 18.5 19.8 25.5 23.7 28.3 27.8 24.0 29.8 29.2 DE - EN PYP-SCFG Table 3: Results for the SMT experiments in B LEU . The baseline is produced using a full G IZA ++ run. The M ODEL 1 I NITIALISATION col"
D12-1021,N10-1033,1,0.859874,"SCFG, rules are associated with probabilities such that the probabilities of all rewrites of a particular LHS category sum to 1. Translation with SCFGs is carried out by parsing the source language with the monolingual source language projection of the grammar (using standard monolingual parsing algorithms), which induces a parallel tree structure and translation in the target language (Chiang, 2007). Alignment or synchronous parsing is the process of concurrently parsing both the source and target sentences, uncovering the derivation or derivations that give rise to a string pair (Wu, 1997; Dyer, 2010). Our goal is to infer the most probable SCFG derivations that explain a corpus of parallel sentences, given a nonparametric prior over probabilistic SCFGs. In this work we will consider grammars with a single nonterminal category X. 3.2 Pitman-Yor Process SCFG Before training we have no way of knowing how many rules will be needed in our grammar to adequately represent the data. By using the PitmanYor process as a prior on the parameters of a synchronous grammar we can formulate a model which prefers smaller numbers of rules that are reused often, thereby avoiding degenerate grammars consisti"
D12-1021,N04-1035,0,0.0933862,"Missing"
D12-1021,P06-1085,0,0.00679291,"Our model goes further by allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hΣ, ∆, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). Σ is a finite set of source language terminal symbols, ∆ is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewri"
D12-1021,N03-1017,0,0.234232,"tistics (in words). translation evaluation.3 The ZH - EN data is of a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1 BASELINE I NITIALISATION W EAK M1 I NIT. S TRONG HMM I NIT. UR - EN MT 09 ZH - EN MT 03-08 E URO PARL 23.1 29.4 28.4 18.5 19.8 25.5 23.7 28.3 27.8 24.0 29.8 29.2 DE - EN PYP-SCFG Table 3: Results for the SMT experiments in B LEU . The baseline is pr"
D12-1021,W02-1018,0,0.176665,"ents in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn synchronous grammars encoding complex translation phenomena. 223 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 223–232, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2 Prior Work The goal of directly inducing phrasal translation models from parallel corpora has received a lot of attention in the NLP and SMT literature. Marcu and Wong (2002) presented an ambitious maximum likelihood model and EM inference algorithm for learning phrasal translation representations. The first issue this model faced was a massive parameter space and intractable inference. However a more subtle issue is that likelihood based models of this form suffer from a degenerate solution, resulting in the model learning whole sentences as phrases rather than minimal units of translation. DeNero et al. (2008) recognised this problem and proposed a nonparametric Bayesian prior for contiguous phrases. This had the dual benefits of biasing the model towards learni"
D12-1021,P11-1064,0,0.396565,"2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn synchronous grammars en"
D12-1021,J03-1002,0,0.0262314,"able 2: Corpora statistics (in words). translation evaluation.3 The ZH - EN data is of a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1 BASELINE I NITIALISATION W EAK M1 I NIT. S TRONG HMM I NIT. UR - EN MT 09 ZH - EN MT 03-08 E URO PARL 23.1 29.4 28.4 18.5 19.8 25.5 23.7 28.3 27.8 24.0 29.8 29.2 DE - EN PYP-SCFG Table 3: Results for the SMT experiments in B LEU"
D12-1021,2001.mtsummit-papers.68,0,0.0103018,"generate synthetic data, and the five most probable inferred rules by our model. T RAIN ( SRC ) T RAIN ( TRG ) D EV ( SRC ) D EV ( TRG ) ZH - EN UR - EN DE - EN NIST NIST E UROPARL 8.6M 9.5M 22K 27K 1.2M 1.0M 18K 16K 34M 36M 26K 28K Table 2: Corpora statistics (in words). translation evaluation.3 The ZH - EN data is of a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1"
D12-1021,P06-1124,0,0.642911,"alisation. Our model goes further by allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hΣ, ∆, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). Σ is a finite set of source language terminal symbols, ∆ is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a"
D12-1021,C96-2141,0,0.588149,"each of the language pairs and experimental conditions described below. We used the approach of Newman et al. (2007) to distribute the sampler across multiple threads. The strength θ and discount d hyperparameters of the Pitman-Yor Processes, and the terminal penalty φ (Section 3.3), were inferred using slice sampling (Neal, 2000). The Gibbs sampler requires an initial set of derivations from which to commence sampling. In our experiments we investigated both weak and a strong initialisations, the former based on word alignments from IBM Model 1 and the latter on alignments from an HMM model (Vogel et al., 1996). For decoding we used the word alignments implied by the derivations in the final sample to extract a Hiero grammar with the same standard set of relative frequency, length, and language model features used for the baseline. Weak Initialisation Our first translation experiments ascertain the degree to which our proposed Gibbs sampling inference algorithm is able to learn good synchronous derivations for the PYP-SCFG model. A number of prior works on alignment with Gibbs samplers have only evaluated models initialised with the more complex G IZA ++ alignment models (Blunsom et al., 2009; DeNer"
D12-1021,J97-3002,0,0.851103,"e-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our result"
D12-1021,P08-1012,0,0.693183,"rt in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our mode"
D12-1021,P02-1040,0,\N,Missing
D13-1008,P06-2005,0,0.119477,"Missing"
D13-1008,2005.iwslt-1.8,0,0.0618985,"Missing"
D13-1008,P05-1074,0,0.444565,"Missing"
D13-1008,P11-1131,0,0.0205445,"Missing"
D13-1008,J92-4003,0,0.0391747,"Missing"
D13-1008,J93-2003,0,0.0256659,"Missing"
D13-1008,W11-2107,0,0.0305025,"Missing"
D13-1008,P08-1115,0,0.0452721,"Missing"
D13-1008,N13-1073,1,0.862904,"Missing"
D13-1008,P11-1137,0,0.00905253,"Missing"
D13-1008,N13-1037,0,0.00849129,"Missing"
D13-1008,W11-2210,0,0.028087,"Missing"
D13-1008,P11-1038,0,0.106633,"Missing"
D13-1008,D12-1039,0,0.166445,"Missing"
D13-1008,N03-1017,0,0.0474429,"Missing"
D13-1008,P07-2045,1,0.0119874,"Missing"
D13-1008,P13-1018,1,0.283107,"Missing"
D13-1008,J03-1002,0,0.0062214,"Missing"
D13-1008,J04-4002,0,0.110137,"Missing"
D13-1008,P03-1021,0,0.0134582,"Missing"
D13-1008,P02-1040,0,0.105491,"Missing"
D13-1008,P11-1002,0,0.0235515,"Missing"
D13-1008,J03-3002,0,0.0406716,"Missing"
D13-1008,P12-1021,0,0.0203959,"Missing"
D13-1008,C96-2141,0,0.0951178,"Missing"
D13-1008,N13-1050,0,0.1235,"Missing"
D13-1008,W13-2515,0,0.541542,"Missing"
D13-1008,D13-1007,0,0.0725911,"Missing"
D13-1008,2010.iwslt-papers.14,1,\N,Missing
D13-1111,P11-1022,0,0.0364316,"w much improvement from system combination. In Table 3, we break down the scores according to 1-best BLEU+1 quartiles, as done in Figure 1.7 In general, we find the largest gains for the lowBLEU translations. For the two worst BLEU quartiles, we see gains of 1.2 to 2.5 BLEU points, while the gains shrink or disappear entirely for the best quartile. This may be a worthwhile trade-off: a large improvement in the worst translations may be more significant to users than a smaller degredation on sentences that are already being translated well. In addition, quality estimation (Specia et al., 2011; Bach et al., 2011) could be used to automatically determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for"
D13-1111,J92-4003,0,0.0565547,"model of “bad” translations). This yielded 4 features. The procedure was then repeated using POS tags instead of words, for 8 features in total. Google 5-Grams (G OOG): Translations were compared to the Google 5-gram corpus (LDC2006T13) to compute: the number of 5-grams that matched, the number of 5-grams that missed, and a set of indicator features that fire if the fraction of 5grams that matched in the sentence was greater than {0.05, 0.1, 0.2, . . . , 0.9}, for a total of 12 features. Word Cluster LMs (WCLM): Using an implementation provided by Liang (2005), we performed Brown clustering (Brown et al., 1992) on 900k English sentences, including the NC corpus and random sentences from Gigaword. We clustered words that appeared at least twice, once with 300 clusters and again with 1000. We then replaced words with their clusters in a large corpus consisting of the WMT news data, Gigaword, and the NC data. An additional cluster label was used for unknown words. For each of the clusterings (300 and 1000), we estimated 5- and 7-gram LMs with Witten-Bell smoothing (Witten and Bell, 1991). We added 4 features to the reranker, one for the log-probability of the translation under each of the word cluster"
D13-1111,W13-2239,0,0.0691083,"t closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when consi"
D13-1111,W08-0336,0,0.0128042,"ata and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE 1, a 764-sentence subset of MT05 as TUNE 2, and MT08 as TEST . For ZH→EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was"
D13-1111,P05-1022,0,0.0176181,"Missing"
D13-1111,D10-1059,0,0.0797957,"s are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a generic dissimilarity function ∆(·, ·) that specifies how two solutions differ. Our first contribution is a family of dissimilarity function"
D13-1111,J07-2003,0,0.0562394,"r x. Derivations are coupled with translations and we define Tx ⊆ Yx × Hx as the set of possible hy, hi pairs for x. We use a linear model with a parameter vector w and a vector φ(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: ˆ = argmax w |φ(x, y, h) hˆ y, hi (1) hy,hi∈Tx where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions φ. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a greedy iterative procedure as follows. Let y1 be the model-best translation (Eq. 1). On the m-th iteration, the m-th best (diverse) translation is obtained as hym , hm i = argmax w φ(x, y, h) + hy,hi∈Tx m−1 X λj ∆(yj , y) Dissimilarity Functions for MT When designing a dissimilarity function ∆(·, ·) for MT, we want to consider variation both in individual word choice"
D13-1111,J05-1003,0,0.0155194,"Missing"
D13-1111,P09-1064,0,0.028759,"r n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Task"
D13-1111,N12-1059,0,0.402172,"bility to the negated count for each n-gram in previous diverse translations, and sets to zero all other n-grams’ log-probabilities and back-off weights. The advantage of this dissimilarity function is its simplicity. It can be easily used with any translation system that uses n-gram language models without any change to the decoder. Indeed, we use both phrase-based and hierarchical phrase-based models in our experiments below. 4 Related Work MT researchers have recently started to consider diversity in the context of system combination (Macherey and Och, 2007). Most closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems train"
D13-1111,D12-1065,0,0.0160887,"e to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Taskar, 2011) and discovery of topical threads in document collections (Gillenwater et al., 2012). Unfortunately, in the structured setting, DPPs make severely restric1102 tive assumptions on the scoring function, while our framework does not. 5 Experimental Setup We now embark on an extensive empirical evaluation of the framework presented above. We begin by analyzing our diverse sets of translations, showing how they differ from standard M -best lists (Section 6), followed by three tasks that illustrate how diversity can be exploited to improve translation quality: system combination (Section 7), discriminative reranking (Section 8), and a novel human postediting task (Section 9). In th"
D13-1111,2010.amta-papers.34,0,0.0181645,"veraged over the sentences in each quartile. As shown in the plot, the ranges of 20-diverse lists subsume those of 20-best lists, though the medians of diverse 3 The optimal values of λ were 0.005 for AR→EN and 0.01 for ZH→EN and DE→EN. Since these values depend on the scale of the weights learned by MERT, they are difficult to interpret in isolation. 1104 System Combination Experiments One way to evaluate the quality of our diverse lists is to use them in system combination, as was similarly done by Devlin and Matsoukas (2012) and Cer et al. (2013). We use the system combination framework of Heafield and Lavie (2010b), which has an open-source implementation (Heafield and Lavie, 2010a).4 We use our baseline systems (trained on TUNE 1) to generate lists for system combination on TUNE 2 and TEST. We compare M -best lists, unique M -best lists, and M -diverse lists, with M ∈ {10, 15, 20}.5 For each choice of list type and M , we trained the system combiner on TUNE 2 and tested on TEST with the learned parameters. System combination hyperparameters (whether to use feature length normalization; the size of the k-best lists generated by the system combiner during tuning, k ∈ {300, 600}) were chosen to maximize"
D13-1111,W11-2123,0,0.00371833,"to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE 1. We used the learned parameters to generate M -best and diverse lists for TUNE 2 and TEST to use for subsequent experiments. 5.3 1 best 20 best 200 best 1000 best unique 20 best unique 200 best 20 diverse 20 div × 10 best 20 div × 50 best Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function ∆n (§3.2) and the λj weights on the dissimilarity terms in Eq. (2). Though"
D13-1111,2008.amta-srw.3,0,0.0500625,"ch sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used"
D13-1111,2009.iwslt-papers.4,0,0.0288142,"rom the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with mod"
D13-1111,D11-1125,0,0.0159599,", and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used a slack-rescaled structured support vector machine (Tsochantaridis et al., 2005) with L2 regularization. As a sentencelevel loss, we used negated BLEU+1. We used the 1-slack cutting-plane algorithm of Joachims et al. (2009) for optimization during learning.8 A more detailed description of the reranker is provided in the supplementary material. We used 5-fold cross-validation on TUNE 2 to choose the regularization parameter C from the set {0.01, 0.1, 1, 10}. We selected the value yielding the highest average"
D13-1111,P08-1067,0,0.0110543,"em makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions"
D13-1111,N03-1017,0,0.0799251,"Hx , where Hx is the set of possible values of h for x. Derivations are coupled with translations and we define Tx ⊆ Yx × Hx as the set of possible hy, hi pairs for x. We use a linear model with a parameter vector w and a vector φ(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: ˆ = argmax w |φ(x, y, h) hˆ y, hi (1) hy,hi∈Tx where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions φ. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a greedy iterative procedure as follows. Let y1 be the model-best translation (Eq. 1). On the m-th iteration, the m-th best (diverse) translation is obtained as hym , hm i = argmax w φ(x, y, h) + hy,hi∈Tx m−1 X λj ∆(yj , y) Dissimilarity Functions for MT When designing a dissimilarity function ∆(·, ·) for MT, we want to consi"
D13-1111,P07-2045,1,0.0187723,"03k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (St"
D13-1111,N10-1078,0,0.0440076,"t in typical M -best lists, thereby hindering its ability to correctly rank diverse lists at test time. These results suggest that part of the benefit of using diverse lists comes from seeing a larger portion of the output space during training. 9 Human Post-Editing Experiments We wanted to determine whether diverse translations could be helpful to users struggling to understand the output of an imperfect MT system. We consider a post-editing task in which users are presented with translation output without the source sentence, and are asked to improve it. This setting has been studied; e.g., Koehn (2010) presented evidence that monolingual speakers could often produce improved translations for this task, occasionally reaching the level of an expert translator. Here, we use a novel variation of this task in which multiple translations are shown to editors. We compare the use of entries from an M -best list and entries from a diverse list. Again, the original source sentence is not provided. Our goal is to determine whether multiple, diverse translations can help users to more accurately guess the meaning of the original sentence than entries from a standard M -best list. If so, commercial MT s"
D13-1111,N09-1046,1,0.452862,"0M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE 1, a 764-sentence subset of MT05 as TUNE 2, and MT08 as TEST . For ZH→EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of"
D13-1111,W12-3123,0,0.0120141,"system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 1109 Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness of our approach is its computa"
D13-1111,W06-1673,0,0.0863389,"of Diversity in Machine Translation Kevin Gimpel∗ Dhruv Batra† Chris Dyer‡ Gregory Shakhnarovich∗ ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637, USA † Virginia Tech, Blacksburg, VA 24061, USA ‡ Carnegie Mellon University, Pittsburgh, PA 15213, USA Corresponding author: kgimpel@ttic.edu Abstract Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT. 1 Introduction From the perspective of user interaction, the id"
D13-1111,N04-1022,0,0.0225903,"q. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over set"
D13-1111,P09-1019,1,0.418986,"est output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a generic dissimilarity function ∆(·, ·) that spec"
D13-1111,P03-1051,0,0.0141558,"ge pair has two tuning and one test set: TUNE 1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE 2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR→EN and ZH→EN and one for DE→EN. For AR→EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE 1, a 764-sentence subset of MT05 as TUNE 2, and MT08 as TEST . For ZH→EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT20"
D13-1111,P09-1067,0,0.0139587,"at requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers divers"
D13-1111,2011.mtsummit-papers.58,0,0.199694,"ttings and do not show much improvement from system combination. In Table 3, we break down the scores according to 1-best BLEU+1 quartiles, as done in Figure 1.7 In general, we find the largest gains for the lowBLEU translations. For the two worst BLEU quartiles, we see gains of 1.2 to 2.5 BLEU points, while the gains shrink or disappear entirely for the best quartile. This may be a worthwhile trade-off: a large improvement in the worst translations may be more significant to users than a smaller degredation on sentences that are already being translated well. In addition, quality estimation (Specia et al., 2011; Bach et al., 2011) could be used to automatically determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5,"
D13-1111,2011.eamt-1.12,0,0.0112461,"f the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 1109 Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness of our approac"
D13-1111,2009.mtsummit-posters.20,0,0.038311,"he BLEU score of the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 1109 Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness"
D13-1111,C04-1072,0,0.0155123,"Missing"
D13-1111,N03-1033,0,0.0637169,"Missing"
D13-1111,D07-1105,0,0.0285552,"nal language model in ARPA format that sets the logprobability to the negated count for each n-gram in previous diverse translations, and sets to zero all other n-grams’ log-probabilities and back-off weights. The advantage of this dissimilarity function is its simplicity. It can be easily used with any translation system that uses n-gram language models without any change to the decoder. Indeed, we use both phrase-based and hierarchical phrase-based models in our experiments below. 4 Related Work MT researchers have recently started to consider diversity in the context of system combination (Macherey and Och, 2007). Most closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination"
D13-1111,D08-1065,0,0.0185506,"ate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a generic dissimilarity funct"
D13-1111,D08-1076,0,0.0130331,"s commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M -best solutions from a probabilistic model using a gener"
D13-1111,P02-1038,0,0.108968,"of all strings in a source language. For an x ∈ X, let Yx denote the set of its possible translations y in the target language. MT models typically include a latent variable that captures the derivational structure of the translation process. Regardless of its specific form, we refer to this variable as a derivation h ∈ Hx , where Hx is the set of possible values of h for x. Derivations are coupled with translations and we define Tx ⊆ Yx × Hx as the set of possible hy, hi pairs for x. We use a linear model with a parameter vector w and a vector φ(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: ˆ = argmax w |φ(x, y, h) hˆ y, hi (1) hy,hi∈Tx where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions φ. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a g"
D13-1111,J03-1002,0,0.016745,"ng us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE 1, MT05 as TUNE 2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE 1, the 2009 test set as TUNE 2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for"
D13-1111,N04-1021,0,0.0168026,"ly determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the"
D13-1111,P03-1021,0,0.0466511,", discriminative reranking (Section 8), and a novel human postediting task (Section 9). In the remainder of this section, we describe details of our experimental setup. 5.1 Language Pairs and Datasets We use three language pairs: Arabic-to-English (AR→EN), Chinese-to-English (ZH→EN), and German-to-English (DE→EN). For AR→EN and DE→ EN , we used a phrase-based model (Koehn et al., 2003) and for ZH→EN we used a hierarchical phrase-based model (Chiang, 2007). Each language pair has two tuning and one test set: TUNE 1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE 2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR→EN and ZH→EN and one for DE→EN. For AR→EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens an"
D13-1111,P02-1040,0,0.10263,"sed for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE 1. We used the learned parameters to generate M -best and diverse lists for TUNE 2 and TEST to use for subsequent experiments. 5.3 1 best 20 best 200 best 1000 best unique 20 best unique 200 best 20 diverse 20 div × 10 best 20 div × 50 best Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function ∆n (§3.2) and the λj weights on the dissimilarity terms in Eq. (2). Though our framework permits different λj for each j, we use a single λ v"
D13-1111,P12-1101,0,0.0231153,"s the percentage of words whose lexical translation probability falls below a threshold. We also include versions of the first 2 features normalized by the translation length, for a total of 5 I NV M OD 1 features. Large LM (LLM): We created a large 4-gram LM by interpolating LMs from the WMT news data, Gigaword, Europarl, and the DE→EN news commentary (NC) corpus to maximize likelihood of a heldout development set (WMT08 test set). We used the average per-word log-probability as the single feature function in this category. Syntactic LM (S YN): We used the syntactic treelet language model of Pauls and Klein (2012) to compute two features: the translation log probability and the length-normalized log probability. Finite/Non-Finite Verbs (V ERB): We ran the Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) on each translation and added four features: the fraction of words tagged as finite/non-finite verbs, and the fraction of verbs that are finite/nonfinite.9 9 Words tagged as MD, VBP, VBZ, and VBD were counted 1106 Reranking features N/A (baseline) None + I NV M OD 1 + LLM, S YN + V ERB, D ISC + G OOG + WCLM AR → EN best div 50.1 50.5 50.7 50.3 50.8 50.5 51.1 50.4 51.3 50.7 51.3 51.2 51.8 ZH"
D13-1111,N07-1029,0,0.0239987,"Missing"
D13-1111,W03-0402,0,0.0324323,"e Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal be"
D13-1111,N04-1023,0,0.0100353,"LEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6 They reported +0.8 BLEU from system combination for AR → EN , and saw a further +0.5–0.7 from their new features. 7 Quartile points are: 39, 49, 61 for AR→EN; 25, 36, and 47 for ZH→EN; and 14.5, 21.1, and 30.3 for DE→EN. 1105 gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M -best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M -best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadolla"
D13-1111,2008.amta-papers.18,0,0.0323456,"field’s inception. It is the way we interact with commercial MT services (such as Google Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M -best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Need for Diversity. Unfortunately, M -best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M -best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to"
D13-1111,W12-3102,0,\N,Missing
D13-1111,W13-2201,0,\N,Missing
D13-1174,2010.amta-papers.4,0,0.126235,"tions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several"
D13-1174,W13-2205,1,0.788135,"monolingual text). The setup is identical except for the addition of sparse 9 http://www.statmt.org/wmt13/ translation-task.html 10 http://sw.globalvoicesonline.org 11 http://www.aakkl.helsinki.fi/cameel/ corpus/intro.htm 1684 EN → RU 14.7±0.1 15.7±0.1 EN → HE 15.8±0.3 16.8±0.4 EN→ SW 18.3±0.1 18.7±0.2 16.2±0.1 16.7±0.1 17.6±0.1 — 19.0±0.1 — rule shape indicator features and bigram cluster features. In these large scale conditions, the BLEU score improves from 18.8 to 19.6 with the addition of word clusters and reaches 20.0 with synthetic phrases. Details regarding this system are reported in Ammar et al. (2013). 7 Related Work Translation into morphologically rich languages is a widely studied problem and there is a tremendous amount of related work. Our technique of synthesizing translation options to improve generation of inflected forms is closely related to the factored translation approach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-proces"
D13-1174,P08-1087,0,0.0835203,"slation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the le"
D13-1174,N03-2002,0,0.0801637,"ontext to generate additional local translation options and by leaving the choice of the full sentence translation to the decoder, we sidestep the difficulty of computing features on target translations hypotheses. However, many morphological processes (most notably, agreement) are most best modeled using target language context. To capture target context effects, we depend on strong target language models. Therefore, an important extension of our work is to explore the interaction of our approach with more sophisticated language models that more directly model morphology, e.g., the models of Bilmes and Kirchhoff (2003), or, alternatively, ways to incorporate target language context in the inflection model. We also achieve language independence by exploiting unsupervised morphological segmentations in the absence of linguistically informed morphological analyses. Code for replicating the experiments is available from https://github.com/eschling/morphogen; further details are available in (Schlinger et al., 2013). Acknowledgments This work was supported by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533. We would like to thank Kim Spasaro for"
D13-1174,P08-1024,0,0.0115532,"li word wakiwapiga will produce the following features: ψprefix[−3][wa] (µ) = 1, ψprefix[−1][wa] (µ) = 1. 4 Statistics of the parallel corpora used to train the inflection model are summarized in Table 1. It is important to note here that our richly parameterized model is trained on the full parallel training corpus, not just on a handful of development sentences (which are typically used to tune MT system parameters). Despite this scale, training is simple: the inflection model is trained to discriminate among different inflectional paradigms, not over all possible target language sentences (Blunsom et al., 2008) or learning from all observable rules (Subotin, 2011). This makes the training problem relatively tractable: all experiments in this paper were trained on a single processor using a Cython implementation of the SGD optimizer. For our largest model, trained on 3.3M Russian words, n = 231K × m = 336 features were produced, and 10 SGD iterations were performed in less than 16 hours. 4.1 ψprefix[−2][ki] (µ) = 1, Inflection Model Parameter Estimation To set the parameters W and V of the inflection prediction model (Eq. 1), we use stochastic gradient descent to maximize the conditional log-likeliho"
D13-1174,J93-2003,0,0.036659,"Missing"
D13-1174,N06-1003,0,0.0465741,"uistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora"
D13-1174,D07-1007,0,0.027646,"mplex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and ta"
D13-1174,2012.eamt-1.60,0,0.0154987,"based language model to be especially 8 For Swahili and Hebrew, n = 6; for Russian, n = 7. helpful here and capture some basic agreement patterns that can be learned more easily on dense clusters than from plain word sequences. Table 4: Translation quality (measured by BLEU) averaged over 3 MIRA runs. Baseline +Class LM +Synthetic unsupervised supervised Detailed corpus statistics are given in Table 1: • The Russian data consist of the News Commentary parallel corpus and additional monolingual data crawled from news websites.9 • The Hebrew parallel corpus is composed of transcribed TED talks (Cettolo et al., 2012). Additional monolingual news data is also used. • The Swahili parallel corpus was obtained by crawling the Global Voices project website10 for parallel articles. Additional monolingual data was taken from the Helsinki Corpus of Swahili.11 We evaluate translation quality by translating and measuring the BLEU score of a 2000–3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability (Clark et al., 2011). Table 4 reports the results. For all languages, using class language models improves over the baseline. When synthetic phrases are added, s"
D13-1174,P07-1005,0,0.0415702,"morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphras"
D13-1174,W09-0436,0,0.0550669,"endent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned"
D13-1174,2011.iwslt-evaluation.19,0,0.0452575,"slation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora to produce new inflections possibly unseen in the bilingual training data. Our method decomposes into two simple independent steps"
D13-1174,J07-2003,0,0.121987,"process of producing a translation for a word (or phrase) into two steps. First, a meaning-bearing stem is chosen and then an appropriate inflection is selected using a feature-rich discriminative model that conditions on the source context of the word being translated. Rather than attempting to directly produce fullsentence translations using such an elementary process, we use our model to generate translations of individual words and short phrases that augment— on a sentence-by-sentence basis—the inventory of translation rules obtained using standard translation rule extraction techniques (Chiang, 2007). We call these synthetic phrases. The major advantages of our approach are: (i) synthesized forms are targeted to a specific translation context; (ii) multiple, alternative phrases may be generated with the final choice among rules left to the global translation model; (iii) virtually no language-specific engineering is necessary; (iv) any phrase- or syntax-based decoder can be used without modification; and (v) we can generate forms that were not attested in the bilingual training data. The paper is structured as follows. We first present our “translate-and-inflect” model for predicting lexi"
D13-1174,P11-2031,1,0.516392,"lel corpus and additional monolingual data crawled from news websites.9 • The Hebrew parallel corpus is composed of transcribed TED talks (Cettolo et al., 2012). Additional monolingual news data is also used. • The Swahili parallel corpus was obtained by crawling the Global Voices project website10 for parallel articles. Additional monolingual data was taken from the Helsinki Corpus of Swahili.11 We evaluate translation quality by translating and measuring the BLEU score of a 2000–3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability (Clark et al., 2011). Table 4 reports the results. For all languages, using class language models improves over the baseline. When synthetic phrases are added, significant additional improvements are obtained. For the English–Russian language pair, where both supervised and unsupervised analyses can be obtained, we notice that expert-crafted morphological analyzers are more efficient at improving translation quality. Globally, the amount of improvement observed varies depending on the language; this is most likely indicative of the quality of unsupervised morphological segmentations produced and the kinds of gram"
D13-1174,P11-1004,0,0.126155,"odel morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complic"
D13-1174,P10-4002,1,0.764306,"Missing"
D13-1174,2012.eamt-1.6,0,0.224483,"Missing"
D13-1174,E12-1068,0,0.191071,"h sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing"
D13-1174,W08-0302,1,0.823134,"e in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” p"
D13-1174,P05-1071,0,0.0166254,"rvised methods. 3.1 The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3 • Dependency parsing with TurboParser (Martins et al., 2010), a non-projective dependency 1679 Supervised Morphology The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased analyzers and annotated corpora to train the disambiguation models. As a result, such analyzers are only available for a small number of languages, and, as a practical matter, each analyzer (which resulted from different development efforts) operates differently from the others. We therefore focus on using supervised analysis for a single target language, Russian. We use the analysis tool of Sharoff et al. (2008) which produces for each word"
D13-1174,P01-1035,0,0.0193571,"Missing"
D13-1174,C00-1042,0,0.268241,"Missing"
D13-1174,2010.amta-papers.33,0,0.211534,"ard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Hab"
D13-1174,W08-0704,0,0.0306035,"sls independently from θs ; and a stem σ ∼ θσ . (c) Concatenate prefixes, the stem, and suffixes: w = p1 +· · ·+plp +σ+s1 +· · ·+sls . We use blocked Gibbs sampling to sample segmentations for each word in the training vocabulary. Because of our particular choice of priors, it possible to approximately decompose the posterior over the arcs of a compact finite-state machine. Sampling a segmentation or obtaining the most likely segmentation a posteriori then reduces to familiar FST operations. This model is reminiscent of work on learning morphology using adaptor grammars (Johnson et al., 2006; Johnson, 2008). The inferred morphological grammar is very sensitive to the Dirichlet hyperparameters (αp , αs , ασ ) and these are, in turn, sensitive to the number of types in the vocabulary. Using αp , αs  ασ  1 tended to recover useful segmentations, but we have not yet been able to find reliable generic priors for these values. Therefore, we selected them empirically to obtain a stem vocabulary size on the parallel data that is one-to-one with English.4 Future work 4 Our default starting point was to use αp = αs = 10 , ασ = 10−4 and then to adjust all parameters by factors of 10. −6 Table 1: Corpus s"
D13-1174,D07-1091,0,0.150963,"17.6±0.1 — 19.0±0.1 — rule shape indicator features and bigram cluster features. In these large scale conditions, the BLEU score improves from 18.8 to 19.6 with the addition of word clusters and reaches 20.0 with synthetic phrases. Details regarding this system are reported in Ammar et al. (2013). 7 Related Work Translation into morphologically rich languages is a widely studied problem and there is a tremendous amount of related work. Our technique of synthesizing translation options to improve generation of inflected forms is closely related to the factored translation approach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment"
D13-1174,D07-1104,0,0.0572931,"Missing"
D13-1174,D10-1004,1,0.475972,"s of English text.3 We then extract binary features from e using this information, by considering the aligned source word ei , its preceding and following words, and its syntactic neighbors. These are detailed in Figure 2. 3 Morphological Grammars and Features We now describe how to obtain morphological analyses and convert them into feature vectors (ψ) for our target languages, Russian, Hebrew, and Swahili, using supervised and unsupervised methods. 3.1 The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3 • Dependency parsing with TurboParser (Martins et al., 2010), a non-projective dependency 1679 Supervised Morphology The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased anal"
D13-1174,P07-1017,0,0.256142,", a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and"
D13-1174,W07-0704,0,0.0236609,"proach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it"
D13-1174,D13-1174,1,0.106134,"Missing"
D13-1174,H05-1060,1,0.344355,"upervised and unsupervised methods. 3.1 The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3 • Dependency parsing with TurboParser (Martins et al., 2010), a non-projective dependency 1679 Supervised Morphology The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased analyzers and annotated corpora to train the disambiguation models. As a result, such analyzers are only available for a small number of languages, and, as a practical matter, each analyzer (which resulted from different development efforts) operates differently from the others. We therefore focus on using supervised analysis for a single target language, Russian. We use the analysis tool of Sharoff et al. (2008) whi"
D13-1174,P12-2063,0,0.0334722,"h (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 201"
D13-1174,P11-1024,0,0.261482,"ix[−3][wa] (µ) = 1, ψprefix[−1][wa] (µ) = 1. 4 Statistics of the parallel corpora used to train the inflection model are summarized in Table 1. It is important to note here that our richly parameterized model is trained on the full parallel training corpus, not just on a handful of development sentences (which are typically used to tune MT system parameters). Despite this scale, training is simple: the inflection model is trained to discriminate among different inflectional paradigms, not over all possible target language sentences (Blunsom et al., 2008) or learning from all observable rules (Subotin, 2011). This makes the training problem relatively tractable: all experiments in this paper were trained on a single processor using a Cython implementation of the SGD optimizer. For our largest model, trained on 3.3M Russian words, n = 231K × m = 336 features were produced, and 10 SGD iterations were performed in less than 16 hours. 4.1 ψprefix[−2][ki] (µ) = 1, Inflection Model Parameter Estimation To set the parameters W and V of the inflection prediction model (Eq. 1), we use stochastic gradient descent to maximize the conditional log-likelihood of a training set consisting of pairs of source (En"
D13-1174,P08-1059,0,0.65058,"at the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface"
D13-1174,W13-2234,1,0.711961,"to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora to produce new inflections possibly unseen in the bilingual training data. Our method decomposes into two simple independent steps involving well-understood discriminative models. By relying on source-side context to generate additional local translation options and by leaving the choice of the full sentence translat"
D13-1174,P10-1047,0,0.059066,"cessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic struct"
D13-1174,sharoff-etal-2008-designing,0,\N,Missing
D14-1108,N13-1037,0,0.0101107,"t al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007), and the expense of creating Penn Treebank–style annotations (Marcus et al., 1993). This paper presents T WEEBOPARSER, the first syntactic dependency parser designed explicitly for English tweets. We developed this parser following current best practices in empirical NLP: we annotate a corpus (T WEEBANK) and train the parameters of a statistical parsing algorithm. Our research contributions include: • a survey of key challenges posed by syntactic analysis of tweets (by humans or machines) and decisions motivated"
D14-1108,C96-1058,0,0.325768,"is used in parsing algorithms that seek to find: parse∗ (x) = argmax w⊺ g(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word w"
D14-1108,J92-4003,0,0.124367,"x p or xg , and transitions that consider unselected tokens as children, are eliminated. In order to allow the scores to depend on unselected tokens between xc and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is t"
D14-1108,N13-1070,0,0.00887336,"$a with the coordinator and labeling Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key differences from YM are in coordination structures (discussed in §3.2; YM makes the first conjunct the head) and possessive structures, in which the possessor is the child of the clitic, which is the child of the semantic head, e.g., the > king > ’s > horses. 3.4 Intrinsic Quality Our approach to developing this initi"
D14-1108,W06-2920,0,0.0727057,"Missing"
D14-1108,N09-1037,0,0.0604952,"Missing"
D14-1108,P14-1070,0,0.0175782,"c function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy on developing and respecting conventions (or makin"
D14-1108,D07-1101,0,0.0132253,"jectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasib"
D14-1108,W02-1001,0,0.20767,"Missing"
D14-1108,W11-0809,0,0.0137621,"Missing"
D14-1108,P12-1022,0,0.0128621,"the task of selecting tokens with a syntactic function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy"
D14-1108,W08-1301,0,0.0281126,"Missing"
D14-1108,P11-2008,1,0.814385,"Missing"
D14-1108,N13-1013,0,0.0174979,"ally modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our datasets. (A tweet with k annotations in th"
D14-1108,J98-4004,0,0.0947122,"Colors highlight token selection (gray; §2.1), multiword expressions (blue; §2.2), multiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence s"
D14-1108,P08-1068,0,0.0484886,"end on unselected tokens between xc and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from"
D14-1108,D08-1017,1,0.807895,"erienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our datasets. (A tweet with k"
D14-1108,D11-1022,1,0.315387,", which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs in"
D14-1108,P09-1039,1,0.732309,"ultiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets,"
D14-1108,D10-1004,1,0.488796,"Missing"
D14-1108,P05-1012,0,0.0428376,"errors included, for example, an incorrect dependency relation between an auxiliary verb and the main verb (like ima > [have to]). Minor errors included an incorrect attachment between two modifiers of the same head, as in the > only > [grocery store]—the correct annotation would have two attachments to a single head, i.e. the > [grocery store] < only (or equivalent). feature vector representation g is used in parsing algorithms that seek to find: parse∗ (x) = argmax w⊺ g(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald a"
D14-1108,H05-1066,0,0.195682,"Missing"
D14-1108,P10-1001,0,0.00934067,"approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have"
D14-1108,W07-2216,0,0.0321536,"al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all tree"
D14-1108,D10-1125,0,0.0181274,"Missing"
D14-1108,P14-5021,1,0.819279,"Missing"
D14-1108,P08-1108,0,0.0093581,"losely as possible.7 2. An experienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our"
D14-1108,C14-1177,0,0.0252561,"Missing"
D14-1108,N13-1039,1,0.599524,"Missing"
D14-1108,P14-2128,0,0.0325991,"Missing"
D14-1108,J93-2004,0,0.0527718,"tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein,"
D14-1108,P13-2109,1,0.553015,"Missing"
D14-1108,P92-1017,0,0.491182,"s whose roots are marked by the ** symbol. Schneider et al.’s GFL offers some additional features, only some of which we made use of in this project. One important feature allows an annotator to leave the parse underspecified in some ways. We allowed our annotators to make use of this feature; however, we excluded from our training and testing data any parse that was incomplete (i.e., any parse that contained multiple disconnected fragments with no explicit root, excluding unselected tokens). Learning to parse from incomplete annotations is a fascinating topic explored in the past (Hwa, 2001; Pereira and Schabes, 1992) and, in the case of tweets, left for future work. An important feature of GFL that we did use is special notation for coordination structures. For the coordination structure in Figure 1, for example, the notation is: $a :: {♥ want} :: the attachments specially for postprocessing, following Schneider et al. (2013). In our evaluation (§5), these are treated like other attachments. {&} where $a creates a new node in the parse tree as it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordi"
D14-1108,A97-1004,0,0.0375716,"der TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets, however, often contain multiple sentences or fragments, which we call “utterances,” each with its own syntactic root disconnected from the others. The selected tokens in Figure 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-struc"
D14-1108,W06-1616,0,0.0166238,"ion of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first col"
D14-1108,D11-1141,0,0.491647,"f traditional publications such as news reports, social media text closely represents language as it is used by people in their everyday lives. These informal texts, which account for ever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on twee"
D14-1108,Q14-1016,1,0.746015,"tags) average accuracy in the task of selecting tokens with a syntactic function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators s"
D14-1108,W13-2307,1,0.848519,"Missing"
D14-1108,D08-1016,0,0.00740608,"ing) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first collection of parts we adap"
D14-1108,P14-2068,0,0.0232384,"; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007),"
D14-1108,P10-1040,0,0.0319044,"and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from the Penn Treebank. Indeed, Foster et a"
D14-1108,P07-1031,0,0.0119923,"re 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-structure annotations, as is typically done using the Penn Treebank, is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the T WEEBANK corpus, highlighting data selection (§3.1), the an"
D14-1108,D13-1015,0,0.0116876,", is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the T WEEBANK corpus, highlighting data selection (§3.1), the annotation process (§3.2), important convention choices (§3.3), and measures of quality (§3.4). 3.1 Data Selection We added manual dependency parses to 929 tweets (12,318 tokens) drawn from the POS-tagged Twitter corpus of Owoputi et al. (2013), which are tokenized and contain manually annotated POS tags. Owoputi et al.’s data consists of two parts. The first, originally annotated by Gimpel et al. (2011), consists of tweets sampled from a particular day, October 27, 2010—t"
D14-1108,W03-3023,0,0.0804945,"s it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordinator and conjunct as parent. For learning to parse, we transform GFL’s coordination structures into specially-labeled dependency parses collapsing nodes like $a with the coordinator and labeling Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key di"
D14-1108,N09-1057,0,\N,Missing
D14-1108,I11-1100,0,\N,Missing
D14-1108,D07-1112,0,\N,Missing
D14-1108,J13-1009,0,\N,Missing
D14-1158,N09-1053,0,0.0288986,"r methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax ) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work. Outline: We first review existing n-gram smoothing methods (§2) and then present the intuition behind the key components of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We"
D14-1158,D08-1024,0,0.0259081,"low rank trigram models. For SmallRussian the ranges were {1e−5, 5e−5, 1e−4} for both the low rank bigram and the low rank trigram models. For statistical validity, 10 test sets of size equal to the original test set were generated by randomly sampling sentences with replacement from the original test set. Our method outperforms “intMKN” with gains similar to that on the smaller datasets. As shown in Table 3, our method obtains fast training times even for large datasets. 6 Machine Translation Task Table 4 presents results for the MT task, translating from English to Russian7 . We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6 As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7 the best score at WMT 2013 was 19.9 (Bojar et al., 2013) 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weigh"
D14-1158,J07-2003,0,0.0338502,"ineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus avai"
D14-1158,P11-2031,1,0.778325,"he MT task, translating from English to Russian7 . We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6 As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7 the best score at WMT 2013 was 19.9 (Bojar et al., 2013) 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connect"
D14-1158,P10-4002,1,0.832454,"am machine translation (MT) task. We have made the code for our approach publicly available 3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3 . • Small-Engli"
D14-1158,W11-2123,0,0.0366918,"3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3 . • Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test -"
D14-1158,J10-4005,0,0.0183816,"f ngram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task. 1 Introduction Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010). The predominant approach to language modeling is the n-gram model, wherein the probability of a word sequence P (w1 , . . . , w` ) is decomposed using the chain rule, and then a Markov assumption is made: P (w1 , . . . , w` ) ≈ Q` i−1 P (w i |wi−n+1 ). While this assumption subi=1 stantially reduces the modeling complexity, parameter estimation remains a major challenge. Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have"
D14-1158,D13-1024,0,0.0301227,"at leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax ) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work. Outline: We first review existing n-gram smoothing methods (§2) and then present the intuition behind the key components of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We also include a comparison"
D14-1158,P06-1124,0,0.0530922,"mes to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive"
D14-1158,D13-1140,0,0.0649532,"d retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6 As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7 the best score at WMT 2013 was 19.9 (Bojar et al., 2013) 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009)"
D14-1158,P02-1040,0,0.0898218,"of O(n), it is much faster than other recent methods for language modeling such as neural networks and conditional exponential family models where exact computation of the normalizing constant costs O(V ). 5 Experiments To evaluate PLRE, we compared its performance on English and Russian corpora with several vari2 for derivation see proof of Lemma 4 in the supplementary material 1493 ants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) syste"
D14-1158,P13-1005,0,0.0266141,"utation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which√can reduce the cost of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts. 8 Conclusion We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as well as low rank matrices and tensors, our method captures both the fine-grained and coarse structures in word sequences. Our discounting strategy preserves the marginal constraint and thus generalizes Kneser Ney, and under slight changes can also e"
D14-1158,D11-1104,0,0.0163873,"rarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is t"
D14-1158,W97-0309,0,0.378024,"milar to how some smoothing methods modify their lower order distributions. Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V ≈ 1 × 106 ) leading to fast training times. This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax ) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related wo"
D14-1158,W13-2201,0,\N,Missing
D14-1210,N10-1083,0,0.0851391,"Missing"
D14-1210,J07-2003,0,0.171693,"moments. We evaluate their performance on a Chinese–English translation task. The results indicate that we can achieve significant gains over the baseline with both approaches, but in particular the momentsbased estimator is both faster and performs better than EM. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) treat the translation problem as a context-free parsing problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to p"
D14-1210,N13-1015,1,0.849514,"milar, but in lieu of sparse feature functions in the spectral case, EM uses partial counts estimated with the current set of parameters. The nature of EM allows it to be susceptible to local optima, while the spectral approach comes with guarantees on obtaining the global optimum (Cohen et al., 2014). Lastly, computing the SVD and estimating parameters in the low-rank space is a one-shot operation, as opposed to the iterative procedure of EM, and therefore is much more computationally efficient. 4.1 Estimation with Spectral Method We generalize the parameter estimation algorithm presented in Cohen et al. (2013) to the syn5 We filtered rules with arity 3 and above (i.e., containing more than 3 NTs on the RHS). While the L-SCFG formalism is perfectly capable of handling such cases, it would have resulted in higher order tensors for our parameter structures. 1956 Inputs: Training examples (r(i) , t(i,1) , t(i,2) , t(i,3) , o(i) , b(i) ) for i ∈ {1 . . . M }, where r(i) is a context free rule; t(i,1) , t(i,2) , and t(i,3) are inside trees; o(i) is an outside tree; and b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function φ that maps inside trees t to feature-vectors φ(t) ∈ Rd . A function"
D14-1210,N13-1029,0,0.0166761,"thm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different derivations therefore their inside-outside algorithm is O(n6 ). In the syntax-directed translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question of whether we can incorpor"
D14-1210,2006.amta-papers.8,0,0.0329779,"r1 , . . . , rN together with values h1 , . . . , hN for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a • Arity 2 (binary) rules (R2 ): 3rd -order tensor"
D14-1210,D10-1014,0,0.0162481,"unsom et al. (2008) present a Bayesian model for synchronous grammar induction, and place an appropriate nonparametric prior on the parameters. However, their starting point is to estimate a synchronous grammar with multiple categories from parallel data (using the word alignments as a prior), while we aim to refine a fixed grammar with additional latent states. Furthermore, their estimation procedure is extremely expensive and is restricted to learning up to five NT categories, via a series of mean-field approximations. Another approach is to explicitly attach a realvalued vector to each NT: Huang et al. (2010) use an external source-language parser for this purpose and score rules based on the similarity between a source sentence parse and the information contained in this vector, which explicitly requires the integration of a good-quality source-language parser. The EM-based algorithm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different d"
D14-1210,W01-1812,0,0.0708033,"he right and ×0 when multiplying on the left of the matrix. The decoder computes marginal probabilities for each skeletal rule in the 1 This operation is sometimes called a contraction. parse forest of a source sentence by marginalizing over the latent states, which in practice corresponds to simple tensor-vector products. This operation is not dependent on the manner in which the parameters were estimated. Figure 1 presents the tensor version of the inside-outside algorithm for decoding L-SCFGs. The algorithm takes as input the parse forest of the source sentence represented as a hypergraph (Klein and Manning, 2001), which is computed using a bottom-up parser with Earley-style rules similar to the algorithm in Chiang (2007). Hypergraphs are a compact way to represent a forest of multiple parse trees. Each node in the hypergraph corresponds to an NT span, and can have multiple incoming and outgoing hyperedges. Hyperedges, which connect one or more tail nodes to a single head node, correspond exactly to rules, and tail or head nodes correspond to children (RHS NTs) or parent (LHS NT). The function B(q) returns all incoming hyperedges to a node q, i.e., all rules such that the LHS NT of the rule corresponds"
D14-1210,N03-1017,0,0.0451428,"n-generalizable rules are utilized. Hence, the hope is that this work pro1953 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1964, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics motes the move towards translation models that directly model the conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N , m, ns , nt , π, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set"
D14-1210,D12-1021,1,0.84686,"ng problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petro"
D14-1210,D07-1072,0,0.0262621,"owever, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petrov et al., 2006; Liang et al., 2007; Cohen et al., 2014), where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation. We first generalize the concept of latent PCFGs to latent-variable SCFGs (§2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Subsequently, two methods for parameter estimation are presented (§4): a spectral approach based on the method of moments, and an EM-based likelihood maximization. Results on a Chinese–English"
D14-1210,J06-4004,0,0.0963096,"Missing"
D14-1210,P11-1105,0,0.0177486,"0 ln(sum) 2 ln(sum)at word: Span starting 1 I ’ll bring it . Span starting at word: 0 I go away . I ’d like a shampoo and style . I ’d like a shampoo and style . I ’d like a shampoo and style . (d) MLE (e) Spectral m = 16 RI (f) EM m = 16 Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based pa"
D14-1210,P05-1010,0,0.835872,"ls that directly model the conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N , m, ns , nt , π, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set consists of only two symbols, X and a goal symbol S. • [m] is the set of possible hidden states associated with NTs. Aligned pairs of NTs across the source and target languages share the same hidden state. • [ns ] is the set of source side words, i.e., the source-side vocabulary, with [ns ] ∩ N = ∅. • [nt ] is the"
D14-1210,P10-4002,1,0.799568,"e upon a minimal grammar baseline with only a single category, but the spectral approach does better. In fact, it matches the performance of the standard H I ERO baseline, despite learning on top of a minimal grammar. 5.1 TRAIN (SRC) TRAIN (TGT) DEV (SRC) DEV (TGT) TEST (SRC) TEST (TGT) Data and Baselines The ZH-EN data is the BTEC parallel corpus (Paul, 2009); we combine the first and second development sets in one, and evaluate on the third development set. The development and test sets are evaluated with 16 references. Statistics for the data are shown in Table 1. We used the CDEC decoder (Dyer et al., 2010) to extract word alignments and the baseline hierarchical grammars, MERT tuning, and decoding. We used a 4-gram language model built from the target-side of the parallel training data. The Python-based implementation of the tensor-based decoder, as well as the parameter estimation algorithms is available at github.com/asaluja/spectral-scfg/. The baseline HIERO system uses a grammar extracted by applying the commonly used heurisZH-EN 334K 366K 7K 7.6K 3.8K 3.9K Table 1: Corpus statistics (in words). For the target DEV and TEST statistics, we take the first reference. tics (Chiang, 2007). Each r"
D14-1210,P11-1065,0,0.0309594,"Missing"
D14-1210,P13-1033,0,0.0115376,"tral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov models. This idea was extended to L-PCFGs (Cohen et al., 2014), and our approach can be seen as a bilingual or synchronous generalization. 7 Conclusion In this work, we presented an approach to refine synchronous grammars used in MT by inferring the laten"
D14-1210,N04-1035,0,0.144233,"Missing"
D14-1210,P06-1121,0,0.490662,"face statistics associated with the phrase pairs or rules. The weights of these features are then learned using a discriminative training algorithm (Och, 2003; Chiang, 2012, inter alia). In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making parameter estimation more tractable.3 To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word alignments, as opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applicable rules can be combinatorially larger than if we just consider the set of rules that cannot be formed from other rules, namely the minimal rules. The rule types across all sentence pairs are combined to form a minimal grammar.4 To extract a set of minimal rules, we use the linear-time extraction algorithm of Zhang et al. (2008). We give a rough description of their m"
D14-1210,J08-3004,0,0.013094,"ether with values h1 , . . . , hN for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a • Arity 2 (binary) rules (R2 ): 3rd -order tensor in the case of a binar"
D14-1210,P03-1021,0,0.131506,"composed of synchronous s-trees, which can be acquired from word alignments. Normally in phrase-based translation models, we consider all possible phrase 2 In practice, the term m3 |G |can be replaced with a smaller term, which separates the rules in G by the number of NTs on the RHS. This idea relates to the notion of “effective grammar size” which we discuss in §5. 1955 pairs consistent with the word alignments and estimate features based on surface statistics associated with the phrase pairs or rules. The weights of these features are then learned using a discriminative training algorithm (Och, 2003; Chiang, 2012, inter alia). In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making parameter estimation more tractable.3 To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word alignments, as opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees con"
D14-1210,P02-1040,0,0.091384,"me described in Matsuzaki et al. (2005), but found that it provided little benefit. 1958 procedure (Berg-Kirkpatrick et al., 2010) and we leave this extension for future work. 5 Experiments The goal of the experimental section is to evaluate the performance of the latent-variable SCFG in comparison to a baseline without any additional NT annotations (M IN -G RAMMAR), and to compare the performance of the two parameter estimation algorithms. We also compare L-SCFGs to a H IERO baseline (Chiang, 2007). The language pair of evaluation is Chinese–English (ZH-EN). We score translations using BLEU (Papineni et al., 2002). The latent-variable model is integrated into the standard MT pipeline by computing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in Figure 1 with the parameters estimated through the algorithms in Figure 2, and is added as a feature for the rule during MERT (Och, 2003). These probabilities are conditioned on the LHS (X), and are thus joint probabilities for a source-target RHS pair. We also write out as features the conditional relative frequencies Pˆ (e|f ) and Pˆ (f |e) as estimated by our latent-variable model, i.e., conditioned on the s"
D14-1210,P06-1055,0,0.894834,"2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petrov et al., 2006; Liang et al., 2007; Cohen et al., 2014), where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation. We first generalize the concept of latent PCFGs to latent-variable SCFGs (§2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Subsequently, two methods for parameter estimation are presented (§4): a spectral approach based on the method of moments, and an EM-based likelihood maximization. Results o"
D14-1210,P11-1086,0,0.0149769,"a shampoo and style . I ’d like a shampoo and style . I ’d like a shampoo and style . (d) MLE (e) Spectral m = 16 RI (f) EM m = 16 Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov"
D14-1210,N09-1027,0,0.0192305,"ser. The EM-based algorithm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different derivations therefore their inside-outside algorithm is O(n6 ). In the syntax-directed translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question"
D14-1210,J97-3002,0,0.377768,"problem as a context-free parsing problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique ha"
D14-1210,zhang-etal-2004-interpreting,0,0.0152996,"IERO baseline, but it only uses rules extracted from a minimal grammar, whose size is a fraction of the HIERO grammar. The gains seem to level off at this rank; additional ranks seem to add noise to the parameters. Feature-wise, additional lexical and length features add little, prob1960 ably because much of this information is encapsulated in the rule indicator features. For EM, m = 16 outperforms the minimal grammar baseline, but is not at the level of the spectral results. All EM, spectral, and MLE results are statistically significant (p < 0.01) with respect to the M IN G RAMMAR baseline (Zhang et al., 2004), and the improvement over the H IERO baseline achieved by the m = 16 rule indicator configuration is also statistically significant. The two estimation algorithms differ significantly in their estimation time. Given a feature covariance matrix, the spectral algorithm (SVD, which was done with Matlab, and correlation computation steps) for m = 16 took 7 minutes, while the EM algorithm took 5 minutes for each iteration with this rank. 5.4 Analysis Figure 3 presents a comparison of the nonterminal span marginals for two sentences in the development set. We visualize these differences through a h"
D14-1210,C08-1136,0,0.128119,"opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applicable rules can be combinatorially larger than if we just consider the set of rules that cannot be formed from other rules, namely the minimal rules. The rule types across all sentence pairs are combined to form a minimal grammar.4 To extract a set of minimal rules, we use the linear-time extraction algorithm of Zhang et al. (2008). We give a rough description of their method below, and refer the reader to the original paper for additional details. The algorithm returns a complete minimal derivation tree for each word-aligned sentence pair, and generalizes an approach for finding all common intervals (pairs of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase) between two permutations (Uno and Yagiura, 2000) to sequences with many-to-many alignment links between the two sides, as in word alignment. The key idea is to encode all phrase pairs of a sentence alignmen"
D14-1210,W06-3119,0,0.0804889,"Missing"
D14-1210,2009.iwslt-evaluation.1,0,\N,Missing
D15-1041,Q13-1034,0,0.0297403,"Missing"
D15-1041,D14-1082,0,0.480661,"m incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portuga"
D15-1041,P14-2111,0,0.0181423,"hment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar˜aes (2015) learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks. Our approach is similar to theirs. Others have used character-based models as features to improve existing models. For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. 6 Conclusion We have presented several interesting findings. First, we add new evidence t"
D15-1041,D07-1022,1,0.91229,"on practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags) • Words + POS: words and POS tags (§3"
D15-1041,W15-3904,0,0.0444823,"Missing"
D15-1041,P15-1033,1,0.075747,"ecially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parser of Dyer et al. (2015) can learn to generate nonprojective trees. We do this by augmenting its transition operations We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are s"
D15-1041,P11-2124,0,0.0240466,"orted (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zha"
D15-1041,W13-4907,1,0.945071,"on University, Pittsburgh, PA, USA ♣ Marianas Labs, Pittsburgh, PA, USA ♥ Computer Science & Engineering, University of Washington, Seattle, WA, USA miguel.ballesteros@upf.edu, chris@marianaslabs.com, nasmith@cs.washington.edu Abstract The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’s form often provides strong evidence regarding its grammatical role in morphologically rich languages (Ballesteros, 2013, inter alia), this has promise to improve accuracy and statistical efficiency relative to traditional approaches that treat each word type as opaque and independently modeled. In the traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in th"
D15-1041,Q13-1033,0,0.0347924,"sualization was produced using t-SNE; see http: //lvdmaaten.github.io/tsne/. word of the input sentence; however, at test time these results could be cached. On average, Words parses a sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons9 We are using a machine with 32 Intel Xeon CPU E52650 at 2.00GHz; the parser runs on a single core. 355 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features. For English and Chi"
D15-1041,W13-4916,0,0.0362755,"Missing"
D15-1041,P08-1043,0,0.0336093,"Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predict"
D15-1041,W14-6111,0,0.046747,"traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parse"
D15-1041,P13-1088,0,0.0104461,"te (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr (u, v) or gr (v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. Predicting Parser Decisions 3 The parser uses a probab"
D15-1041,P82-1020,0,0.877457,"Missing"
D15-1041,D10-1125,0,0.00949521,"t performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang"
D15-1041,seeker-kuhn-2012-making,0,0.013972,"se vectors and a (learned) representation of their tag to produce the representation w. As in §3.1, a linear map (V) is applied and passed through a component-wise ReLU. n o → ← x = max 0, V[w; w; t] + b sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean ´ (Choi, 2013), Polish (Swidzi´ nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in conte"
D15-1041,J93-2004,0,0.0644981,"98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped when the learned model’s UAS stops improving on the development set, and this model is used to parse the test set. No pretraining of a"
D15-1041,de-marneffe-etal-2006-generating,0,0.0548307,"Missing"
D15-1041,D13-1032,0,0.0302696,"Missing"
D15-1041,D13-1170,0,0.00309807,"s. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr (u, v) or gr (v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. Predicting Parser Decision"
D15-1041,W06-2933,0,0.0487182,"Missing"
D15-1041,nivre-etal-2006-talbanken05,0,0.0429632,"LU. n o → ← x = max 0, V[w; w; t] + b sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean ´ (Choi, 2013), Polish (Swidzi´ nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same Chinese and English This process is shown schematically in Figure 3"
D15-1041,W07-2218,0,0.0081179,"ges show that the parsing model benefits from incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro"
D15-1041,W04-0308,0,0.155705,"ng the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard parsing algorithm (Nivre, 2004). At each time step t, a transition action is applied that alters these data structures by pushing or popping words from the stack and the buffer; the operations are listed in Figure 1. Along with the discrete transitions above, the parser calculates a vector representation of the states of B, S, and A; at time step t these are denoted by bt , st , and at , respectively. The total parser state at t is given by pt = max {0, W[st ; bt ; at ] + d} it = σ(Wix xt + Wih ht−1 + Wic ct−1 + bi ) ft = 1 − it ct = ft ct−1 + it tanh(Wcx xt + Wch ht−1 + bc ) ot = σ(Wox xt + Woh ht−1 + Woc ct + bo ) ht = ot"
D15-1041,N03-1033,0,0.216746,"Missing"
D15-1041,P09-1040,0,0.427849,"stm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory (it ) or forget (ft ). We refer interested readers to the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt , the previous hidden state ht−1 , and the memory cell ct−1 : with a SWAP operation (Nivre, 2009) (§2.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard pars"
D15-1041,P06-3009,0,0.199759,"rs + POS. It is common practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags) • Words +"
D15-1041,P15-1032,0,0.60146,"oduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational"
D15-1041,P08-1101,0,0.0599117,"he SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped wh"
D15-1041,D08-1059,0,0.407672,"he SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped wh"
D15-1041,P13-1013,0,0.0124566,"2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a). To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper. tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), wh"
D15-1041,P15-1117,0,0.0362065,"gs of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Associat"
D15-1041,D15-1176,1,\N,Missing
D15-1041,W13-4917,0,\N,Missing
D15-1041,Q14-1017,0,\N,Missing
D15-1041,vincze-etal-2010-hungarian,0,\N,Missing
D15-1161,P14-2131,0,0.0497195,"uage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning words representations tha"
D15-1161,D14-1082,0,0.00523589,"local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextu"
D15-1161,P14-1129,0,0.0182429,"pic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the le"
D15-1161,D14-1012,0,0.0194663,"data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depends on both the syntactic relation from nicely, which narrows down the list of candidates to verbs, and on the semant"
D15-1161,P12-1092,0,0.059041,"e a large drop on this semantically oriented task. Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Co"
D15-1161,D13-1176,0,0.00900077,"g global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted wor"
D15-1161,D14-1108,1,0.740924,"ction words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently"
D15-1161,N15-1144,1,0.0648252,"ver, this model does not scale well as b increases as it requires V × dw more parameters for each new word in the window. Finally, setting a good value for b is difficult as larger values may introduce a degenerative behavior in the model, as more effort is spent predicting words that are conditioned on unrelated words, while smaller values of b may lead to cases where the window size is not large enough include words that are semantically related. For syntactic tasks, it has been shown that increasing the window size can adversely impact in the quality of the embeddings (Bansal et al., 2014; Lin et al., 2015). 2.2 CBOW with Attention We present a solution to these problems while maintaining the efficiency underlying the bag-ofwords model, and allowing it to consider contextual words within the window in a non-uniform way. We first rewrite the context window c as: X c= ai (wi )wi (2) i∈[−b,b]−{0} (1) where Oc corresponds to the projection of the context vector c onto the vocabulary V and v is a one-hot representation. For larger vocabularies it is inefficient to compute the normalizer P > v∈V exp v Oc. Solutions for problem are using the hierarchical softmax objective function (Mikolov et al., 2013"
D15-1161,N15-1142,1,0.386366,"isbon, Portugal Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning wo"
D15-1161,P14-1140,0,0.0283868,"Missing"
D15-1161,D07-1043,0,0.00495254,"embeddings in the domain of part-of-speech tagging in both supervised (Ling et al., 2015b) and unsupervised tasks (Lin et al., 2015). This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters. Part-of-speech induction. The work in (Lin et al., 2015) attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision. We use the same dataset, obtained from the ConLL 2007 shared task (Nivre et al., 2007) Scoring is performed using the V-measure (Rosenberg and Hirschberg, 2007), which is used to predict syntactic classes at the word level. It has been shown in (Lin et al., 2015) that word embeddings learnt from structured skip-ngrams tend to work better at this task, mainly because it is less sensitive to larger window sizes. These results are consistent with our observations found in Table 1, in rows “Skip-ngram” and “SSkip-ngram”. We can observe that our attention based CBOW model (row “CBOW Attention”) improves over these results for both tasks and also the original CBOW model (row “CBOW”). 1369 CBOW Skip-ngram SSkip-ngram CBOW Attention POS Induction 50.40 33.86"
D15-1161,D13-1170,0,0.00265235,"sented in (Ling et al., 2015b) using the same hyper-parameters. Results on the POS accuracy on the test set are reported on Table 1. We can observe our model can obtain similar results compared to the structured skip-ngram model on this task, while training the model is significantly faster. The gap between the usage of different embeddings is not as large as in POS induction, as this is a supervised task, where pre-training generally leads to smaller improvements. 3.3 Semantic Evaluation To evaluate the quality of our vectors in terms of semantics, we use the sentiment analysis task (Senti) (Socher et al., 2013), which is a binary classification task for movie reviews. We simply use the mean of the word vectors of words in a sentence, and use them as features in an `2 regularized logistic regression classifier. We use the standard training/dev/test split and report accuracy on the test set in table 1. We can see that in this task, our models do not perform as well as the CBOW and Skipngram model, which hints that our model is learning embeddings that learn more towards syntax. This is expected as it is generally uncommon for embeddings to outperform existing models on both syntactic and semantic task"
D15-1161,P10-1040,0,0.138833,"better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depen"
D15-1161,D15-1176,1,\N,Missing
D15-1161,D07-1096,0,\N,Missing
D15-1176,afonso-etal-2002-floresta,0,0.054909,"., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. eat embedings for words words1 .This 5 cats NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous representations and the same hyperparametrization as in language modeling (Section 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for characters and the wor"
D15-1176,D15-1041,1,0.487282,"eling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic units to derive the embeddings for the word. On POS tagging, our models using characters alone can still achieve comparable or better results than state-of-the-art systems, without the need to manually engineer such lexical features. Although both language modeling and POS tagging both benefit strongly from morphological cues, the success of our models in languages with impover"
D15-1176,D13-1008,1,0.162387,"susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regul"
D15-1176,N15-1142,1,0.116424,"y are not found labelled in the training data, the model would be dependent on context to determine their tags, which could lead to errors in ambiguous contexts. Unsupervised training methods such as the Skip-n-gram model (Mikolov et al., 2013) can be used to pretrain the word representations on unannotated corpora. If such pretraining places cat, dog and snake near each other in vector space, and the supervised POS data contains evidence that cat and dog are nouns, our model will be likely to label snake with the same tag. We train embeddings using English wikipedia with the dataset used in (Ling et al., 2015), and the Structured Skip-n-gram model. Results using pre-trained word lookup tables and the C2W with the pre-trained word lookup tables as additional parameters are shown in rows “word(sskip)” and “C2W + word(sskip)”. We can observe that both systems can obtain improvements over their random initializations (rows “word” and (C2W)). Finally, we also found that when using the C2W model in conjunction pre-trained word embeddings, that adding a non-linearity to the representations extracted from the C2W model eC w improves the results over using a simple linear trans+feat no no yes yes yes yes no"
D15-1176,P14-1140,0,0.0196322,"tors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly with other tasks with"
D15-1176,W13-3512,0,0.437991,"are not independent. The wellknown “past tense debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have rel"
D15-1176,D14-1082,0,0.0447531,"Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly with other tasks with large amounts of data an"
D15-1176,J93-2004,0,0.0572265,"ls Experiments: Part-of-speech Tagging As a second illustration of the utility of our model, we turn to POS tagging. As morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering features (Nakagawa et al., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. eat embedings for words words1 .This 5 cats NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous r"
D15-1176,D09-1078,0,0.0117625,"from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from b"
D15-1176,D13-1176,0,0.0372417,"onclude in Section 7. 2 Word Vectors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly wit"
D15-1176,kang-choi-2000-automatic,0,0.0359805,"tion according to the lexical model. Convolutional model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be r"
D15-1176,D15-1278,0,0.049515,"Missing"
D15-1176,D12-1088,1,0.613165,"Missing"
D15-1176,D13-1032,0,0.0412355,"Missing"
D15-1176,P11-2009,0,0.0797599,"Missing"
D15-1176,N15-1186,0,0.0289151,"se debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively regular effects, many words with lexical"
D15-1176,E09-1087,0,0.00853859,"Missing"
D15-1176,W13-2515,0,0.0187849,"al model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence ou"
D15-1176,D12-1089,0,0.00506125,"xically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic un"
D15-1176,D14-1179,0,\N,Missing
D15-1176,W03-2405,0,\N,Missing
D15-1243,P12-1015,0,0.0404941,"i et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus. The third dataset is SimLex-999 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Eac"
D15-1243,J90-1003,0,0.350901,"which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the vector space.5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ Uk ΣVk T (Landauer and Dumais, 1997). 3 https://code.google.com/p/word2vec 4 https://github.com/wlin12/wang2vec 5 http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semant"
D15-1243,W06-1670,0,0.0317102,"Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0."
D15-1243,P14-5004,1,0.611603,"sis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classifier. Finally, we evaluate vectors on the metaphor detection (Metaphor) (Tsvetkov et al., 6 https://github.com/mfaruqui/ retrofitting 7 We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014). 8 http://qwone.com/~jason/20Newsgroups 2051 2014a).9 The system uses word vectors as features in a random forest classifier to label adjective-noun pairs as literal/metaphoric. We report the system accuracy in 5-fold cross validation. 5 Results To test the efficiency of QVEC in capturing the semantic content of word vectors, we evaluate how well QVEC’s scores correspond to the scores of word vector models on semantic benchmarks. We compute the Pearson’s correlation coefficient r to quantify the linear relationship between the scorings. We begin with comparison of QVEC with one extrinsic task"
D15-1243,P15-2076,1,0.309428,"Missing"
D15-1243,N15-1184,1,0.864444,"Missing"
D15-1243,N13-1092,0,0.0368039,"Missing"
D15-1243,D14-1012,0,0.0362549,"of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequent"
D15-1243,J15-4004,0,0.243621,"Missing"
D15-1243,N15-1142,1,0.522248,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,H93-1061,0,0.140934,"our model obtains high correlation (0.34 ≤ r ≤ 0.89) with the extrinsic tasks (§5). 2 Linguistic Dimension Word Vectors The crux of our evaluation method lies in quantifying the similarity between a distributional word vector model and a (gold-standard) linguistic re2049 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb le"
D15-1243,I08-2105,0,0.0125447,"thods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0.67 ··· VB . MOTI"
D15-1243,D14-1162,0,0.112475,"Missing"
D15-1243,D15-1036,0,0.233502,"Missing"
D15-1243,N13-1076,1,0.805767,"Missing"
D15-1243,D13-1170,0,0.0376388,"ed lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequently, it is not clear how to score a non-interp"
D15-1243,P14-1024,1,0.829115,"Missing"
D15-1243,tsvetkov-etal-2014-augmenting-english,1,0.809802,"Missing"
D15-1243,P10-1040,0,0.162082,"f word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at htt"
D15-1243,P14-1074,0,0.0118254,"9 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with Yogatama and Smith (2014). For example, a classification task is between two categories of Sports: baseball vs hockey. We report the average classification accuracy across the four tasks. Our next downstream semantic task is the sentiment analysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classif"
D15-1243,D13-1196,0,0.0612772,"nexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguisti"
D15-1243,D15-1161,1,0.0792052,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,P14-2131,0,\N,Missing
D15-1284,W09-2004,0,0.0176664,"cation performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorou"
D15-1284,P11-2016,0,0.0936674,"ics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they"
D15-1284,N15-1070,1,0.765615,"ing distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5 https://code.google.com/p/word2vec/ We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. 2369 6 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense"
D15-1284,P12-2030,0,0.0360941,"stances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generation, there are few studies"
D15-1284,H05-1067,0,0.36465,"d phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they learned statistical patterns of text in N-gra"
D15-1284,P15-1070,0,0.0619138,"ncongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5 , we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5 https://code.google.com/p/word2vec/ We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. 2369 6 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whol"
D15-1284,W06-1625,0,0.213259,"n for Computational Linguistics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognitio"
D15-1284,N12-2012,0,0.0706261,"First, a universal definition of humor is hard to achieve, because different people hold different understandings of even the same sentence. Second, humor is always situated in a broader context that sometimes requires a lot of external knowledge to fully understand it. For example, consider the sentence, “The one who invented the door knocker got a No Bell prize” and “Veni, Vidi, Visa: I came, I saw, I did a little shopping”. One needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor (Raz, 2012), such as wordplay, irony and sarcasm, but there exist few formal taxonomies of humor characteristics. Thus it is almost impossible to design a general algorithm that can classify all the different types of humor, since even human cannot perfectly classify all of them. Although it is impossible to understand universal humor characteristics, one can still capture the possible latent structures behind humor (Bucaria, 2004; Binsted and Ritchie, 1997). In this work, we uncover several latent semantic structures behind humor, in terms of meaning incongruity, ambiguity, phonetic style and personal a"
D15-1284,W05-1614,0,0.109392,"e high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devot"
D15-1284,P05-3029,0,0.0413941,"r positive instances will have high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and g"
D15-1284,N03-1033,0,0.0277565,"occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger (Toutanova et al., 2003) to identify Noun, Verb, Adj, Adv. Then we consider the possible meanings of such words {w1 , w2 · · · wk } via WordNet and Qkcalculate the sense combinations as log( i=1 nwi ). nwi is the total number of senses of word wi . • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, hu"
D15-1284,P06-1134,0,0.0105404,"ds {w1 , w2 · · · wk } via WordNet and Qkcalculate the sense combinations as log( i=1 nwi ). nwi is the total number of senses of word wi . • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive)"
D15-1284,H05-1044,0,0.031023,"tility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive) Polarity: the number of occurrences of all Negative (Positive) words. 7 Path Similarity: http://www.nltk.org/howto/ wordnet.html • Weak (Strong) Subjectivity: the number of occurrences of all Weak (Strong) Subjectivity oriented words in a sentence. It is the linguistic expression of people’s opinions, evaluations, beliefs or speculations. 4.4 Phonetic Style Many humorous texts play with sounds, creating incongruous sound"
D15-1284,P12-1074,0,\N,Missing
D16-1116,D15-1138,0,0.0747526,"Missing"
D16-1116,P13-2009,0,0.013306,"bes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution"
D16-1116,Q13-1005,0,0.117977,"rsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address dat"
D16-1116,D14-1134,0,0.0245851,"Missing"
D16-1116,P14-1133,0,0.0415207,"onment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first appro"
D16-1116,P16-1004,0,0.63439,"e prediction of a query on the G EO corpus which is a frequently used benchmark for semantic parsing. The corpus contains 880 questions about US geography together with executable queries representing those questions. We follow the approach established by Zettlemoyer and Collins (2005) and split the corpus into 600 training and 280 test cases. Following common practice, we augment the dataset by referring to the database during training and test time. In particular, we use the database to identify and anonymise variables (cities, states, countries and rivers) following the method described in Dong and Lapata (2016). Most prior work on the G EO corpus relies on standard semantic parsing methods together with custom heuristics or pipelines for this corpus. The recent paper by Dong and Lapata (2016) is of note, as it uses a sequence-to-sequence model for training which is the unidirectional equivalent to S2S, and also to the decoder part of our S EQ 4 network. 3.2 Open Street Maps The second task we tackle with our model is the NL MAPS dataset by Haas and Riezler (2016). The dataset contains 1,500 training and 880 testing instances of natural language questions with corresponding machine readable queries o"
D16-1116,N04-1035,0,0.0239608,"n the higher percentages then too little unsupervised data to meaningfully improve the model. 6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016"
D16-1116,N16-1088,0,0.19687,"etrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in Σx from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step. These serve as continuous relaxations of discrete samples, providing a differentiable estimator of the expected reconstruction log likelihood. We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the G EO Q UERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks. In most settings, the semi-supervised model outperforms the supervised model, both when trained on additional generated data as well as on subsets of the existing data. 1078 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078–1087, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Dataset Example G EO what are the high points of states surrounding mississip"
D16-1116,P16-1002,0,0.406241,"objects, etc). Paths within that maze are created by randomly sampling start and end positions. 4 Model Experiments We evaluate our model on the three tasks in multiple settings. First, we establish a supervised baseline to compare the S2S model with prior work. Next, we 5 Our randomly generated unsupervised datasets can be downloaded from http://deepmind.com/ publications 1082 Accuracy Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) Liang et al. (2013) Kwiatkowski et al. (2011) Zhao and Huang (2014) Kwiatkowski et al. (2013) 79.3 86.1 87.9 88.6 88.9 89.0 Dong and Lapata (2016) Jia and Liang (2016)6 84.6 89.3 S2S S EQ 4 86.5 87.3 Table 2: Non-neural and neural model results on G EO Q UERY using the train/test split from (Zettlemoyer and Collins, 2005). train our S EQ 4 model in a semi-supervised setting on the entire dataset with the additional monomodal training data described in the previous section. Finally, we perform an “ablation” study where we discard some of the training data and compare S2S to S EQ 4. S2S is trained solely on the reduced data in a supervised manner, while S EQ 4 is once again trained semi-supervised on the same reduced data plus the machine readable part of the"
D16-1116,C12-1083,1,0.256175,"nsupervised data to meaningfully improve the model. 6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free gram"
D16-1116,D12-1040,0,0.0355422,"Missing"
D16-1116,P13-1022,0,0.0405195,"Missing"
D16-1116,D11-1140,0,0.063695,"6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), wh"
D16-1116,D13-1161,0,0.036316,"Missing"
D16-1116,P11-1060,0,0.0545052,"and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to dracon"
D16-1116,J13-2005,0,0.0543262,"Missing"
D16-1116,Q16-1017,0,0.0270309,"from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to draconian assumptions (Ammar et al., 2014) to make marginalisation tractable. While our model is not exactly marginalisable either, the continuous relaxation makes training far more tractable. A related idea was recently presented in G¨ulc¸ehre et al. (2015), who use monolingual data to improve machine translation by fusing a sequence-to-sequence model and a language model. 7 Conclusion We described a method for augmenting a supervised sequence t"
D16-1116,Q14-1030,0,0.0168947,"all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. Th"
D16-1116,N06-1056,0,0.581285,"inalisation, we introduce a novel differentiable alternative for draws from a softmax which can be used with the reparametrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in Σx from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step. These serve as continuous relaxations of discrete samples, providing a differentiable estimator of the expected reconstruction log likelihood. We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the G EO Q UERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks. In most settings, the semi-supervised model outperforms the supervised model, both when trained on additional generated data as well as on subsets of the existing data. 1078 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078–1087, c Austin, Texas, November 1-5, 2016. 201"
D16-1116,D07-1071,0,0.82963,"Missing"
D16-1116,P15-1109,0,0.0223581,"tive model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms. 1 Introduction Neural approaches, in particular attention-based sequence-to-sequence models, have shown great promise and obtained state-of-the-art performance for sequence transduction tasks including machine translation (Bahdanau et al., 2015), syntactic constituency parsing (Vinyals et al., 2015), and semantic role labelling (Zhou and Xu, 2015). A key requirement for effectively training such models is an abundance of supervised data. In this paper we focus on learning mappings from input sequences x to output sequences y in domains where the latter are easily obtained, but annotation in the form of (x, y) pairs is sparse or expensive to produce, and propose a novel architecture that accommodates semi-supervised training on sequence transduction tasks. To this end, we augment the transduction objective (x 7→ y) with an autoencoding objective where the input sequence is treated as a latent variable (y 7→ x 7→ y), enabling training fr"
D16-1116,N16-1160,0,\N,Missing
D16-1124,P14-2023,0,0.0264448,"ons of a standard neural model (or δ distributions) trained on PTB, and count based distributions trained on PTB, WSJ, and GW are added one-by-one using the standard static and proposed LSTM interpolation methods. From the results, we can see that when only PTB data is used, the methods have similar results, but with the more diverse data sets the proposed method edges out its static counterpart.7 7 In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015). Interp Lin. LSTM δ+PTB 95.1 95.3 +WSJ 70.5 68.3 +GW 65.8 63.5 Table 4: PTB perplexity for interpolation between neural (δ) LMs and count-based models. 7 Related Work Acknowledgements A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G¨ulc¸ehre et al., 2015). Perhaps most relevant is Hsu (2007)’s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities αn"
D16-1124,N15-1083,0,0.0194106,"999) and adapt them based on the distribution of the current document, albeit in a linear model. There has also been work incorporating binary n-gram features into neural language models, which allows for more direct learning of ngram weights (Mikolov et al., 2011), but does not afford many of the advantages of the proposed model such as the incorporation of count-based probability estimates. Finally, recent works have compared ngram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015). 8 neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al., 2015). In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions. Conclusion and Future Work In this paper, we proposed a framework for language modeling that generalizes both neural network and count-based n-gram LMs. This allowed us to learn more effective interpolation functions for count-based n-grams, and to create ne"
D16-1124,D07-1090,0,0.0149178,"Dyer‡ † Carnegie Mellon University, USA ‡ Google DeepMind, United Kingdom Abstract and Goodman, 1996). Recently, there has been a focus on LMs based on neural networks (Nakamura et al., 1990; Bengio et al., 2006; Mikolov et al., 2010), which have shown impressive improvements in performance over count-based LMs. On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs (Chen et al., 2015; Williams et al., 2015) are trained on a fraction of the data of their count-based counterparts (Brants et al., 2007). In this paper we focus on a class of LMs, which we will call mixture of distributions LMs (MODLMs; §2). Specifically, we define MODLMs as all LMs that take the following form, calculating the probabilities of the next word in a sentence wi given preceding context c according to a mixture of several component probability distributions Pk (wi |c): Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability an"
D16-1124,N03-2003,0,0.0524999,"ts the proposed method edges out its static counterpart.7 7 In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015). Interp Lin. LSTM δ+PTB 95.1 95.3 +WSJ 70.5 68.3 +GW 65.8 63.5 Table 4: PTB perplexity for interpolation between neural (δ) LMs and count-based models. 7 Related Work Acknowledgements A number of alternative methods focus on interpolating LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G¨ulc¸ehre et al., 2015). Perhaps most relevant is Hsu (2007)’s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities αn (c) in Eq. 3 instead of λ(c), and does not cover interpolation of n-gram components, non-linearities, or the connection with neural network LMs. Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999"
D16-1124,P96-1041,0,0.606559,"dly used language modeling paradigm is that of count-based LMs, usually smoothed n-grams (Witten and Bell, 1991; Chen 1 Work was performed while GN was at the Nara Institute of Science and Technology and CD was at Carnegie Mellon University. Code and data to reproduce experiments is available at http://github.com/neubig/modlm P (wi |c) = K X k=1 λk (c)Pk (wi |c). (1) Here, λk (c) is a function that defines P the mixture weights, with the constraint that K k=1 λk (c) = 1 for all c. This form is not new in itself, and widely used both in the calculation of smoothing coefficients for n-gram LMs (Chen and Goodman, 1996), and interpolation of LMs of various varieties (Jelinek and Mercer, 1980). The main contribution of this paper is to demonstrate that depending on our definition of c, λk (c), and Pk (wi |c), Eq. 1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and 1163 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1163–1172, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network"
D16-1124,H92-1020,0,0.479706,"ing LMs of multiple varieties such as in-domain and out-of-domain LMs (Bulyko et al., 2003; Bacchiani et al., 2006; G¨ulc¸ehre et al., 2015). Perhaps most relevant is Hsu (2007)’s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities αn (c) in Eq. 3 instead of λ(c), and does not cover interpolation of n-gram components, non-linearities, or the connection with neural network LMs. Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities (Della Pietra et al., 1992; Kneser and Steinbiss, 1993; Rosenfeld, 1996; Iyer and Ostendorf, 1999) and adapt them based on the distribution of the current document, albeit in a linear model. There has also been work incorporating binary n-gram features into neural language models, which allows for more direct learning of ngram weights (Mikolov et al., 2011), but does not afford many of the advantages of the proposed model such as the incorporation of count-based probability estimates. Finally, recent works have compared ngram and neural models, finding that neural models often perform better in perplexity, but n-grams"
D16-1124,P11-2005,0,0.0174869,"rpolated LMs by normalizing the discounted distribution: i−1 PN D (wi |wi−n+1 ) = PJ i−1 PD (wi |wi−n+1 ) j=1 PD (wi i−1 = j|wi−n+1 ) , which allows us to replace β(·) for α(·) and PN D (·) for PM L (·) in Eq. 3, and proceed as normal. Kneser–Ney (KN; Kneser and Ney (1995)) and Modified KN (Chen and Goodman, 1996) smoothing further improve discounted LMs by adjusting the counts of lower-order distributions to more closely match their expectations as fallbacks for higher order distributions. Modified KN is currently the defacto standard in n-gram LMs despite occasional improvements (Teh, 2006; Durrett and Klein, 2011), and we will express it as PKN (·). 3.2 Neural LMs as Mixtures of Distributions In this section we demonstrate how neural network LMs can also be viewed as an instantiation of the MODLM framework. Feed-forward neural network LMs: Feedforward LMs (Bengio et al., 2006; Schwenk, 2007) are LMs that, like n-grams, calculate the probability of the next word based on the previous i−1 words. Given context wi−N +1 , these words are converted into real-valued word representation veci−1 tors ri−N +1 , which are concatenated into an overi−1 all representation vector q = ⊕(ri−N +1 ), where ⊕(·) is the vec"
D16-1124,C90-3038,0,0.318275,"at http://github.com/neubig/modlm P (wi |c) = K X k=1 λk (c)Pk (wi |c). (1) Here, λk (c) is a function that defines P the mixture weights, with the constraint that K k=1 λk (c) = 1 for all c. This form is not new in itself, and widely used both in the calculation of smoothing coefficients for n-gram LMs (Chen and Goodman, 1996), and interpolation of LMs of various varieties (Jelinek and Mercer, 1980). The main contribution of this paper is to demonstrate that depending on our definition of c, λk (c), and Pk (wi |c), Eq. 1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and 1163 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1163–1172, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (§3). This observation is useful theoretically, as it provides a single mathematical framework that encompasses several widely used classes of LMs. It is also useful practically, in that this new view of these traditional models allows us to create new models that combine the desirable"
D16-1124,N15-1020,0,0.0319479,"al language models, which allows for more direct learning of ngram weights (Mikolov et al., 2011), but does not afford many of the advantages of the proposed model such as the incorporation of count-based probability estimates. Finally, recent works have compared ngram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrinsic tasks (Baltescu and Blunsom, 2015) and better modeling of rare words (Chen et al., 2015). 8 neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al., 2015). In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions. Conclusion and Future Work In this paper, we proposed a framework for language modeling that generalizes both neural network and count-based n-gram LMs. This allowed us to learn more effective interpolation functions for count-based n-grams, and to create neural LMs that incorporate information from count-based models. As the framework discussed here is general, it is also possible that they could be used in other ta"
D16-1153,W02-2004,0,0.0260502,"Missing"
D16-1153,W03-0423,0,0.0343727,"posed. Despite the noisy supervision provided in the target language, transferring from Turkish and Uzbek provides a +14.1 F1 improvement over a state of the art monolingual model trained on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Mo"
D16-1153,W14-4012,0,0.0384515,"Missing"
D16-1153,W99-0612,0,0.0745531,"target language, transferring from Turkish and Uzbek provides a +14.1 F1 improvement over a state of the art monolingual model trained on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very"
D16-1153,W15-3904,0,0.0447429,"Missing"
D16-1153,W03-0425,0,0.103213,"nsfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016)."
D16-1153,P12-1073,0,0.0342719,"imilar architecture to Lample et al. (2016), replacing the LSTMs with Gated Recurrent Units (Cho et al., 2014). However, Yang et al. (2016) also tackle multi task and multi-lingual joint training scenarios. Most of the models cited so far are monolingual either because they use hand crafted features and language specific resources or because of deepseated assumptions. For example a change in orthography, lexicon, script, word order or addition of complex morphology makes transfer impossible. This is the central challenge that we tackle. There has been much less work catering to this scenario. Kim et al. (2012) use weak annotations from Wikipedia metadata and parallel data for multi lingual NER. Yang et al. (2016) addresses the use case of multilingual joint training, which assumes there is sufficient data available in all languages. Nothman et al. (2013) also operate under the assumption of availability of Wikipedia data. To the best of our knowledge, a scenario involving transfer of a model trained in one (or more) source language(s) to another language with little or no labeled data, different script, different morphology, different lexicon, lack of transliteration, non-mutual intelligibility etc"
D16-1153,W03-0428,0,0.0902775,"trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The follow"
D16-1153,N16-1030,1,0.138311,"or engineering features have been developed. A more challenging task is to design a model that retains competence in monolingual scenarios and can easily be transferred to a low resource language with minimum overhead in terms of data annotation requirements and feature engineering. This transfer setting introduces additional challenges such as varying character usage conventions across languages with same script, differing scripts, lack of NE transliteration, varying morphology, different lexicons and mutual non-intelligibility to name a few. We propose the following changes over prior work (Lample et al., 2016) to address the challenges of the low-resource transfer setting. We use: 1. Language universal phonological character representations instead of orthographic ones. 2. Attention over characters of a word while labeling it with an NE category. 1462 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1462–1472, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Figure 1: Attentional LSTM-CRF architecture. li denotes the encoding of a word and its left context (forward LSTM) while ri includes only right context (backward LS"
D16-1153,P09-1116,0,0.0162238,"1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a monolingual focus. Collobert et al. (20"
D16-1153,N15-1142,1,0.780835,"put sequence X is: Y ∗ = arg max S(X, Y ) (3) Y ∈YX Normally, evaluating the partition function over the exponential space of all possible labelings would be intractable. However, as described in (Lafferty et al., 2001), this can be done efficiently for linear chain CRFs using the forward backward algorithm. 2.4 Word Representations The inputs to our model are in the form of type level word representations (figure 2). Motivated by the distributional hypothesis (Harris, 1954; Firth, 1957) we use word embeddings as inputs. In the monolingual scenario, we use structured skipgram word embeddings (Ling et al., 2015a). For the transfer scenario, embeddings can optionally be trained using techniques like multi CCA described in (Ammar et al., 2016). By learning a linear transformation from a shared vector space between languages, the model may acquire some transfer capability to the target language. We use character bi-LSTMs to handle the Out Of Vocabulary (OOV) problem as in (Ling et al., 2015b). However, just as a distributional hypothesis exists for words, prior work (Tsvetkov and Dyer, 2015; Tsvetkov et al., 2015) suggests phonological character representations capture inherent similarities between cha"
D16-1153,L16-1529,1,0.78301,"al representations enables model transfer from one or more source languages to a target language with no extra effort, even when the languages use different scripts. We demonstrate that while attention over characters of words has marginal utility in monolingual and high resource settings, it greatly improves the statistical efficiency of the model in 0-shot and low resource transfer settings. We do require a mapping from a language’s script to phonological feature space which is script specific and not task specific. This presents little or no overhead due to existence of tools like PanPhon (Littell et al., 2016). 2 Our Approach Figure 1 provides a high level overview of our model. We model the words of a sentence at the type level and the token level. At the type level (ignorant of sentential context), we use bidirectional character LSTMs as in figure 2 to compose characters of a word to obtain its word representation and concatenate this with a word embedding that captures distributional semantics. This can memorize entities or capture morphological and suffixal clues 1463 Figure 2: Type level word representations - l denotes a word prefix encoding (by forward char LSTM) while r denotes a word suffi"
D16-1153,P16-1101,0,0.0170303,"lorian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a monolingual focus. Collobert et al. (2011), Turian et al. (2010), and Ando and Zhang (2005) use uns"
D16-1153,W03-0430,0,0.142459,"incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features w"
D16-1153,W09-1119,0,0.0627792,", semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a"
D16-1153,W02-2024,0,0.69861,"Missing"
D16-1153,N15-1062,1,0.920448,"ul type level phenomena during inference and improves the statistical efficiency of the model in certain scenarios. Having described our intuitions, we now provide mathematical details of our model. 2.1 Figure 3: Use of Epitran and PanPhon to enable transfer across orthographies in Uyghur (Perso-Arabic script) and Turkish (Latin script), thus making the equivalence across scripts apparent. We concatenate the feature vectors from PanPhon and 1-hot encodings of the corresponding IPA characters and use these as inputs to the character bi-LSTMs. This shift to IPA space is motivated by prior work (Tsvetkov et al., 2015; Tsvetkov and Dyer, 2015) which demonstrated the value of projecting orthographic surface forms of words into a phonological space for detecting loan words, transliteration and cognates even in language pairs that exhibit significant typological, morphological and phonological differences. Our underlying assumption is that named entities are likely to be transliterated or retain pronunciation patterns across languages. Additionally, phenomena such as vowel harmony manifest explicitly in IPA representation and can potentially be helpful for NER. Foreign named entities for example, need not obe"
D16-1153,P10-1040,0,0.0188753,"ple et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a monolingual focus. Collobert et al. (2011), Turian et al. (2010), and Ando and Zhang (2005) use unsupervised features in conjunction with engineered features capturing capitalization, character categories and gazetteer matches. Collobert et al. (2011) use a Convolutional Neural Network (CNN) over the sequence of word embeddings. Huang et al. (2015) instead use bi-directional LSTMs over the sequence of words, along with spelling and orthographic features. The most recent work eliminates feature engineering altogether and combines CRFs with LSTMs which can model long sequences while remembering appropriate past context. Lample et al. (2016) proposed an archi"
D16-1153,C96-1071,0,0.110928,"on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin a"
D16-1180,P16-1231,0,0.391211,"ing standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding consensus. We address"
D16-1180,D15-1041,1,0.89892,"Missing"
D16-1180,D16-1211,1,0.803959,"Missing"
D16-1180,D12-1133,0,0.212269,"ensemble pipeline. Accuracy. All scores are shown in Table 5. First, consider the neural FOG parser trained with Hamming cost (CH in the second-to-last row). This is a very strong benchmark, outperforming many higherorder graph-based and neural network models on all three datasets. Nonetheless, training the same model with distillation cost gives consistent improvements for all languages. For English, we see that this model comes close to the slower ensemble it was trained to simulate. For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre (2012) for LAS. Effects of Pre-trained Word Embedding. As an ablation study, we ran experiments on English without pre-trained word embedding, both with the Hamming and distillation costs. The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS, compared to 93.6 UAS and 91.1 LAS for the model with distillation cost. This result further showcases the consistent improvements from using the distillation cost across different settings and languages. We conclude that “soft targets” derived from ensemble uncertainty offer useful guidance, through the distillation cost function and discriminativ"
D16-1180,D14-1082,0,0.22734,"ized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neu"
D16-1180,P15-1033,1,0.267775,"sentropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the"
D16-1180,C96-1058,0,0.199002,"ing function that considers only a single dependency arc at a time. (We suppress dependency labels; there are various ways to incorporate them, discussed later.) To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm ; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003). Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the t"
D16-1180,N10-1112,1,0.815815,"POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors. Second, 6 Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experiment"
D16-1180,D16-1139,0,0.0603475,"ensemble training to re-lexicalize a transfer model for a target language. We similarly use an ensemble of models as a supervision for a sin10 Our cost is zero when the correct arc is predicted, regardless of what the soft target thinks, something a compression model without gold supervision cannot do. gle model. By incorporating the ensemble uncertainty estimates in the cost function, our approach is cheaper, not requiring any marginalization during training. An additional difference is that we learn from the gold labels (“hard targets”) rather than only ensemble estimates on unlabeled data. Kim and Rush (2016) proposed a distillation model at the sequence level, with application in sequence-to-sequence neural machine translation. There are two primary differences with this work. First, we use a global model to distill the ensemble, instead of a sequential one. Second, Kim and Rush (2016) aim to distill a larger model into a smaller one, while we propose to distill an ensemble instead of a single model. 8 Conclusions We demonstrate that an ensemble of 20 greedy stack LSTMs (Dyer et al., 2015) can achieve state of the art accuracy on English dependency parsing. This approach corresponds to minimum Ba"
D16-1180,Q16-1023,0,0.116166,"w cost function, inspired by the notion of “soft targets” (Hinton et al., 2015). The essential idea is to derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse"
D16-1180,P10-1001,0,0.0124705,"o derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples (h, m, `), where h is the index of a head, m the index of"
D16-1180,D14-1081,0,0.0194869,"he ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples (h, m, `), where h is the index of a head, m the index of a modifier, and ` a dependency label (or rel"
D16-1180,N15-1142,1,0.446195,"oves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library.8 Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncert"
D16-1180,C12-2077,0,0.0564198,"Missing"
D16-1180,D08-1017,1,0.760634,"cost—and use it in discriminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parse"
D16-1180,P13-2109,1,0.895869,"Missing"
D16-1180,P05-1012,0,0.876112,"(h, m, `), where h is the index of a head, m the index of a modifier, and ` a dependency label (or relation type). Most dependency parsers are constrained to return y that form a directed tree. A first-order graph-based (FOG; also known as “arc-factored”) dependency parser exactly solves y ˆ(x) = arg max y∈T (x) X s(h, m, x), (h,m)∈y | {z S(y,x) (1) } where T (x) is the set of directed trees over x, and s is a local scoring function that considers only a single dependency arc at a time. (We suppress dependency labels; there are various ways to incorporate them, discussed later.) To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm ; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based par"
D16-1180,H05-1066,0,0.452646,"Missing"
D16-1180,P08-1108,0,0.0151135,"criminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike mod"
D16-1180,W03-3017,0,0.0218202,"016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003). Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the time of this writing employ neural networks (Andor et al., 2016). 1 https://github.com/adhigunasurya/ distillation_parser.git 1745 Let hy (m) denote the parent of xm in y (using a special null symbol when m is the root of the tree), and hy0 (m) denotes the parent of xm in the predicted tree y 0 . Given two dependency parses of"
D16-1180,N10-1003,0,0.026416,"denoted with an underline. The † sign indicates the use of predicted tags for Chinese in the original publication, although we report accuracy using gold Chinese tags based on private correspondence with the authors. ered a FOG parser, though future work might investigate any parser amenable to training to minimize a cost-aware loss like the structured hinge. 7 Related Work Our work on ensembling dependency parsers is based on Sagae and Lavie (2006) and Surdeanu and Manning (2010); an additional contribution of this work is to show that the normalized ensemble votes correspond to MBR parsing. Petrov (2010) proposed a similar model combination with random initializations for phrase-structure parsing, using products of constituent marginals. The local optima in his base model’s training objective arise from latent variables instead of neural networks (in our case). Model distillation was proposed by Bucilˇa et al. (2006), who used a single neural network to simulate a large ensemble of classifiers. More recently, Ba and Caruana (2014) showed that a single shal1751 low neural network can closely replicate the performance of an ensemble of deep neural networks in phoneme recognition and object dete"
D16-1180,N06-2033,0,0.0848776,"ble, N = 10, MST ensemble, N = 15, MST ensemble, N = 20, MST Consensus and Minimum Bayes Risk Despite the recent success of neural network dependency parsers, most prior works exclusively report single-model performance. Ensembling neural network models trained from different random starting points is a standard technique in a variety of problems, such as machine translation (Sutskever et al., 2014) and constituency parsing (Vinyals et al., 2015). We aim to investigate the benefit of ensembling independently trained neural network dependency parsers by applying the parser ensembling method of Sagae and Lavie (2006) to a collection of N strong neural network base parsers. Here, each base parser is an instance of the greedy, transition-based parser of Dyer et al. (2015), known as the stack LSTM parser, trained from a different random initial estimate. Given a sentence x, the consensus FOG parser (Eq. 1) defines score s(h, m, x) as the number of base parsers that include the attachment (h, m), which we denote votes(h, m).2 An example of this scoring function with an ensemble of 20 models is shown in Figure 1 We assign to dependency (h, m) the label most frequently selected by the base parsers that attach m"
D16-1180,P06-2101,0,0.0302359,"nd compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors. Second, 6 Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2"
D16-1180,N10-1091,0,0.261328,"ndencies version 3.3.0 (De Marneffe and Manning, 2008) Penn Treebank task. As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015).3 Table 1 shows that ensembles, even with small N , strongly outperform a single stack LSTM parser. Our ensembles of greedy, locally normalized parsers perform comparably to the best previously reported, due to Andor et al. (2016), which uses a beam (width 32) for training and decoding. 4 What is Ensemble Uncertainty? While previous works have already demonstrated the merit of ensembling in dependency parsing (Sagae and Lavie, 2006; Surdeanu and Manning, 2010), usually with diverse base parsers, we consider whether the posterior marginals estimated by pˆ((h, m) ∈ Y |x) = votes(h, m)/N can be interpreted. We conjecture that disagreement among base parsers about where to attach xm (i.e., uncertainty in the posterior) is a sign of difficulty or am3 We use the standard data split (02–21 for training, 22 for development, 23 for test), automatically predicted partof-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https:// github.com/clab/lstm-parser, each with a different random initialization; this dif"
D16-1180,N13-1126,0,0.0166262,"Missing"
D16-1180,N03-1033,0,0.0210996,"ate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library.8 Since the ensemble is used to obtain the uncertainty on the training set,"
D16-1180,P16-1218,0,0.223292,"notion of “soft targets” (Hinton et al., 2015). The essential idea is to derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a"
D16-1180,P15-1032,0,0.0231937,"ns required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding"
D16-1180,C02-1145,0,0.117563,"isk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data a"
D16-1180,K15-1015,0,0.0197952,"Missing"
D16-1180,D08-1059,0,0.0578633,"single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike model combination techniqu"
D16-1180,P11-2033,0,0.0443312,"Missing"
D16-1180,P15-1112,0,0.0208299,"Missing"
D16-1202,Q15-1008,0,0.366058,"Missing"
D16-1202,W11-0611,0,0.0317273,"suggests that colors and words are associated in the brain. The brain uses different regions to perceive various modalities, but processing a color word activates the same brain region as the color it denotes (del Prado Mart´ın et al., 2006; Simmons et al., 2007). Closer to NLP, the relationship between visual stimuli and their linguistic descriptions by humans has been explored extensively through automatic text generation from images (Kiros et al., 2014; Karpathy and Fei-Fei, 2014; Xu et al., 2015). Color association with word semantics has also been investigated in several previous papers (Mohammad, 2011; Heer and Stone, 2012; Andreas and Klein, 2014; McMahan and Stone, 2015). 7 Conclusion In this paper, we introduced a computational model to predict a point in color space from the sequence of characters in the color’s name. Using a large set of color–name pairs obtained from a color design forum, we evaluate our model on a “color Turing test” and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. We also investigate the reverse mapping, from colors to names. We compare a conditional LSTM language model to a new latent-vari"
D16-1202,D16-1243,0,0.205224,"Missing"
D16-1202,W14-1607,0,\N,Missing
D16-1211,P16-1231,0,0.489171,".94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87 90.75 88.14 91.44 89.29 93.22 91.23 German UAS LAS 88.09 85.24 88.56 86.15 89.80 87.29 90.34 88.17 89.6 86.0 89.12 86.95 90.91 89.15 Japanese UAS LAS 93.10 92.28 – – 93.47 92.70 – – – – 93.71 92.85 93.65 92.84 Spanish UAS LAS 89.08 85.03 90.76 87.48 89.53 85.69 91.09 87.95 88.3 85.4 91.01 88.14 92.62 89.95 Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudoprojective parsing. Y’15 and A’16 are beam = 1 parsers from Yazdani and Henderson (2015) and Andor et al. (2016), respectively. A’16-beam is the parser with beam larger than 1 by Andor et al. (2016). Bold numbers indicate the best results among the greedy parsers. The error-exploring dynamic-oracle training always improves over static oracle training controlling for the transition system, but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Ando"
D16-1211,W15-2210,0,0.0477838,"Missing"
D16-1211,P05-1022,0,0.0279624,"English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87"
D16-1211,D14-1082,0,0.316369,"urns out to be very beneficial for the configurations that make use of external embeddings. Indeed, these configurations achieve high accuracies and sharp class distributions early on in the training process. The parser is trained to maximize the likelihood of a correct action zg at each parsing state pt according to Equation 1. When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is the marginal likelihood of all correct actions,3 X p(zg |pt ) = p(zgi |pt ). (3) zgi ∈zg 3 Experiments Following the same settings of Chen and Manning (2014) and Dyer et al (2015) we report results4 in the English PTB and Chinese CTB-5. Table 1 shows the results of the parser in its different configurations. The table also shows the best result obtained with the static oracle (obtained by rerunning Dyer et al. parser) for the sake of comparison between static and dynamic training strategies. Method Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) + pre-training: Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66"
D16-1211,P15-1033,1,0.660448,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are typically stored in a stack data structure, and the words remaining to be processed. to capture information from the entirety of the state, without resorting to locality assumptions that were common in most other transition-based par"
D16-1211,C12-1059,1,0.947246,"ces, given words. At test time, the parser makes greedy decisions according to the learned model. Although this setup obtains very good performance, the training and testing conditions are mismatched in the following way: at training time the historical context of an action is always derived from the gold standard (i.e., perfectly correct past actions), but at test time, it will be a model prediction. In this work, we adapt the training criterion so as to explore parser states drawn not only from the training data, but also from the model as it is being learned. To do so, we use the method of Goldberg and Nivre (2012; 2013) to dynamically chose an optimal (relative to the final attachment accuracy) action given an imperfect history. By interpolating between algorithm states sampled from the model and those sampled from the training data, more robust predictions at test time can be made. We show that the technique can be used to improve the strong parser of Dyer et al. 2 Parsing Model and Parameter Learning Our departure point is the parsing model described by Dyer et al. (2015). We do not describe the model in detail, and refer the reader to the original work. At each stage t of the parsing process, the p"
D16-1211,Q13-1033,1,0.93704,"ing with Exploration Improves a Greedy Stack LSTM Parser Miguel Ballesteros♦ Yoav Goldberg♣ Chris Dyer♠ Noah A. Smith♥ ♦ NLP Group, Pompeu Fabra University, Barcelona, Spain ♣ Computer Science Department, Bar-Ilan University, Ramat Gan, Israel ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA, USA miguel.ballesteros@upf.edu, yoav.goldberg@gmail.com, cdyer@google.com, nasmith@cs.washington.edu Abstract We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Mats"
D16-1211,Q14-1010,1,0.946318,"if the best tree that can be reached after taking the action is no worse (in terms of accuracy with respect to the gold tree) than the best tree that could be reached prior to taking that action. Goldberg and Nivre (2013) define the arcdecomposition property of transition systems, and show how to derive efficient dynamic oracles for transition systems that are arc-decomposable.2 Unfortunately, the arc-standard transition system does 2 Specifically: for every parser configuration p and group of not have this property. While it is possible to compute dynamic oracles for the arc-standard system (Goldberg et al., 2014), the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack. As the dynamic oracle has to be queried for each parser state seen during training, the use of this dynamic oracle will make the training runtime several times longer. We chose instead to switch to the arc-hybrid transition system (Kuhlmann et al., 2011), which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O(1) dynamic oracle, resulting in only negligible increase to training runtime. We implemented the dynamic oracle to the arc-hybrid s"
D16-1211,W13-5709,1,0.953811,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The framework of training with exploration using dynamic oracles suggested by Goldberg and Nivre (2012; 2013) provides answers to these questions. While the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take for any valid parser state. In contrast to static oracles that derive a canonical state sequence for each gold parse tree and say nothing about states that deviate from this canonical path, the dynamic oracle is well defined for states that result from parsing mistakes, and they may produce more than a single gold action for a given state. Under the dynamic oracle framework, an action is said to be optimal for a state if the best tree that can be reached afte"
D16-1211,P15-2042,0,0.0776288,"Missing"
D16-1211,D14-1099,0,0.0889626,"Missing"
D16-1211,Q14-1011,0,0.0127702,"rained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; R"
D16-1211,W13-3518,1,0.840178,"clude results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012"
D16-1211,P11-1068,0,0.126938,"Missing"
D16-1211,P05-1013,0,0.0596932,"rc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Andor et al., 2016)5 we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task (Hajiˇc et al., 2009). Since some of the treebanks contain nonprojective sentences and archybrid does not allow nonprojective trees, we use the pseudo-projective approach (Nivre and Nilsson, 2005). We used predicted part-of-speech tags provided by the CoNLL 2009 shared task organizers. We also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several resea"
D16-1211,W03-3017,0,0.353357,"ssuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous de"
D16-1211,W04-0308,0,0.52657,"ror-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (some"
D16-1211,J08-4003,0,0.0983095,"on history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called t"
D16-1211,P00-1061,0,0.278952,"brid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59"
D16-1211,P15-3004,0,0.0262076,"Missing"
D16-1211,Q16-1014,0,0.0220862,"nish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training"
D16-1211,W03-3023,0,0.18219,"nd Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection o"
D16-1211,K15-1015,0,0.032838,"ize 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training regimens that make the learned model more robust to test-time prediction errors. Solutions based on curriculum learning (Bengio et al., 2015), expected loss training (Shen et al., 2015), and reinforcement learning have been proposed (Ranzato et al., 2016). Finally, abandoning greedy search in favor of approximate global search offers an alternative solution to the problems with greedy search (Andor et al., 2016), and has been analyzed as well (Kulesza and Pereira"
D16-1254,P16-1231,0,0.0514973,"cts a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure. Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence. In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions. Recent models include the greedy StackLSTM model of Dyer et al. (2015) and the globally normalized feed-forward networks of Andor et al. (2016). These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers. One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013). When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams. Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of"
D16-1254,D12-1133,0,0.0602576,"Missing"
D16-1254,P13-1104,0,0.290773,"the space of possible paths to the predicted tree is an important component of high-accuracy parsers. One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013). When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams. Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of the highest-scoring beam. Another approach to solving this issue is given by Choi and McCallum (2013). They introduced selectional branching, which involves performing an initial greedy parse, and then using confidence estimates on each prediction to spawn additional beams. Relative to standard beam-search, this reduces the average number of predictions required to parse a sentence, resulting in a speed-up. In this paper, we introduce heuristic backtracking, which expands on the ideas of selectional branching by integrating a search strategy based on a heuristic function (Pearl, 1984): a function which estimates 2313 Proceedings of the 2016 Conference on Empirical Methods in Natural Language"
D16-1254,P04-1015,0,0.742986,", we look at all explored nodes that still have at least one unexplored child, and choose the node with the lowest heuristic confidence (see Section 3.2). We rewind our stack, buffer, and action history to that state, and execute the highest-scoring transition from that node that has not yet been explored. At this point, we are again in a fully-unexplored node, and can greedily parse just as before until we reach another leaf. Once we have generated b leaves, we score them all and return the transition sequence leading up to the highest-scoring leaf as the answer. Just as in previous studies (Collins and Roark, 2004), we use the n11 n12 n13 n14 ... n1l n22 n23 n24 ... n32 n33 n34 n42 n43 n44 n11 n12 n13 n14 ... n1l n2l n22 n23 n24 ... n2l ... n3l n32 n34 ... n3l ... n4l n42 (a) Beam Search n11 n4l (b) Dynamic Beam Search n12 n13 n14 ... n1l n22 n23 n24 ... n2l n33 n34 ... n44 ... n11 n12 n13 n14 ... n1l n22 n23 n24 ... n2l n3l n34 ... n3l n4l n44 ... n4l (c) Selectional Branching (d) Heuristic Backtracking Figure 1: Visualization of various decoding algorithms sum of the log probabilities of all individual transitions as the overall score for the parse. 3.2 We do this in the following way: H(nji ) = (V (n"
D16-1254,P15-1033,1,0.933825,"arsing, one of the most prominent dependency parsing techniques, constructs a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure. Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence. In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions. Recent models include the greedy StackLSTM model of Dyer et al. (2015) and the globally normalized feed-forward networks of Andor et al. (2016). These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers. One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013). When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams. Common methods for pruning are removing all beams below some"
D16-1254,W04-0308,0,0.333279,"the search early if we have found an answer that we believe to be the gold parse, saving time proportional to the number of backtracks remaining. We compare the performance of these various decoding algorithms with the Stack-LSTM parser (Dyer et al., 2015), and achieve slightly higher accuracy than beam search, in significantly less time. the valid transition actions that may be taken in the current state. The objective function is: 2 3.1 Transition-Based Parsing With Stack-LSTM Our starting point is the model described by Dyer et al. (2015).1 The parser implements the arc-standard algorithm (Nivre, 2004) and it therefore makes use of a stack and a buffer. In (Dyer et al., 2015), the stack and the buffer are encoded with Stack-LSTMs, and a third sequence with the history of actions taken by the parser is encoded with another Stack-LSTM. The three encoded sequences form the parser state pt defined as follows, pt = max {0, W[st ; bt ; at ] + d} , (1) where W is a learned parameter matrix, bt , st and at are the stack LSTM encoding of buffer, stack and the history of actions, and d is a bias term. The output pt (after a component-wise rectified linear unit (ReLU) nonlinearity (Glorot et al., 2011"
D16-1254,P15-1032,0,0.0242667,"Missing"
D16-1254,K15-1015,0,0.0545119,"Missing"
D16-1254,D08-1059,0,0.0323662,"Missing"
D16-1254,P11-2033,0,0.173805,"Missing"
D16-1254,P15-1117,0,0.124944,"Missing"
D17-1197,P16-1154,0,0.231329,"an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. Figure 1 depicts examples of REs in the context of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe gener"
D17-1197,P16-1014,0,0.0547892,"Missing"
D17-1197,N10-1061,0,0.034711,"orks on task oriented dialogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly. Recipe generation was proposed in (Kiddon et al., 2016). They use attention mechanism over the checklists, whereas our work models explicit references to them. Context dependent language models (Mikolov et al., 2010; Jozefowicz et al., 2016; Mikolov et al., 2010; Ji et al., 2015; Wang and Cho, 2015) are proposed to capture long term dependency of text. There are also lots of works on coreference resolution (Haghighi and Klein, 2010; Wiseman et al., 2016). We are the first to combine coreference with language modeling, to the best of our knowledge. 5 Conclusion We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three applications, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore fu"
D17-1197,D16-1032,0,0.126535,"made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. Figure 1 depicts examples of REs in the context of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Wi"
D17-1197,D16-1127,0,0.0690347,"text of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Williams and Zweig, 2016; Shang et al., 2015; Wen et al., 2016). Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system 1850 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850–1859 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics says “the nirala is a nice restaurant”,"
D17-1197,P16-1057,1,0.749091,"Missing"
D17-1197,P15-1002,0,0.0118737,"think…Go ahead [Linda]2 … thanks goes to [you]1 … Figure 1: Reference-aware language models. Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of na"
D17-1197,P15-1152,0,0.0333048,"Missing"
D17-1197,N15-1020,0,0.0273631,"rst, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Williams and Zweig, 2016; Shang et al., 2015; Wen et al., 2016). Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system 1850 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850–1859 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics says “the nirala is a nice restaurant”, it refers to the restaurant name the nirala from the database."
D17-1197,D15-1199,0,0.0573329,"ontext coref [I]1 think…Go ahead [Linda]2 … thanks goes to [you]1 … Figure 1: Reference-aware language models. Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key"
D17-1197,N16-1114,0,0.015614,"ogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly. Recipe generation was proposed in (Kiddon et al., 2016). They use attention mechanism over the checklists, whereas our work models explicit references to them. Context dependent language models (Mikolov et al., 2010; Jozefowicz et al., 2016; Mikolov et al., 2010; Ji et al., 2015; Wang and Cho, 2015) are proposed to capture long term dependency of text. There are also lots of works on coreference resolution (Haghighi and Klein, 2010; Wiseman et al., 2016). We are the first to combine coreference with language modeling, to the best of our knowledge. 5 Conclusion We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three applications, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore further based on our fram"
D18-1412,S07-1018,0,0.0955337,"meNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference. For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material. Frame SRL. Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold. We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007). Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; T¨ackstr¨om et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015). The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017). In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer. In their relational"
D18-1412,P98-1013,0,0.599303,"ssification among (i) noun phrase (or prepositional phrase, for frame SRL); (ii) any other category; and (iii) null. In Figure 1, for the span “encouraging them”, the constituent identity scaffold label is 1, the nonterminal label is S|VP, the non-terminal and parent label is S|VP+par=PP, and the common nonterminals label is set to OTHER. 3 Semantic Role Labeling We contribute a new SRL model which contributes a strong baseline for experiments with syntactic scaffolds. The performance of this baseline itself is competitive with state-of-the-art methods (§7). FrameNet. In the FrameNet lexicon (Baker et al., 1998), a frame represents a type of event, situation, or relationship, and is associated with a set of semantic roles, called frame elements. A frame can be evoked by a word or phrase in a sentence, called a target. Each frame element of an evoked frame can then be realized in the sentence as a sentential span, called an argument (or it can be unrealized). Arguments for a given frame do not overlap. (2) where wc is a parameter vector associated with category c. We sum the log loss terms for all the spans in a sentence to give its loss: X L2 (x, z) = − log p(zi: j |xi: j ). (3) 4.2 5 In the OntoNote"
D18-1412,P17-1110,0,0.015502,"es stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task"
D18-1412,D16-1245,0,0.0211932,"(or, by extension, a larger vocabulary), though that might be a factor for frame SRL. A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017). This, along with other recent approaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL. Coreference. We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3. Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Con"
D18-1412,P16-1061,0,0.0192322,"(or, by extension, a larger vocabulary), though that might be a factor for frame SRL. A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017). This, along with other recent approaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL. Coreference. We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3. Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Con"
D18-1412,P17-1044,1,0.303465,"e on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models"
D18-1412,P18-1192,0,0.120004,"rmance by 1 F1 , and our common nonterminal scaffold by 3.1 F1 .6 Prec. Rec. F1 Kshirsagar et al. (2015) Yang and Mitchell (2017) (Rel) Yang and Mitchell (2017) (Seq) †Yang and Mitchell (2017) (All) 66.0 71.8 63.4 70.2 60.4 57.7 66.4 60.2 63.1 64.0 64.9 65.5 Semi-CRF baseline 67.8 66.2 67.0 68.1 68.8 69.4 69.2 67.4 68.2 68.0 69.0 67.7 68.5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2"
D18-1412,copestake-flickinger-2000-open,0,0.0377689,"nformation into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better gen"
D18-1412,S12-1029,1,0.796445,"sible labeled segmentations. The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function: X αj = αi−1 exp{Ψ(s, x) + cost(s, s∗ )}, (7) s=hi, j,yi: j i j−i6D where Z = αn , under the base case α0 = 1. The prediction under the model can be calculated using a similar dynamic program with the following recurrence where γ0 = 1: γ j = max γi−1 exp Ψ(s, x). s=hi, j,yi: j i j−i6D (8) Our model formulation enforces that arguments do not overlap. We do not enforce any other SRL constraints, such as non-repetition of core frame elements (Das et al., 2012). 5.3 Input Span Representation This section describes the neural architecture used to obtain the span embedding, vi: j , corresponding to a span xi: j and the target in consideration, t = htstart , tend i. For the scaffold task, since the syntactic treebank does not contain annotations for semantic targets, we use the last verb in the sentence as a placeholder target, wherever target features are used. If there are no verbs, we use the first token in the sentence as a placeholder target. The parameters used to learn v are shared between the tasks. We construct an embedding for the span using"
D18-1412,W06-1673,0,0.0377993,"sting alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization (or some approximation to it) in order to reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyan"
D18-1412,J13-4006,0,0.0237581,"mation to it) in order to reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T 2 , and it need not overlap the T 1 training data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic pars"
D18-1412,P18-1035,0,0.0286481,"ant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebank D2 with sentences x, each with a phrase-structure tree z. 4.1 Loss Each task has an associated loss, and we seek to minimize the combination of task losses, X X L1 (x, y)"
D18-1412,P15-2036,1,0.898483,"SRL. Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold. We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007). Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; T¨ackstr¨om et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015). The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017). In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer. In their relational model (Rel), they treat the same problem as a span classification problem. Finally, they introduce an ensemble to integerate both models, and use an integer linear program for inference satisfying SRL constraints. Though their model does not do any syntactic pruning, it does use syntactic features for argument identification and labeling.5 Notably"
D18-1412,D15-1112,0,0.0644176,"Missing"
D18-1412,D17-1018,1,0.481129,"sks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models (§2). Our approach"
D18-1412,J02-3001,0,0.475993,"nimizes an auxiliary supervised loss function, derived from a syntactic treebank. The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a val"
D18-1412,N18-2108,1,0.84095,"rate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1 . All the above are orthogonal to our approach, and could be incorporated to yield higher gains. 8 Discussion To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline. We consider a breakdown of the performance by the syntactic phrase types of the arguments, provided in FrameNet7 in Figure 2. Not surpris7 We"
D18-1412,P02-1031,0,0.06036,"g, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost"
D18-1412,Q13-1018,0,0.0760573,"Missing"
D18-1412,N10-1112,1,0.71015,"arameters of the semi-CRF are learned to maximize a criterion related to the conditional loglikelihood of the gold-standard segments in the training corpus (§5.2). The learner evaluates and adjusts segment scores ψ(sk , x) for every span in the sentence, which in turn involves learning embedded representations for all spans (§5.3). 5.2 Softmax-Margin Objective Typically CRF and semi-CRF models are trained to maximize a conditional log-likelihood objective. In early experiments, we found that incorporating a structured cost was beneficial; we do so by using a softmax-margin training objective (Gimpel and Smith, 2010), a “cost-aware” variant X L1 = − (x,s∗ )∈D1 Z(x, s∗ ) = X log exp Ψ(s∗ , x) , Z(x, s∗ ) exp {Ψ(s, x) + cost(s, s∗ )}. (4) (5) s We design the cost function so that it factors by predicted span, in the same way Ψ does: X X cost(s, s∗ ) = cost(s, s∗ ) = I(s &lt; s∗ ). (6) s∈s s∈s The softmax-margin criterion, like log-likelihood, is globally normalized over all of the exponentially many possible labeled segmentations. The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function: X αj = αi−1 exp{Ψ(s, x) + cost(s, s∗ )}, (7) s=hi, j,y"
D18-1412,D17-1206,0,0.0301315,"ods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task’s outputs y (semantic ro"
D18-1412,D13-1152,0,0.0594194,"Missing"
D18-1412,P18-2058,1,0.905144,"rmance by 1 F1 , and our common nonterminal scaffold by 3.1 F1 .6 Prec. Rec. F1 Kshirsagar et al. (2015) Yang and Mitchell (2017) (Rel) Yang and Mitchell (2017) (Seq) †Yang and Mitchell (2017) (All) 66.0 71.8 63.4 70.2 60.4 57.7 66.4 60.2 63.1 64.0 64.9 65.5 Semi-CRF baseline 67.8 66.2 67.0 68.1 68.8 69.4 69.2 67.4 68.2 68.0 69.0 67.7 68.5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2"
D18-1412,W08-2124,0,0.0676864,"Missing"
D18-1412,D12-1074,0,0.084146,"Missing"
D18-1412,P10-1142,0,0.0212275,"from a syntactic treebank. The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never nee"
D18-1412,J05-1004,0,0.492254,"r, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never need to run a syntactic parsing algorithm. Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for two SRL tasks (§5) and coreference resolution (§6). Our models use the strongest ava"
D18-1412,P17-1186,1,0.932782,"n As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models (§2). Our approach, the syntactic scaf"
D18-1412,Q15-1003,0,0.0552312,"Missing"
D18-1412,N18-1135,1,0.842326,"5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2 Clark and Manning (2016a) 79.2 70.4 74.6 66.8 57.0 61.5 71.0 56.5 63.0 69.9 58.0 63.4 62.1 53.9 57.7 63.8 54.3 58.7 63.5 55.5 59.2 64.2 65.3 65.7 Lee et al. (2017) 78.4 73.4 75.8 68.6 61.8 65.0 62.7 59.0 60.8 67.2 + common nonterminals 78.4 74.3 76.3 68.7 62.9 65.7 62.9 60.2 61.5 67.8 Model MUC Prec. Rec. F1 Table 3: Coreference resolution r"
D18-1412,D14-1162,0,0.0809139,"still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. Additionally, these methods explore architectural variations in RNN layers for including supervision, whereas we focus on incorporating supervision with minimal changes to the baseline architecture. To the best of our knowledge, such simplified syntactic scaffolds have not been tried before. Word embeddings. Our definition of a scaffold task almost includes stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tas"
D18-1412,N18-1202,1,0.82975,"here is only one task about whose performance we are concerned, denoted T 1 (in this paper, T 1 is either SRL or coreference resolution). We use the term “scaffold” to refer to a second task, T 2 , that can be combined with T 1 during multitask learning. A scaffold task is only used during training; it holds no intrinsic interest beyond biasing the learning of T 1 , and after learning is completed, the scaffold is discarded. A syntactic scaffold is a task designed to steer the (shared) model toward awareness of syntactic 1 This excludes models initialized with deep, contextualized embeddings (Peters et al., 2018), an approach orthogonal to ours. structure. It could be defined through a syntactic parser that shares some parameters with T 1 ’s model. Since syntactic parsing is costly, we use simpler syntactic prediction problems (discussed below) that do not produce whole trees. As with multitask learning in general, we do not assume that the same data are annotated with outputs for T 1 and T 2 . In this work, T 2 is defined using phrase-structure syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013). We experiment with three settings: one where the corpus for T 2 does"
D18-1412,W12-4501,0,0.144135,"Missing"
D18-1412,J08-2005,0,0.160547,"jective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We"
D18-1412,D16-1264,0,0.0416351,"d syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never need to run a syntactic parsing algorithm. Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for"
D18-1412,J08-2002,0,0.054317,"Missing"
D18-1412,N16-1114,0,0.073824,"pared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T 1 and T 2 output. 3 Related Work We briefly contrast the syntactic scaffold with existing alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization (or some approximation"
D18-1412,D17-1128,0,0.427202,"SRL and coreference). Compared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T 1 and T 2 output. 3 Related Work We briefly contrast the syntactic scaffold with existing alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization"
D18-1412,P16-1147,0,0.0316632,"raining data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with. While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. A"
D18-1412,P15-1109,0,0.217767,"petitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural"
D18-1412,P16-2038,0,0.0317744,"mantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with. While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. Additionally, these methods explore architectural v"
D18-1412,K16-1019,1,0.877792,"o reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T 2 , and it need not overlap the T 1 training data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech"
D19-1419,P08-1115,0,0.0326284,"etworks is an extremely challenging task, and that scaling complete and incomplete methods to large models remains an open challenge. Representations of Combinatorial Spaces. Word lattices and hypergraphs are data structures that have often been used to efficiently represent and process exponentially large numbers of sentences without exhaustively enumerating them. Applications include automatic speech recognition (ASR) output rescoring (Liu et al., 2016), machine translation of ASR outputs (Bertoldi et al., 2007), paraphrase variants (Onishi et al., 2010), and word segmentation alternatives (Dyer et al., 2008). The specifications used to characterise the space of adversarial attacks are likewise a compact representation, and the algorithms discussed below operate on them without exhaustive enumeration. 4084 3 Methodology 3.2 We assume a fixed initial vector representation z0 of a given input sentence z 1 (e.g. the concatenation of pretrained word embeddings) and use a neural network model, i.e. a series of differentiable transformations hk : zk = hk (zk 1) k = 1, . . . , K 3.1 Verification Verification is the process of examining whether the output of a model satisfies a given specification. Formal"
D19-1419,P18-2006,0,0.675706,"ation violation (e.g., prediction changes). Deep models have been shown to be vulnerable against adversarial input perturbations (Szegedy et al., 2013; Kurakin et al., 2016). Small, semantically invariant input alterations can lead to drastic changes in predictions, leading to poor performance on adversarially chosen samples. Recent work (Jia and Liang, 2017; Belinkov and Bisk, 2018; Ettinger et al., 2017) also exposed the vulnerabilities of neural NLP models, e.g. with small § logits Upper bounds Introduction ‡ FC great event ReLU Abstract Conv {j.welbl}@cs.ucl.ac.uk character perturbations (Ebrahimi et al., 2018) or paraphrases (Ribeiro et al., 2018; Iyyer et al., 2018). These adversarial attacks highlight often unintuitive model failure modes and present a challenge to deploying NLP models. Common attempts to mitigate the issue are adversarial training (Ebrahimi et al., 2018) and data augmentation (Belinkov and Bisk, 2018; Li et al., 2017), which lead to improved accuracy on adversarial examples. However, this might cause a false sense of security, as there is generally no guarantee that stronger adversaries could not circumvent defenses to find other successful attacks (Carlini and Wagner, 2017; Ath"
D19-1419,W17-5401,0,0.0266626,"Missing"
D19-1419,N13-1092,0,0.0234753,"Missing"
D19-1419,P18-2103,0,0.0141732,"of this work: i) limited model depth: we only investigated models with few layers. IBP bounds are likely to become looser as the number of layers increases. ii) limited model types: we only studied models with CNN and fully connected layers. 4090 We focused on the HotFlip attack to showcase specification verification in the NLP context, with the goal of understanding factors that impact its effectiveness (e.g. the perturbation space volume, see Section 4.6). It is worth noting that symbol substitution is general enough to encompass other threat models such as lexical entailment perturbations (Glockner et al., 2018), and could potentially be extended to the addition of pre/postfixes (Jia and Liang, 2017; Wallace et al., 2019). Interesting directions of future work include: tightening IBP bounds to allow applicability to deeper models, investigating bound propagation in other types of neural architectures (e.g. those based on recurrent networks or self-attention), and exploring other forms of specifications in NLP. 6 Conclusion We introduced formal verification of text classification models against synonym and character flip perturbations. Through experiments, we demonstrated the effectiveness of the prop"
D19-1419,N18-1170,0,0.19593,"been shown to be vulnerable against adversarial input perturbations (Szegedy et al., 2013; Kurakin et al., 2016). Small, semantically invariant input alterations can lead to drastic changes in predictions, leading to poor performance on adversarially chosen samples. Recent work (Jia and Liang, 2017; Belinkov and Bisk, 2018; Ettinger et al., 2017) also exposed the vulnerabilities of neural NLP models, e.g. with small § logits Upper bounds Introduction ‡ FC great event ReLU Abstract Conv {j.welbl}@cs.ucl.ac.uk character perturbations (Ebrahimi et al., 2018) or paraphrases (Ribeiro et al., 2018; Iyyer et al., 2018). These adversarial attacks highlight often unintuitive model failure modes and present a challenge to deploying NLP models. Common attempts to mitigate the issue are adversarial training (Ebrahimi et al., 2018) and data augmentation (Belinkov and Bisk, 2018; Li et al., 2017), which lead to improved accuracy on adversarial examples. However, this might cause a false sense of security, as there is generally no guarantee that stronger adversaries could not circumvent defenses to find other successful attacks (Carlini and Wagner, 2017; Athalye et al., 2018; Uesato et al., 2018). Rather than conti"
D19-1419,D17-1215,0,0.323933,"t “great event” that is propagated through a model. At each layer, this shape deforms itself, but can be bounded by axis-parallel bounding boxes, which are propagated similarly. Finally, in logit space, we can compute an upper bound on the worst-case specification violation (e.g., prediction changes). Deep models have been shown to be vulnerable against adversarial input perturbations (Szegedy et al., 2013; Kurakin et al., 2016). Small, semantically invariant input alterations can lead to drastic changes in predictions, leading to poor performance on adversarially chosen samples. Recent work (Jia and Liang, 2017; Belinkov and Bisk, 2018; Ettinger et al., 2017) also exposed the vulnerabilities of neural NLP models, e.g. with small § logits Upper bounds Introduction ‡ FC great event ReLU Abstract Conv {j.welbl}@cs.ucl.ac.uk character perturbations (Ebrahimi et al., 2018) or paraphrases (Ribeiro et al., 2018; Iyyer et al., 2018). These adversarial attacks highlight often unintuitive model failure modes and present a challenge to deploying NLP models. Common attempts to mitigate the issue are adversarial training (Ebrahimi et al., 2018) and data augmentation (Belinkov and Bisk, 2018; Li et al., 2017), wh"
D19-1419,E17-2004,0,0.0735112,"(Jia and Liang, 2017; Belinkov and Bisk, 2018; Ettinger et al., 2017) also exposed the vulnerabilities of neural NLP models, e.g. with small § logits Upper bounds Introduction ‡ FC great event ReLU Abstract Conv {j.welbl}@cs.ucl.ac.uk character perturbations (Ebrahimi et al., 2018) or paraphrases (Ribeiro et al., 2018; Iyyer et al., 2018). These adversarial attacks highlight often unintuitive model failure modes and present a challenge to deploying NLP models. Common attempts to mitigate the issue are adversarial training (Ebrahimi et al., 2018) and data augmentation (Belinkov and Bisk, 2018; Li et al., 2017), which lead to improved accuracy on adversarial examples. However, this might cause a false sense of security, as there is generally no guarantee that stronger adversaries could not circumvent defenses to find other successful attacks (Carlini and Wagner, 2017; Athalye et al., 2018; Uesato et al., 2018). Rather than continuing the race with adversaries, formal verification (Baier and Katoen, 2008; Barrett and Tinelli, 2018; Katz et al., 2017) offers a different approach: it aims at providing provable guarantees to a given model specification. In the case of adversarial robustness, such a spec"
D19-1419,N16-1018,0,0.031467,"Missing"
D19-1419,P10-2001,0,0.0120493,"sification. We highlight that the verification of neural networks is an extremely challenging task, and that scaling complete and incomplete methods to large models remains an open challenge. Representations of Combinatorial Spaces. Word lattices and hypergraphs are data structures that have often been used to efficiently represent and process exponentially large numbers of sentences without exhaustively enumerating them. Applications include automatic speech recognition (ASR) output rescoring (Liu et al., 2016), machine translation of ASR outputs (Bertoldi et al., 2007), paraphrase variants (Onishi et al., 2010), and word segmentation alternatives (Dyer et al., 2008). The specifications used to characterise the space of adversarial attacks are likewise a compact representation, and the algorithms discussed below operate on them without exhaustive enumeration. 4084 3 Methodology 3.2 We assume a fixed initial vector representation z0 of a given input sentence z 1 (e.g. the concatenation of pretrained word embeddings) and use a neural network model, i.e. a series of differentiable transformations hk : zk = hk (zk 1) k = 1, . . . , K 3.1 Verification Verification is the process of examining whether the o"
D19-1419,D14-1162,0,0.0842674,"Missing"
D19-1419,N18-1202,0,0.0387422,"Missing"
D19-1419,P18-1079,0,0.314761,"es). Deep models have been shown to be vulnerable against adversarial input perturbations (Szegedy et al., 2013; Kurakin et al., 2016). Small, semantically invariant input alterations can lead to drastic changes in predictions, leading to poor performance on adversarially chosen samples. Recent work (Jia and Liang, 2017; Belinkov and Bisk, 2018; Ettinger et al., 2017) also exposed the vulnerabilities of neural NLP models, e.g. with small § logits Upper bounds Introduction ‡ FC great event ReLU Abstract Conv {j.welbl}@cs.ucl.ac.uk character perturbations (Ebrahimi et al., 2018) or paraphrases (Ribeiro et al., 2018; Iyyer et al., 2018). These adversarial attacks highlight often unintuitive model failure modes and present a challenge to deploying NLP models. Common attempts to mitigate the issue are adversarial training (Ebrahimi et al., 2018) and data augmentation (Belinkov and Bisk, 2018; Li et al., 2017), which lead to improved accuracy on adversarial examples. However, this might cause a false sense of security, as there is generally no guarantee that stronger adversaries could not circumvent defenses to find other successful attacks (Carlini and Wagner, 2017; Athalye et al., 2018; Uesato et al., 201"
D19-1419,P18-1041,0,0.0299225,"Missing"
D19-1419,D13-1170,0,0.0231423,"3 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4083–4093, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics any altered – but semantically invariant – input change. In this paper, we study verifiable robustness, i.e., providing a certificate that for a given network and test input, no attack or perturbation under the specification can change predictions, using the example of text classification tasks, Stanford Sentiment Treebank (SST) (Socher et al., 2013) and AG News (Zhang et al., 2015). The specification against which we verify is that a text classification model should preserve its prediction under character (or synonym) substitutions in a character (or word) based model. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation (IBP) (Gowal et al., 2018; Mirman et al., 2018; Dvijotham et al., 2018) to compute worst case bounds on specification satisfaction, as illustrated in Figure 1. Since these bounds can be computed efficiently, we can furthermore derive an auxiliary objective for models to bec"
D19-1594,D11-1033,0,0.0432156,"Missing"
D19-1594,R13-1008,0,0.0488253,"Missing"
D19-1594,P07-1033,0,0.0180784,"Missing"
D19-1594,N19-1423,0,0.0100792,"equence model which does not (Hochreiter and Schmidhuber, 1997). Here, both models receive the benefit of in-domain training on the lexicallysimilar literature. But regardless of how much indomain training data is made available, the explicitly phrase-structural RNNG always offers a better account of the EEG signal. 7 500,000 Figure 3: Comparison between RNNG with explicit hierarchical representations and LSTM without such representations. Lower WAIC indicates a better fit to human electrophysiological data. 2011). Our focus on large training sets responds to recent work in language modeling (Devlin et al., 2019; Radford et al., 2019). The research reported here differs in two ways from these earlier studies. First, we consider the interaction between domain and amount of training data, rather than examining each variable in isolation. Second, we investigate the impact of these variables on cognitive modeling, which reveals a pattern that is different from what we observe in the standard perplexity evaluation. We focus on text genre, rather than online adaptation as van Schijndel and Linzen (2018) do. Despite coming at the problem from a different direction (and using EEG rather than selfpaced readin"
D19-1594,P15-1033,1,0.845002,"omes from a different domain.5 5 EEG Regression model Surprisal values from a beam search parser based on RNNG are entered as predictors into a regression model of scalp voltages. Models are fit with the brm function in R and model fits were compared using Bayesian model comparison (Vehtari et al., 2017). These regression models include random intercepts for participant along-side predictors to account for factors of non-interest that nevertheless are known to influence sentence processing difficulty (see e.g. Goodkind and Bick3 Alice-like 400 The RNNG hyperparameters are: 2-layer stack LSTM (Dyer et al., 2015), 450 hidden units, and an initial SGD learning rate of 0.3, decayed exponentially with a factor of 0.1 applied after the tenth epoch. 4 The per-action perplexity is computed based on the joint probability of strings (x) and trees (y), denoted as p(x, y), which therefore aggregate the perplexity of the next-word prediction with the perplexity of tree-building actions. Approximate inference methods such as important sampling can be used to derive an estimate of p(x) (Dyer et al., 2016). 5 While relative perplexity levels internal to a genre are comparable in virtue of a shared vocabulary, absol"
D19-1594,N19-1004,0,0.013225,"and in virtue of their higher temporal resolution, are more detailed than reading times or plausibility judgments. Hale et al. (2018) were the first to model them using a neural parser. In that study, textual training data came from the same book as did the human participants’ stimuli. While this yielded a model that was quite well-matched to the EEG modeling task, its training data was confined to just 1543 sentences (24K words). In contrast, recent studies suggest that neural language models require substantially more data to achieve humanlike linguistic competence (Gulordava et al., 2018; Futrell et al., 2019; Frank and Hoeks, 2019). We investigate this question of data size together with a contrast between textual domains or “genres”. Modeling human neural signals, we find that in-domain training leads to a better and better fit as more examples are added to the training set whereas with out-of-domain data, more examples do not help. This is interesting given the consistent reductions in language modeling perplexity with more data, which we observe across both domains. We further find that, across all amounts of training data, models that incorporate linguisticallyplausible phrase structure achie"
D19-1594,W01-0521,0,0.0384253,"aptation1 which arises in many areas of NLP. This paper revisits domain adaptation in the context of human-like parsing. With this humanlike aspect in mind, we consider models that use linguistically-plausible trees (see Frank, 2011 for a review) and operate incrementally from left to right (e.g. Steedman, 2000). We quantify the fit to human language performance using freelyavailable electrophysiological data (henceforth: 1 It remains quite difficult to reconcile human-like incremental parsing with high performance out-of-domain; many researchers take a nonincremental whole-sentence approach (Gildea, 2001; Baucom et al., 2013; Joshi et al., 2018). EEG) that was elicited by a pre-existing literary text (Brennan and Hale, 2019). These EEG data come from a naturalistic stimulus, and in virtue of their higher temporal resolution, are more detailed than reading times or plausibility judgments. Hale et al. (2018) were the first to model them using a neural parser. In that study, textual training data came from the same book as did the human participants’ stimuli. While this yielded a model that was quite well-matched to the EEG modeling task, its training data was confined to just 1543 sentences (24"
D19-1594,W18-0102,0,0.0367165,"Missing"
D19-1594,N18-1108,0,0.0234681,"a naturalistic stimulus, and in virtue of their higher temporal resolution, are more detailed than reading times or plausibility judgments. Hale et al. (2018) were the first to model them using a neural parser. In that study, textual training data came from the same book as did the human participants’ stimuli. While this yielded a model that was quite well-matched to the EEG modeling task, its training data was confined to just 1543 sentences (24K words). In contrast, recent studies suggest that neural language models require substantially more data to achieve humanlike linguistic competence (Gulordava et al., 2018; Futrell et al., 2019; Frank and Hoeks, 2019). We investigate this question of data size together with a contrast between textual domains or “genres”. Modeling human neural signals, we find that in-domain training leads to a better and better fit as more examples are added to the training set whereas with out-of-domain data, more examples do not help. This is interesting given the consistent reductions in language modeling perplexity with more data, which we observe across both domains. We further find that, across all amounts of training data, models that incorporate linguisticallyplausible"
D19-1594,J93-2004,0,0.0642203,"word corpus (Graff et al., 2005). This sampling was made regardless of the particular national source, i.e. Agence France-Press, New York Times or Xinhua News Agency. Sentences in this sample were, on average, 20 words long. This out-of-domain text had a CosineTop50 dissimilarity level of 0.56. 3.3 Presumptive Trees Both genres were parsed using a reimplementation of the Berkeley parser (Petrov et al., 2006) to yield presumptively-correct, “silver-grade” trees. This Berkeley parser was trained on a diverse set of annotated data. These include the Penn Treebank’s Wall Street Journal materials (Marcus et al., 1993), the Question Treebank (Judge et al., 2006), Ontonotes (e.g. Pradhan and Ramshaw, 2017) and Parsing-the-Web Corpora (Petrov and McDonald, 2012). In a manual inspection of randomly sampled silver trees, the only obvious mistakes were tagging errors (1 newspaper, 2 literature), which are not harmful since RNNGs do not use part of speech tags. Indeed, the bracketing and phrase labels on these silver trees appeared to be fully consistent with the Penn Treebank Bracketing Guidelines (Bies et al., 1995). Before being passed to the RNNG as training data, these trees were post-processed to remove emp"
D19-1594,N06-1020,0,0.0653108,"ent from what we observe in the standard perplexity evaluation. We focus on text genre, rather than online adaptation as van Schijndel and Linzen (2018) do. Despite coming at the problem from a different direction (and using EEG rather than selfpaced reading) our results agree with van Schijndel and Linzen in suggesting that some kind of adaptation must be going on in human language comprehension. 8 Conclusion Related Work The importance of domain adaptation in NLP has been well-established in earlier work (see Daum´e III, 2007 and footnote 1), including applications to parsing (Sarkar, 2001; McClosky et al., 2006; Søgaard and Rishøj, 2010; Weiss et al., 2015). Our approach to in-domain data selection is closely related to earlier work in language modeling and machine translation (Keller and Lapata, 2003; Moore and Lewis, 2010; Axelrod et al., These comparisons confirm that genre matters. If surprisal describes human linguistic expectations, then we can say that those expectations are bettermodeled by a parsing system that benefits from indomain training. This would follow if, as Kaan and Chun (2018) have suggested, people are able to very rapidly adjust their syntactic expectations to match a particul"
D19-1594,N01-1021,1,0.716188,"models that benefit from large training data. 2 Methodology We proceed by comparing parsing systems that are based on Recurrent Neural Network Grammars (Dyer et al., 2016; Wilcox et al., 2019, henceforth; RNNG) and trained according to fourteen 5846 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5846–5852, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics complexity metric: amount of training data: genre: surprisal of hypotheses in beam (Hale, 2001; Roark et al., 2009) 39832,{100, 250, 500, 750}K, 1M and 1437575 sentences newspaper text (Graff et al., 2005) and lexically-similar literature (Gutenberg) Table 1: Training regimes different regimes. These training regimes cross seven different amounts of training data with two different genres of writing. The question across all regimes is: how well does a complexity metric derived from these parsers improve the modeling of EEG data from human participants engaged in language comprehension of the same text? Note that the dependent variable here is not parsing performance or language modelin"
D19-1594,P10-2041,0,0.0174531,"d using EEG rather than selfpaced reading) our results agree with van Schijndel and Linzen in suggesting that some kind of adaptation must be going on in human language comprehension. 8 Conclusion Related Work The importance of domain adaptation in NLP has been well-established in earlier work (see Daum´e III, 2007 and footnote 1), including applications to parsing (Sarkar, 2001; McClosky et al., 2006; Søgaard and Rishøj, 2010; Weiss et al., 2015). Our approach to in-domain data selection is closely related to earlier work in language modeling and machine translation (Keller and Lapata, 2003; Moore and Lewis, 2010; Axelrod et al., These comparisons confirm that genre matters. If surprisal describes human linguistic expectations, then we can say that those expectations are bettermodeled by a parsing system that benefits from indomain training. This would follow if, as Kaan and Chun (2018) have suggested, people are able to very rapidly adjust their syntactic expectations to match a particular genre. Indeed, these expectations seems to be phrase5849 structural in nature. Certainly the presence of unigram nuisance predictors in the EEG regression (section 5) and the comparatively worse performance of the"
D19-1594,P18-1254,1,0.844696,"dman, 2000). We quantify the fit to human language performance using freelyavailable electrophysiological data (henceforth: 1 It remains quite difficult to reconcile human-like incremental parsing with high performance out-of-domain; many researchers take a nonincremental whole-sentence approach (Gildea, 2001; Baucom et al., 2013; Joshi et al., 2018). EEG) that was elicited by a pre-existing literary text (Brennan and Hale, 2019). These EEG data come from a naturalistic stimulus, and in virtue of their higher temporal resolution, are more detailed than reading times or plausibility judgments. Hale et al. (2018) were the first to model them using a neural parser. In that study, textual training data came from the same book as did the human participants’ stimuli. While this yielded a model that was quite well-matched to the EEG modeling task, its training data was confined to just 1543 sentences (24K words). In contrast, recent studies suggest that neural language models require substantially more data to achieve humanlike linguistic competence (Gulordava et al., 2018; Futrell et al., 2019; Frank and Hoeks, 2019). We investigate this question of data size together with a contrast between textual domai"
D19-1594,P06-1055,0,0.0389866,"ness Robert Hichens Table 2: Alice-like books from Project Gutenberg tence length in this selection was 17 words. 3.2 Newspaper text In the second genre, we randomly sampled news articles from the English Gigaword corpus (Graff et al., 2005). This sampling was made regardless of the particular national source, i.e. Agence France-Press, New York Times or Xinhua News Agency. Sentences in this sample were, on average, 20 words long. This out-of-domain text had a CosineTop50 dissimilarity level of 0.56. 3.3 Presumptive Trees Both genres were parsed using a reimplementation of the Berkeley parser (Petrov et al., 2006) to yield presumptively-correct, “silver-grade” trees. This Berkeley parser was trained on a diverse set of annotated data. These include the Penn Treebank’s Wall Street Journal materials (Marcus et al., 1993), the Question Treebank (Judge et al., 2006), Ontonotes (e.g. Pradhan and Ramshaw, 2017) and Parsing-the-Web Corpora (Petrov and McDonald, 2012). In a manual inspection of randomly sampled silver trees, the only obvious mistakes were tagging errors (1 newspaper, 2 literature), which are not harmful since RNNGs do not use part of speech tags. Indeed, the bracketing and phrase labels on the"
D19-1594,P82-1020,0,0.719649,"Missing"
D19-1594,P18-1110,0,0.0307816,"Missing"
D19-1594,P06-1063,0,0.0292414,"Missing"
D19-1594,J03-3005,0,0.0788147,"a different direction (and using EEG rather than selfpaced reading) our results agree with van Schijndel and Linzen in suggesting that some kind of adaptation must be going on in human language comprehension. 8 Conclusion Related Work The importance of domain adaptation in NLP has been well-established in earlier work (see Daum´e III, 2007 and footnote 1), including applications to parsing (Sarkar, 2001; McClosky et al., 2006; Søgaard and Rishøj, 2010; Weiss et al., 2015). Our approach to in-domain data selection is closely related to earlier work in language modeling and machine translation (Keller and Lapata, 2003; Moore and Lewis, 2010; Axelrod et al., These comparisons confirm that genre matters. If surprisal describes human linguistic expectations, then we can say that those expectations are bettermodeled by a parsing system that benefits from indomain training. This would follow if, as Kaan and Chun (2018) have suggested, people are able to very rapidly adjust their syntactic expectations to match a particular genre. Indeed, these expectations seems to be phrase5849 structural in nature. Certainly the presence of unigram nuisance predictors in the EEG regression (section 5) and the comparatively wo"
D19-1594,D09-1034,0,0.0409188,"benefit from large training data. 2 Methodology We proceed by comparing parsing systems that are based on Recurrent Neural Network Grammars (Dyer et al., 2016; Wilcox et al., 2019, henceforth; RNNG) and trained according to fourteen 5846 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5846–5852, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics complexity metric: amount of training data: genre: surprisal of hypotheses in beam (Hale, 2001; Roark et al., 2009) 39832,{100, 250, 500, 750}K, 1M and 1437575 sentences newspaper text (Graff et al., 2005) and lexically-similar literature (Gutenberg) Table 1: Training regimes different regimes. These training regimes cross seven different amounts of training data with two different genres of writing. The question across all regimes is: how well does a complexity metric derived from these parsers improve the modeling of EEG data from human participants engaged in language comprehension of the same text? Note that the dependent variable here is not parsing performance or language modeling perplexity of the R"
D19-1594,N01-1023,0,0.239197,"that is different from what we observe in the standard perplexity evaluation. We focus on text genre, rather than online adaptation as van Schijndel and Linzen (2018) do. Despite coming at the problem from a different direction (and using EEG rather than selfpaced reading) our results agree with van Schijndel and Linzen in suggesting that some kind of adaptation must be going on in human language comprehension. 8 Conclusion Related Work The importance of domain adaptation in NLP has been well-established in earlier work (see Daum´e III, 2007 and footnote 1), including applications to parsing (Sarkar, 2001; McClosky et al., 2006; Søgaard and Rishøj, 2010; Weiss et al., 2015). Our approach to in-domain data selection is closely related to earlier work in language modeling and machine translation (Keller and Lapata, 2003; Moore and Lewis, 2010; Axelrod et al., These comparisons confirm that genre matters. If surprisal describes human linguistic expectations, then we can say that those expectations are bettermodeled by a parsing system that benefits from indomain training. This would follow if, as Kaan and Chun (2018) have suggested, people are able to very rapidly adjust their syntactic expectati"
D19-1594,D18-1499,0,0.0354427,"Missing"
D19-1594,C10-1120,0,0.0192872,"e in the standard perplexity evaluation. We focus on text genre, rather than online adaptation as van Schijndel and Linzen (2018) do. Despite coming at the problem from a different direction (and using EEG rather than selfpaced reading) our results agree with van Schijndel and Linzen in suggesting that some kind of adaptation must be going on in human language comprehension. 8 Conclusion Related Work The importance of domain adaptation in NLP has been well-established in earlier work (see Daum´e III, 2007 and footnote 1), including applications to parsing (Sarkar, 2001; McClosky et al., 2006; Søgaard and Rishøj, 2010; Weiss et al., 2015). Our approach to in-domain data selection is closely related to earlier work in language modeling and machine translation (Keller and Lapata, 2003; Moore and Lewis, 2010; Axelrod et al., These comparisons confirm that genre matters. If surprisal describes human linguistic expectations, then we can say that those expectations are bettermodeled by a parsing system that benefits from indomain training. This would follow if, as Kaan and Chun (2018) have suggested, people are able to very rapidly adjust their syntactic expectations to match a particular genre. Indeed, these ex"
D19-1594,P15-1032,0,0.0194258,"ty evaluation. We focus on text genre, rather than online adaptation as van Schijndel and Linzen (2018) do. Despite coming at the problem from a different direction (and using EEG rather than selfpaced reading) our results agree with van Schijndel and Linzen in suggesting that some kind of adaptation must be going on in human language comprehension. 8 Conclusion Related Work The importance of domain adaptation in NLP has been well-established in earlier work (see Daum´e III, 2007 and footnote 1), including applications to parsing (Sarkar, 2001; McClosky et al., 2006; Søgaard and Rishøj, 2010; Weiss et al., 2015). Our approach to in-domain data selection is closely related to earlier work in language modeling and machine translation (Keller and Lapata, 2003; Moore and Lewis, 2010; Axelrod et al., These comparisons confirm that genre matters. If surprisal describes human linguistic expectations, then we can say that those expectations are bettermodeled by a parsing system that benefits from indomain training. This would follow if, as Kaan and Chun (2018) have suggested, people are able to very rapidly adjust their syntactic expectations to match a particular genre. Indeed, these expectations seems to b"
D19-1594,N19-1334,0,0.0134669,"given the consistent reductions in language modeling perplexity with more data, which we observe across both domains. We further find that, across all amounts of training data, models that incorporate linguisticallyplausible phrase structure achieve a better fit to human EEG data than models that do not. This suggests that phrase structure should play an important role in human-like models of language comprehension, even in models that benefit from large training data. 2 Methodology We proceed by comparing parsing systems that are based on Recurrent Neural Network Grammars (Dyer et al., 2016; Wilcox et al., 2019, henceforth; RNNG) and trained according to fourteen 5846 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5846–5852, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics complexity metric: amount of training data: genre: surprisal of hypotheses in beam (Hale, 2001; Roark et al., 2009) 39832,{100, 250, 500, 750}K, 1M and 1437575 sentences newspaper text (Graff et al., 2005) and lexically-similar literature (Gutenberg) Table 1: Training regime"
E14-1042,W12-3102,0,0.0382064,"Missing"
E14-1042,2012.eamt-1.60,0,0.015108,"Missing"
E14-1042,J07-2003,0,0.0847015,"Missing"
E14-1042,P11-2031,1,0.0803537,"Missing"
E14-1042,P10-4002,1,0.708845,"Missing"
E14-1042,N13-1073,1,0.315558,"Missing"
E14-1042,W12-3160,0,0.196093,"Missing"
E14-1042,2009.mtsummit-btm.7,0,0.0406991,"Missing"
E14-1042,2010.amta-papers.21,0,0.748015,"Missing"
E14-1042,P13-2121,0,0.016193,"Missing"
E14-1042,D11-1125,0,0.0158403,"Missing"
E14-1042,N10-1062,0,0.501773,"Missing"
E14-1042,C08-1064,0,0.0808059,"Missing"
E14-1042,D08-1076,0,0.0144994,"Missing"
E14-1042,W04-3225,0,0.568137,"Missing"
E14-1042,C88-2101,0,0.104491,"Missing"
E14-1042,P03-1021,0,0.0557394,"Missing"
E14-1042,2012.amta-papers.14,0,0.558928,"Missing"
E14-1042,2013.mtsummit-papers.24,0,0.521722,"Missing"
E14-1042,2009.mtsummit-posters.20,0,0.0815556,"Missing"
E14-1042,P06-1124,0,0.0644723,"Missing"
E14-1042,C04-1059,0,0.022975,"Missing"
E14-1042,2012.amta-wptp.10,0,0.107696,"Missing"
E14-1042,N10-1079,0,0.418105,"Missing"
E14-1042,P02-1040,0,0.0936261,"Missing"
E14-1049,P05-1074,0,0.0214439,"Missing"
E14-1049,D13-1167,0,0.00450737,"nd at http://cs.cmu. edu/˜mfaruqui/soft.html. Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that captur"
E14-1049,J90-1003,0,0.136179,"matrix. We construct a word co-occurrence frequency matrix F for a given training corpus where each row w, represents one word in the corpus and every column c, is the context feature in which the word is observed. In our case, every column is a word which occurs in a given window length around the target word. For scalability reasons, we only select words with frequency greater than 10 as features. We also remove the top 100 most frequent words (mostly stop words) from the column features. We then replace every entry in the sparse frequency matrix F by its pointwise mutual information (PMI) (Church and Hanks, 1990; Turney, 2001) resulting in X. PMI is designed to give a high value to xij where there is a interesting relation between wi and cj , a small or negative value of xij indicates that the occurrence of wi in cj is uninformative. Finally, we factorize the matrix X using singular value decomposition (SVD). SVD decomposes X into the product of three matrices: X = U ΨV > (7) where, U and V are in column orthonormal form and Ψ is a diagonal matrix of singular values (Golub and Van Loan, 1996). We obtain a reduced dimensional representation of words from size |V |to k: A = U k Ψk (8) 4.2 Semantic Rela"
E14-1049,P08-1088,0,0.0150447,"Missing"
E14-1049,P12-1092,0,0.395239,"Missing"
E14-1049,C12-1089,0,0.853281,"y associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word Acknowledgements We thanks Kevin Gimpel, Noa"
E14-1049,N13-1090,0,0.627131,",1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1 Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. 462 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–471, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics space embeddings of two different vocabularies where rows represent words. Since the two vocabularies are of different sizes (n1 and n2 ) and there might not exist translation for every word 0 0 of Σ in Ω, let Σ ⊆ Σ where every word in Σ 0 is translated to one other word3 in Ω ⊆ Ω and Σ"
E14-1049,P10-4002,1,0.113625,"and Spanish we used the WMT-20115 monolingual news corpora and for French we combined the WMT-2011 and 20126 monolingual news corpora so that we have around 300 million tokens for each language to train the word vectors. For CCA, a one-to-one correspondence between the two sets of vectors is required. Obviously, the vocabulary of two languages are of different sizes and hence to obtain one-to-one mapping, for every English word we choose a word from the other language to which it has been aligned the maximum number of times7 in a parallel corpus. We got these word alignment counts using cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English{German, French, Spanish}. 5 http://www.statmt.org/wmt11/ http://www.statmt.org/wmt12/ 7 We also tried weighted average of vectors across all aligned words and did not observe any significant difference in results. 6 8 465 See section 5.5 for further discussion on vector length. Lang En De-En Fr-En Es-En Average Dim 640 512 512 512 – WS-353 46.7 68.0 68.4 67.2 56.6 WS-SIM 56.2 74.4 73.3 71.6 64.5 WS-REL 36.5 64.6 65.7 64.5 51.0 RG-65 50.7 75.5 73.5 70.5 62.0 MC-30 42.3 81.9 81.3 78.2 65.5 MTur"
E14-1049,P13-2136,1,0.54011,"er continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly different for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact that both SG and SVD vectors have similar behavior across language pairs can be treated as"
E14-1049,N10-1135,0,0.0939216,"Missing"
E14-1049,D13-1168,0,0.259867,"Missing"
E14-1049,N01-1026,0,0.0175523,"nt for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact that both SG and SVD vectors have similar behavior across language pairs can be treated as evidence that semantics or information at a conceptual level (since both of them basically model word cooccurrence counts) transfers well across languages (Dyvik, 2004) although syntax has been projected across languages as well (Hwa et al., 2005; Yarowsky and Ngai, 2001). The pattern of results in the case of RNN vectors are indicative of the fact that these vectors encode syntactic information as explained in §6 which might not generalize well as compared to semantic information. 7 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilingual context into word representations generated using only monolingual information and shown its applicability across three different ways of generating monolingual vectors on a variety of evaluation benchmarks. These word representations obtained after using multilingual eviden"
E14-1049,P06-2124,0,0.0125756,"t al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly different for every language pair. SG vectors show a uniform pattern across different language pairs and"
E14-1049,W05-0804,0,0.0545703,"ased word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly different for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact th"
E14-1049,P00-1054,0,0.167896,"ys have a fixed length of 80, they are just shown in the plots for comparison. 468 (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly diff"
E14-1049,D13-1141,0,0.826271,"ations obtained after using multilingual evidence perform significantly better on the evaluation tasks compared to the monolingual vectors. We have also shown that our method is more suitable for vectors that encode semantic information than those that encode syntactic information. Our work suggests that multilingual evidence is an important resource even for purely monolingual, semantically aware applications. The tool for projecting word vectors can be found at http://cs.cmu. edu/˜mfaruqui/soft.html. Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a mu"
E14-1049,N12-1052,0,0.105356,"Missing"
E14-1049,P10-1040,0,0.0970205,"ur method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1 Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. 462 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–"
E14-1049,J06-3003,0,0.00817052,"rd representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1 Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. 462 Proceedings of the 14th Confere"
E14-1049,N09-1003,0,\N,Missing
E14-1065,W13-2205,1,0.897909,"Missing"
E14-1065,bojar-etal-2010-data,0,0.0134346,"the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does not include Russian, Hebrew and Hindi, which are a subject of this research. 10 Since TED translation is a voluntary effort, not all talks are available in all languages. 11 Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing to domain mismatch. 620 TED id 1 39 142 228 248 451 535 TED talk Al Gore, 15 Ways to Avert a Climate Crisis, 2006 Aubrey de Grey: A roadmap to end aging, 2005 Alan Russell: The potential of regenerative medicine, 2006 Alan Kay shares a powerful idea about ideas, 2007 Alisa Miller: The news about the news, 2008 Bill Gates: Mosquitos, malaria and education, 2009 Al Gore warns on latest climate trends, 2009 Table 1: Test set of TED talks. ASR errors in synthetic phrase tables corresponds to the portion of errors that ou"
E14-1065,N06-1003,0,0.0932478,"Missing"
E14-1065,2012.eamt-1.60,0,0.0130906,"cmu.edu/cgi-bin/cmudict 5 p1 ·p2 ||p1 ||·||p2 || 618 pronunciation variants. To create a phone confusion transducer T maps source to target phone sequences by performing a number of edit operations. Allowed edits are: tells the chelsea CH EH L S IY Figure 4: Pseudo-ASR output generation example for a bigram tells the. Phonetic edits are Substitute(T, CH), Substitute(Z, S), Delete(DH). • Deletion of a consonant (mapping to ). • Doubling of a vowel. • Insertion of one or two phones in the end of a sequence from the list of possible suffixes: S (-s), IX NG (-ing), D (-ed). translated TED talks (Cettolo et al., 2012b).6 English is the source language in all the experiments. In expASR we used tst2011–the official test set of the SLT track of the IWSLT 2011 evaluation campaign on the English-French language pair (Federico et al., 2011).7 This test set comprises reference transcriptions of 8 talks (approximately 1.1h of speech, segmented to 818 utterances), 1-best hypotheses from five different ASR systems, a ROVER combination of four systems (Fiscus, 1997), and three sets of lattices produced by the participants of the IWSLT 2011 ASR track. In this set of experiments we compare baseline systems performance"
E14-1065,N03-1017,0,0.00534194,"s (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does not include Russian, Hebrew and Hindi, which are a subject of this resea"
E14-1065,D13-1174,1,0.836913,". English has strong constraints on sequences of consonants; the sequence [zdr], for example, cannot be a legal EnWe adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2 We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). 3 These are the reasons why in context-dependent acoustic modeling different HMM models are trained for different contexts. 617 Source phrase tells the story tell their story tells a story tell the story tell a story tell that story tell their stories tells the stories tells her story chelsea star Target phrase raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire Original phrase translation features f1 , f 2 , f 3 , f 4 , f 5 f1 , f 2 , f 3 , f 4 , f 5 f1 ,"
E14-1065,P11-2031,1,0.862394,"Missing"
E14-1065,P07-2045,1,0.014774,"German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does"
E14-1065,P08-1115,0,0.022545,"tenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using o"
E14-1065,moore-2002-fast,0,0.0125955,"experiments the ASR outputs and lattices in standard lattice format (SLF) were produces by the participants of IWSLT 2011 evaluation campaign. 5.2 We train and test MT using the TED corpora in all five languages. For French, German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that fe"
E14-1065,W13-2234,1,0.918146,"hat can restrict allowed sequences of phones. English has strong constraints on sequences of consonants; the sequence [zdr], for example, cannot be a legal EnWe adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2 We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). 3 These are the reasons why in context-dependent acoustic modeling different HMM models are trained for different contexts. 617 Source phrase tells the story tell their story tells a story tell the story tell a story tell that story tell their stories tells the stories tells her story chelsea star Target phrase raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire raconte l’histoire Original phrase translation features f1 , f 2 , f"
E14-1065,P03-1021,0,0.0154168,"evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed Test 843 501 735 540 300 Table 2: Number of sentences in training, dev and expMultilingual test corpora. 5.1 MT ASR In the expMultilingual set of experiments, we employ the JANUS Recognition Toolkit that features the IBIS single pass decoder (Soltau et 9 After we conducted our experiments, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does not include Russian, Hebrew and Hindi, which are a subject of this research. 10 Since TED translation is a voluntary e"
E14-1065,P10-2001,0,0.017716,"ld improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using only a source language"
E14-1065,C04-1168,0,0.0360232,"ations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using only a source language pronunciation dictionary and language model (i.e., without an acoustic model), and use th"
E14-1065,P02-1040,0,0.088177,"Missing"
E14-1065,2012.iwslt-papers.18,0,0.200039,"ambiguating context and are subject to reduced pronunciations (Goldwater et al., 2010). One would expect that training an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to Abstract We propose a novel technique for adapting text-base"
E14-1065,P10-4002,1,\N,Missing
E14-1065,W14-3315,1,\N,Missing
E14-1065,2011.iwslt-evaluation.1,0,\N,Missing
E14-1065,2011.iwslt-evaluation.11,0,\N,Missing
E17-1117,P16-1231,0,0.223878,"Missing"
E17-1117,C02-1126,0,0.0882932,"Missing"
E17-1117,D16-1257,0,0.541811,"n all past actions. The joint probability estimate p(x, y) can be used for both phrase-structure parsing (finding arg maxy p(y |x)) and language modeling (finding p(x) by marginalizing over the set of possible parses for x). Both inference problems can be solved using an importance sampling procedure.4 We report all RNNG performance based on the corrigendum to Dyer et al. (2016). 3 Composition is Key Given the same data, under both the discriminative and generative settings RNNGs were found to parse with significantly higher accuracy than (respectively) the models of Vinyals et al. (2015) and Choe and Charniak (2016) that represent y as a “linearized” sequence of symbols and parentheses without explicitly capturing the tree structure, or even constraining the y to be a well-formed tree (see Table 1). Vinyals et al. (2015) directly predict the sequence of nonterminals, “shifts” (which consume a terminal symbol), and parentheses from left to right, conditional on the input terminal sequence x, while Choe and Charniak (2016) used a sequential LSTM language model on the same linearized trees to create a generative variant of the Vinyals et al. (2015) model. The generative model is used to re-rank parse candid"
E17-1117,P97-1003,0,0.860051,"guistics: Volume 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequen"
E17-1117,de-marneffe-etal-2006-generating,0,0.0559403,"Missing"
E17-1117,P81-1022,0,0.79275,"Missing"
E17-1117,N16-1024,1,0.631277,"ial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model’s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis. 1 Introduction In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; Dyer et al., 2016), designed to model syntactic derivations of sentences. We focus on RNNGs as generative probabilistic models over trees, as summarized in §2. Fitting a probabilistic model to data has often been understood as a way to test or confirm some aspect of a theory. We talk about a model’s assumptions and sometimes explore its parameters or posteriors over its latent variables in order to gain understanding of what it “discovers” from the data. In some sense, such models can be thought of as mini-scientists. Neural networks, including RNNGs, are capable of representing larger classes of hypotheses tha"
E17-1117,Q16-1023,0,0.0670931,"Missing"
E17-1117,P02-1017,0,0.525775,"(that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.01) IBM (0.25) both (0.02) stocks (0.03) and ("
E17-1117,P03-1054,0,0.278815,"e 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequence of actions to construct"
E17-1117,D15-1278,0,0.0303018,"f understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on"
E17-1117,N16-1082,0,0.0221556,"put the verb as the head of the verb phrase. Another interesting finding is that the model pays attention to polarity information, where negations are almost always assigned nontrivial attention weights.7 Furthermore, we find that the model attends to the conjunction terminal in conjunctions of verb phrases (e.g., “VP → VP and VP”, 10), reinforcing the similar finding for conjunction of noun phrases. PPs. In almost all cases, the model attends to the preposition terminal instead of the noun phrases or complete clauses under it, regardless of the type of preposition. Even when the preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3"
E17-1117,P92-1017,0,0.673069,"ecessary. If the endocentric hypothesis is true (that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.0"
E17-1117,P06-1055,0,0.0272857,"information, even when trained on a fairly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The"
E17-1117,E09-1080,0,0.0214366,"ly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models"
E17-1117,D16-1159,0,0.0208633,"ads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sen"
E17-1117,D16-1137,0,0.0306962,", essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG"
E17-1117,W07-2416,0,0.0109927,"preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3 2.5 2 1.5 1 ADJP VP NP PP QP SBAR Figure 3: Average perplexity of the learned attention vectors on the test set (blue), as opposed to the average perplexity of the uniform distribution (red), computed for each major phrase type. 5.2 Comparison to Existing Head Rules To better measure the overlap between the attention vectors and existing head rules, we converted the trees in PTB §23 into a dependency representation using the attention weights. In this case, the attention weight functions as a “dynamic” head rule, where all other constituents within the same composed phrase are considered t"
E17-1117,J98-4004,0,0.453425,"and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and nonterminal (Johnson, 1998; Klein and Manning, 2003) augmentations. The conjecture that fine-grained nonterminal rules and labels can be discovered given weaker bracketing structures was based on several studies (Chiang Conclusion We probe what recurrent neural network grammars learn about syntax, through ablation scenarios and a novel variant with a gated attention mechanism on the composition function. The composition function, a key differentiator between the RNNG and other neural models of syntax, is crucial for good performance. Using the attention vectors we discover that the model is learning something similar t"
E17-2061,P16-1008,0,0.0186923,"en proven in practice, existing models of attention use contentbased addressing and have made only limited use of the historical attention masks. However, lessons from better word alignment priors in latent variable translation model suggests value for modeling attention dependent of content. A challenge in modeling dependencies between previous and subsequent attention decisions is that source sentences are of different lengths, so we need models that can deal with variable numbers of predictions across variable lengths. While other work has sought to address this problem (Cohn et al., 2016; Tu et al., 2016; Feng et al., 2016), these 2 2.1 Model Neural Machine Translation NMT directly models the condition probability p(y|x) of target sequence y = {y1 , ..., yT } given source sequence x = {x1 , ..., xS }, where xi , yj are tokens in source sequence and target sequence respectively. Sutskever et al. (2014) and Bahdanau et al. (2014) are slightly different in choosing the encoder and decoder network. Here we choose the 383 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 383–387, c Valencia, Spain, April 3-7,"
E17-2061,P16-2058,0,0.0245339,"Missing"
E17-2061,C16-1290,0,0.029246,"tice, existing models of attention use contentbased addressing and have made only limited use of the historical attention masks. However, lessons from better word alignment priors in latent variable translation model suggests value for modeling attention dependent of content. A challenge in modeling dependencies between previous and subsequent attention decisions is that source sentences are of different lengths, so we need models that can deal with variable numbers of predictions across variable lengths. While other work has sought to address this problem (Cohn et al., 2016; Tu et al., 2016; Feng et al., 2016), these 2 2.1 Model Neural Machine Translation NMT directly models the condition probability p(y|x) of target sequence y = {y1 , ..., yT } given source sequence x = {x1 , ..., xS }, where xi , yj are tokens in source sequence and target sequence respectively. Sutskever et al. (2014) and Bahdanau et al. (2014) are slightly different in choosing the encoder and decoder network. Here we choose the 383 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 383–387, c Valencia, Spain, April 3-7, 2017. 2017 Associati"
E17-2061,P15-1001,0,0.0279021,"e plain SGD to train the model and set the batch size to be 128. We rescale the gradient whenever its norm is greater than 3. We use an initial learning rate of 0.7. For English-German, we start to halve the learning rate every epoch after training for 8 epochs. We train the model for a total of 12 epochs. For Chinese-English, we start to halve the learning rate every two epochs after training for 10 epochs. We train the model for a total of 18 epochs. To investigate the effect of window size 2k + 1, we report results for k = 0, 5, i.e., windows of size 1, 11. Experiments & Results 3.1 test1 (Jean et al., 2015) RNNSearch RNNSearch + UNK replace i 3 Model Data sets We experiment with two data sets: WMT EnglishGerman and NIST Chinese-English. • English-German The German-English data set contains Europarl, Common Crawl and News Commentary corpus. We remove the sentence pairs that are not German or English in a similar way as in (Jean et al., 2015). There are about 4.4 million sentence pairs after preprocessing. We use newstest2013 set as validation and newstest2014, newstest2015 as test. • Chinese-English We use FIBS and LDC2004T08 Hong Kong News data set for Chinese-English translation. There are abou"
E17-2061,D13-1176,0,0.0156692,"tended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality. 1 Introduction In contrast to earlier approaches to neural machine translation (NMT) that used a fixed vector representation of the input (Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013), attention mechanisms provide an evolving view of the input sentence as the output is generated (Bahdanau et al., 2014). Although attention is an intuitively appealing concept and has been proven in practice, existing models of attention use contentbased addressing and have made only limited use of the historical attention masks. However, lessons from better word alignment priors in latent variable translation model suggests value for modeling attention dependent of content. A challenge in modeling dependencies between previous and subsequent attention decisions is that source sentences are o"
E17-2061,D15-1166,0,0.581304,"he vector of i-th word is WS xi . We get the annotations of word i by summarizing the information of neighboring words using bidirectional LSTMs: → − − −−− → −−→ h i =LSTM(hi−1 , WS xi ) ← − ← −−− − ←−− h i =LSTM(hi+1 , WS xi ) … (1) (2) p(yj |y<j , x). αi−k,j−1 … αi,j−1 … αi+k,j−1 yj−2 αi−k,j−2 … αi,j−2 … αi+k,j−2 y1 αi−k,1 … αi,1 xi attention matrix … αi+k,1 xi+k s˜j = tanh(W1 [sj , cj ] + b1 ) (8) pj =softmax(W2 s˜j ) (9) We feed cj to the next step, this explains the cj−1 term in Eq. 4. The above attention mechanism follows that of Vinyals et al. (2015). Similar approach has been used in (Luong et al., 2015a). This is slightly different from the attention mechanism used in (Bahdanau et al., 2014), we find empirically it works better. One major limitation is that attention at each time step is not directly dependent of each other. However, in machine translation, the next word to attend to highly depends on previous steps, neighboring words are more likely to be selected in next time step. This above attention mechanism fails to capture these important characteristics. In the following, we attach a dynamic memory vector to the original static memory hi , to keep track of how many times this word"
E17-2061,P15-1002,0,0.118847,"he vector of i-th word is WS xi . We get the annotations of word i by summarizing the information of neighboring words using bidirectional LSTMs: → − − −−− → −−→ h i =LSTM(hi−1 , WS xi ) ← − ← −−− − ←−− h i =LSTM(hi+1 , WS xi ) … (1) (2) p(yj |y<j , x). αi−k,j−1 … αi,j−1 … αi+k,j−1 yj−2 αi−k,j−2 … αi,j−2 … αi+k,j−2 y1 αi−k,1 … αi,1 xi attention matrix … αi+k,1 xi+k s˜j = tanh(W1 [sj , cj ] + b1 ) (8) pj =softmax(W2 s˜j ) (9) We feed cj to the next step, this explains the cj−1 term in Eq. 4. The above attention mechanism follows that of Vinyals et al. (2015). Similar approach has been used in (Luong et al., 2015a). This is slightly different from the attention mechanism used in (Bahdanau et al., 2014), we find empirically it works better. One major limitation is that attention at each time step is not directly dependent of each other. However, in machine translation, the next word to attend to highly depends on previous steps, neighboring words are more likely to be selected in next time step. This above attention mechanism fails to capture these important characteristics. In the following, we attach a dynamic memory vector to the original static memory hi , to keep track of how many times this word"
E17-2061,P02-1040,0,0.105818,"nd News Commentary corpus. We remove the sentence pairs that are not German or English in a similar way as in (Jean et al., 2015). There are about 4.4 million sentence pairs after preprocessing. We use newstest2013 set as validation and newstest2014, newstest2015 as test. • Chinese-English We use FIBS and LDC2004T08 Hong Kong News data set for Chinese-English translation. There are about 1.5 million sentences pairs. We use MT 02, 03 as validation and MT 05 as test. For both data sets, we tokenize the text with tokenizer.perl. Translation quality is evaluated in terms of tokenized BLEU scores (Papineni et al., 2002) with multi-bleu.perl. 3.3 Results The results of English-German and ChineseEnglish are shown in Table 2 and 3 respectively. 385 src ref baseline our model src ref baseline our model She was spotted three days later by a dog walker trapped in the quarry Drei Tage sp¨ater wurde sie von einem Spazierg¨anger im Steinbruch in ihrer misslichen Lage entdeckt Sie wurde drei Tage sp¨ater von einem Hund entdeckt . Drei Tage sp¨ater wurde sie von einem Hund im Steinbruch gefangen entdeckt . At the Metropolitan Transportation Commission in the San Francisco Bay Area , officials say Congress could very si"
E17-2061,P16-1160,0,\N,Missing
J16-2005,W10-0710,0,0.108835,"lations within the same document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quality control. Thus, in order to find good translations, subsequent post-editing and/or ranking is generally necessary. 3. The Intra-Document Alignment (IDA) Model As discussed above, conte"
J16-2005,W12-2108,0,0.0274338,"weets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volumes of data involved, we can find a large amount of content. We must efficiently detect multilingual tweets, as monolingual tweets do not contain translations. Although language identification is a well-studied problem (Zissman 1996; Gottron and Lipka 2010; Greenhill 2011), even in the microblog domain (Bergsma et al. 2012), these assume that only one language is present within a document and cannot be directly applied to this problem. Furthermore, because of the magnitude of tweets that must be processed, many of the proposed solutions cannot be applied due to their computational complexity. In our work, we propose an efficient implementation for large-scale detection of multilingual documents. Crowdsourcing for parallel data extraction - To tune the parameters of our parallel data extractor and perform MT experiments, user-verified annotations must be obtained. To obtain this, we propose a simple crowdsourcing"
J16-2005,C10-2010,0,0.0214145,"the translation of xai . Model 1 naively assigns a uniform prior probability to all alignment configurations. Although this is an obviously flawed assumption, the posterior alignment probability under Model 1 (i.e., PM1 (a |x, y)) is surprisingly informative. More robust models make less-naive prior assumptions and generally produce higher-quality alignments, but the uniform prior probability assumption simplifies the complexity of performing inference. Despite its simplicity, Model 1 has shown particularly good performance as a component in sentence alignment systems (Xu, Zens, and Ney 2005; Braune and Fraser 2010). Some work on parallel data extraction has also focused on extracting parallel segments from comparable corpora (Smith, Quirk, and Toutanova 2010; Munteanu, Fraser, and Marcu 2004). Smith et al. (2010) uses conditional random fields to identify parallel segments from comparable Wikipedia documents (since Wikipedia documents in multiple languages are not generally translations of each other, although they are about the same topics). The work of Jehl, Hieber, and Riezler (2012) uses cross-lingual information retrieval techniques to extract candidate English–Arabic translations from Twitter. The"
J16-2005,J93-2003,0,0.116606,"Missing"
J16-2005,N13-1073,1,0.853048,"Missing"
J16-2005,2014.iwslt-papers.7,0,0.0308749,"also possible to focus the extraction in one particular type of phenomena. For example, the work on mining parenthetical translations (Lin et al. 2008), which attempts to find translations within the same document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quali"
J16-2005,W06-1008,0,0.0705046,"Missing"
J16-2005,P91-1023,0,0.541067,"n features - In informal domains, there are many terms that are not translated, such as hashtags (e.g., #twitter), at mentions (e.g., @NYC), numbers, and people’s names. The presence of such repeated terms in the same tweet can be a strong cue for detecting translations. Hence, we define features that trigger if a given word type occurs in a pair within a tweet. The word types considered are hashtags, at mentions, numbers, and words beginning with capital letters. Length feature - It has been known that the length differences between parallel sentences can be modeled by a normal distribution (Gale and Church 1991). Thus, we used parallel training data (used to train the alignment model) in the respective language pair to determine (µ˜ , σ˜ 2 ), which lets us calculate the likelihood of two hypothesized segments being parallel. For each language pair s, t, we train separate classifiers for each language pair on annotated parallel data Dgold (s, t). The method used to obtain the necessary annotations is described in Section 5. Intrinsic evaluation. The quality of the classifier can be determined in terms of precision and recall. We count one as a true positive (tp) if we correctly identify a parallel twe"
J16-2005,P11-2008,0,0.0478665,"Missing"
J16-2005,J11-4003,0,0.0223833,"te pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volumes of data involved, we can find a large amount of content. We must efficiently detect multilingual tweets, as monolingual tweets do not contain translations. Although language identification is a well-studied problem (Zissman 1996; Gottron and Lipka 2010; Greenhill 2011), even in the microblog domain (Bergsma et al. 2012), these assume that only one language is present within a document and cannot be directly applied to this problem. Furthermore, because of the magnitude of tweets that must be processed, many of the proposed solutions cannot be applied due to their computational complexity. In our work, we propose an efficient implementation for large-scale detection of multilingual documents. Crowdsourcing for parallel data extraction - To tune the parameters of our parallel data extractor and perform MT experiments, user-verified annotations must be obtaine"
J16-2005,P11-1038,0,0.0259193,"till with me or what?) and nonstandard abbreviations (idk! smh). Automated language processing tools (e.g., those that perform linguistic analysis or translation) face particular difficulty with this new kind of content. On one hand, these have been developed with the conventions of more edited genres in mind. For example, they often make strong assumptions about orthographic and lexical uniformity (e.g., that there is just one way to spell you, and that cool, cooool, and cooooool represent completely unrelated lexical items). While modeling innovations are helping to relax these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling et al. 2015), a second serious challenge is that many of the annotated text resources that our tools are learned from are drawn from edited genres, and poor generalization from edited to user-generated genres is a major source of errors (Gimpel et al. 2011; Kong et al. 2014). In this work, we present methods for finding naturally occurring parallel data on social media sites that is suitable for training machine translation (MT) systems. In MT, the domain mismatch problem is quite acute because most existing sources of parallel data are governmen"
J16-2005,W12-3153,0,0.0424014,"Missing"
J16-2005,D07-1103,0,0.0213949,"hus, for each language, we extract all data from Wikipedia up to a limit of 100K lines in order to keep the model compact. 7.1.3 Translation Lexicons. The IDA model uses translation lexicons to determine the translation score, as described in Section 3.1.1, which are estimated using parallel corpora. More specifically, we use the aligner described in Dyer, Chahuneau, and Smith (2013) to obtain the bidirectional alignments from the parallel sentences. Afterwards, we intersect the bidirectional alignments to obtain sure alignment points. Finally, we prune the lexicon using significance pruning (Johnson et al. 2007) with the threshold α +  (as defined in that work). The intersection and the pruning are performed to reduce the size of the lexicon to make the look-up faster. The breakdown of the different lexicons built is shown in Table 1. 7.2 Building Gold Standards To train and test the classifier described in Section 4.3, and perform MT experiments, a corpus of annotated tweets is needed for different language pairs. Table 2 summarizes the annotated corpora for the two domains (column Source) and the different language pairs (column Language Pair). We also report the method used to obtain the annotati"
J16-2005,D14-1108,1,0.842008,"hey often make strong assumptions about orthographic and lexical uniformity (e.g., that there is just one way to spell you, and that cool, cooool, and cooooool represent completely unrelated lexical items). While modeling innovations are helping to relax these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling et al. 2015), a second serious challenge is that many of the annotated text resources that our tools are learned from are drawn from edited genres, and poor generalization from edited to user-generated genres is a major source of errors (Gimpel et al. 2011; Kong et al. 2014). In this work, we present methods for finding naturally occurring parallel data on social media sites that is suitable for training machine translation (MT) systems. In MT, the domain mismatch problem is quite acute because most existing sources of parallel data are governmental, religious, or commercial, which are quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also"
J16-2005,I08-2120,0,0.190188,"he elaboration of this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are"
J16-2005,P08-1113,0,0.0607684,"Missing"
J16-2005,D13-1008,1,0.90112,"e quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publ"
J16-2005,D15-1176,1,0.850166,"Missing"
J16-2005,W14-3356,1,0.854061,"parallel segment is correct. If so, the material is extracted, otherwise it is discarded. This research is an extension of the preliminary work described in Ling et al. (2013), in which we obtained over 1 million Chinese–English parallel segments from Sina Weibo, using only their public application program interface (API). This automatically extracted parallel data yielded substantial translation quality improvements in translating microblog text and modest improvements in translating edited news. Following this work, we developed a method for crowdsourcing judgments about parallel segments (Ling et al. 2014), which was then used to build gold standard data for other language pairs and for the Twitter domain. This article extends these two papers in several ways: r 310 Improved language pair detection - The previous work assumes that the language pair is formed by two languages with different unicode ranges, such as English–Chinese, and does not support the extraction of parallel data if the languages share the same unicode range (such as English–Portuguese). This issue is addressed in this article, where we present a novel approach for finding multilingual tweets. Ling et al. r r Mining Parallel"
J16-2005,P13-1018,1,0.731322,"e quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publ"
J16-2005,Q14-1003,0,0.0561496,"Missing"
J16-2005,J05-4003,0,0.318213,"the appropriately aligned parallel segments. Obviously, Ds,t will contain a considerable number of non-parallel segments, as multilingual messages in Tmult are not guaranteed to contain translated material. Furthermore, we must also consider errors from misalignments of the IDA model and misclassifications of the multilingual message detector. Thus, in order to identify messages that are actually parallel, a final identification step is necessary. 4.3 Identification Given a candidate sentence pair (s, t), many existing methods for detecting parallel data can be applied (Resnik and Smith 2003; Munteanu and Marcu 2005), as this problem becomes a regular unstructured bitext identification problem. In our initial work (Ling et al. 2013), we simply defined a threshold τ on the IDA model score, which was determined empirically. To obtain better results we train a logistic regression classifier for each language pair, similar to that presented in Munteanu and Marcu (2005), which detects whether two segments are parallel in a given language pair by looking at features of the candidate pair. Training is performed to maximize the classification decisions on annotated candidate pairs. 324 Ling et al. Mining Parallel"
J16-2005,N04-1034,0,0.0600954,"Missing"
J16-2005,P03-1021,0,0.0143381,"Missing"
J16-2005,N13-1039,1,0.773659,"regarding existing languages allows the detector to estimate the language probabilities more accurately. As we are using a character trigram model, a large amount of data is not required to saturate the model probabilities. Thus, for each language, we extract all data from Wikipedia up to a limit of 100K lines in order to keep the model compact. 7.1.3 Translation Lexicons. The IDA model uses translation lexicons to determine the translation score, as described in Section 3.1.1, which are estimated using parallel corpora. More specifically, we use the aligner described in Dyer, Chahuneau, and Smith (2013) to obtain the bidirectional alignments from the parallel sentences. Afterwards, we intersect the bidirectional alignments to obtain sure alignment points. Finally, we prune the lexicon using significance pruning (Johnson et al. 2007) with the threshold α +  (as defined in that work). The intersection and the pruning are performed to reduce the size of the lexicon to make the look-up faster. The breakdown of the different lexicons built is shown in Table 1. 7.2 Building Gold Standards To train and test the classifier described in Section 4.3, and perform MT experiments, a corpus of annotated"
J16-2005,P02-1040,0,0.0967467,"Missing"
J16-2005,P14-2110,0,0.0370301,"Missing"
J16-2005,W12-3152,0,0.0412989,"Missing"
J16-2005,J03-3002,0,0.467405,"s many challenges to current NLP and MT methods. As part of the elaboration of this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient"
J16-2005,N10-1063,0,0.0245615,"1 (i.e., PM1 (a |x, y)) is surprisingly informative. More robust models make less-naive prior assumptions and generally produce higher-quality alignments, but the uniform prior probability assumption simplifies the complexity of performing inference. Despite its simplicity, Model 1 has shown particularly good performance as a component in sentence alignment systems (Xu, Zens, and Ney 2005; Braune and Fraser 2010). Some work on parallel data extraction has also focused on extracting parallel segments from comparable corpora (Smith, Quirk, and Toutanova 2010; Munteanu, Fraser, and Marcu 2004). Smith et al. (2010) uses conditional random fields to identify parallel segments from comparable Wikipedia documents (since Wikipedia documents in multiple languages are not generally translations of each other, although they are about the same topics). The work of Jehl, Hieber, and Riezler (2012) uses cross-lingual information retrieval techniques to extract candidate English–Arabic translations from Twitter. These candidates are then refined using a more expressive model to identify translations (Xu, Weischedel, and Nguyen 2001). It is also possible to focus the extraction in one particular type of phenomena."
J16-2005,N12-1079,0,0.0918698,"following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volum"
J16-2005,C10-1124,0,0.3794,"this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but"
J16-2005,D14-1122,0,0.0205903,"oth in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publicists) who tweet in multiple languages; we see"
J16-2005,2005.eamt-1.37,0,0.0799727,"Missing"
J16-2005,P11-1122,0,0.136156,"document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quality control. Thus, in order to find good translations, subsequent post-editing and/or ranking is generally necessary. 3. The Intra-Document Alignment (IDA) Model As discussed above, content-based filtering is a method f"
J16-2005,J93-1004,0,\N,Missing
J16-2005,N03-1017,0,\N,Missing
J17-2002,Q16-1031,1,0.843163,"information in a joint model. Similar improvements may be achieved in an outof-domain scenario. Even though the parser is greedy, it provides very consistent results comparable with the best parsers of the state-of-the-art. We even obtained further improvement as demonstrated with the experiments with the dynamic oracles, that provide a push over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their"
J17-2002,P16-1231,0,0.190862,"w view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a"
J17-2002,P81-1022,0,0.756303,"Missing"
J17-2002,D12-1133,0,0.163485,"Missing"
J17-2002,Q13-1034,0,0.205338,"Missing"
J17-2002,W06-2920,0,0.0983953,"ddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section 6.2.1. 6.2.3 Data for the Dynamic Oracle. Because the arc-hybrid transition-based parsing algorithm is limited to fully projective trees, we use the same data as in Section 6.2.1, which makes it comparable with the basic model that uses standard word representations and a static oracle arc-standard algorithm. 6.2.4 CoNLL-2009 Data. We also report results with all the CoNLL 2009 data sets (Hajiˇc et al. 2009) to make a complete c"
J17-2002,P05-1022,0,0.0472943,"al embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, finding that they are quite benefic"
J17-2002,D14-1082,0,0.112043,"ork in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack"
J17-2002,C14-1078,0,0.146438,"Missing"
J17-2002,P13-1104,0,0.246028,"Missing"
J17-2002,P14-2111,0,0.126115,"Missing"
J17-2002,D07-1022,1,0.889561,"Missing"
J17-2002,W15-3904,0,0.0920647,"Missing"
J17-2002,P15-1033,1,0.882509,"therefore also investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,W13-5709,1,0.94387,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The dynamic oracles framework for training with exploration, suggested by Goldberg and Nivre (2012, 2013), provides answers to these questions. Although the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). 5.2.1 Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take under a given parser state. In contrast to static oracles that derive a canonical sequence for each gold parse tree and say nothing about parsing states that do not stem from this canonical path, the dynamic oracle is well-defined for states that result from parsing mistakes, and may produce more than a single gold action for a given state. Under the dynamic oracle framework, a parsing action is said to be optimal in a given state if the best tree that"
J17-2002,P11-2124,1,0.874063,"nt neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio, and Satta 2014; Honnibal, Goldb"
J17-2002,C12-1059,1,0.936735,"Missing"
J17-2002,Q13-1033,1,0.949811,"eaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled with the stack LSTM. Figure 4 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Because a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. 3.3.3 Arc-Hybrid. For the dynamic oracle training scenario, described in Section 5.2, we switch to the arc-hybrid transition system, which is amenable to an efficient dynamic oracle (Goldberg and Nivre 2013). The arc-hybrid system is summarized in Figure 5. The SHIFT and REDUCE - RIGHT transitions are the same as in arc-standard. However, the REDUCE - LEFT transition pops the top of the stack and attaches it as a child of the first item in the buffer. Although it is possible to extend the arc-hybrid system to support nonprojectivity by adding a SWAP transition, this extension would invalidate an important guarantee enabling efficient dynamic oracles.10 We therefore restrict the dynamic-oracle experiments to the fully projective English and Chinese treebanks. In order to parse nonprojective trees,"
J17-2002,Q14-1010,1,0.935088,"Missing"
J17-2002,P08-1043,1,0.846251,"014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio"
J17-2002,P13-2111,1,0.891593,"work uses continuous-valued relaxations of these. 3. Dependency Parser We now turn to the problem of learning representations of dependency parser states. We preserve the standard data structures of a transition-based dependency parser, namely, a buffer of words to be processed (B) and a stack (S) of partially constructed syntactic elements. Each stack element is augmented with a continuous-space vector embedding representing a word and, in the case of S, any of its syntactic dependents. Additionally, we introduce a third stack (A) to represent the history of transition actions taken by the 5 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. 316 } ) T L( IF DSH RE od am … pt | {z TO P {z S Greedy Transition-Based Dependency Parsing with Stack LSTMs B P TO | Ballesteros et al. } amod ; was made TO |{z } an decision overhasty P root ; REDUCE-LEFT(amod) A SHIFT … Figure 2 Parser state computation encountered while parsing the sentence an overhasty decision was made. Here S designates the stack of partially constructed dependency subtre"
J17-2002,D14-1099,0,0.164827,"Missing"
J17-2002,W09-1201,0,0.137744,"Missing"
J17-2002,N03-1014,0,0.736917,"ounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LS"
J17-2002,P04-1013,0,0.567345,"Missing"
J17-2002,P13-1088,0,0.0439757,"and qz is a bias term for action z. The set A(S, B) represents the valid transition actions that may be taken given the current contents of the stack and buffer.9 Because pt = f (st , bt , at ) encodes information about all previous decisions made by the parser, the chain rule may be invoked to write the probability of any valid sequence of parse transitions z conditional on the input as: p(z |w) = |z| Y t=1 p(zt |pt ) (2) 3.2 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Hermann and Blunsom 2013; Socher et al. 2011, 2013a, 2013b). We follow previous work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed in Section 4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S,"
J17-2002,W13-3518,1,0.937487,"Missing"
J17-2002,Q14-1011,0,0.100404,"Missing"
J17-2002,P11-1068,0,0.272942,"Missing"
J17-2002,N16-1030,1,0.677399,"Missing"
J17-2002,D14-1081,0,0.0383284,"crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by the stack, buffer, an"
J17-2002,N15-1142,1,0.846911,"and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with probability 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained Word Embeddings. There are several options for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches that discard order perform less well (Bansal, Gimpel, and Livescu 2014); therefore, we used a variant of the skip n-gram model introduced by Ling et al. (2015a), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. 4.2 Modeling Characters Instead of Words Following Ling et al. (2015b), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber 2005). When the parser initiates the learning process and populates the buffer with all the words from the sentence, it reads the words character by character from left to right and computes a continuous-space vector embedding the character se"
J17-2002,D15-1176,1,0.903245,"Missing"
J17-2002,de-marneffe-etal-2006-generating,0,0.292516,"Missing"
J17-2002,D13-1032,0,0.0288931,"Experiments with Static Oracle and Standard Word Representations We report results on five experimental configurations per language, as well as the Chen and Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (−POS), the stack LSTM parsing model without pretrained language model embeddings (−pretraining), the 13 Training: 02-21. Development: 22. Test: 23. 14 Training: 001–815, 1001–1136. Development: 886–931, 1148–1151. Test: 816–885, 1137–1147. ¨ 15 The POS tags were calculated with MarMot tagger (Mueller, Schmid, and Schutze 2013) by the best ¨ performing system of the SPMRL Shared Task (Bjorkelund et al. 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 16 Because the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4,996 sentences of the training set as a development set. 328 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 1 Unlabeled attachment scores and labeled attachment scores on the development sets (top) and the final test"
J17-2002,W03-3017,0,0.216793,"ril 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—which is unboun"
J17-2002,W04-0308,0,0.395191,"embeddings of the head, dependent, and relation and applying a linear operator and a component-wise nonlinearity as follows: c = tanh (U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. Th"
J17-2002,J08-4003,0,0.37078,"4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S, B) is the complete set of parser actions discussed in Section 3.3, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT transition is obligatory (Nivre 2008). 318 Ballesteros et al. det Greedy Transition-Based Dependency Parsing with Stack LSTMs amod an overhasty mod an decision cc2 rel det head mod overhasty c1 head rel amod decision Figure 3 The representation of a dependency subtree (top) is computed by recursively applying composition functions to hhead, modifier, relationi triples. In the case of multiple dependents of a single head, the recursive branching order is imposed by the order of the parser’s reduce transition (bottom). in the order they are “reduced” in the parser, as illustrated in Figure 3. Each node in this expanded syntactic tr"
J17-2002,P09-1040,0,0.801651,"(U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. The arc-standard transition inventory (Nivre 2004) is given in Figure 4. The SHIFT transition moves a word from the buffer to the stack,"
J17-2002,W06-2933,0,0.104845,"Missing"
J17-2002,P05-1013,0,0.662475,"B B B (v, v), B Dependency — r u→v r u←v — Figure 4 Parser transitions of the arc-standard system (with swap, Section 3.3.2) indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. (gr (x, y), x) refers to the composition function presented in 3.2. the stack. The arc-standard system allows building all and only projective trees. In order to parse nonprojective trees, this can be combined with the pseudo-projective approach (Nivre and Nilsson 2005) or follow what is presented in Section 3.3.2. 3.3.2 Arc-Standard with Swap. In order to deal with nonprojectivity, the arc-standard system can be augmented with a SWAP transition (Nivre 2009). The SWAP transition removes the second-to-top item from the stack and pushes it back to the buffer, allowing for the creation of nonprojective trees. We only use this transition when the training data set contains nonprojective trees. The inclusion of the SWAP transition requires breaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled"
J17-2002,nivre-etal-2006-talbanken05,0,0.111357,"82.15 79.04 92.57 90.21 Words + POS UAS LAS 86.85 85.36 93.04 90.87 Words + Chars UAS LAS 81.90 78.81 92.56 90.38 Words + Chars + POS UAS LAS 86.92 85.49 92.75 90.62 Test: Language Chinese English 6.4.3 Comparison with State-of-the-Art Parsers. Table 5 shows a comparison with state-ofthe-art parsers. We include greedy transition-based parsers that, like ours, do not apply beam search. For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006), who also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. For English and Chinese, we report our results of the best parser on the develpment set in Section 6.3—which is Words + POS but with pretrained word embeddings. We also show the best reported results on these data sets. For the SPMRL data ¨ sets, the best performing system of the shared task is either Bjoreklund et al. (2013) or ¨ Bjorkelund et al. (2014), which are better than our system. Note that the comparison is harsh to our system, which does not use unlabeled data nor any"
J17-2002,J17-2002,1,0.0512826,"Missing"
J17-2002,P00-1061,0,0.0437166,"that make use of external embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, findin"
J17-2002,W14-6111,0,0.0979038,"Missing"
J17-2002,W13-4917,1,0.910835,"Missing"
J17-2002,seeker-kuhn-2012-making,0,0.0639458,"uage model word embeddings were generated from the AFE portion of the English Gigaword corpus (version 5), and from the complete Chinese Gigaword corpus (version 2), as segmented by the Stanford Chinese Segmenter (Tseng et al. 2005). 6.2.2 Data to Test the Character-Based Representations and Static Oracle for Training. For the character-based representations we applied our model to the treebanks of the SPMRL ¨ Shared Task (Seddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section"
J17-2002,P13-1045,0,0.0704272,"3, Number 2 manually crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by"
J17-2002,D13-1170,0,0.00710017,"Missing"
J17-2002,K16-1019,1,0.850904,"sh over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their help with the parsing algorithms. This work was sponsored in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533, and in part by NSF CAREER grant IIS-1054319. Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENS"
J17-2002,P15-1150,0,0.158426,"Missing"
J17-2002,P07-1080,0,0.609122,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,W07-2218,0,0.744034,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,P15-3004,0,0.0618368,"so investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,N03-1033,0,0.365748,"Missing"
J17-2002,P06-3009,0,0.158184,"xisting models. For instance, Chrupala (2014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different w"
J17-2002,I05-3027,0,0.0858346,"Missing"
J17-2002,Q16-1014,0,0.139104,"Missing"
J17-2002,P15-1113,0,0.105221,"Missing"
J17-2002,P15-1032,0,0.317311,"parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can"
J17-2002,W03-3023,0,0.341093,"ised version received: 6 April 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—whi"
J17-2002,K15-1015,0,0.465378,"finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack pointer that is manipulated by push and pop o"
J17-2002,D15-1251,1,0.901456,"Missing"
J17-2002,P16-1147,0,0.0322281,"rd embeddings are useful for other languages we also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training set-up as in Section 4; for English and Chinese we used the same pretrained word embeddings as in previous experiments, for German we pretrained embeddings using the monolingual training data from the WMT 2015 data set22 , and for Spanish we used the Spanish Gigaword version 3. The results for the parser with character-based representations on these data sets (last line of the table) were published by Andor et al. (2016). In Zhang and Weiss (2016), it is also possible to find results of the same version of the parser on the Universal Dependency treebanks (Nivre et al. 2015). 21 We report the performance of these parsers in the most comparable set-up, that is, with beam = 1 or greedy. 22 http://www.statmt.org/wmt15/translation-task.html. 336 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 7 Results of the parser in its different versions including comparison with other systems. St. refers to static oracle with the arc-standard parser and Dyn. refers to dynamic oracle with the arc-hybrid parser with α"
J17-2002,D08-1059,0,0.177172,"Missing"
J17-2002,P11-2033,0,0.0417267,"tions on this sequence, and r the complete contents of the stack of partially constructed syntactic structures. This global sensitivity of the state representation contrasts with most previous work in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which"
J17-2002,P15-1117,0,0.148064,"ders only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be unders"
J17-2002,N07-1050,0,\N,Missing
J17-2002,D15-1041,1,\N,Missing
J17-2002,W13-4916,0,\N,Missing
J17-2002,J13-1002,1,\N,Missing
J17-2002,P14-6005,0,\N,Missing
J17-2002,W14-6110,0,\N,Missing
J17-2002,Q14-1017,0,\N,Missing
J17-2002,W13-4907,1,\N,Missing
J17-2002,P14-2131,0,\N,Missing
J17-2002,C14-1076,1,\N,Missing
J17-2002,W15-2210,0,\N,Missing
J17-2002,P15-2042,0,\N,Missing
K15-1003,J07-3004,0,0.0232518,") 0 A(y |y) = min 1, p(y |θ LCTX , θ RCTX ) For completeness, we note that the probability of a tree y given only the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the tree"
K15-1003,C08-1008,1,0.86999,"equence ADJ NOUN frequently occurs between the tags DET and VERB. This DET—VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN . From this, we might deduce that DET — VERB is a likely context for a noun phrase. CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability. However, since there is nothing intrinsic about the POS pair DET—VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data. Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000; Steedman and Baldridge, 2011) categories reflect universal grammatical properties. CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents. For example, a category might encode that “this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence” instead of simply V"
K15-1003,N07-1018,0,0.284036,"design of models and learning procedures that result in better parsing tools. Given our desire to train NLP models in low-supervision scenarios, the possibility of constructing inductive biases out of universal properties of language is enticing: if we can do this well, then it only needs to be done once, and can be applied to any language or domain without adaptation. In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model, we develop a blocked sampler based on that of Johnson et al. (2007) to sample parse trees for sentences in the raw training corpus according to their posterior probabilities. However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios. pler categories by encoding a notion of category simp"
K15-1003,P02-1017,0,0.256356,"it the properties of CCG. Our experiments show that our model outperforms a baseline in which this context information is not captured. 1 Introduction Learning parsers from incomplete or indirect supervision is an important component of moving NLP research toward new domains and languages. But with less information, it becomes necessary to devise ways of making better use of the information that is available. In general, this means constructing inductive biases that take advantage of unannotated data to train probabilistic models. One important example is the constituentcontext model (CCM) of Klein and Manning (2002), which was specifically designed to capture the linguistic observation made by Radford (1988) that there are regularities to the contexts in which constituents appear. This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts. For example, 22 Proceedings of the 19th Conference on Computational Language Learning, pages 22–31, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics able data in the context of studying child language acquisition (e.g., Villavicencio, 2002; Goldwater, 2007), we are interested in applying t"
K15-1003,Q13-1007,0,0.0395254,"Missing"
K15-1003,D14-1107,0,0.0241254,"he interaction between universal grammar and observ23 n s s
p np np/n The pp n man (s
p)/pp pp/np walks to np work n/n n s
p The lazy dog sleeps Figure 2: Higher-level category n subsumes the categories of its constituents. Thus, n should have a strong prior on combinability with its adjacent supertags np/n and s
p. Figure 1: CCG parse for “The man walks to work.” Parameters: θ ROOT ∼ Dir(αROOT , θ ROOT-0 ) θtBIN ∼ Dir(αBIN , θ BIN -0 ) θtUN ∼ Dir(αUN , θ UN -0 ) θtTERM ∼ Dir(αTERM , θtTERM -0 ) λt ∼ Dir(αλ , λ0 ) θtLCTX ∼ Dir(αLCTX , θtLCTX -0 ) θtRCTX ∼ Dir(αRCTX , θtRCTX -0 ) We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 np/n Generative Model ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T Sentence: In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. do s ∼ Cat(θ ROOT ) y |s ∼ SCM(s) until the tree y is valid The CCG formalism is said to be naturally associativ"
K15-1003,I11-1049,0,0.0375395,"Missing"
K15-1003,J93-2004,0,0.0511752,"robability of a tree y given only the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introd"
K15-1003,D10-1120,0,0.0495496,"Missing"
K15-1003,bosco-etal-2000-building,0,0.0377197,"≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Gigaword 5 corp"
K15-1003,J99-1004,0,0.0388977,"adverb that modifies a verb); and (4) operators may occur at different rates, as given by pfwd . We can use PCAT to define priors on our production parameters that bias our model toward rules 1 Note that this version has also updated the probability definitions for modifiers to be sums, incorporating the fact that any A/A is also a A/B (likewise for AA). This ensures that our grammar defines a valid probability distribution. 2 The probability distribution over categories is guaranteed to be proper so long as pterm > 12 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). Non-terminal production prior means For the root, binary, and unary parameters, we want to choose prior means that encode our bias 25 that result in a priori more likely categories:3 atoms have features associated, then the atoms are allowed to unify if the features match, or if at least one of them does not have a feature. In defining κ, it is also important to ignore possible arguments on the wrong side of the combination since they can be consumed without affecting the connection between the two. To achieve this for κ(t, u), it is assumed that it is possible to consume all preceding argum"
K15-1003,J07-4004,0,0.0423889,"n should have a strong prior on combinability with its adjacent supertags np/n and s
p. Figure 1: CCG parse for “The man walks to work.” Parameters: θ ROOT ∼ Dir(αROOT , θ ROOT-0 ) θtBIN ∼ Dir(αBIN , θ BIN -0 ) θtUN ∼ Dir(αUN , θ UN -0 ) θtTERM ∼ Dir(αTERM , θtTERM -0 ) λt ∼ Dir(αλ , λ0 ) θtLCTX ∼ Dir(αLCTX , θtLCTX -0 ) θtRCTX ∼ Dir(αRCTX , θtRCTX -0 ) We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 np/n Generative Model ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T Sentence: In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. do s ∼ Cat(θ ROOT ) y |s ∼ SCM(s) until the tree y is valid The CCG formalism is said to be naturally associative since a constituent label is often able to combine on either the left or the right. As a motivating example, consider the sentence “The lazy dog sleeps”, as shown in Figure 2. The word lazy, with category n/n, can either combine"
K15-1003,D12-1075,1,0.861669,"categories:  σ · 1/|T | if κ(t, r) σ > 1 right P (r |t) = 1/|T | otherwise  σ · PCAT (r) if κ(t, r) σ > 1 right PCAT (r |t) = PCAT (r) otherwise θ ROOT-0 (t) = PCAT (t) θ BIN -0 (hu, vi) = PCAT (u) · PCAT (v) θ UN -0 (hui) = PCAT (u) For simplicity, we assume the production-type mixture prior to be uniform: λ0 = h 13 , 31 , 31 i. 3.2 Terminal production prior means We employ the same procedure as our previous work for setting the terminal production prior distributions θtTERM -0 (w) by estimating word-givencategory relationships from the weak supervision: the tag dictionary and raw corpus (Garrette and Baldridge, 2012; Garrette et al., 2015).4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags. These counts are then combined with estimates of the “openness” of each tag in order to assess its likelihood of appearing with new words. 3.3 Context parameter prior means In order to encourage our model to choose trees in which the constituent labels “fit” into their supertag contexts, we want to bias our context parameters toward context categories that"
K15-1003,C10-1122,0,0.0163412,"the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introducing special conjunction rules. In"
K15-1003,W14-1615,1,0.846747,"allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios. pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger (Garrette et al., 2014). In this paper, we present a novel parsing model that is designed specifically for the capacity to capture both of these universal, intrinsic properties of CCG. We do so by extending our previous, PCFG-based parsing model to include parameters that govern the relationship between constituent categories and the preterminal categories (also known as supertags) to the left and right. The advantage of modeling context within a CCG framework is that while CCM must learn which contexts are likely purely from the data, the CCG categories give us obvious a priori information about whether a context i"
K15-1003,D07-1071,0,0.0387248,"n we would have to take a score of zero for that sentence: every dependency would be “wrong”. Thus, it is important that we make a best effort to find a parse. To accomplish this, we implemented a parsing backoff strategy. The parser first tries to find a valid parse that has either sdcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the “deletion” strategy employed by Zettlemoyer and Collins (2007), but we do it directly in the grammar. We add unary rules of the form hDi→u 29 the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels. Other researchers have shown positive results for grammar induction by introducing relatively small amounts of linguistic knowledge. Naseem et al. (2010) induced dependency parsers by handconstructing a small set of linguistically-universal dependency rules and using them as soft constraints during learning. These rules were useful for disambiguating between various structures in"
K16-1019,J13-1002,1,0.83594,"data structures: a syntactic stack St , a semantic stack Mt —each containing partially built structures—and a buffer of input words Bt . Our algorithm also places partial syntactic and semantic parse structures onto the front of the buffer, so it is also implemented as a stack. Each arc in the output corresponds to a transition (or “action”) chosen based on the current state; every transition modifies the state by updating St , Mt , and Bt to St+1 , Mt+1 , and Bt+1 , respectively. While each state may license several valid actions, each action 2 This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), in contrast to Henderson et al. (2013), who initialized with root at the buffer front. 3 Note that in the original arc-eager algorithm (Nivre, 2008), S HIFT and R IGHT-A RC actions move the item on the buffer front to the stack, whereas we only copy it (to allow the semantic operations to have access to it). 1 https://github.com/clab/ joint-lstm-parser 188 and Congress has other fied and the algorithm returns to syntactic transitions. This implies that, for each word, its leftside syntactic dependencies are resolved before its left-side semantic dependencies. An example run of the algorithm"
K16-1019,D15-1041,1,0.865442,"le 3: Comparison on the CoNLL 2009 English test set. The first block presents results of other models evaluated for both syntax and semantics on the CoNLL 2009 task. The second block presents our models. The third block presents the best published models, each using its own syntactic preprocessing. corporate morphological features where available; this could potentially improve performance, especially in highly inflective languages like Czech. An alternative might be to infer word-internal representations using character-based word embeddings, which was found beneficial for syntactic parsing (Ballesteros et al., 2015). Language Catalan Chinese Czech English German Japanese Spanish Average #1 C+’09 81.84 76.38 83.27 87.00 82.44 85.65 81.90 82.64 #2 Z+ ’09a 83.01 76.23 80.87 87.69 81.22 85.28 83.31 82.52 #3 G+ ’09 82.66 76.15 83.21 86.03 79.59 84.91 82.43 82.14 7 Conclusion We presented an incremental, greedy parser for joint syntactic and semantic dependency parsing. Our model surpasses the performance of previous joint models on the CoNLL 2008 and 2009 English tasks, without using expert-crafted, expensive features of the full syntactic parse. Joint 82.40 79.27 79.53 87.45 81.05 80.91 83.11 81.96 Acknowled"
K16-1019,W09-1206,0,0.209308,"Missing"
K16-1019,C10-3009,0,0.177351,"Missing"
K16-1019,D13-1152,0,0.0460906,"Missing"
K16-1019,W08-2122,0,0.59599,"al setWe present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008–9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics. 1 Introduction We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2"
K16-1019,W05-0620,0,0.265775,"Missing"
K16-1019,W08-2134,0,0.144763,"updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the development performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB. 5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expensive system. State-of"
K16-1019,J13-4006,0,0.565554,"and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic parse tree, 187 Proceedings of the 20th SIGN"
K16-1019,W09-1207,0,0.0159494,"performs the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expensive system. State-of-the-art SRL systems (shown in the last block of Table 3) which use advances orthogonal to the contributions in this paper, perform better than our models. Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Bj¨orkelund et al., 2009; Bj¨orkelund et al., 2010; Roth and W"
K16-1019,P82-1020,0,0.887895,"Missing"
K16-1019,W08-2138,0,0.0752159,"Missing"
K16-1019,W08-2123,0,0.016286,"L 2009 English test set on a single core. This is almost 2.5 times faster than the pipeline model of Lei et al. (2015) (439.9±42 seconds) on the same machine.8 Table 2: Joint parsers: comparison on the CoNLL 2008 test (WSJ+Brown) set. sitions is derived, showing the benefit of learning a representation for the entire algorithmic state. Several other joint learning models have been proposed (Llu´ıs and M`arquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models. The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system’s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower. CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013), demonstrating the benefit of our entire-parser-state repre"
K16-1019,D09-1059,0,0.230367,"Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We beli"
K16-1019,P14-1112,0,0.0304826,"of the U.S. Army Research Office or the U.S. Government. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Table 4: Comparison of macro F1 scores on the multilingual CoNLL 2009 test set. 6 a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset. There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and M`arquez, 2005), their approach inspired Zhou a"
K16-1019,P15-1033,1,0.571352,"features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design. Our system’s performance does not ma"
K16-1019,D15-1112,0,0.418532,"Missing"
K16-1019,D15-1169,0,0.122887,"Missing"
K16-1019,S15-1033,0,0.183514,"Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for"
K16-1019,P10-1113,0,0.0151724,"oject MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Table 4: Comparison of macro F1 scores on the multilingual CoNLL 2009 test set. 6 a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset. There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and M`arquez, 2005), their approach inspired Zhou and Xu (2015), who achieved state-of-the-art results using deep bidirectional LSTMs. Our approach for dependency-based SRL is not directly compara"
K16-1019,W09-1205,0,0.35133,"oduction We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic"
K16-1019,N15-1142,1,0.750894,"in the English CoNLL 2009 training data. where v and u are vectors corresponding to atomic words or composed parse fragments; l and r are learned vector representations for syntactic and semantic labels respectively. Syntactic and semantic parameters are separated (Zs , es and Zm , em , respectively). Finally, for predicates, we use another recursive function to compose the word representation, v with a learned representation for the dismabiguated sense of the predicate, p: gd (v, p) = tanh(Zd [v; p] + ed ) Pretrained Embeddings Following Dyer et al. (2015), “structured skipgram” embeddings (Ling et al., 2015) were used, trained on the English (AFP section), German, Spanish and Chinese Gigaword corpora, with a window of size 5; training was stopped after 5 epochs. For out-of-vocabulary words, a randomly initialized vector of the same dimension was used. Figure 5: Example of a joint parse tree fragment with vector representations shown at each node. The vectors are obtained by recursive composition of representations of head, dependent, and label vectors. Syntactic dependencies and labels are in green, semantic in blue. gs (v, u, l) = tanh(Zs [v; u; l] + es ) (5) Training Training the classifier req"
K16-1019,J02-3001,0,0.506702,"Joint Syntactic-Semantic Parsing with Stack LSTMs Swabha Swayamdipta♣ Miguel Ballesteros♦ Chris Dyer♠ Noah A. Smith♥ ♣ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA ♦ Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data st"
K16-1019,W08-2124,0,0.0851174,"Missing"
K16-1019,P15-1032,0,0.050578,"Missing"
K16-1019,Q13-1018,0,0.0957373,"Missing"
K16-1019,J93-2004,0,0.0604819,"Missing"
K16-1019,W08-2127,0,0.227225,"nt descent was used and updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the development performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB. 5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expe"
K16-1019,W04-2705,0,0.160463,"Missing"
K16-1019,W09-1209,0,0.713032,"ts of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design. Our system’s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Bj¨orkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (T¨ackstr¨om et al., 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al., 2015). Many advances in those systems are orthogonal to our model, and we expect future work to achieve further gains by integrating them. Because our system is very fast— with an end-to-end runtime of 177.6±18 seconds to parse the CoNLL 2009 English test data on a single core—we believe it will be useful in practical setWe present a transition-based parser that jointl"
K16-1019,P15-1109,0,0.238586,"Missing"
K16-1019,J08-4003,0,0.451374,"are expected to reopen soon expect.01 reopen.01 C-A1 A1 AM-TMP A1 Figure 1: Example of a joint parse. Syntactic dependencies are shown by arcs above the sentence and semantic dependencies below; predicates are marked in boldface. C- denotes continuation of argument A1. Correspondences between dependencies might be close (between expected and to) or not (between reopen and all). 2.2 There are separate sets of syntactic and semantic transitions; the former manipulate S and B, the latter M and B. All are formally defined in Table 1. The syntactic transitions are from the “arceager” algorithm of Nivre (2008). They include: • S-S HIFT, which copies3 an item from the front of B and pushes it on S. • S-R EDUCE pops an item from S. • S-R IGHT(`) creates a syntactic dependency. Let u be the element at the top of S and v be the element at the front of B. The new dependency has u as head, v as dependent, and label `. u is popped off S, and the resulting structure, rooted at u, is pushed on S. Finally, v is copied to the top of S. • S-L EFT(`) creates a syntactic dependency with label ` in the reverse direction as S-R IGHT. The top of S, u, is popped. The front of B, v, is replaced by the new structure,"
K16-1019,P09-1040,0,0.412162,"pty. Actions that pop from a stack (S-R EDUCE and M-R EDUCE) are forbidden when that stack is empty. We disallow actions corresponding to the same dependency, or the same predicate to be repeated in the sequence. Repetitive M-S WAP transitions are disallowed to avoid infinite swapping. Finally, as noted above, we restrict the parser to syntactic actions until it needs to shift an item from B to S, after which it can only execute semantic actions until it executes an M-S HIFT. Asymptotic runtime complexity of this greedy algorithm is linear in the length of the input, following the analysis by Nivre (2009).5 Because SRL graphs allow a node to be a semantic argument of two parents—like all in the example in Figure 1—M-L EFT and M-R IGHT do not remove the dependent from the semantic stack and buffer respectively, unlike their syntactic equivalents, S-L EFT and S-R IGHT. We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues: • M-S WAP swaps the top two items on M , to allow for crossing semantic arcs. • M-P RED(p) marks the item at the front of B as a semantic predicate with the sense p, and replaces it with the disambiguated predicate. The CoNLL 2009"
K16-1019,J05-1004,0,0.264767,"Missing"
K16-1019,D14-1045,0,0.175162,"er Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA ♦ Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used i"
K16-1019,W08-2121,0,0.31456,"Missing"
K16-1019,W05-0636,0,0.0398908,"ures at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic parse tree, 187 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 187–197, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics has a deterministic effect on the state of the algorithm. Initially, S0 and M0 are empty, and B0 contains the input sentence with the first word at the front of B and a special root symbol at the end.2"
K16-1019,Q15-1003,0,0.171472,"Missing"
K16-1019,J08-2002,0,0.328138,"Missing"
K19-1022,K17-3004,0,0.0306181,"on the languages discussed in this work, use discriminative models. Kanayama et al. (2017) had tremendous success on Japanese using a wildly different approach. They train a model to identify likely syntactic heads, then assume that all other words simply attach in a left-branching structure, which works due to the strictly head-final nature of Japanese. Dozat et al. (2017) train a discriminative neural parser which uses a BiLSTM to generate hidden representations of each word (Kiperwasser and Goldberg, 2016). These representations are used to score arcs, which are greedily added to the tree. Björkelund et al. (2017) perform best on Arabic, using an ensemble of many different types of bottom-up discriminative parsers. They have each of twelve parsers score potential arcs, learn a weighting function to combine them, and use the Chu-Liu-Edmonds algorithm (Chu, 1965; Edmonds, 1967) to output final parses. All three of these discriminative models are very effective for analysis of a sentence, none of them are able to be converted into a similar generative model. At best, the biaffine model of Dozat et al. (2017) could generate a bag of dependencies without order information, which makes it impractical as the"
K19-1022,P15-2142,0,0.0158506,"its subject noun, may have large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative parsing model with no independence assumptions based on sigmoid belief networks (Neal, 1992) instead of RNNs. The CoNLL 2017 shared task saw many different models succeed at parsing Universal Dependencies. Most of the top contenders, including the be"
K19-1022,N18-1086,0,0.0321065,"Missing"
K19-1022,K17-3002,0,0.35251,"el, and our bottom-up model to obtain a score for the parse from each. We combine these scores using weights learned to optimize performance on the development set (Och, 2003). 3 Data Sets 3.2 Baseline Models On the language modeling task we compare against a standard LSTM-based language model baseline (Mikolov et al., 2010), using 1024dimensional 2-layer LSTM cells, and optimized using Adam (Kingma and Ba, 2014). For the parsing task we compare against the discriminative parser of Dyer et al. (2015), a bottom-up transition-based parser that uses stackLSTMs, as well as the overall top system (Dozat et al., 2017) from the 2017 CoNLL shared task on multilingual dependency parsing (Zeman et al., 2017). That work uses a discriminative graphbased parser that uses a biaffine scoring function to score each potential arc. Moreover, it uses character-level representations to deal with morphology and a PoS tagger more sophisticated than UDPipe – two major changes from the shared task’s default pipeline. These two differences afford them a substantial advantage over our approach which only modifies the parsing step of the pipeline. Finally, we show the results of an oracle system looking at the 1000-best lists"
K19-1022,P15-1033,1,0.909014,"e gold-standard data using in our language modeling experiments. In the second, we again train on gold data, but use UDPipe (Straka and Straková, 2017) to segment, tokenize, and POS tag the dev and test sets starting from raw text, following the default scenario and most participants in the CoNLL 2017 shared task. very STOP-L STOP-R the very tall old man Figure 4: Examples of embedding two subtrees in the top-down model. A subtree is embedded using an LSTM over its child subtrees (solid lines) with a gated residual connection from the root word to the final embedding (dotted lines). parser of Dyer et al. (2015), a discriminative neural stack-LSTM-based bottom-up parser, as our proposal distribution q(x, y) and compute the approximate marginal using N = 1000 samples per P p(x,y) sentence: p(x) ≈ N1 N i=1 q(x,y) . 2.4 Parsing Evaluation through Reranking In order to evaluate our model as a parser we would ideally like to efficiently find the MAP parse tree given an input sentence. Unfortunately, due to the unbounded dependencies across the sequences of actions used by our models this inference is infeasible. As such, we instead rerank a list of 1000 samples produced by the baseline discriminative pars"
K19-1022,N16-1024,1,0.899149,"tom up and the other top down, which profoundly changes the estimation problem faced by the learner. We evaluate the two models on three typologically different languages: English, Arabic, and Japanese. We find that both generative models improve parsing performance over a discriminative baseline, but, in contrast to RNNGs, they are significantly less effective than non-syntactic LSTM language models. Little difference between the tree construction orders is observed for either parsing or language modeling. 1 Chris Dyer DeepMind cdyer@google.com Introduction Recurrent neural network grammars (Dyer et al., 2016, RNNGs) are syntactic language models that use predicted syntactic structures to determine the topology of the recurrent networks they use to predict subsequent words. Not only can they learn to model language better than non-syntactic language models, but the conditional distributions over parse trees given sentences produce excellent parsers (Fried et al., 2017). In this paper, we introduce and evaluate two new dependency syntax language models which are based on a recurrent neural network (RNN) 1 We release code for these two models, which can be found at https://github.com/armatthews/ dep"
K19-1022,P17-2025,0,0.08465,"han non-syntactic LSTM language models. Little difference between the tree construction orders is observed for either parsing or language modeling. 1 Chris Dyer DeepMind cdyer@google.com Introduction Recurrent neural network grammars (Dyer et al., 2016, RNNGs) are syntactic language models that use predicted syntactic structures to determine the topology of the recurrent networks they use to predict subsequent words. Not only can they learn to model language better than non-syntactic language models, but the conditional distributions over parse trees given sentences produce excellent parsers (Fried et al., 2017). In this paper, we introduce and evaluate two new dependency syntax language models which are based on a recurrent neural network (RNN) 1 We release code for these two models, which can be found at https://github.com/armatthews/ dependency-lm. 2 In this work, we limit ourselves to models that are capable only of generating projective dependency trees. 227 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 227–237 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 0 $ Top-down 1 sings 2 John 3 STOP-L 5 STOP-L 6 12 STOP-R wel"
K19-1022,W03-3017,0,0.181107,"n on why the two models’ performances are so similar. Unfortunately the fact that the models use different conditioning contexts makes direct comparison of sub-sentential scores impossible. The topdown model, which generates the verb before its subject noun, may have large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative"
K19-1022,P04-1013,0,0.112698,"Missing"
K19-1022,D16-1073,0,0.0253279,"ave large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative parsing model with no independence assumptions based on sigmoid belief networks (Neal, 1992) instead of RNNs. The CoNLL 2017 shared task saw many different models succeed at parsing Universal Dependencies. Most of the top contenders, including the best scoring systems on"
K19-1022,K17-3028,0,0.0672997,"Missing"
K19-1022,K17-3001,0,0.150413,"nning over entries from oldest to newest. The resulting vector h is then passed through an MLP, and then a softmax over the three possible action types. If the SHIFT action is taken, the vector h is re-used and passed through a separate MLP and softmax over the vocabulary to choose an individual word to generate. If one of the two REDUCE actions is chosen, the top two elements from the stack are popped, concatenated (with the head-to-be first, followed by the child), and passed through an MLP. The result is a vector representing a new subtree that is then pushed onto the stack. Kuncoro et al. (2017) Marginalization Traditionally a language model takes a sentence x and assigns it a probability p(x). Since our syntax-based language models jointly predicts the probability p(x, y) of a sequence of terminals x and a tree y, we must marginalize over trees to get the total P probability assigned to a sentence x, p(x) = y∈T (x) p(x, y), where T (x) represents the set of all possible dependency trees over a sentence x. Unfortunately the size of T (x) grows exponentially in the length of x, making explicit marginalization infeasible. Instead we use importance sampling to approximate the marginal ("
K19-1022,P03-1021,0,0.0141966,"ces of actions used by our models this inference is infeasible. As such, we instead rerank a list of 1000 samples produced by the baseline discriminative parser, a combination process that has been shown to improve performance by combining the different knowledge learned by the discriminative and generative models (Fried et al., 2017). For each hypothesis parse in the sample list we query the discriminative parser, our top-down model, and our bottom-up model to obtain a score for the parse from each. We combine these scores using weights learned to optimize performance on the development set (Och, 2003). 3 Data Sets 3.2 Baseline Models On the language modeling task we compare against a standard LSTM-based language model baseline (Mikolov et al., 2010), using 1024dimensional 2-layer LSTM cells, and optimized using Adam (Kingma and Ba, 2014). For the parsing task we compare against the discriminative parser of Dyer et al. (2015), a bottom-up transition-based parser that uses stackLSTMs, as well as the overall top system (Dozat et al., 2017) from the 2017 CoNLL shared task on multilingual dependency parsing (Zeman et al., 2017). That work uses a discriminative graphbased parser that uses a biaf"
K19-1022,K17-3009,0,0.0165193,"English, Japanese, and Arabic, as provided for the 2017 CoNLL shared task on universal dependency parsing. In all languages we convert all singleton terminal symbols to a special UNK token. See Table 1 for details regarding the size of these data sets. For language modeling we evaluate using the gold sentence segmentations, word tokenizations, and part of speech tags given in the data. For parsing, we evaluate in two scenarios. In the first, we train and test on the same gold-standard data using in our language modeling experiments. In the second, we again train on gold data, but use UDPipe (Straka and Straková, 2017) to segment, tokenize, and POS tag the dev and test sets starting from raw text, following the default scenario and most participants in the CoNLL 2017 shared task. very STOP-L STOP-R the very tall old man Figure 4: Examples of embedding two subtrees in the top-down model. A subtree is embedded using an LSTM over its child subtrees (solid lines) with a gated residual connection from the root word to the final embedding (dotted lines). parser of Dyer et al. (2015), a discriminative neural stack-LSTM-based bottom-up parser, as our proposal distribution q(x, y) and compute the approximate margina"
K19-1022,Q16-1023,0,0.165867,". Unfortunately the fact that the models use different conditioning contexts makes direct comparison of sub-sentential scores impossible. The topdown model, which generates the verb before its subject noun, may have large entropy when choosing the verb, but an easier time choosing the subject since it can condition on the verb limiting its choices to appropriate semantic classes, person, 233 Top-Down Bottom-Up Structure 4.97 7.42 Terminals 67.9 63.3 6 Related Work Most work on discriminative dependency parsing follows the bottom-up paradigm (Nivre, 2003; Nivre et al., 2007; Dyer et al., 2015; Kiperwasser and Goldberg, 2016), but top-down models have also shown some promise (Zhang et al., 2015). Generative dependency models go back to Hays (1964), but most existing such models (Buys and Blunsom, 2015; Jiang et al., 2016) have relied on independence assumptions whether used for parsing, unsupervised dependency induction, or language modeling. Buys and Blunsom (2015) also describe a generative bottom-up neural parser, but use hand-crafted input features and limit the model to third-order features. Titov and Henderson (2010) explore a generative parsing model with no independence assumptions based on sigmoid belief"
K19-1022,E17-1117,1,0.800545,"sing an LSTM running over entries from oldest to newest. The resulting vector h is then passed through an MLP, and then a softmax over the three possible action types. If the SHIFT action is taken, the vector h is re-used and passed through a separate MLP and softmax over the vocabulary to choose an individual word to generate. If one of the two REDUCE actions is chosen, the top two elements from the stack are popped, concatenated (with the head-to-be first, followed by the child), and passed through an MLP. The result is a vector representing a new subtree that is then pushed onto the stack. Kuncoro et al. (2017) Marginalization Traditionally a language model takes a sentence x and assigns it a probability p(x). Since our syntax-based language models jointly predicts the probability p(x, y) of a sequence of terminals x and a tree y, we must marginalize over trees to get the total P probability assigned to a sentence x, p(x) = y∈T (x) p(x, y), where T (x) represents the set of all possible dependency trees over a sentence x. Unfortunately the size of T (x) grows exponentially in the length of x, making explicit marginalization infeasible. Instead we use importance sampling to approximate the marginal ("
K19-1022,J93-2004,0,0.0647921,"eneration events show, the top-down model generates recursively from the root, whereas the bottomup model generates from left to right. ing distracted by accidental correlations), while in the other it may be more distant. These differences thus imply that the two models will have different structural biases, but it is not at all clear whether one should out perform the other. We therefore explore to what extent this choice of construction order affects performance, and we evaluate the proposed models on language modeling and parsing tasks across three typologically different languages (§3). (Marcus et al., 1993). Finally, we observe only minimal differences in language modeling performance for top-down and bottom-up models. This result is surprising in light of how different the estimation problems are, but it is a clear demonstration of the ability of RNNs to learn to extract relevant features from data presented in any different but consistent orders. Our findings (§4) show that, like RNNGs, generative dependency models make good parsers. Given the small scale of the Universal Dependency corpora, this result is also in line with previous work which shows that joint generative models offer very samp"
L16-1529,P13-2054,0,0.122817,"e most useful orthographic features in named-entity recognition – capitalization – is absent, as the language’s Perso-Arabic script does not make a distinction between uppercase and lowercase letters. We describe a system for deriving an inferred capitalization value from closely related languages by phonological similarity, and illustrate the system using several related Western Iranian languages. Keywords: Kurdish, named-entity recognition, phonology 1. Introduction In constructing a named-entity recognition system for Sorani Kurdish (Gautier, 1998; Thackston, 2006; Walther and Sagot, 2010; Esmaili and Salavati, 2013), a low-resource Western Iranian language written in a Perso-Arabic script, we were faced with a dilemma: one of the most useful orthographic features in named-entity recognition— capitalization—is absent in Perso-Arabic writing. However, within the Western Iranian family there are several languages, including Kurmanji Kurdish, Zazaki, and Tajik, that are written in Latin or Cyrillic scripts and therefore do feature capitalization. This article details the process we developed and the challenges we faced in attempting to infer a “surrogate” capitalization feature for Sorani named entity recogn"
L16-1529,qian-etal-2010-python,0,0.24998,"egments—both simple (consisting of a single letter) and complex (consisting of a letter and one or more diacritics/modifiers—with their corresponding definitions in terms of articulatory features. A second (very simple) script validates Unicode IPA files (UTF-8 only) against this comprehensive table. 6 5 Word-initially, /ɛ/ is expressed with a preceding hamza, which eliminates the ambiguity between word-initial /ɛ/ and word-initial /h/ (Thackston, 2006); we also encountered this form when /ɛ/ follows another vowel. During the course of the project, we developed an improved version of Unitran (Qian et al., 2010) which can be used to generate a lookup table of this type rapidly. 7 This resource (PanPhon) will be made available through the ELRA Catalogue of LRs. 3320 Additionally, PanPhon includes a Python library with numerous utility functions for manipulating articulatory feature vectors and a Python class for interacting with the comprehensive IPA feature database. This class includes methods for querying the database in various ways, for querying IPA segment inventories, for fixed width pattern matching based on articulatory features, for calculating the sonority of an IPA segment, and for impleme"
N09-1046,I08-1033,0,0.0127771,"and Jong, 2003). A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system. These segmentation lattices improve translation quality (over an already strong baseline) in three typologically distinct languages (German, Hungarian, Turkish) when translating into English. Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al."
N09-1046,J96-1002,0,0.00794484,"ccur. Second, the heuristic scoring offers little insight into which segmentations should be included in a lattice. We would like our model to consider a wide variety of segmentations of any word (including perhaps hypothesized morphemes that are not in the dictionary), to make use of a rich set of features, and to have a probabilistic interpretation of each hypothesized split (to incorporate into the downstream decoder). We decided to use the class of maximum entropy models, which are probabilistically sound, can make use of possibly many overlapping features, and can be trained efficiently (Berger et al., 1996). We thus define a model of the conditional probability distribution P r(sN 1 |w), where w is a surface form and sN is the segmented form consisting of N 1 segments as: P r(sN 1 |w) P exp i λi hi (sN 1 , w) P P = 0 exp λ h s0 i i i (s , w) (4) To simplify inference and to make the lattice representation more natural, we only make use of local feature functions that depend on properties of each segment: 3.3 P r(sN 1 |w) ∝ exp 3.1 X i λi N X hi (sj , w) (5) j From model to segmentation lattice The segmentation model just introduced is equivalent to a lattice where each vertex corresponds to a pa"
N09-1046,W08-0336,0,0.109703,"uild segmentation lattices for MT Chris Dyer Laboratory for Computational Linguistics and Information Processing Department of Linguistics University of Maryland College Park, MD 20742, USA redpony AT umd.edu Abstract native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input t"
N09-1046,J07-2003,0,0.0404818,"f the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003). For each language pair, the corpus was aligned twice, once in its non-segmented variant and once using the single-best segmentation variant. For translation, we used a bottom-up parsing decoder that uses cube pruning to intersect the lan4 http://www.statmt.org/wmt09 411 guage model with the target side of the synchronous grammar. The grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f (¯ e|f¯), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(f¯|G), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest repr"
N09-1046,2008.amta-papers.7,0,0.174546,"he minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries. However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices."
N09-1046,P08-1115,0,0.659858,"ngle morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries. However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source"
N09-1046,N06-2013,0,0.356385,"ximum entropy model to build segmentation lattices for MT Chris Dyer Laboratory for Computational Linguistics and Information Processing Department of Linguistics University of Maryland College Park, MD 20742, USA redpony AT umd.edu Abstract native speaker of the language. However, there are often advantages to using elements larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of"
N09-1046,E03-1076,0,0.412064,"choosing 19 German newspaper articles, identifying all words greater than 6 characters is length, and segmenting each word so that the resulting units could be translated compositionally into English. This resulted in 489 training sentences corresponding to 564 paths for the dev set (which was drawn from 15 articles), and 279 words (302 paths) for the test set (drawn from the remaining 4 articles). 3 aufnahme A maximum entropy segmentation model We now turn to the problem of modeling word segmentation in a way that facilitates lattice construction. As a starting point, we consider the work of Koehn and Knight (2003) who observe that in most languages that exhibit compounding, the mor408 phemes used to construct compounds frequently also appear as individual tokens. Based on this observation, they propose a model of word segmentation that splits compound words into pieces found in the dictionary based on a variety heuristic scoring criteria. While these models have been reasonably successful (Koehn et al., 2008), they are problematic for two reasons. First, there is no principled way to incorporate additional features (such as phonotactics) which might be useful to determining whether a word break should"
N09-1046,N03-1017,0,0.0064024,"the Turkish-English data corresponds to the training and test sets used in the work of Oflazer and Durgar ElKahlout (2007). Corpus statistics for all language pairs are summarized in Table 3. We note that in all language pairs, the 1 BEST segmentation variant of the training data results in a significant reduction in types. Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 iterations of Model 1, 5 of the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003). For each language pair, the corpus was aligned twice, once in its non-segmented variant and once using the single-best segmentation variant. For translation, we used a bottom-up parsing decoder that uses cube pruning to intersect the lan4 http://www.statmt.org/wmt09 411 guage model with the target side of the synchronous grammar. The grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f (¯ e|f¯), the ‘lexical translation’ log probabilities in both dir"
N09-1046,P07-2045,1,0.0147401,"et al., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a 406 In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word. This model enables generation of diverse, accurate segmentation lattices from a single model that are appropriate for use in decoders that accept word lattices as input, such as Moses (Koehn et al., 2007). Since our model relies a small number of dense features, its parameters can be tuned using very small amounts of manually created reference lattices. Furthermore, since these parameters were chosen to have valid interpretation across a variety of languages, we find that the weights estimated for one apply quite well to another. We show that these lattices significantly improve translation quality when translating into English from three languages exhibiting productive compounding: German, Turkish, and Hungarian. The paper is structured as follows. In the next secHuman Language Technologies:"
N09-1046,W08-0318,0,0.317261,"ignificant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines. 1 Introduction Compound words pose significant challenges to the lexicalized models that are currently common in statistical machine translation. This problem has been widely acknowledged, and the conventional solution, which has been shown to work well for many language pairs, is to segment compounds into their constituent morphemes using either morphological analyzers or empirical methods and then to translate from or to this segmented variant (Koehn et al., 2008; Dyer et al., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a 406 In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word. This model enables generation of diverse, accurate segmentation lattices from a single model that are appropriate for use in decoders that accept word lattices as input, such as"
N09-1046,P07-1039,0,0.0108012,"number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model. Another strand of inquiry that is closely related is the work on adjusting the source language segmentation to match the granularity of the target language as a way of improving translation. The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007). 7 Conclusions and future work In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system. These segmentation lattices improve translation quality (over an already strong baseline) in three typologically distinct languages (German, Hungarian, Turkish) when translating into English. Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al., 2005) or hand-cr"
N09-1046,D08-1076,0,0.0280135,"glish language model log probability, log f (¯ e|f¯), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(f¯|G), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest representation of the decoder’s hypothesis space (Macherey et al., 2008). The weights were independently optimized for each language pair and each experimental condition. 5.2 Segmentation lattice results In this section, we report the results of an experiment to see if the compound lattices constructed using our maximum entropy model yield better translations than either an unsegmented baseline or a baseline consisting of a single-best segmentation. For each language pair, we define three conditions: BASELINE, 1 BEST, and LATTICE. In the BASELINE condition, a lowercased and tokenized (but not segmented) version of the test data is translated using the grammar deri"
N09-1046,J03-1002,0,0.00338009,"-English systems used were distributed as part of the 2009 EACL Workshop on Machine Translation,4 and the Turkish-English data corresponds to the training and test sets used in the work of Oflazer and Durgar ElKahlout (2007). Corpus statistics for all language pairs are summarized in Table 3. We note that in all language pairs, the 1 BEST segmentation variant of the training data results in a significant reduction in types. Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 iterations of Model 1, 5 of the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003). For each language pair, the corpus was aligned twice, once in its non-segmented variant and once using the single-best segmentation variant. For translation, we used a bottom-up parsing decoder that uses cube pruning to intersect the lan4 http://www.statmt.org/wmt09 411 guage model with the target side of the synchronous grammar. The grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English l"
N09-1046,W07-0704,0,0.0164161,"Missing"
N09-1046,P02-1040,0,0.104581,"he grammar rules were extracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f (¯ e|f¯), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(f¯|G), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest representation of the decoder’s hypothesis space (Macherey et al., 2008). The weights were independently optimized for each language pair and each experimental condition. 5.2 Segmentation lattice results In this section, we report the results of an experiment to see if the compound lattices constructed using our maximum entropy model yield better translations than either an unsegmented baseline or a baseline consisting of a single-best segmentation. For each language pair, we define three conditions: BASELINE,"
N09-1046,N09-1024,0,0.0105461,"nd Turkish), we have demonstrated that the parameters obtained in one language work surprisingly well for others. Thus, with virtually no cost, this model can be used with a variety of diverse languages. While these results are already quite satisfying, there are a number of compelling extensions to this work that we intend to explore in the future. First, unsupervised segmentation approaches offer a very compelling alternative to the manually crafted segmentation lattices that we created. Recent work suggests that unsupervised segmentation of inflectional affixal morphology works quite well (Poon et al., 2009), and extending this work to compounding morphology should be feasible, obviating the need for expensive hand-crafted reference lattices. Second, incorporating target language information into a segmentation model holds considerable promise for inducing more effective translation models that perform especially well for segmentation lattice inputs. Acknowledgments Special thanks to Kemal Oflazar and Reyyan Yeniterzi of Sabancı University for providing the Turkish-English corpus and to Philip Resnik, Adam Lopez, Trevor Cohn, and especially Phil Blunsom for their helpful suggestions. This researc"
N09-1046,2006.amta-papers.25,0,0.0213505,"xtracted from the word aligned parallel corpus and scored as described in Chiang (2007). The features used by the decoder were the English language model log probability, log f (¯ e|f¯), the ‘lexical translation’ log probabilities in both directions (Koehn et al., 2003), and a word count feature. For the lattice systems, we also included the unnormalized log p(f¯|G), as it is defined in Section 3, as well as an input word count feature. The feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of BLEU and 1-TER (Papineni et al., 2002; Snover et al., 2006) using the minimum error training algorithm on a packed forest representation of the decoder’s hypothesis space (Macherey et al., 2008). The weights were independently optimized for each language pair and each experimental condition. 5.2 Segmentation lattice results In this section, we report the results of an experiment to see if the compound lattices constructed using our maximum entropy model yield better translations than either an unsegmented baseline or a baseline consisting of a single-best segmentation. For each language pair, we define three conditions: BASELINE, 1 BEST, and LATTICE."
N09-1046,W04-1118,0,0.153457,"s again resumption tonband ton band wiederaufnahme Figure 2: Manually created reference lattices for the two words from Figure 1. Although only a subset of all linguistically plausible segmentations, each path corresponds to a plausible segmentation for word-for-word German-English translation. Table 1: German-English dictionary fragment for words present in Figure 1. come plausible translations. However, using a strategy of “over segmentation” and relying on phrase models to learn the non-compositional translations has been shown to degrade translation quality significantly on several tasks (Xu et al., 2004; Habash and Sadat, 2006). We thus desire lattices containing as little oversegmentation as possible. We have now have a concept of a “gold standard” segmentation lattice for translation: it should contain all linguistically motivated segmentations that also correspond to plausible word-for-word translations into English. Figure 2 shows an example of the reference lattice for the two words we just discussed. For the experiments in this paper, we generated a development and test set by randomly choosing 19 German newspaper articles, identifying all words greater than 6 characters is length, and"
N09-1046,2005.iwslt-1.18,0,0.0762208,"ts larger than single morphemes as the minimal lexical unit for MT, since they may correspond more closely to the units of translation. Unfortunately, determining the optimal segmentation is challenging, typically requiring extensive experimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008). Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008). Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input. Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries. However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficien"
N09-1046,E06-1006,0,0.268979,"quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines. 1 Introduction Compound words pose significant challenges to the lexicalized models that are currently common in statistical machine translation. This problem has been widely acknowledged, and the conventional solution, which has been shown to work well for many language pairs, is to segment compounds into their constituent morphemes using either morphological analyzers or empirical methods and then to translate from or to this segmented variant (Koehn et al., 2008; Dyer et al., 2008; Yang and Kirchhoff, 2006). But into what units should a compound word be segmented? Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a 406 In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word. This model enables generation of diverse, accurate segmentation lattices from a single model that are appropriate for use in decoders that accept word lattices as input, such as Moses (Koehn et al., 2007). Since our model re"
N09-4001,W08-0207,1,0.821425,"Lin is an assistant professor in the iSchool at the University of Maryland, College Park. He joined the faculty in 2004 after completing his Ph.D. in Electrical Engineering and Computer Science at MIT. Dr. Lin’s research interests lie at the intersection of natural language processing and information retrieval. 1 Proceedings of NAACL HLT 2009: Tutorials, pages 1–2, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics He leads the University of Maryland’s effort in the Google/IBM Academic Cloud Computing Initiative. Dr. Lin has taught two semester‐long Hadoop courses [2] and has given numerous talks about MapReduce to a wide audience. Chris Dyer is a Ph.D. student at the University of Maryland, College Park, in the Department of Linguistics. His current research interests include statistical machine translation, machine learning, and the relationship between artificial language processing systems and the human linguistic processing system. He has served on program committees for AMTA, ACL, COLING, EACL, EMNLP, NAACL, ISWLT, and the ACL Workshops on Machine translation, and is one of the developers of the Moses open source machine translation toolkit. He has p"
N10-1033,D08-1023,0,0.424986,"d a variant of the phrasal ITG described by Zhang et al. (2008).12 Figure 3 plots the average run-time of the two algorithms as a function of the Arabic sentence length. The two-parse approach is far more efficient. In total, aligning the 80k sentence pairs in the corpus completed in less than 4 hours with the two-parse algorithm but required more than 1 week with the baseline algorithm.13 “Hiero” grammars. An alternative approach to computing a synchronous parse forest is based on cube pruning (Huang and Chiang, 2007). While more commonly used to integrate a target m-gram LM during decoding, Blunsom et al. (2008), who required synchronous parses to discriminatively train 10 How tight these bounds are depends on the ambiguity in the grammar w.r.t. the input: to generate n3 edges, every item in every cell must be derivable by every combination of its subspans. Most grammars are substantially less ambiguous. 11 Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al., 2008), a detailed analysis of containing and higher rank grammars is left to future work. 12 The"
N10-1033,J07-2003,0,0.715248,"cribes a bottom-up O(n6 ) synchronous parsing algorithm for ITGs, a binary SCFG with a restricted form. For general grammars, the situation is even worse: the problem has been shown to be NP-hard (Satta and Peserico, 2005). Even if we restrict ourselves to binary ITGs, the ∗ This work was supported in part by the GALE program of DARPA, Contract No. HR0011-06-2-001. The author wishes to thank Philip Rensik, Adam Lopez, Phil Blunsom, and Jason Eisner for helpful discussions. 1 SCFGs have enjoyed a resurgence in popularity as the formal basis for a number of statistical translation systems, e.g. Chiang (2007). However, translation requires only the manipulation of SCFGs using monolingual parsing algorithms. 2 It is assumed that n = |f |≈ |e|. O(n6 ) run-time makes large-scale learning applications infeasible. The usual solution is to use a heuristic search that avoids exploring edges that are likely (but not guaranteed) to be low probability (Zhang et al., 2008; Haghighi et al., 2009). In this paper, we derive an alternative synchronous parsing algorithm starting from a conception of parsing with SCFGs as a composition of binary relations. This enables us to factor the synchronous parsing problem"
N10-1033,P09-1104,0,0.0181766,"thor wishes to thank Philip Rensik, Adam Lopez, Phil Blunsom, and Jason Eisner for helpful discussions. 1 SCFGs have enjoyed a resurgence in popularity as the formal basis for a number of statistical translation systems, e.g. Chiang (2007). However, translation requires only the manipulation of SCFGs using monolingual parsing algorithms. 2 It is assumed that n = |f |≈ |e|. O(n6 ) run-time makes large-scale learning applications infeasible. The usual solution is to use a heuristic search that avoids exploring edges that are likely (but not guaranteed) to be low probability (Zhang et al., 2008; Haghighi et al., 2009). In this paper, we derive an alternative synchronous parsing algorithm starting from a conception of parsing with SCFGs as a composition of binary relations. This enables us to factor the synchronous parsing problem into two successive monolingual parses. Our algorithm runs more efficiently than O(n6 ) with many grammars (including those that required using heuristic search with other parsers), making it possible to take advantage of synchronous parsing without developing search heuristics; and the SCFGs are not required to be in a normal form, making it possible to easily parse with more com"
N10-1033,P07-1019,0,0.0601242,"orithm and the O(n6 ) ITG parsing algorithm on an Arabic-English phrasal ITG alignment task. We used a variant of the phrasal ITG described by Zhang et al. (2008).12 Figure 3 plots the average run-time of the two algorithms as a function of the Arabic sentence length. The two-parse approach is far more efficient. In total, aligning the 80k sentence pairs in the corpus completed in less than 4 hours with the two-parse algorithm but required more than 1 week with the baseline algorithm.13 “Hiero” grammars. An alternative approach to computing a synchronous parse forest is based on cube pruning (Huang and Chiang, 2007). While more commonly used to integrate a target m-gram LM during decoding, Blunsom et al. (2008), who required synchronous parses to discriminatively train 10 How tight these bounds are depends on the ambiguity in the grammar w.r.t. the input: to generate n3 edges, every item in every cell must be derivable by every combination of its subspans. Most grammars are substantially less ambiguous. 11 Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al.,"
N10-1033,J09-4009,0,0.0222572,"ding to a span [i, j] in the input sentence; and the synchronous variant defines a table in 4 dimensions, with cells corresponding to a source span [s, t] and a target span [u, v]. The bottom of the chart is initialized first, and pairs of items are combined from bottom to top. Since combining items from the n4 cells involves considering two split points (one source, one target), it is not hard to see that this algorithm runs in time O(n6 ). 3 Generalizing the algorithm to higher rank grammars is possible (Wu, 1997), as is converting a grammar to a weakly equivalent binary form in some cases (Huang et al., 2009). 263 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 263–266, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics n the target language. parsing, which is our focus for the is paper, is the problem of finding on, or the forest of derivations, of which, because parsed (with a monolingual parser), get sentence pair !f, e&quot;. This forest of the parallel structure of the rules, induces a forest 2.2 Parsing, intersection, and composition seful in learningin problems it of translations the target since language."
N10-1033,N09-1049,0,0.0667691,"Missing"
N10-1033,H05-1101,0,0.0405047,"me way that finite state transducers (FSTs) generalize finite state automata (FSAs).1 Synchronous parsing is the problem of finding the best derivation, or forest of derivations, of a source and target sentence pair hf, ei under an SCFG, G.2 Solving this problem is necessary for several applications, for example, optimizing how well an SCFG translation model fits parallel training data. Wu (1997) describes a bottom-up O(n6 ) synchronous parsing algorithm for ITGs, a binary SCFG with a restricted form. For general grammars, the situation is even worse: the problem has been shown to be NP-hard (Satta and Peserico, 2005). Even if we restrict ourselves to binary ITGs, the ∗ This work was supported in part by the GALE program of DARPA, Contract No. HR0011-06-2-001. The author wishes to thank Philip Rensik, Adam Lopez, Phil Blunsom, and Jason Eisner for helpful discussions. 1 SCFGs have enjoyed a resurgence in popularity as the formal basis for a number of statistical translation systems, e.g. Chiang (2007). However, translation requires only the manipulation of SCFGs using monolingual parsing algorithms. 2 It is assumed that n = |f |≈ |e|. O(n6 ) run-time makes large-scale learning applications infeasible. The"
N10-1033,P95-1022,0,0.178855,"Missing"
N10-1033,J97-3002,0,0.907945,"uned search. 1 Introduction Synchronous context free grammars (SCFGs) generalize monolingual context-free grammars to generate strings concurrently in pairs of languages (Lewis and Stearns, 1968) in much the same way that finite state transducers (FSTs) generalize finite state automata (FSAs).1 Synchronous parsing is the problem of finding the best derivation, or forest of derivations, of a source and target sentence pair hf, ei under an SCFG, G.2 Solving this problem is necessary for several applications, for example, optimizing how well an SCFG translation model fits parallel training data. Wu (1997) describes a bottom-up O(n6 ) synchronous parsing algorithm for ITGs, a binary SCFG with a restricted form. For general grammars, the situation is even worse: the problem has been shown to be NP-hard (Satta and Peserico, 2005). Even if we restrict ourselves to binary ITGs, the ∗ This work was supported in part by the GALE program of DARPA, Contract No. HR0011-06-2-001. The author wishes to thank Philip Rensik, Adam Lopez, Phil Blunsom, and Jason Eisner for helpful discussions. 1 SCFGs have enjoyed a resurgence in popularity as the formal basis for a number of statistical translation systems, e"
N10-1033,P08-1012,0,0.497721,"011-06-2-001. The author wishes to thank Philip Rensik, Adam Lopez, Phil Blunsom, and Jason Eisner for helpful discussions. 1 SCFGs have enjoyed a resurgence in popularity as the formal basis for a number of statistical translation systems, e.g. Chiang (2007). However, translation requires only the manipulation of SCFGs using monolingual parsing algorithms. 2 It is assumed that n = |f |≈ |e|. O(n6 ) run-time makes large-scale learning applications infeasible. The usual solution is to use a heuristic search that avoids exploring edges that are likely (but not guaranteed) to be low probability (Zhang et al., 2008; Haghighi et al., 2009). In this paper, we derive an alternative synchronous parsing algorithm starting from a conception of parsing with SCFGs as a composition of binary relations. This enables us to factor the synchronous parsing problem into two successive monolingual parses. Our algorithm runs more efficiently than O(n6 ) with many grammars (including those that required using heuristic search with other parsers), making it possible to take advantage of synchronous parsing without developing search heuristics; and the SCFGs are not required to be in a normal form, making it possible to ea"
N10-1033,W06-3119,0,0.0404372,"parse forest is based on cube pruning (Huang and Chiang, 2007). While more commonly used to integrate a target m-gram LM during decoding, Blunsom et al. (2008), who required synchronous parses to discriminatively train 10 How tight these bounds are depends on the ambiguity in the grammar w.r.t. the input: to generate n3 edges, every item in every cell must be derivable by every combination of its subspans. Most grammars are substantially less ambiguous. 11 Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al., 2008), a detailed analysis of containing and higher rank grammars is left to future work. 12 The restriction that phrases contain exactly a single alignment point was relaxed, resulting in much larger and more ambiguous grammars than those used in the original work. 13 A note on implementation: our ITG aligner was minimal; it only computed the probability of the sentence pair using the inside algorithm. With the two-parse aligner, we stored the complete forest during both the first and second parses. 60 haustive search over all n4 span pairs without awareness"
N10-1033,C08-1144,0,\N,Missing
N10-1033,P08-1024,0,\N,Missing
N10-1128,P08-1024,0,0.262288,"of a weighted CFG and t(e|f0 ) to be an FST, the quantity (1), which sums over all reorderings (and derivations), can be computed in polynomial time with dynamic programming composition, as described in §2.2. 3.2 translation distribution with a penalty term due to the prior: X λi ∂L Ep(d,a|e,f;Λ) [hi ] − Ep(e,d,a|f;Λ) [hi ] − 2 = ∂λi σ he,fi Conditional training While it is straightforward to use expectation maximization to optimize the joint likelihood of the parallel training data with a latent variable model, instead we use a log-linear parameterization and maximize conditional likelihood (Blunsom et al., 2008; Petrov and Klein, 2008). This enables us to employ a rich set of (possibly overlapping, non-independent) features to discriminate among translations. The probability of a derivation from source to reordered source to target is thus written in terms of model parameters Λ = {λi } as: P exp i λi · Hi (e, d, f0 , a, f) 0 p(e, d, f , a|f; Λ) = Z(f; Λ) X X 0 0 where Hi (e, d, f , a, f) = hi (f , r) + hi (f, s) r∈d s∈a The derivation probability is globally normalized by the partition Z(f; Λ), which is just the sum of the numerator for all derivations of f (corresponding to any e). The Hi (written"
N10-1128,W09-0436,0,0.0626448,"finition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then our model. Table 2: Translation results (BLEU) Condition BTEC Chinese-Eng. Arabic-Eng. Mono 47.4 29.0 41.2 PB 51.8 30.9 45.8 Hiero 52.4 32.1 46.6 Forest 54.1 32.4 44.9 use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but ofte"
N10-1128,J07-2003,0,0.345992,"uction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years (Wu, 1997; Chiang, 2007). Compared to phrase-based models, which can be represented as finite-state transducers (FSTs, Kumar et al. (2006)), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1 Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized phrase pairs."
N10-1128,P05-1066,0,0.408016,"er process, but the forests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the f"
N10-1128,P81-1022,0,0.424667,"dealing with FSTs that define binary relations over strings, not FSAs defining strings, this operation is more properly composition. However, since CFG/FSA intersection is less 5 For computational tractability, we only consider all permutations only when the number of children is less than 5, otherwise we exclude permutations where a child moves more than 4 positions away from where it starts. 860 cumbersome to describe, we present the algorithm in terms of intersection. To compute the composition of a reordering forest, G, with an FSA, F , we will make use of a variant of Earley’s algorithm (Earley, 1970). Let weighted finite-state automaton F = hΣ, Q, q0 , qfinal , δ, wi. Σ is a finite alphabet; Q is a set of states; q0 and qfinal ∈ Q are start and accept states, respectively,6 δ is the transition function Q × Σ → 2Q , and w is the transition cost function Q × Q → R. We use variables that refer to states in the FSA with the letters q, r, and s. We use x to represent a variable that is an element of Σ. Variables u and v represent costs. X and Y are non-terminals. Lowercase Greek letters are strings of terminals and non-terminals. The function δ(q, x) returns the state(s) that are reachable fro"
N10-1128,D08-1089,0,0.0505788,"es, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have"
N10-1128,P07-1019,0,0.0794581,"Missing"
N10-1128,N09-1049,0,0.0236448,"Missing"
N10-1128,N03-1017,0,0.0467756,"last, and English is a head-initial language, where heads come first. As a result, the usual order for a declarative sentence in English is SVO (subject-verb-object), but in Japanese, it is SOV, and the desired translation is John-ga ringo-o [an apple] tabeta [ate]. In summary, when translating from English into Japanese, it is usually necessary to move verbs from their position between the subject and object to the end of the sentence. This reordering can happen in two ways, which we depict in Figure 1. In the derivation on the left, a memorized phrase pair captures the movement of the verb (Koehn et al., 2003). In the other derivation, the source is first reordered into target word order and then translated, using smaller translation units. In addition, we have assumed that the phrase translations were learned from a parallel corpus that is in the original ordering, so the reordering forest F should include derivations of phrase-size units in the source order as well as the target order. 2 Note that forests are isomorphic to context-free grammars. For example, what is referred to as the ‘parse forest’, and understood to encode all derivations of a sentence s under some grammar, can also be understo"
N10-1128,P09-1019,1,0.105321,"er state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we converted the coefficients on the reordering features into a single reordering feature which then had a coefficient assigned to it. This technique is similar to what is done with logarithmic opinion pools, only the learned model is not a probability distribution (Smith et al., 2005). Once we collapsed the reordering weights into a single feature, we used the techniques described by Kumar et al. (2009) to optimize the feature weights to maximize corpus BLEU on a held-out development set. 5.2 Translation results Scores on a held-out test set are reported in Table 2 using case-insensitive BLEU with 4 reference translations (16 for BTEC) using the original definition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then o"
N10-1128,P03-1051,0,0.0414838,"Missing"
N10-1128,D09-1005,0,0.0104559,"der using our child-permutation rules; however, if the source VP is modified by a modal particle, the parser makes the particle the parent of the VP, and it is no longer possible to move the subject to the first position in the sentence. Richer reordering rules are needed to address this problem. 9 Only sentences that can be generated by the model can be used in training. 863 Other solutions to the reachability problem include targeting reachable oracles instead of the reference translation (Li and Khudanpur, 2009) or making use of alternative training criteria, such as minimum risk training (Li and Eisner, 2009). 4.1 Features We briefly describe the feature functions we used in our model. These include the typical dense features used in translation: relative phrase translation frequencies p(e|f ) and p(f |e), ‘lexically smoothed’ translation probabilities plex (e|f ) and plex (f |e), and a phrase count feature. For the reordering model, we used a binary feature for each kind of rule used, for example φVP→V NP (a) would fire once for each time the rule VP → V NP was used in a derivation, a. For the Arabic-English condition, we observed that the parse trees tended to be quite flat, with many repeated n"
N10-1128,N09-2003,0,0.0159856,"e ‘middle child’ between the V and the object constituent. This can be reordered into an English SVO order using our child-permutation rules; however, if the source VP is modified by a modal particle, the parser makes the particle the parent of the VP, and it is no longer possible to move the subject to the first position in the sentence. Richer reordering rules are needed to address this problem. 9 Only sentences that can be generated by the model can be used in training. 863 Other solutions to the reachability problem include targeting reachable oracles instead of the reference translation (Li and Khudanpur, 2009) or making use of alternative training criteria, such as minimum risk training (Li and Eisner, 2009). 4.1 Features We briefly describe the feature functions we used in our model. These include the typical dense features used in translation: relative phrase translation frequencies p(e|f ) and p(f |e), ‘lexically smoothed’ translation probabilities plex (e|f ) and plex (f |e), and a phrase count feature. For the reordering model, we used a binary feature for each kind of rule used, for example φVP→V NP (a) would fire once for each time the rule VP → V NP was used in a derivation, a. For the Arab"
N10-1128,P08-1023,0,0.0724852,"entence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particul"
N10-1128,J03-1002,0,0.0034079,"Missing"
N10-1128,N03-1028,0,0.0085968,", we also make use of a spherical Gaussian prior on the value of Λ with mean 0 and variance σ 2 , which helps prevent overfitting of the model (Chen and Rosenfeld, 1998). Our objective is thus to select Λ minimizing: Y ||Λ||2 L = − log p(e|f; Λ) − 2σ 2 he,fi = − X he,fi [log Z(e, f; Λ) − log Z(f; Λ)] − ||Λ||2 2σ 2 The gradient of L with respect to the feature weights has a parallel form; it is the difference in feature expectations under the reference distribution and the 862 The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). However, rather than matching the feature expectations in the model to an observable feature value, we have to sum over the latent structure that remains after observing our target e, which makes the form of the first summand an expectation rather than just a feature function value. 3.2.1 Computing the objective and gradient The objective and gradient that were just introduced can be computed in two steps. Given a training pair he, fi, we generate the forest of reorderings F from f as described in §2.1. We then compose this grammar with T , the FST representing the translation model, which y"
N10-1128,P05-1003,0,0.00859833,"ion model so that it contained phrases from both the original order and the 1-best reordering. To be competitive with other state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we converted the coefficients on the reordering features into a single reordering feature which then had a coefficient assigned to it. This technique is similar to what is done with logarithmic opinion pools, only the learned model is not a probability distribution (Smith et al., 2005). Once we collapsed the reordering weights into a single feature, we used the techniques described by Kumar et al. (2009) to optimize the feature weights to maximize corpus BLEU on a held-out development set. 5.2 Translation results Scores on a held-out test set are reported in Table 2 using case-insensitive BLEU with 4 reference translations (16 for BTEC) using the original definition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based dist"
N10-1128,takezawa-etal-2002-toward,0,0.100444,"Missing"
N10-1128,D09-1105,0,0.340561,"t is also intuitively satisfying because from a task perspective, we are not concerned with values of f0 , but only with producing a good translation e. 3.1 A probabilistic translation model with a latent reordering variable The translation model we use is a two phase process. First, source sentence f is reordered into a targetlike word order f0 according to a reordering model r(f0 |f). The reordered source is then transduced into the target language according to a translation model t(e|f0 ). We require that r(f0 |f) can be represented by orderings from word aligned parallel corpora, refer to Tromble and Eisner (2009). 861 u Y − →γ∈G [X → α • Y β, q, s] : u [Y → γ•, s, r] : v [X → αY • β, q, r] : u ⊗ v Goal state: [S 0 → S•, q0 , qfinal ] Figure 5: Weighted logic program for computing the intersection of a weighted FSA and a weighted CFG. a recursion-free probabilistic context-free grammar, i.e. a forest as in §2.1, and that t(e|f0 ) is represented by a (cyclic) finite-state transducer, as in Figure 2. Since the reordering forest may define multiple derivations a from f to a particular f0 , and the transducer may define multiple derivations d from f0 to a particular translation e, we marginalize over these"
N10-1128,D07-1077,0,0.0692492,"rests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the finite-state part ha"
N10-1128,J97-3002,0,0.519504,"ase transduction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years (Wu, 1997; Chiang, 2007). Compared to phrase-based models, which can be represented as finite-state transducers (FSTs, Kumar et al. (2006)), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1 Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized"
N10-1128,N09-1028,0,0.0748437,"ecover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the finite-state part handle local phrasa"
N10-1128,P01-1067,0,0.638561,"NP NP ate an apple In this grammar, the phrases John and an apple are fixed and only the VP contains ordering ambiguity. 2.1 Reordering forests based on source parses Many kinds of reordering forests are possible; in general, the best one for a particular language pair will be one that is easiest to create given the resources available in the source language. It will also be the one that most compactly expresses the source reorderings that are most likely to be useful for translation. In this paper, we consider a particular kind of reordering forest that is inspired by the reordering model of Yamada and Knight (2001).4 These are generated by taking a source language parse tree and ‘expanding’ each node so that it 4 One important difference is that our translation model is not restricted by the structure of the source parse tree; i.e., phrases used in transduction need not correspond to constituents in the source reordering forest. However, if a phrase does cross a constituent boundary between constituents A and B, then translations that use that phrase will have A and B adjacent. f John ate an apple John ate an apple an apple ate f' John ate an apple John e ジョンが リンゴを 食べた ジョンが John-ga ringo-o tabeta John-g"
N10-1128,W06-3108,0,0.0602485,"sing the original definition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then our model. Table 2: Translation results (BLEU) Condition BTEC Chinese-Eng. Arabic-Eng. Mono 47.4 29.0 41.2 PB 51.8 30.9 45.8 Hiero 52.4 32.1 46.6 Forest 54.1 32.4 44.9 use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just"
N10-1128,W06-3119,0,0.0224709,"ning examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1best parsing"
N10-1128,C08-1144,0,0.0249409,"e ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1 Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized phrase pairs. pairs with substantial structural differences (and thus requiring long-range reordering during translation), SCFG models have come to outperform the best FST models (Zollmann et al., 2008). In this paper, we explore a new way to take advantage of the computational benefits of CFGs during translation. Rather than using a single SCFG to both reorder and translate a source sentence into the target language, we break the translation process into a two step pipeline where (1) the source language is reordered into a target-like order, with alternatives encoded in a context-free forest, and (2) the reordered source is transduced into the target language using an FST that represents phrasal correspondences. While multi-step decompositions of the translation problem have been proposed b"
N10-1128,I05-3027,0,\N,Missing
N10-4001,W08-0207,1,0.519078,"Missing"
N10-4001,J90-1003,0,\N,Missing
N10-4001,W09-3209,0,\N,Missing
N10-4001,J93-2003,0,\N,Missing
N10-4001,C96-2141,0,\N,Missing
N10-4001,N09-4002,0,\N,Missing
N10-4001,A92-1018,0,\N,Missing
N10-4001,W02-2018,0,\N,Missing
N10-4001,D07-1090,0,\N,Missing
N10-4001,W09-0401,0,\N,Missing
N10-4001,W08-0333,1,\N,Missing
N10-4001,J98-1004,0,\N,Missing
N10-4001,N10-1021,0,\N,Missing
N10-4001,D08-1044,1,\N,Missing
N10-4001,N03-1017,0,\N,Missing
N10-4001,N03-1028,0,\N,Missing
N10-4001,J03-1002,0,\N,Missing
N10-4001,P01-1005,0,\N,Missing
N10-4001,D09-1079,0,\N,Missing
N10-4001,N10-1062,0,\N,Missing
N13-1025,P08-1024,0,0.549714,"a specific rule is used in a translation. Early experiments (Liang et al., 2006) used the structured perceptron to tune a phrase-based system on a large subset of the training data, showing improvements when using rule indicator features, word alignment features, and POS tag features. Another early attempt (Tillmann and Zhang, 2006) used phrase pair and word features in a block SMT system trained using stochastic gradient descent for a convex loss function, but did not compare to MERT. Problems of overfitting and degenerate derivations were tackled with a probabilistic latent variable model (Blunsom et al., 2008) which used rule indicator features yet failed to improve upon the MERT baseline for the standard Hiero features. 249 Difficulties in Large-Scale Training Discriminative training for machine translation is complicated by several factors. First, both translation rules and feature weights are learned from parallel data. If the same data is used for both tasks, overfitting of the weights is very possible.1 Second, the standard MT cost function, BLEU (Papineni et al., 2002), does not decompose additively over training instances (because of the “brevity penalty”) and so approximations are used—thes"
N13-1025,N12-1047,0,0.372997,"2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned weight, which count how oft"
N13-1025,D08-1024,0,0.352008,"ight 30.115 30.120 (a) Four representative frequent sparse features. 30.110 4 Held-Out Line Search Algorithm 30.105 BLEU Sentence Level Approximations to BLEU Finally, we note that discriminative training methods often use a sentence level approximation to BLEU. It has been shown that optimizing corpus level BLEU versus sentence level BLEU can lead to improvements of up to nearly .4 BLEU points on the test set (Nakov et al., 2012). Possible fixes to this problem include using a proper sentence level metric such a METEOR (Denkowski and Lavie, 2011) or a pseudo-corpus from the last few updates (Chiang et al., 2008). However, in light of the result from section 3.1 that tuning on the dev set is still better than tuning on a held-out portion of the training data, we observe that tuning a corpus level metric on a highquality dev set from the same domain as the test set probably leads to the best translation quality. Attempts to improve upon this strong baseline lead us to the development of the HOLS algorithm which we describe next. 28.0 BLEU 30.0 3.3 −2 30.100 poorly scaled for rule feature weights. Changing the weights for one of the common features changes the BLEU score by almost 2.5 BLEU points, while"
N13-1025,P05-1033,0,0.0932255,"e much sparser features without abandoning the proven dense features; however, extremely sparse features leads to problems of scaling in the optimization problem as we will show. 3.1 Training Data and Overfitting One of the big questions in discriminative training of machine translation systems is why standard machine learning techniques can perform so poorly when applied to large-scale learning on the training data. Figure 1 shows a good example of this. The structured SVM (Tsochantaridis et al., 2004; Cherry and Foster, 2012) was used to learn the weights for a Chinese-English Hiero system (Chiang, 2005) with just eight features, using stochastic gradient descent (SGD) for online learning (Bottou, 1998; Bottou, 2010). The weights were initialized from MERT values tuned on a 2k-sentence dev set (MT06), and the figure shows the progress of the online method during a single pass through the 300ksentence Chinese-English FBIS training set. As the training progresses in Figure 1, BLEU scores on the training data go up, but scores on the 1 Previous work has attempted to mitigate the risk of overfitting through careful regularization (Blunsom et al., 2008; Simianer et al., 2012). 30 Table 1: MERT on"
N13-1025,P11-2031,1,0.859433,"MT05 MT08nw MT05wb Train Dev Test Train (FBIS) Dev (MT06) Test (MT02-03) MT08 24M 1M 1797 1,056 813 547 89K 1,359 1,133 302K 1,664 1,797 1,357 Tokens Source Target 594M 7M 31M 13K 236K 7K 144K 5K 116K 5K 89K 2.1M 1.7M 34K 28K 29K 24K 1M 9.3M 4K 192K 5K 223K 4K 167K derivations for the input x, and cost(yi , y) is add one smoothing sentence level BLEU.7 Except where noted, all experiments are repeated 5 times and results are averaged, initial weights for the dense features are drawn from a standard normal, and initial weights for the sparse features are set to zero. We evaluate using MultEval (Clark et al., 2011) and report standard deviations across optimizer runs and significance at p = .05 using MultEval’s built-in permutation test. In the large-scale experiments for HOLS, we only run the full optimizer once, and report standard deviations using multiple runs of the last MERT run (i.e. the last line search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which us"
N13-1025,W11-2107,0,0.0302107,"st lists. The plots indicate that the BLEU score is 29.0 27.0 −1 0 1 2 Weight 30.115 30.120 (a) Four representative frequent sparse features. 30.110 4 Held-Out Line Search Algorithm 30.105 BLEU Sentence Level Approximations to BLEU Finally, we note that discriminative training methods often use a sentence level approximation to BLEU. It has been shown that optimizing corpus level BLEU versus sentence level BLEU can lead to improvements of up to nearly .4 BLEU points on the test set (Nakov et al., 2012). Possible fixes to this problem include using a proper sentence level metric such a METEOR (Denkowski and Lavie, 2011) or a pseudo-corpus from the last few updates (Chiang et al., 2008). However, in light of the result from section 3.1 that tuning on the dev set is still better than tuning on a held-out portion of the training data, we observe that tuning a corpus level metric on a highquality dev set from the same domain as the test set probably leads to the best translation quality. Attempts to improve upon this strong baseline lead us to the development of the HOLS algorithm which we describe next. 28.0 BLEU 30.0 3.3 −2 30.100 poorly scaled for rule feature weights. Changing the weights for one of the comm"
N13-1025,P10-4002,1,0.867269,"search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HOLS. It is defined as ˜= L n  X i=1 X − log ~ ~ f (xi ,y)−cost(yi ,y) ew· y∈Gen(xi ) + log X"
N13-1025,N12-1023,0,0.864849,"son to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned we"
N13-1025,W11-2123,0,0.0251385,"g 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HOLS. It is defined as ˜= L n  X i=1 X − log ~ ~ f (xi ,y)−cost(yi ,y) ew· y∈Gen(xi ) + log X e w· ~ f~(xi ,y)+cost(yi ,y)  y∈G"
N13-1025,D11-1125,0,0.568108,"imianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of"
N13-1025,P06-1096,0,0.510523,"Missing"
N13-1025,C12-1121,0,0.401001,"Missing"
N13-1025,P02-1038,0,0.0582837,"e details of our algorithm that addresses these issues, give results on three language pairs, and conclude. Techniques for distributed learning and feature selection for the perceptron loss using rule indicator, rule shape, and source side-bigram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in"
N13-1025,J03-1002,0,0.00723126,"and report standard deviations using multiple runs of the last MERT run (i.e. the last line search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HO"
N13-1025,P03-1021,0,0.587438,". Not only does this two-phase learning approach prevent overfitting, the second pass optimizes corpus-level BLEU of the Viterbi translation of the decoder. We demonstrate significant improvements using sparse rule indicator features in three different translation tasks. To our knowledge, this is the first large-scale discriminative training algorithm capable of showing improvements over the MERT baseline with only rule indicator features in addition to the standard MERT features. 1 Introduction This paper is about large scale discriminative training of machine translation systems. Like MERT (Och, 2003), our procedure directly optimizes the cost of the Viterbi output on corpus-level metrics, but does so while scaling to millions of features. The training procedure, which we call the Held-Out Line Search algorithm (HOLS), is a two-phase iterative batch optimization procedure consisting of (1) a gradient calculation on a differentiable approximation to the loss on a large amount of parallel training While sparse features are successfully used in many NLP systems, such parameterizations pose a number of learning challenges. First, since any one feature is likely to occur infrequently, a large a"
N13-1025,P02-1040,0,0.0870489,"t compare to MERT. Problems of overfitting and degenerate derivations were tackled with a probabilistic latent variable model (Blunsom et al., 2008) which used rule indicator features yet failed to improve upon the MERT baseline for the standard Hiero features. 249 Difficulties in Large-Scale Training Discriminative training for machine translation is complicated by several factors. First, both translation rules and feature weights are learned from parallel data. If the same data is used for both tasks, overfitting of the weights is very possible.1 Second, the standard MT cost function, BLEU (Papineni et al., 2002), does not decompose additively over training instances (because of the “brevity penalty”) and so approximations are used—these often have problems with the length (Nakov et al., 2012). Finally, state-of-the-art MT systems make extensive good use of “dense” features, such as the log probability of translation decisions under a simpler generative translation model. Our goal is to begin to use much sparser features without abandoning the proven dense features; however, extremely sparse features leads to problems of scaling in the optimization problem as we will show. 3.1 Training Data and Overfi"
N13-1025,W10-1748,0,0.0488262,"tly been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule"
N13-1025,2012.amta-papers.14,0,0.114435,"tive training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned weight, which count how often a specific rule is"
N13-1025,P12-1002,1,0.93084,"poor scaling (since MT 248 Proceedings of NAACL-HLT 2013, pages 248–258, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics decoding is so expensive, it is not feasible to make many passes through large amounts of training data, so optimization must be efficient). We then present the details of our algorithm that addresses these issues, give results on three language pairs, and conclude. Techniques for distributed learning and feature selection for the perceptron loss using rule indicator, rule shape, and source side-bigram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011"
N13-1025,P06-2101,0,0.0864597,"ape, and source side-bigram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameteri"
N13-1025,P06-1091,0,0.0494195,"criminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned weight, which count how often a specific rule is used in a translation. Early experiments (Liang et al., 2006) used the structured perceptron to tune a phrase-based system on a large subset of the training data, showing improvements when using rule indicator features, word alignment features, and POS tag features. Another early attempt (Tillmann and Zhang, 2006) used phrase pair and word features in a block SMT system trained using stochastic gradient descent for a convex loss function, but did not compare to MERT. Problems of overfitting and degenerate derivations were tackled with a probabilistic latent variable model (Blunsom et al., 2008) which used rule indicator features yet failed to improve upon the MERT baseline for the standard Hiero features. 249 Difficulties in Large-Scale Training Discriminative training for machine translation is complicated by several factors. First, both translation rules and feature weights are learned from parallel"
N13-1025,D07-1080,0,0.295509,"ram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translati"
N13-1025,W11-2160,0,0.028091,"ions using multiple runs of the last MERT run (i.e. the last line search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HOLS. It is defined as ˜= L n"
N13-1039,P11-1087,0,0.0199181,"of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to go.” 7 http://www.ark.c"
N13-1039,D11-1052,0,0.403712,": u = “you”) and prepositions (C: fir = “for”). There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many"
N13-1039,J92-4003,0,0.437604,"Missing"
N13-1039,E03-1009,0,0.019282,"lusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the"
N13-1039,P11-1137,1,0.142556,"g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as"
N13-1039,N13-1037,0,0.383435,"rs corresponding to some of these words. This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data. Introduction Online conversational text, typified by microblogs, chat, and text messages,1 is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text"
N13-1039,D11-1142,0,0.0448156,"g minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and §6.2. 6 Experiments We are primarily concerned with performance on our annotated datasets described"
N13-1039,P11-2008,1,0.727026,"Missing"
N13-1039,P11-1038,0,0.422239,"to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult. We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc. These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexic"
N13-1039,P08-1068,0,0.420847,"t for social media analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha ha"
N13-1039,P11-1037,0,0.0938106,"e are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for b"
N13-1039,P12-3005,0,0.246426,"Missing"
N13-1039,J93-2004,0,0.061082,"Missing"
N13-1039,N10-1004,0,0.161793,"regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text. We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets. We release all of these resources to the research community: • an open-source part-of-"
N13-1039,W96-0213,0,0.684593,"community: • an open-source part-of-speech tagger for online conversational text (§2); • unsupervised Twitter word clusters (§3); • an improved emoticon detector for conversational text (§4); 380 Proceedings of NAACL-HLT 2013, pages 380–390, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics • POS annotation guidelines (§5.1); and • a new dataset of 547 manually POS-annotated tweets (§5). 2 MEMM Tagger Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1 , and is parameterized by a multiclass logistic regression: p(yt = k |y t−1 , x, t; β) ∝   P (obs) (trans) exp βyt−1 ,k + j βj,k fj (x, t) We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A). Our feature sets will be discussed below in detail. Decoding. For experiments reported in this paper, we use the O(|x|K"
N13-1039,D11-1141,0,0.44769,"tactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results i"
N13-1039,P05-1044,1,0.328937,"demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about t"
N13-1039,N12-1052,0,0.100512,"s—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa aha"
N13-1039,P10-1040,0,0.242904,"analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha"
N13-1039,P02-1053,0,0.00928357,"y inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and"
N13-1039,petrov-etal-2012-universal,0,\N,Missing
N13-1073,J93-2003,0,0.254903,"l are also important. These play a crucial role in many scenarios such as parallel data mining and rapid large scale experimentation, and as subcomponents of other models or training and inference algorithms. For these reasons, IBM Models 1 and 2, which support exact inference in time Θ(|f |· |e|), continue to be widely used. This paper argues that both of these models are suboptimal, even in the space of models that permit such computationally cheap inference. Model 1 assumes all alignment structures are uniformly 2 Model Our model is a variation of the lexical translation models proposed by Brown et al. (1993). Lexical translation works as follows. Given a source sentence f with length n, first generate the length of the target sentence, m. Next, generate an alignment, a = ha1 , a2 , . . . , am i, that indicates which source word (or null token) each target word will be a translation of. Last, generate the m output words, where each ei depends only on fai . The model of alignment configurations we propose is a log-linear reparameterization of Model 2. 1 Model 2 has independent parameters for every alignment position, conditioned on the source length, target length, and current target index. 644 Pro"
N13-1073,N13-1140,1,0.531297,"ored this issue for training Model 3, we found that EM tended to find poor values for p0 , producing alignments that were overly sparse. By fixing the value at p0 = 0.08, we obtained minimal AER. Second, like Riley and Gildea (2012), we found that small values of α improved the alignment error rate, although the impact was not particularly strong over large ranges of 5 http://www.statmt.org/wmt12 While this computational effort is a small relative to the total cost in EM training, in algorithms where λ changes more rapidly, for example in Bayesian posterior inference with Monte Carlo methods (Chahuneau et al., 2013), this savings can have substantial value. 6 647 Table 1: CPU time (hours) required to train alignment models in one direction. Language Pair Chinese-English French-English Arabic-English Tokens 17.6M 117M 368M Model 4 2.7 17.2 63.2 Log-linear 0.2 1.7 6.0 Table 2: Alignment quality (AER) on the WMT 2012 French-English and FBIS Chinese-English. Rows with EM use expectation maximization to estimate the θf , and ∼Dir use variational Bayes. Model Model 1 Model 1 Model 2 Log-linear Log-linear Model 4 Estimator EM ∼Dir EM EM ∼Dir EM FR - EN ZH - EN 29.0 26.6 21.4 18.5 16.6 10.4 56.2 53.6 53.3 46.5 4"
N13-1073,J07-2003,0,0.00975985,"bove (§3.3). The algorithms are 7 As an anonymous reviewer pointed out, it is a near certainty that tuning of these hyperparameters for each alignment task would improve results; however, optimizing hyperparameters of alignment models is quite expensive. Our intention is to show that it is possible to obtain reasonable (if not optimal) results without careful tuning. compared in terms of (1) time required for training; (2) alignment error rate (AER, lower is better);8 and (3) translation quality (BLEU, higher is better) of hierarchical phrase-based translation system that used the alignments (Chiang, 2007). Table 1 shows the CPU time in hours required for training (one direction, English is generated). Our model is at least 10× faster to train than Model 4. Table 3 reports the differences in BLEU on a held-out test set. Our model’s alignments lead to consistently better scores than Model 4’s do.9 5 Conclusion We have presented a fast and effective reparameterization of IBM Model 2 that is a compelling replacement for for the standard Model 4. Although the alignment quality results measured in terms of AER are mixed, the alignments were shown to work exceptionally well in downstream translation"
N13-1073,N06-2013,0,0.00674238,"Missing"
N13-1073,J03-1002,0,0.124565,"mpute the posterior probability over alignments using the above probabilities, p(ai |ei , f, m, n) = Under our model, the marginal likelihood of a sentence pair hf, ei can be computed exactly in time (1) Finally, since all words in e (and their alignments) are conditionally independent,3 p(e |f) = Marginals p(ei , ai |f, m, n) . p(ei |f, m, n) = m Y p(ei i=1 m X n Y i=1 j=0 |f, m, n) δ(ai |i, m, n) × θ(ei |fai ). 2 Vogel et al. (1996) hint at a similar reparameterization of Model 2; however, its likelihood and its gradient are not efficient to evaluate, making it impractical to train and use. Och and Ney (2003) likewise remark on the overparameterization issue, removing a single variable of the original conditioning context, which only slightly improves matters. 645 3 We note here that Brown et al. (1993) derive their variant of this expression by starting with the joint probability of an alignment and translation, marginalizing, and then reorganizing common terms. While identical in implication, we find the direct probabilistic argument far more intuitive. 3.2 Efficient Partition Function Evaluation Evaluating and maximizing the data likelihood under log-linear models can be computationally expensi"
N13-1073,P12-2060,0,0.0202891,"probabilities decrease by a factor of r = exp −λ n per step. To compute the value of the partition, we only need to evaluate the unnormalized probabilities at j↑ and j↓ and then use the following identity, which gives the sum of the first ` terms of a geometric series (Courant and Robbins, 1996): s` (g1 , r) = ` X g1 rk−1 = g1 k=1 1 − r` . 1−r as counts and normalizing (Brown et al., 1993). In the experiments reported in this paper, we make the further assumption that θf ∼ Dirichlet(µ) where µi = 0.01 and approximate the posterior distribution over the θf ’s using a mean-field approximation (Riley and Gildea, 2012).4 During the M-step, the λ parameter must also be updated to make the E-step posterior distribution over alignment points maximally probable under δ(· |i, m, n). This maximizing value cannot be computed analytically, but a gradient-based optimization can be used, where the first derivative (here, for a single target word) is: ∇λ L = Ep(ai |ei ,f,m,n) [h(i, ai , m, n)]   − Eδ(j 0 |i,m,n) h(i, j 0 , m, n) The first term in this expression (the expected value of h under the E-step posterior) is fixed for the duration of each M-step, but the second term’s value (the derivative of the log-partit"
N13-1073,C96-2141,0,0.377167,"the probability that the ith word of e is ei can be computed as: p(ei , ai |f, m, n) = δ(ai |i, m, n) × θ(ei |fai ) n X p(ei |f, m, n) = p(ei , ai = j |f, m, n). j=0 We can also compute the posterior probability over alignments using the above probabilities, p(ai |ei , f, m, n) = Under our model, the marginal likelihood of a sentence pair hf, ei can be computed exactly in time (1) Finally, since all words in e (and their alignments) are conditionally independent,3 p(e |f) = Marginals p(ei , ai |f, m, n) . p(ei |f, m, n) = m Y p(ei i=1 m X n Y i=1 j=0 |f, m, n) δ(ai |i, m, n) × θ(ei |fai ). 2 Vogel et al. (1996) hint at a similar reparameterization of Model 2; however, its likelihood and its gradient are not efficient to evaluate, making it impractical to train and use. Och and Ney (2003) likewise remark on the overparameterization issue, removing a single variable of the original conditioning context, which only slightly improves matters. 645 3 We note here that Brown et al. (1993) derive their variant of this expression by starting with the joint probability of an alignment and translation, marginalizing, and then reorganizing common terms. While identical in implication, we find the direct probabi"
N13-1076,N10-1083,0,0.0254529,"ected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupe"
N13-1076,D08-1092,0,0.0201754,"s starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This"
N13-1076,W10-2906,0,0.0157732,"5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not dep"
N13-1076,J07-2003,0,0.0858748,"Missing"
N13-1076,W06-1670,0,0.725634,"Missing"
N13-1076,W03-1022,0,0.189249,"applications communication ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. Introduction A taxonomic view of lexical semantics groups word senses/usages into categories of varying granularities. WordNet supersense tags denote coarse semantic classes, including person and artifact (for nouns) and motion and weather (for verbs); these categories can be taken as the top level of a taxonomy. Nominal supersense tagging (Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annota"
N13-1076,P11-1061,0,0.0261324,"e morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel"
N13-1076,P10-4002,1,0.808987,"Missing"
N13-1076,elkateb-etal-2006-building,0,0.0311891,"(Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective"
N13-1076,2005.eamt-1.15,0,0.0305449,"bout topical domain performance we were able to find that holds across annotators and systems, in contrast with the stark topical effects observed by Mohit et al. (2012) for NER. 665 noun coverage gains track overall improvements. If QCRI produces better translations, why is cdec more useful for supersense tagging? As noted in §3.3, cdec gives word-level alignments (even when the decoder uses large phrasal units for translation), allowing for more precise projections.11 We suspect this is especially important in light of findings that larger phrase sizes are indicative of better translations (Gamon et al., 2005), so these are exactly the cases where we expect the translations to be valuable. 5 Conclusion To our knowledge, this is the first study of automatic Arabic supersense tagging. We have shown empirically that an MT-in-the-middle technique is most effective of several approaches that do not require labeled training data. Analysis sheds light on several challenges that would need to be overcome for better Arabic lexical semantic tagging. Acknowledgments We thank Wajdi Zaghouani for the translation of the Arabic Wikipedia MT set, Francisco Guzman and Preslav Nakov for the output of QCRI’s MT syste"
N13-1076,P05-1071,0,0.0961947,"Missing"
N13-1076,N06-2015,0,0.0206598,"hunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English,"
N13-1076,P12-1073,0,0.0128156,"is; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the train"
N13-1076,N03-1017,0,0.00971947,"Missing"
N13-1076,P07-2045,1,0.0132485,"Missing"
N13-1076,J94-2001,0,0.08432,"nstructs the Arabic-to-English mapping {1→person11 , 4→location43 , {5, 6}→artifact76 }, resulting in the tagging shown in the bottom row. weighted to favor earlier senses (presumed by lexicographers to be more frequent) and then the supersense with the greatest aggregate weight is selected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with oth"
N13-1076,P07-1123,0,0.019554,"Missing"
N13-1076,H93-1061,0,0.0657483,"Missing"
N13-1076,E12-1017,1,0.866478,"Missing"
N13-1076,P02-1040,0,0.0859619,"Missing"
N13-1076,N12-1090,0,0.0532349,"ords that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et al., 2012), which augmented a named entity corpu"
N13-1076,P08-2030,0,0.0741844,"Missing"
N13-1076,P12-2050,1,0.801661,"toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and co"
N13-1076,W04-3207,1,0.770277,"ts with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in"
N13-1076,2006.amta-papers.25,0,0.0975624,"Missing"
N13-1076,N01-1026,0,0.0399772,"t sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely du"
N13-1076,H01-1035,0,0.0177244,"(for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch),"
N13-1076,D08-1063,0,0.11663,"0 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et"
N13-1076,W05-0909,0,\N,Missing
N13-1140,2010.amta-papers.4,0,0.0192714,"Missing"
N13-1140,N03-2002,0,0.0424451,"ggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid mode"
N13-1140,bojar-prokopova-2006-czech,0,0.0273209,"hoices for the base word distribution: • As a baseline, we use a uniform base distribution over the target vocabulary: G0w = U(V ). • We define a stem distribution Gs [f ] for each source word f , a shared pattern distribution Gp , and set G0w [f ] = MP(Gs [f ], Gp ). In this case, we obtain the model depicted in Fig. 2. The stem and the pattern models are also given PY priors with uniform base distribution (G0s = U(S)). Finally, we put uninformative priors on the alignment distribution parameters: p0 ∼ Beta(α, β) is collapsed and λ ∼ Gamma(k, θ) is inferred using Metropolis-Hastings. ↵, from Bojar and Prokopová (2006). The morphological analyzer is provided by Xerox. Results Results are shown in Table 5. Our lightly parameterized model performs much better than IBM Model 4 in these small-data conditions. With an identical model, we find PY priors outperform traditional multinomial distributions. Adding morphology further reduced the alignment error rate, for both languages. Model Model 4 EM PY-U(V ) PY-U(S) p0 d w , ✓w d s , ✓s e Gw Gs G0s Gp G0p Table 5: Word alignment experiments on English-Turkish (en-tr) and English-Czech (en-cs) data. d p , ✓p Figure 2: Our alignment model, represented as a graphical"
N13-1140,J92-1002,0,0.752922,"Missing"
N13-1140,J93-2003,0,0.0700196,"tical model, we find PY priors outperform traditional multinomial distributions. Adding morphology further reduced the alignment error rate, for both languages. Model Model 4 EM PY-U(V ) PY-U(S) p0 d w , ✓w d s , ✓s e Gw Gs G0s Gp G0p Table 5: Word alignment experiments on English-Turkish (en-tr) and English-Czech (en-cs) data. d p , ✓p Figure 2: Our alignment model, represented as a graphical model. Experiments We evaluate the alignment error rate of our models for two language pairs with rich morphology on the target side. We compare to alignments inferred using IBM Model 4 trained with EM (Brown et al., 1993),10 a version of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline. We consider two language pairs. English-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus (Tyers and Alperen, 2010) and gold-standard alignments from Çakmak et al. (2012). Our morphological analyzer is identical to the one used in the previous sections. As an example of how our model generalizes better, consider the sentence pair in Fig. 3, taken from the evaluation data. The two words composing the Turkish sentence are not found elsewhere in t"
N13-1140,W05-1107,0,0.0290275,"age models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive"
N13-1140,2012.eamt-1.60,0,0.0172065,"what we try to do by designing a lexicon-free analyzer for Russian. A guesser was developed in three hours; it is prone to over-generation and produces ambiguous analyses for most words but covers a large number of morphological phenomena (gender, case, tense, etc.). For example, the word иврите5 can be correctly analyzed as иврит+Noun+Masc+Prep+Sg but also as the incorrect forms: иврить+Verb+Pres+2P+Pl, иврита+Noun+Fem+Dat+Sg, ивритя+Noun+Fem+Prep+Sg, and more. 3.2 Disambiguation Experiments We train the unigram model on a 1.7M-word corpus of TED talks transcriptions translated into Russian (Cettolo et al., 2012) and evaluate our analyzer against a test set consisting of 1,500 goldstandard analyses obtained from the morphology disambiguation task of the DIALOG 2010 conference (Lyaševskaya et al., 2010).6 Each analysis is composed of a lemma (иврит), a part of speech (Noun), and a sequence of additional functional morphemes (Masc,Prep,Sg). We consider only open-class categories: nouns, ad5 6 иврите = Hebrew (masculine noun, prepositional case) http://ru-eval.ru 1209 jectives, adverbs and verbs, and evaluate the output of our model with three metrics: the lemma accuracy, the part-of-speech accuracy, and"
N13-1140,N07-1048,0,0.0147887,"l., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modelin"
N13-1140,N13-1073,1,0.771743,"ogy. In fact, alignment models are a good candidate for using richer word distributions: they assume a target word distribution conditioned on each source word. When the target language is morphologically rich, classic independence assumptions produce very weak models unless some kind of preprocessing is applied to one side of the corpus. An alternative is to use our unigram model as a word translation distribution for each source word in the corpus. Our alignment model is based on a simple variant of IBM Model 2 where the alignment distribution is only controlled by two parameters, λ and p0 (Dyer et al., 2013). p0 is the probability of the null alignment. For a source sentence f of length n, a target sentence e of length m and a latent alignment a, we define the following alignment link probabilities (j 6= 0):   i j p(ai = j |n, m) ∝ (1 − p0 ) exp −λ − m n λ controls the flatness of this distribution: larger values make the probabilities more peaked around the diagonal of the alignment matrix. Each target word is then generated given a source word and a latent alignment link from the word translation distribution p(ei |fai , Gw ). Note that this is effectively a unigram distribution over target w"
N13-1140,J01-2001,0,0.0647422,"develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sa"
N13-1140,H05-1085,0,0.0802413,"been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general proble"
N13-1140,P05-1071,0,0.0219994,"irilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and"
N13-1140,N06-2013,0,0.165461,"human languages, many NLP systems treat fully inflected forms as the atomic units of language. By assuming independence of lexical stems’ various surface forms, this avoidance approach exacerbates the problem of data sparseness. If it is employed at all, morphological analysis of text tends to be treated as a preprocessing step to other NLP modules. While this latter disambiguation approach helps address data sparsity concerns, it has substantial drawbacks: it requires supervised learning from expert-annotated corpora, and determining the optimal morphological granularity is labor-intensive (Habash and Sadat, 2006). Neither approach fully exploits the finite-state transducer (FST) technology that has been so successful for modeling the mapping between surface forms and their morphological analyses (Karttunen and Beesley, 2005), and the mature collections of high quality transducers that already exist for many languages (e.g., Turkish, Russian, Arabic). Much linguistic knowledge is encoded in such FSTs. In this paper, we develop morphology-aware nonparametric Bayesian language models that bring together hand-written FSTs with statistical modeling and require no token-level annotation. The sparsity issue"
N13-1140,C00-1042,0,0.0608514,"4. 11 ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al."
N13-1140,E09-2008,0,0.0256513,"derable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and 10 We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. 11 ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length princip"
N13-1140,J94-3001,0,0.687315,"Missing"
N13-1140,P84-1038,0,0.450265,"-base model to find the correct alignment (marked in black), while all the other models have no evidence for it and choose an arbitrary alignment (gray points). I was not able to finish my homework f a AER en-tr en-cs 52.1 34.5 43.8 28.9 39.2 25.7 33.8 24.8 ödevimi bitiremedim Figure 3: A complex Turkish-English word alignment (alignment points in gray: EM/PY-U(V ); black: PYU(S)). 6 Related Work English-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments Computational morphology has received considerable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and 10 We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. 11 ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several p"
N13-1140,P07-1017,0,0.0650095,"length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a c"
N13-1140,D11-1080,0,0.0250277,"lds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al"
N13-1140,W07-0704,0,0.0188157,"sed on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most"
N13-1140,P11-1072,0,0.0298152,"the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which m"
N13-1140,H05-1060,1,0.806906,"e, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3,"
N13-1140,P01-1063,0,0.0360378,"ambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored lan"
N13-1140,P08-1084,0,0.0289725,"et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language model"
N13-1140,P06-1124,0,0.44035,"ependent is clearly unsatisfying. We therefore assume that both the stem and the pattern distributions are generated from PY processes, and that MP(Gs , Gp , G ENERATE) is itself the base distribution of a PYP. Pitman-Yor Processes Our work relies extensively on Pitman-Yor processes, which provide a flexible framework for expressing backoff and interpolation relationships and extending standard models with richer word distributions (Pitman and Yor, 1997). They have been shown to match the performance of state-of-the-art language models and to give estimates that follow appropriate power laws (Teh, 2006). A draw from a Pitman-Yor process (PYP), denoted G ∼ PY(d, θ, G0 ), is a discrete distribution over a (possibly infinite) set of events, which we denote abstractly E. The process is parameterized by a discount parameter 0 ≤ d &lt; 1, a strength parameter θ > −d, and a base distribution G0 over the event space E. In this work, our focus is on the base distribution G0 . We place vague priors on the hyperparameters d ∼ U([0, 1]) and (θ + d) ∼ Gamma(1, 1). Inference in PYPs is discussed below. 1207 Gs ∼ PY(ds , θs , G0s ) Gp ∼ PY(dp , θp , G0p ) Gw ∼ PY(d, θ, MP(Gs , Gp , G ENERATE)) A draw Gw from"
N13-1140,W10-1401,0,0.0239317,"d Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the st"
N13-1140,cakmak-etal-2012-word,0,0.0268039,"Missing"
N13-1140,W83-0114,0,\N,Missing
N13-1140,P01-1035,0,\N,Missing
N15-1062,W12-4410,1,0.745643,"Missing"
N15-1062,N09-1067,0,0.0215923,"Missing"
N15-1062,N13-1073,1,0.86988,"Missing"
N15-1062,P97-1040,0,0.107892,"cal processes that alter the appearance of lemmas. To deal with morphological variants, we construct morphological adaptation transducers that optionally strip Arabic concatenative affixes and clitics, and then optionally append Swahili affixes, generating a superset of all possible loanword hypotheses. We obtain the list of Arabic affixes from the Arabic morphological analyzer SAMA (Maamouri et al., 2010); the Swahili affixes are taken from a hand-crafted Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to constraints, rather than learning an ordinal constraint ranking (Boersma and Hayes, 2001; Goldwater a"
N15-1062,P02-1008,0,0.0358064,"that alter the appearance of lemmas. To deal with morphological variants, we construct morphological adaptation transducers that optionally strip Arabic concatenative affixes and clitics, and then optionally append Swahili affixes, generating a superset of all possible loanword hypotheses. We obtain the list of Arabic affixes from the Arabic morphological analyzer SAMA (Maamouri et al., 2010); the Swahili affixes are taken from a hand-crafted Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to constraints, rather than learning an ordinal constraint ranking (Boersma and Hayes, 2001; Goldwater and Faithfulness"
N15-1062,C94-2163,0,0.419115,"d Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to constraints, rather than learning an ordinal constraint ranking (Boersma and Hayes, 2001; Goldwater and Faithfulness constraints MAX - IO - MORPH MAX - IO - C MAX - IO - V DEP - IO - MORPH DEP - IO - V IDENT- IO - C IDENT- IO - C - M IDENT- IO - C - A IDENT- IO - C - S IDENT- IO - C - P IDENT- IO - C - G IDENT- IO - C - E IDENT- IO - V IDENT- IO - V- O IDENT- IO - V- R IDENT- IO - V- F IDENT- IO - V- FIN Markedness constraints no (donor) affix deletion no consonant deletion no vowel deletion no (recipient) affix epenthesis no vowel epenthesis no consonant"
N15-1062,P09-1042,0,0.085628,"Missing"
N15-1062,P12-2027,0,0.025379,"on generation of borrowed phonemes in English–Japanese language pair (the method does not generalize from borrowed phonemes to borrowed words, and does not rely on linguistic insights), we are not aware of any prior work on computational modeling of lexical borrowing. Few papers only mention or tangentially address borrowing, we briefly list them here. Daumé III (2009) focuses on areal effects on linguistic typology, a broader phenomenon that includes borrowing and genetic relations across languages. This study is aimed at discovering language areas based on typological features of languages. Garley and Hockenmaier (2012) train a maxent classifier with character n-gram and morphological features to identify anglicisms (which they compare to loanwords) in an online community of German hip hop fans. List and Moran (2013) have published a toolkit for computational tasks in historical linguistics but remark that “Automatic approaches for borrowing detection are still in their infancy in historical linguistics.” Two related lines of research are transliteration and cognate identification. Knight and Graehl (1998), AlOnaizan and Knight (2002) developed a finite-state generative model of transliteration, and successf"
N15-1062,W06-1107,0,0.0761386,"Missing"
N15-1062,N01-1014,0,0.370066,"put to the borrowing model is a loanword candidate in Swahili/Romanian,8 the outputs are plausible donor words in the Arabic/French monolingual lexicon (i.e., any word in pronunciation dictionary). We train the borrowing model using a small set of training examples, and then evaluate it using a held-out test set. In the rest of this section we describe in detail our datasets, tools, and experimental results. Resources We employ Arabic–English and Swahili–English bitexts to extract a training set (corpora of sizes 5.4M and 14K sentence pairs, respectively), using a cognate discovery technique (Kondrak, 2001). Phonetically and semantically similar strings are classified as cognates; phonetic similarity is the string similarity between phonetic representations, and semantic similarly is approximated by translation.9 We thereby extract Arabic 8 Our model does not provide a mechanism for identifying loanwords in the recipient language; we only model the borrowing process. Classifying loanwords in the recipient language is an interesting but ultimately different problem: the ontological status of words in a lexicon is a difficult problem, even for human experts, however, knowledge of cross-lingual cor"
N15-1062,P13-4003,0,0.0252321,"prior work on computational modeling of lexical borrowing. Few papers only mention or tangentially address borrowing, we briefly list them here. Daumé III (2009) focuses on areal effects on linguistic typology, a broader phenomenon that includes borrowing and genetic relations across languages. This study is aimed at discovering language areas based on typological features of languages. Garley and Hockenmaier (2012) train a maxent classifier with character n-gram and morphological features to identify anglicisms (which they compare to loanwords) in an online community of German hip hop fans. List and Moran (2013) have published a toolkit for computational tasks in historical linguistics but remark that “Automatic approaches for borrowing detection are still in their infancy in historical linguistics.” Two related lines of research are transliteration and cognate identification. Knight and Graehl (1998), AlOnaizan and Knight (2002) developed a finite-state generative model of transliteration, and successfully applied it to Arabic–English named entity translation. Mann and Yarowsky (2001) and Kondrak (2001) identify cognate pairs, based on the learned surface 606 and phonetic similarities, respectively."
N15-1062,littell-etal-2014-morphological,0,0.0281556,"owels in loanword candidates. Morphological adaptation. Both Arabic and Swahili have significant morphological processes that alter the appearance of lemmas. To deal with morphological variants, we construct morphological adaptation transducers that optionally strip Arabic concatenative affixes and clitics, and then optionally append Swahili affixes, generating a superset of all possible loanword hypotheses. We obtain the list of Arabic affixes from the Arabic morphological analyzer SAMA (Maamouri et al., 2010); the Swahili affixes are taken from a hand-crafted Swahili morphological analyzer (Littell et al., 2014). 4 Learning constraint weights Due to the computational problems of working with OT (Eisner, 1997; Eisner, 2002), we make simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with a small linguistically-motivated subset of allowed transformations on donor pronunciations, as described in §3; (2) imposing a priori restrictions on the set of the surface realizations by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights to con"
N15-1062,N01-1020,0,0.301921,"ogical features to identify anglicisms (which they compare to loanwords) in an online community of German hip hop fans. List and Moran (2013) have published a toolkit for computational tasks in historical linguistics but remark that “Automatic approaches for borrowing detection are still in their infancy in historical linguistics.” Two related lines of research are transliteration and cognate identification. Knight and Graehl (1998), AlOnaizan and Knight (2002) developed a finite-state generative model of transliteration, and successfully applied it to Arabic–English named entity translation. Mann and Yarowsky (2001) and Kondrak (2001) identify cognate pairs, based on the learned surface 606 and phonetic similarities, respectively. As our experiments confirm, orthographic and phonetic transliteration and string edit distance methods are not adequate models for the complex borrowing phenomena. 9 Conclusion Given a loanword, our model identifies plausible donor words in a contact language. We show that a discriminative model with Optimality Theoretic features effectively models systematic phonological changes in Arabic–Swahili loanwords. We also found that the model and methodology is generally applicable t"
N15-1062,schultz-schlippe-2014-globalphone,0,0.0656519,"Missing"
N15-1062,P14-1024,1,0.868671,"Missing"
N15-1062,Q13-1001,0,0.0761148,"Missing"
N15-1062,W02-0505,0,\N,Missing
N15-1062,J98-4003,0,\N,Missing
N15-1070,P11-1061,0,0.0146357,"graph, however their approach does not take distributional information from VSMs into account. Thus, to the best of our knowledge, our work presents the first attempt at producing sense grounded VSMs that are symbolically tied to lexical ontologies. From a modelling point of view, it is also the first to outline a unified, principled and extensible framework that effectively combines the symbolic and distributional paradigms of semantics. Both our models leverage the graph structure of ontologies to effectively ground the senses of a VSM. This ties into previous research (Das and Smith, 2011; Das and Petrov, 2011) that propagates information through a factor graph to perform tasks such as frame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing"
N15-1070,P11-1144,0,0.0122982,"walks on the Wordnet graph, however their approach does not take distributional information from VSMs into account. Thus, to the best of our knowledge, our work presents the first attempt at producing sense grounded VSMs that are symbolically tied to lexical ontologies. From a modelling point of view, it is also the first to outline a unified, principled and extensible framework that effectively combines the symbolic and distributional paradigms of semantics. Both our models leverage the graph structure of ontologies to effectively ground the senses of a VSM. This ties into previous research (Das and Smith, 2011; Das and Petrov, 2011) that propagates information through a factor graph to perform tasks such as frame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories"
N15-1070,D10-1113,0,0.00821942,"oothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbased predictive models. We presented an evaluation of 3 semantic tasks on 7 datasets. Our results show"
N15-1070,D08-1094,0,0.0228482,"rame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-"
N15-1070,P10-2017,0,0.00736496,"pective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbase"
N15-1070,N15-1184,1,0.85529,"Missing"
N15-1070,N13-1092,0,0.0538894,"Missing"
N15-1070,C14-1048,0,0.0660298,"and help to distinguish the vectors from their single sense embeddings. 4 Related Work Since Reisinger and Mooney (2010) first proposed a simple context clustering technique to generate multi-prototype VSMs, a number of related efforts have worked on adaptations and improvements relying on the same clustering principle. Huang et al. (2012) train their vectors with a neural network and additionally take global context into account. Neelakantan et al. (2014) extend the popular skip-gram model (Mikolov et al., 2013a) in a non-parametric fashion to allow for different number of senses for words. Guo et al. (2014) exploit bilingual alignments to perform better context clustering during training. Tian et al. (2014) propose a probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work is also similar to Yu and Dredze (2014) and Faruqui et al. (2014), who treat lexicons such as the paraphrase database (PPDB) (Ganitkevitch et al., 2013) or"
N15-1070,P12-1092,0,0.940798,"eanwhile, lexical ontologies, such as WordNet (Miller, 1995) specifically catalog sense inventories and provide typologies that link these senses to one another. These hand-curated ontologies provide a complementary source of information to distributional statistics. Recent research tries to leverage this information to train better VSMs (Yu and Dredze, 2014; Faruqui et al., 2014), but does not tackle the problem of polysemy. Parallely, work on polysemy for VSMs revolves primarily around techniques that cluster contexts to distinguish between different word senses (Reisinger and Mooney, 2010; Huang et al., 2012), but does not integrate ontologies in any way. In this paper we present two novel approaches to integrating ontological and distributional sources of information. Our focus is on allowing already existing, proven techniques to be adapted to produce ontologically grounded word sense embeddings. Our first technique is applicable to any sense-agnostic 683 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 683–693, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics VSM as a post-processing step that perfor"
N15-1070,D07-1031,0,0.00824306,"her than using the full posterior over senses. Also, given that the structured regularizer pΩ (θ) is essentially the retrofitting objective in equation 1, we run retrofitting periodically every k words (with α = 0 in equation 2) instead of lazy updates.2 The following decision rule is used in the “hard” E-step: (t) sij = arg max p(ci |sij ; θ(t) )πij sij (5) In the M-step we use Variational Bayes to update Π with:    (0) exp ψ c˜(wi , sij ) + λπij (t+1) πij ∝ (6) exp (ψ (˜ c(wi ) + λ)) where c˜(·) is the online expected count and ψ(·) is the digamma function. This approach is motivated by Johnson (2007) who found that naive EM leads to poor results, while Variational Bayes is consistently better and promotes faster convergence of the likelihood function. To update the parameters U and V , we use negative sampling (Mikolov et al., 2013a) which is an efficient approximation to the original skip-gram objective. Negative sampling attempts to distinguish between true word pairs in the data, relative to noise. Stochastic gradient descent on the following equation is used to update the model pa2 We find this gives slightly better performance. rameters U and V : L = log σ(ui · vij ) + + X m X log σ("
N15-1070,N13-1090,0,0.501745,"perform other models we compare against. 1 Introduction Vector space models (VSMs) of word meaning play a central role in computational semantics. These represent meanings of words as contextual feature vectors in a high-dimensional space (Deerwester et al., 1990) or some embedding thereof (Collobert and Weston, 2008) and are learned from unannotated corpora. Word vectors in these continuous space representations can be used for meaningful semantic operations such as computing word similarity (Turney, 2006), performing analogical reasoning (Turney, 2013) and discovering lexical relationships (Mikolov et al., 2013b). They have also proved useful in downstream NLP applications such as information retrieval (Manning et al., 2008) and question answering (Tellex et al., 2003), among others. However, VSMs remain flawed because they assign a single vector to every word, thus ignoring the possibility that words may have more than one meaning. For example, the word “bank” can either denote a financial institution or the shore of a river. The ability to model multiple meanings is an important component of any NLP system, given how common polysemy is in language. The lack of sense annotated corpora large enough"
N15-1070,P08-1028,0,0.0298665,"to perform tasks such as frame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM a"
N15-1070,D14-1113,0,0.421675,"ors (i.e. synonyms, hypernyms and hyponyms), tended to have more clearly sense specific vectors. This is expected, since it is these neighborhoods that disambiguate and help to distinguish the vectors from their single sense embeddings. 4 Related Work Since Reisinger and Mooney (2010) first proposed a simple context clustering technique to generate multi-prototype VSMs, a number of related efforts have worked on adaptations and improvements relying on the same clustering principle. Huang et al. (2012) train their vectors with a neural network and additionally take global context into account. Neelakantan et al. (2014) extend the popular skip-gram model (Mikolov et al., 2013a) in a non-parametric fashion to allow for different number of senses for words. Guo et al. (2014) exploit bilingual alignments to perform better context clustering during training. Tian et al. (2014) propose a probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work"
N15-1070,N09-5005,0,0.0139636,"cument level to further enhance the VSM. We distinguish three variants: the original single-sense vectors (SINGLE), a multi-prototype variant (MULTI), – both are available as pre-trained vectors for download4 – and a sense-based version obtained by running retrofitting on the original vectors (RETRO). Skip-gram Vectors (SG) (Mikolov et al., 2013a): We use the word vector tool Word2Vec5 to train skip-gram vectors. We define 6 variants: a single-sense version (SINGLE), two multi-sense variants that were trained by first sense disambiguating the entire corpus using WSD tools, – one unsupervised (Pedersen and Kolhatkar, 2009) (WSD) and the other supervised (Zhong and Ng, 2010) (IMS) – a retrofitted version obtained from the singlesense vectors (RETRO), an EM implementation of the skip-gram model with the structured regularizer as described in section 2.2 (EM+RETRO), and the same EM technique but ignoring the ontology (EM). All models were trained on publicly available WMT20116 English monolingual data. This corpus of 355 million words, although adequate in size, is smaller than typically used billion word corpora. We use this corpus because the WSD baseline involves preprocessing the corpus with sense disambiguati"
N15-1070,P13-1132,0,0.0148761,"probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work is also similar to Yu and Dredze (2014) and Faruqui et al. (2014), who treat lexicons such as the paraphrase database (PPDB) (Ganitkevitch et al., 2013) or WordNet (Miller, 1995) as an auxiliary thesaurus to improve VSMs. However, they do not model senses in any way. Pilehvar et al. (2013) do model senses from an ontology by performing random-walks on the Wordnet graph, however their approach does not take distributional information from VSMs into account. Thus, to the best of our knowledge, our work presents the first attempt at producing sense grounded VSMs that are symbolically tied to lexical ontologies. From a modelling point of view, it is also the first to outline a unified, principled and extensible framework that effectively combines the symbolic and distributional paradigms of semantics. Both our models leverage the graph structure of ontologies to effectively ground"
N15-1070,I11-1079,0,0.0102795,"vised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbased predictive models."
N15-1070,N10-1013,0,0.762795,"ndling polysemy difficult. Meanwhile, lexical ontologies, such as WordNet (Miller, 1995) specifically catalog sense inventories and provide typologies that link these senses to one another. These hand-curated ontologies provide a complementary source of information to distributional statistics. Recent research tries to leverage this information to train better VSMs (Yu and Dredze, 2014; Faruqui et al., 2014), but does not tackle the problem of polysemy. Parallely, work on polysemy for VSMs revolves primarily around techniques that cluster contexts to distinguish between different word senses (Reisinger and Mooney, 2010; Huang et al., 2012), but does not integrate ontologies in any way. In this paper we present two novel approaches to integrating ontological and distributional sources of information. Our focus is on allowing already existing, proven techniques to be adapted to produce ontologically grounded word sense embeddings. Our first technique is applicable to any sense-agnostic 683 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 683–693, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics VSM as a post-proces"
N15-1070,D11-1097,0,0.0117779,"rduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbased predictive models. We presented an evaluation of 3 semantic tasks on 7 datasets. Our results show that our proposed methods are"
N15-1070,I11-1127,0,0.0110009,"g and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while"
N15-1070,C14-1016,0,0.453061,"er and Mooney (2010) first proposed a simple context clustering technique to generate multi-prototype VSMs, a number of related efforts have worked on adaptations and improvements relying on the same clustering principle. Huang et al. (2012) train their vectors with a neural network and additionally take global context into account. Neelakantan et al. (2014) extend the popular skip-gram model (Mikolov et al., 2013a) in a non-parametric fashion to allow for different number of senses for words. Guo et al. (2014) exploit bilingual alignments to perform better context clustering during training. Tian et al. (2014) propose a probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work is also similar to Yu and Dredze (2014) and Faruqui et al. (2014), who treat lexicons such as the paraphrase database (PPDB) (Ganitkevitch et al., 2013) or WordNet (Miller, 1995) as an auxiliary thesaurus to improve VSMs. However, they do not model senses in"
N15-1070,J06-3003,0,0.0185111,"oth the ontology and distributional statistics. Moreover, in most cases our sense-specific models outperform other models we compare against. 1 Introduction Vector space models (VSMs) of word meaning play a central role in computational semantics. These represent meanings of words as contextual feature vectors in a high-dimensional space (Deerwester et al., 1990) or some embedding thereof (Collobert and Weston, 2008) and are learned from unannotated corpora. Word vectors in these continuous space representations can be used for meaningful semantic operations such as computing word similarity (Turney, 2006), performing analogical reasoning (Turney, 2013) and discovering lexical relationships (Mikolov et al., 2013b). They have also proved useful in downstream NLP applications such as information retrieval (Manning et al., 2008) and question answering (Tellex et al., 2003), among others. However, VSMs remain flawed because they assign a single vector to every word, thus ignoring the possibility that words may have more than one meaning. For example, the word “bank” can either denote a financial institution or the shore of a river. The ability to model multiple meanings is an important component of"
N15-1070,Q13-1029,0,0.0113702,"Moreover, in most cases our sense-specific models outperform other models we compare against. 1 Introduction Vector space models (VSMs) of word meaning play a central role in computational semantics. These represent meanings of words as contextual feature vectors in a high-dimensional space (Deerwester et al., 1990) or some embedding thereof (Collobert and Weston, 2008) and are learned from unannotated corpora. Word vectors in these continuous space representations can be used for meaningful semantic operations such as computing word similarity (Turney, 2006), performing analogical reasoning (Turney, 2013) and discovering lexical relationships (Mikolov et al., 2013b). They have also proved useful in downstream NLP applications such as information retrieval (Manning et al., 2008) and question answering (Tellex et al., 2003), among others. However, VSMs remain flawed because they assign a single vector to every word, thus ignoring the possibility that words may have more than one meaning. For example, the word “bank” can either denote a financial institution or the shore of a river. The ability to model multiple meanings is an important component of any NLP system, given how common polysemy is in"
N15-1070,D11-1094,0,0.0272225,"Missing"
N15-1070,P14-2089,0,0.524461,"component of any NLP system, given how common polysemy is in language. The lack of sense annotated corpora large enough to robustly train VSMs, and the absence of fast, high quality word sense disambiguation (WSD) systems makes handling polysemy difficult. Meanwhile, lexical ontologies, such as WordNet (Miller, 1995) specifically catalog sense inventories and provide typologies that link these senses to one another. These hand-curated ontologies provide a complementary source of information to distributional statistics. Recent research tries to leverage this information to train better VSMs (Yu and Dredze, 2014; Faruqui et al., 2014), but does not tackle the problem of polysemy. Parallely, work on polysemy for VSMs revolves primarily around techniques that cluster contexts to distinguish between different word senses (Reisinger and Mooney, 2010; Huang et al., 2012), but does not integrate ontologies in any way. In this paper we present two novel approaches to integrating ontological and distributional sources of information. Our focus is on allowing already existing, proven techniques to be adapted to produce ontologically grounded word sense embeddings. Our first technique is applicable to any sens"
N15-1070,P10-4014,0,0.138375,"variants: the original single-sense vectors (SINGLE), a multi-prototype variant (MULTI), – both are available as pre-trained vectors for download4 – and a sense-based version obtained by running retrofitting on the original vectors (RETRO). Skip-gram Vectors (SG) (Mikolov et al., 2013a): We use the word vector tool Word2Vec5 to train skip-gram vectors. We define 6 variants: a single-sense version (SINGLE), two multi-sense variants that were trained by first sense disambiguating the entire corpus using WSD tools, – one unsupervised (Pedersen and Kolhatkar, 2009) (WSD) and the other supervised (Zhong and Ng, 2010) (IMS) – a retrofitted version obtained from the singlesense vectors (RETRO), an EM implementation of the skip-gram model with the structured regularizer as described in section 2.2 (EM+RETRO), and the same EM technique but ignoring the ontology (EM). All models were trained on publicly available WMT20116 English monolingual data. This corpus of 355 million words, although adequate in size, is smaller than typically used billion word corpora. We use this corpus because the WSD baseline involves preprocessing the corpus with sense disambiguation, which is slow enough that running it on corpora"
N15-1142,P14-2133,0,0.00813576,"ilt using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat). This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations (Andreas and Klein, 2014; Bansal et al., 2014), where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of Collobert et al. (2011) yielded much better results. 1299 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1299–1304, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics In this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quali"
N15-1142,P14-2131,0,0.0361448,"e suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat). This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations (Andreas and Klein, 2014; Bansal et al., 2014), where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of Collobert et al. (2011) yielded much better results. 1299 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1299–1304, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics In this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quality of the embeddings f"
N15-1142,D14-1082,0,0.416746,"o generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of"
N15-1142,P14-1129,0,0.00965647,"they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implement"
N15-1142,E14-1049,1,0.0586898,"isabel.trancoso@inesc-id.pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing"
N15-1142,P11-2008,0,0.0164828,"Missing"
N15-1142,D14-1012,0,0.0227742,"g (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implemented in the Word2Vec tool, However, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur"
N15-1142,P12-1092,0,0.0570707,"yer,awb}@cs.cmu.edu isabel.trancoso@inesc-id.pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging"
N15-1142,D13-1176,0,0.00968706,"riginal models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et"
N15-1142,D14-1108,1,0.588959,"re suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely use"
N15-1142,P14-2050,0,0.0354569,".pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models"
N15-1142,P14-1140,0,0.0252753,"n issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the"
N15-1142,N13-1039,1,0.692184,"Missing"
N15-1142,P10-1040,0,0.185776,"posed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implemented in the Word2Vec tool, However, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while sem"
N15-1142,N15-1069,0,0.0165027,"r the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word repres"
N15-1144,P14-2131,0,0.0355048,"ng (Mikolov et al., 2011; Collobert and Weston, 2008). For the POS induction task, we specifically need embeddings that capture syntactic similarities. Therefore we experiment with two types of embeddings 1311 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1311–1316, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics that are known for such properties: • Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word. Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information. We confirm this finding in our experiments. • Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context. We use the tool word2vec3 and Ling et al. (2015)’s modified version4 to generate both plain and structured skip-gram embeddings in nine languages. 3 Models for POS Induction In this section, we briefly review two classes of models used for POS induction (HMMs"
N15-1144,N10-1083,0,0.0184866,"ts the transition probability and p(wi |t i ) is the emission probability, the probability of a particular tag generating the word at position i.5 We consider two variants of the HMM as baselines: 3https://code.google.com/p/word2vec/ 4https://github.com/wlin12/wang2vec/ 5Terms for the starting and stopping transition probabilities are omitted for brevity. 1312 • p(wi |t i ) is parameterized as a “naïve multinomial” distribution with one distinct parameter for each word type. • p(wi |t i ) is parameterized as a multinomial logistic regression model with hand-engineered features as detailed in (Berg-Kirkpatrick et al., 2010). Gaussian Emissions. We now consider incorporating word embeddings in the HMM. Given a tag t ∈ T, instead of generating the observed word w ∈ V , we generate the (pre-trained) embedding vw ∈ Rd of that word. The conditional probability density assigned to vw |t follows a multivariate Gaussian distribution with mean µ t and covariance matrix Σ t :   exp − 12 (vw − µ t ) > Σ−1 t (v w − µ t ) p(vw ; µ t , Σ t ) = p (2π) d |Σ t | (2) This parameterization makes the assumption that embeddings of words which are often tagged as t are concentrated around some point µ t ∈ Rd , and the concentration"
N15-1144,J92-4003,0,0.205066,"model is a linear-chain CRF with feature vector λ and local feature functions f. 4.1 Choice of POS Induction Models Here, we compare the following models for POS induction: • Baseline: HMM with multinomial emissions (Kupiec, 1992), • Baseline: HMM with log-linear emissions (BergKirkpatrick et al., 2010), • Baseline: CRF autoencoder with multinomial reconstructions (Ammar et al., 2014),7 • Proposed: HMM with Gaussian emissions, and • Proposed: CRF autoencoder with Gaussian reconstructions. In (Ammar et al., 2014), we explored two kinds of reconstructions w: ˆ surface forms and Brown clusters (Brown et al., 1992), and used “stupid multinomials” as the underlying distributions for re-generating w. ˆ Data. To train the POS induction models, we used the plain text from the training sections of the CoNLL-X shared task (Buchholz and Marsi, 2006) (for Danish and Turkish), the CoNLL 2007 shared task (Nivre et al., 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu). For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding u"
N15-1144,W06-2920,0,0.0145533,"1992), • Baseline: HMM with log-linear emissions (BergKirkpatrick et al., 2010), • Baseline: CRF autoencoder with multinomial reconstructions (Ammar et al., 2014),7 • Proposed: HMM with Gaussian emissions, and • Proposed: CRF autoencoder with Gaussian reconstructions. In (Ammar et al., 2014), we explored two kinds of reconstructions w: ˆ surface forms and Brown clusters (Brown et al., 1992), and used “stupid multinomials” as the underlying distributions for re-generating w. ˆ Data. To train the POS induction models, we used the plain text from the training sections of the CoNLL-X shared task (Buchholz and Marsi, 2006) (for Danish and Turkish), the CoNLL 2007 shared task (Nivre et al., 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu). For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding universal POS tag set (Petrov et al., 2012). This is the same set up we used in (Ammar et al., 2014). Gaussian Reconstruction. In this paper, we use ddimensional word embedding reconstructions wˆ i = vw i ∈ Rd , and replace the multi"
N15-1144,P10-4002,1,0.210161,"{1, . . . , d}.9 While the CRF autoencoder with multinomial reconstructions were carefully initialized as p( w,t ˆ |w) = p(t |w) × p( wˆ |t) ∝ p( wˆ |t) × exp λ · |w | X f(t i ,t i−1 , w) (5) i=1 4 Experiments In this section, we attempt to answer the following questions: • §4.1: Do syntactically-informed word embeddings improve POS induction? Which model performs best? • §4.2: What kind of word embeddings are suitable for POS induction? 1313 7We use the configuration with best performance which reconstructs Brown clusters. 8We used the corpus/tokenize-anything.sh script in the cdec decoder (Dyer et al., 2010) to tokenize the corpora from (Quasthoff et al., 2006). The other corpora were already tokenized. In Arabic and Italian, we found a lot of discrepancies between the tokenization used for inducing word embeddings and the tokenization used for evaluation. We expect our results to improve with consistent tokenization. 9Surprisingly, we found that estimating Σ t significantly degrades the performance. This may be due to overfitting (Shinozaki and Kawahara, 2007). Possible remedies include using a prior (Gauvain and Lee, 1994). Gaussian CRF Autoencoder V−measure Multinomial CRF Autoencoder Gaussian"
N15-1144,D07-1031,0,0.0152212,"der. One explanation is that our word embeddings were induced using larger unlabeled corpora than those used to train the POS induction models. The best results are obtained using both word embeddings and feature-rich models using the Gaussian CRF autoencoder model. This set of results suggest that word embeddings and hand-engineered features play complementary roles in POS induction. It is worth noting that the CRF autoencoder model with Gaussian reconstructions did not require careful initialization.11 10We found the V-measure results to be consistent with the many-to-one evaluation metric (Johnson, 2007). We only show one set of results for brevity. 11In (Ammar et al., 2014), we found that careful initialization for the CRF autoencoder model with multinomial reconstructions is necessary. 1314 4.2 Choice of Embeddings Standard skip-gram vs. structured skip-gram. On Gaussian HMMs, structured skip-gram embeddings score moderately higher than standard skipgrams. And as context window size gets larger, the gap widens (as shown in Fig. 2.) The reason may be that structured skip-gram embeddings give each position within the context window its own project matrix, so the smearing effect is not as pron"
N15-1144,N15-1142,1,0.1527,"gs 1311 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1311–1316, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics that are known for such properties: • Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word. Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information. We confirm this finding in our experiments. • Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context. We use the tool word2vec3 and Ling et al. (2015)’s modified version4 to generate both plain and structured skip-gram embeddings in nine languages. 3 Models for POS Induction In this section, we briefly review two classes of models used for POS induction (HMMs and CRF autoencoders), and explain how to generate word embedding observations in each class. We will represent a sentence of length ` as w = hw1 , w2 , . . . , w` i ∈ V ` and a sequence of tags as t"
N15-1144,petrov-etal-2012-universal,0,0.0343714,"tinomials” as the underlying distributions for re-generating w. ˆ Data. To train the POS induction models, we used the plain text from the training sections of the CoNLL-X shared task (Buchholz and Marsi, 2006) (for Danish and Turkish), the CoNLL 2007 shared task (Nivre et al., 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu). For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding universal POS tag set (Petrov et al., 2012). This is the same set up we used in (Ammar et al., 2014). Gaussian Reconstruction. In this paper, we use ddimensional word embedding reconstructions wˆ i = vw i ∈ Rd , and replace the multinomial distribution of the reconstruction model with the multivariate Gaussian distribution in Eq. 2. We again use the Baum– Welch algorithm to estimate µ t ∗ and Σ t ∗ similar to Eq. 3. The only difference is that posterior label probabilities are now conditional on both the input sequence w and the embeddings sequence v, i.e., replace p(t i = t ∗ |v) in Eq. 2 with p(t i = t ∗ |w, v). Setup. In this sectio"
N15-1144,D07-1043,0,0.15676,"ll word embeddings. Left: Models which use standard skip-gram word embeddings (i.e., Gaussian HMM and Gaussian CRF Autoencoder) outperform all baselines on average across languages. Right: comparison between standard and structured skip-grams on Gaussian HMM and CRF Autoencoder. discussed in (Ammar et al., 2014), CRF autoencoder with Gaussian reconstructions were initialized uniformly at random in [−1, 1]. All HMM models were also randomly initialized. We tuned all hyperparameters on the English PTB corpus, then fixed them for all languages. Evaluation. We use the V-measure evaluation metric (Rosenberg and Hirschberg, 2007) to evaluate the predicted syntactic classes at the token level.10 Results. The results in Fig. 1 clearly suggest that we can use word embeddings to improve POS induction. Surprisingly, the feature-less Gaussian HMM model outperforms the strong feature-rich baselines: Multinomial Featurized HMM and Multinomial CRF Autoencoder. One explanation is that our word embeddings were induced using larger unlabeled corpora than those used to train the POS induction models. The best results are obtained using both word embeddings and feature-rich models using the Gaussian CRF autoencoder model. This set"
N15-1144,P14-2044,0,0.01583,"ur findings suggest that, in both models, substantial improvements are possible when word embeddings are used rather than opaque word types. However, the independence assumptions made by the model used to induce embeddings strongly determines its effectiveness for POS induction: embedding models that model short-range context are more effective than those that model longer-range contexts. This result is unsurprising, but it illustrates the lack of an evaluation metric that measures the syntactic (rather than semantic) information in word embeddings. Our results also confirm the conclusions of Sirts et al. (2014) who were likewise able to improve POS induction results, albeit using a custom clustering model based on the the distance-dependent Chinese restaurant process (Blei and Frazier, 2011). Our contributions are as follows: (i) reparameterization of token-level POS induction models to use word embeddings; and (ii) a systematic evaluation of word embeddings with respect to the syntactic information they contain. 2 Vector Space Word Embeddings Word embeddings represent words in a language’s vocabulary as points in a d-dimensional space such that nearby words (points) are similar in terms of their di"
N15-1144,C10-1115,0,0.0267207,"Missing"
N15-1144,C14-1217,0,0.0521622,"tion is the problem of assigning word tokens to syntactic categories given only a corpus of untagged text. In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional random field autoencoder (Ammar et al., 2014). In each model, instead of using a conditional multinomial distribution2 to generate a word token wi ∈ V given a POS tag t i ∈ T, we use a conditional Gaussian distribution and generate a d-dimensional word embedding vw i ∈ Rd given t i . 1Unlike Yatbaz et al. (2014), we leverage easily obtainable and widely used embeddings of word types. 2Also known as a categorical distribution. Our findings suggest that, in both models, substantial improvements are possible when word embeddings are used rather than opaque word types. However, the independence assumptions made by the model used to induce embeddings strongly determines its effectiveness for POS induction: embedding models that model short-range context are more effective than those that model longer-range contexts. This result is unsurprising, but it illustrates the lack of an evaluation metric that meas"
N15-1144,quasthoff-etal-2006-corpus,0,\N,Missing
N15-1184,N09-1003,0,0.562483,"Missing"
N15-1184,N09-1014,0,0.00466202,"nalysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector tr"
N15-1184,P98-1013,0,0.739801,"l., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-b"
N15-1184,P12-1015,0,0.139533,"cts of the representations along with an extrinsic sentiment analysis task. Word Similarity. We evaluate our word representations on a variety of different benchmarks that have been widely used to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans. The second benchmark is the RG-65 (Rubenstein and Goodenough, 1965) dataset that contain 65 pairs of nouns. Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset (Bruni et al., 2012) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus. We calculate cosine similarity between the vectors of two words forming a test item, and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently"
N15-1184,D13-1167,0,0.0169875,"luable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors obtained using any ve"
N15-1184,P11-1061,0,0.0334295,"se to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original v"
N15-1184,P11-1144,1,0.318338,"ue qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original vector training model o"
N15-1184,E14-1049,1,0.393831,"e use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vectors are of length 512.4 4 Semantic Lexicons We use three different semantic lexicons to evaluate their utility in improving the word vectors. We include both"
N15-1184,N13-1092,0,0.314995,"Missing"
N15-1184,W06-3808,0,0.0111527,"elief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performance across tasks, semantic lexicons, and languages and showed that it outperforms existing alternatives. The retrofitting tool is available at: https:// g"
N15-1184,D14-1012,0,0.00727559,"cal semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations"
N15-1184,I05-1067,0,0.0116987,"Spanish. We used the Universal WordNet (de Melo and Weikum, 2009), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information a"
N15-1184,D09-1124,0,0.0382359,"09), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information and retrofitting might be less helpful. We train SG vec12 https://github.c"
N15-1184,P12-1092,0,0.106061,"btained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vector"
N15-1184,J03-1003,0,0.0119063,"Missing"
N15-1184,D11-1122,0,0.00432874,"ilar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performan"
N15-1184,W14-1618,0,0.0675244,"rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently inflected forms of the same verb. There are nine different kinds of relations and overall there are 10,675 syntactic pairs of word tuples. The task is to find a word d that best fits the following relationship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences f"
N15-1184,N13-1090,0,0.687964,"tors. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the word vector space. These vectors were trained on 6 billion words from Wikipedia and English Gigaword 1608 Lexicon PPDB WordNetsyn WordNetall FrameNet Words 102,902 148,730 148,730 10,822 Edges 374,555 304,856 934,705 417,456 Table 1: Approximate size of the graphs obtained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3"
N15-1184,D14-1162,0,0.120168,"Missing"
N15-1184,D13-1170,0,0.00650579,"nship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarsegrained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We train an `2 -regularized logistic regression classifier on the average of the word vectors of a given sentence to predict the coarse-grained sentiment tag at the sentence level, and report the testset accuracy of the classifier. 6 Experiments We first show experiments measuring"
N15-1184,D10-1017,0,0.0133974,"red word vector to be close to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agno"
N15-1184,P10-1149,0,0.00454798,"y induction (Yih et al., 2012) and multi-relational latent semantic analysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information wh"
N15-1184,P10-1040,0,0.0887589,"tery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and pa"
N15-1184,J06-3003,0,0.0129744,", and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level inf"
N15-1184,D12-1111,0,0.0389385,"ons should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors"
N15-1184,P14-2089,0,0.636299,"search on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-pr"
N15-1184,J90-1003,0,\N,Missing
N15-1184,J07-2002,0,\N,Missing
N15-1184,C98-1013,0,\N,Missing
N15-1184,D13-1141,0,\N,Missing
N16-1024,P99-1070,0,0.0566848,"imilar constraints have been proposed to deal with the analogous problem in bottom-up shift-reduce parsers (Sagae and Lavie, 2005). 201 Constraints on generator transitions. The generation algorithm also requires slightly modified constraints. These are: • The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol and n ≥ 1. To designate the set of valid generator transitions, we write AG (T, S, n). This transition set generates trees using nearly the same structure building actions and stack configurations as the “top-down PDA” construction proposed by Abney et al. (1999), albeit without the restriction that the trees be in Chomsky normal form. 3.3 Transition Sequences from Trees Any parse tree can be converted to a sequence of transitions via a depth-first, left-to-right traversal of a parse tree. Since there is a unique depth-first, leftro-right traversal of a tree, there is exactly one transition sequence of each tree. For a tree y and a sequence of symbols x, we write a(x, y) to indicate the corresponding sequence of generation transitions, and b(x, y) to indicate the parser transitions. 3.4 Runtime Analysis A detailed analysis of the algorithmic propertie"
N16-1024,N15-1083,0,0.0397053,"Missing"
N16-1024,E03-1005,0,0.0656796,"Missing"
N16-1024,J92-4003,0,0.647152,"the size of AG (S, T, n), word generation is broken into two parts. First, the decision to generate is made (by predicting GEN as an action), and then choosing the word, conditional on the current parser state. To further reduce the computational complexity of modeling the generation of a word, we use a class-factored softmax (Baltescu pand Blunsom, 2015; Goodman, 2001). By using |Σ| classes for a vocabulary p of size |Σ|, this prediction step runs in time O( |Σ|) rather than the O(|Σ|) of the full-vocabulary softmax. To obtain clusters, we use the greedy agglomerative clustering algorithm of Brown et al. (1992). 4.3 Discriminative Parsing Model 5 Inference via Importance Sampling Our generative model p(x, y) defines a joint distribution on trees (y) and sequences of words (x). To evaluate this as a language model, it is necessary to compute the marginal probability p(x) = P 0 y 0 ∈Y(x) p(x, y ). And, to evaluate the model as a parser, we need to be able to find the MAP parse tree, i.e., the tree y ∈ Y(x) that maximizes p(x, y). However, because of the unbounded dependencies across the sequence of parsing actions in our model, exactly solving either of these inference problems is intractable. To obta"
N16-1024,W15-2108,0,0.16968,"c process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, whe"
N16-1024,P15-2142,0,0.211515,"c process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, whe"
N16-1024,A00-2018,0,0.925699,"le 4: Language model perplexity results. Parsing results on PTB §23 (D=discriminative, G=generative, S=semisupervised). Model Henderson (2004) Socher et al. (2013a) Zhu et al. (2013) Vinyals et al. (2015) – WSJ only Petrov and Klein (2007) Bod (2003) Shindo et al. (2012) – single Shindo et al. (2012) – ensemble Zhu et al. (2013) McClosky et al. (2006) Vinyals et al. (2015) – single Vinyals et al. (2015) – ensemble Discriminative, q(y |x) Generative, pˆ(y |x) type D D D D G G G G S S S S D G Table 3: Parsing results on CTB 5.1. Model Zhu et al. (2013) Wang et al. (2015) Huang and Harper (2009) Charniak (2000) Bikel (2004) Petrov and Klein (2007) Zhu et al. (2013) Wang and Xue (2014) Wang et al. (2015) Discriminative, q(y |x) Generative, pˆ(y |x) type D D D G G G S S S D G F1 89.4 90.4 90.4 90.5 90.1 90.7 91.1 92.4 91.3 92.1 92.5 92.8 89.8 92.4 F1 82.6 83.2 84.2 80.8 80.6 83.3 85.6 86.3 86.6 80.7 82.7 Discussion It is clear from our experiments that the proposed generative model is quite effective both as a parser and as a language model. This is the result of (i) relaxing conventional independence assumptions (e.g., context-freeness) and (ii) inferring continuous representations of symbols alongsi"
N16-1024,D10-1066,0,0.578391,"dling. However, leaving them out reduces the number of transitions by O(n) and also reduces the number of action types, both of which reduce the runtime. Furthermore, standard parsing evaluation scores do not depend on preterminal prediction accuracy. operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the “linearized” parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in previous generative parsing and language modeling work (Roark, 2001, 2004; Charniak, 2010). A further connection is to LL(∗ ) parsing which uses an unbounded lookahead (compactly represented by a DFA) to distinguish between parse alternatives in a top-down parser (Parr and Fisher, 2011); however, our parser uses an RNN encoding of the lookahead rather than a DFA. unprocessed words, rather there is an output buffer (T ), and (ii) instead of a SHIFT operation there are GEN (x) operations which generate terminal symbol x ∈ Σ and add it to the top of the stack and the output buffer. At each timestep an action is stochastically selected according to a conditional distribution that depen"
N16-1024,P15-1033,1,0.788281,"contents (Cho et al., 2014). Since the output buffer and history of actions are only appended to and only contain symbols from a finite alphabet, it is straightforward to apply a standard RNN encoding architecture. The stack (S) is more complicated for two reasons. First, the elements of the stack are more complicated objects than symbols from a discrete alphabet: open nonterminals, terminals, and full trees, are all present on the stack. Second, it is manipulated using both push and pop operations. To efficiently obtain representations of S under push and pop operations, we use stack LSTMs (Dyer et al., 2015). 4.1 Syntactic Composition Function When a REDUCE operation is executed, the parser pops a sequence of completed subtrees and/or tokens (together with their vector embeddings) from the stack and makes them children of the most recent open nonterminal on the stack, “completing” the constituent. To compute an embedding of this new subtree, we use a composition function based on bidirectional LSTMs, which is illustrated in Fig. 6. The first vector read by the LSTM in both the forward and reverse directions is an embedding of the label on the constituent being constructed (in the figure, NP). Thi"
N16-1024,P81-1022,0,0.662754,"lated to the operations used in Earley’s algorithm which likewise introduces nonterminals symbols with its PREDICT 2 Preterminal symbols are, from the parsing algorithm’s point of view, just another kind of nonterminal symbol that requires no special handling. However, leaving them out reduces the number of transitions by O(n) and also reduces the number of action types, both of which reduce the runtime. Furthermore, standard parsing evaluation scores do not depend on preterminal prediction accuracy. operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the “linearized” parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in previous generative parsing and language modeling work (Roark, 2001, 2004; Charniak, 2010). A further connection is to LL(∗ ) parsing which uses an unbounded lookahead (compactly represented by a DFA) to distinguish between parse alternatives in a top-down parser (Parr and Fisher, 2011); however, our parser uses an RNN encoding of the lookahead rather than a DFA. unprocessed words, rather there is an output buffer (T ), and (ii) i"
N16-1024,P06-1121,0,0.0200588,"ther than the importance sampling method based on a discriminative parser, §5) to produce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models. A second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. Previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014). A third consideration regarding how RNNGs, human sentence processing takes place in a left-toright, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is 207 left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010). Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization. 10 Conclusion We intr"
N16-1024,J14-2005,1,0.778993,"nce sampling method based on a discriminative parser, §5) to produce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models. A second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. Previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014). A third consideration regarding how RNNGs, human sentence processing takes place in a left-toright, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is 207 left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010). Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization. 10 Conclusion We introduced recurrent neural n"
N16-1024,N03-1014,0,0.528102,"lationships among words and phrases. RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transitio"
N16-1024,P04-1013,0,0.146212,"latively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down i"
N16-1024,D09-1087,0,0.056023,"Missing"
N16-1024,J91-3004,0,0.723725,"ta. 8 symbol), inverted, and exponentiated to yield the perplexity. Results are summarized in Table 4. 7 Model IKN 5-gram LSTM LM RNNG Related Work Our language model combines work from two modeling traditions: (i) recurrent neural network language models and (ii) syntactic language modeling. Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990). Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neural-network–based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize generative parsing based on a left-corner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a,b). Modeling generation top-down as"
N16-1024,P01-1042,0,0.112989,"generative model. We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model (§5). Experiments show that RNNGs are effective for both language modeling and parsing (§6). Our generative model obtains (i) the best-known parsing results using a single supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential LSTM language models. Surprisingly—although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)— parsing with the generative model obtains significantly better results than parsing with the discriminative model. 2 RNN Grammars Formally, an RNNG is a triple (N, Σ, Θ) consisting of a finite set of nonterminal symbols (N ), a finite set of terminal symbols (Σ) such that N ∩ Σ = ∅, and a collection of neural network parameters Θ. It does not explicitly define rules since these are implicitly characterized by Θ. The algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition-based algorithm, which is outlined in the next section. In"
N16-1024,Q16-1032,0,0.0218427,"top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take. The neural networks we use to model sentences are structured according to the syntax of the sentence being generated. Syntactically structured neural architectures have been explored in a number of applications, including discriminative parsing (Socher et al., 2013a; Kiperwasser and Goldberg, 2016), sentiment analysis (Tai et al., 2015; Socher et al., 2013b), and sentence representation (Socher et al., 2011; Bowman et al., 2006). However, these models have been, without exception, discriminative; this is the first work to use syntactically structured neural models to generate language. Earlier work has demonstrated that sequential RNNs have the capacity to recognize contextfree (and beyond) languages (Sun et al., 1998; Siegelmann and Sontag, 1995). In contrast, our work may be understood as a way of incorporating a context-free inductive bias into the model structure. 9 Outlook RNNGs ca"
N16-1024,N06-1020,0,0.0266107,"Missing"
N16-1024,D13-1032,0,0.02466,"Missing"
N16-1024,N07-1051,0,0.0217429,"Missing"
N16-1024,J01-2004,0,0.40637,"for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a line"
N16-1024,W05-1513,0,0.163106,"forward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down information, but a compl"
N16-1024,P12-1046,0,0.149805,"Missing"
N16-1024,D13-1170,0,0.0582441,"what sort of head it should be looking for as it processes the child node embeddings. The final state of the forward and reverse LSTMs are concatenated, passed through an affine transformation and a tanh nonlinearity to become the subtree embedding.4 Because each of the child node embeddings (u, v, w in Fig. 6) is computed similarly (if it corresponds to an 4 We found the many previously proposed syntactic composition functions inadequate for our purposes. First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task. RE GEDUC NTN E ( NT NP) (VP ) … St z }| (S p(at ) { NP z ut (VP a&lt;t cat Tt }| hungry { The The hungry cat Figure 5: Neural architecture for defining a distribution over at given representations of the stack (St ), output buffer (Tt ) and history of actions (a&lt;t ). Details of the composition architecture of the NP, the action history LSTM, and the other elements of the stack are not shown. This architecture corresponds to the generator"
N16-1024,P15-1150,0,0.365082,"TMs are concatenated, passed through an affine transformation and a tanh nonlinearity to become the subtree embedding.4 Because each of the child node embeddings (u, v, w in Fig. 6) is computed similarly (if it corresponds to an 4 We found the many previously proposed syntactic composition functions inadequate for our purposes. First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task. RE GEDUC NTN E ( NT NP) (VP ) … St z }| (S p(at ) { NP z ut (VP a&lt;t cat Tt }| hungry { The The hungry cat Figure 5: Neural architecture for defining a distribution over at given representations of the stack (St ), output buffer (Tt ) and history of actions (a&lt;t ). Details of the composition architecture of the NP, the action history LSTM, and the other elements of the stack are not shown. This architecture corresponds to the generator state at line 7 of Figure 4. 4.4 x x A discriminative parsing model can be obtained by replacing the embedding of Tt at each tim"
N16-1024,W07-2218,0,0.0387922,"te via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminati"
N16-1024,N03-1033,0,0.190016,"en, 2011). Our importance sampling algorithm uses a conditional proposal distribution q(y |x) with the following properties: (i) p(x, y) > 0 =⇒ q(y | x) > 0; (ii) samples y ∼ q(y |x) can be obtained efficiently; and (iii) the conditional probabilities q(y |x) of these samples are known. While many such distributions are available, the discrim5 Training The parameters in the model are learned to maximize the likelihood of a corpus of trees. 204 For the discriminative parser, the POS tags are processed similarly as in (Dyer et al., 2015); they are predicted for English with the Stanford Tagger (Toutanova et al., 2003) and Chinese with Marmot (Mueller et al., 2013). inatively trained variant of our parser (§4.4) fulfills these requirements: sequences of actions can be sampled using a simple ancestral sampling approach, and, since parse trees and action sequences exist in a one-to-one relationship, the product of the action probabilities is the conditional probability of the parse tree under q. We therefore use our discriminative parser as our proposal distribution. Importance sampling uses importance weights, which we define as w(x, y) = p(x, y)/q(y |x), to compute this estimate. Under this definition, we c"
N16-1024,P15-1110,0,0.219171,"Missing"
N16-1024,P14-1069,0,0.0783975,"Missing"
N16-1024,D15-1199,0,0.0152728,"Missing"
N16-1024,J11-1005,0,0.213134,"use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down information, but a complete path from the root"
N16-1024,P13-1043,0,0.278571,"her assumptions. Assuming our fixed constraint on maximum depth, it is linear. 3.5 Comparison to Other Models Our generation algorithm algorithm differs from previous stack-based parsing/generation algorithms in two ways. First, it constructs rooted tree structures top down (rather than bottom up), and second, the transition operators are capable of directly generating arbitrary tree structures rather than, e.g., assuming binarized trees, as is the case in much prior work that has used transition-based algorithms to produce phrase-structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013). 4 Generative Model RNNGs use the generator transition set just presented to define a joint distribution on syntax trees (y) and words (x). This distribution is defined as a sequence model over generator transitions that is parameterized using a continuous space embedding of the algorithm state at each time step (ut ); i.e., |a(x,y)| p(x, y) = Y p(at |a&lt;t ) t=1 |a(x,y)| = Y t=1 exp r> at ut + bat , > a0 ∈AG (Tt ,St ,nt ) exp ra0 ut + ba0 P and where action-specific embeddings ra and bias vector b are parameters in Θ. The representation of the algorithm state at time t, ut , is computed by com"
N16-1030,D16-1211,1,0.107089,"Missing"
N16-1030,W10-1703,0,0.0202334,"layer just before the input to the bidirectional LSTM in Figure 1. We observe a significant improvement in our model’s performance after using dropout (see table 5). 5 Experiments This section presents the methods we use to train our models, the results we obtained on various tasks and the impact of our networks’ configuration on model performance. 5.1 Training For both models presented, we train our networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with 2 (Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009) a learning rate of 0.01 and a gradient clipping of 5.0. Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014). Although we observe faster convergence using these methods, none of them perform as well as SGD with gradient clipping. Our LSTM-CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100. Tuning this dimension did not significantly impact model performance. We set the dropout rate to 0.5. Using higher rates negatively impacted our results, while s"
N16-1030,W02-2004,0,0.0600038,"Missing"
N16-1030,W99-0612,0,0.105164,"ddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers. Language independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recognition by co-training character-level (word-internal) and token-level (context) features. Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information. Finally, there is currently a lot of interest in models for NER that use letter-based representations. Gillick et"
N16-1030,W02-2007,0,0.09417,"Missing"
N16-1030,P15-1033,1,0.569042,"heme. 3 Transition-Based Chunking Model As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representations of the multi-token names (e.g., the name Mark Watney is composed into a single representation). This model relies on a stack data structure to incrementally construct chunks of the input. To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a “stack pointer.” While sequential LSTMs model sequences from left to right, stack LSTMs permit embedding of a stack of objects that are both added to (using a push operation) and removed from (using a pop operation). This allows the Stack-LSTM to work like a stack that maintains a “summary embedding” of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity. 263 Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the StackLSTM model since in this paper we merely use the same architec"
N16-1030,W11-2202,0,0.0426334,"(2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers. Language independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recognition by co-training character-level (word-internal) and token-level (context) features. Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information. Finally, there is currently a lot of interest in models for NER that use letter-based representations. Gillick et al. (2015) model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character-based representations into their encoder model. Chiu and Nichols (2015) employ"
N16-1030,W03-0425,0,0.259283,"ng Chiu and Nichols (2015) Chiu and Nichols (2015)* LSTM-CRF (no char) LSTM-CRF S-LSTM (no char) S-LSTM F1 89.59 83.78 90.90 90.10 90.05 90.90 89.9 91.2 90.69 90.77 90.20 90.94 87.96 90.33 ing character-level word embeddings resulted in an increase of about +0.74. For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5. Model LSTM LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF S-LSTM S-LSTM S-LSTM S-LSTM S-LSTM Table 1: English NER results (CoNLL-2003 test set). * indicates models trained with the use of external labeled data Model Florian et al. (2003)* Ando and Zhang (2005a) Qi et al. (2009) Gillick et al. (2015) Gillick et al. (2015)* LSTM-CRF – no char LSTM-CRF S-LSTM – no char S-LSTM F1 72.41 75.27 75.72 72.08 76.22 75.06 78.76 65.87 75.66 ent configurations. “pretrain” refers to models that include pretrained word embeddings, “char” refers to models that include character-based modeling of words, “dropout” refers to models that include dropout rate. cates models trained with the use of external labeled data F1 77.05 78.6 78.08 82.84 73.14 81.74 69.90 79.88 Table 3: Dutch NER (CoNLL-2002 test set). * indicates models trained with the us"
N16-1030,D11-1072,0,0.0309592,"Missing"
N16-1030,P09-1116,0,0.033325,"r-based representations to achieve competitive performance; we hypothesize that the LSTM-CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs; however, the Stack-LSTM model consumes the words one by one and it just relies on the word representations when it chunks words. 5.4 Network architectures Our models had several components that we could tweak to understand their impact on the overall performance. We explored the impact that the CRF, the character-level representations, pretraining of our Model Collobert et al. (2011)* Lin and Wu (2009) Lin and Wu (2009)* Huang et al. (2015)* Passos et al. (2014) Passos et al. (2014)* Luo et al. (2015)* + gaz Luo et al. (2015)* + gaz + linking Chiu and Nichols (2015) Chiu and Nichols (2015)* LSTM-CRF (no char) LSTM-CRF S-LSTM (no char) S-LSTM F1 89.59 83.78 90.90 90.10 90.05 90.90 89.9 91.2 90.69 90.77 90.20 90.94 87.96 90.33 ing character-level word embeddings resulted in an increase of about +0.74. For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5. Model LSTM LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF S-LSTM S-LSTM S-L"
N16-1030,D15-1161,1,0.175082,"here, (i) a bidirectional LSTM with a sequential conditional random layer above it (LSTM-CRF; §2), and (ii) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM; §3). Second, token-level evidence for “being a name” includes both orthographic evidence (what does the word being tagged as a name look like?) and distributional evidence (where does the word being tagged tend to occur in a corpus?). To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b). Our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence (§4). Experiments in English, Dutch, German, and Spanish show that we are able to obtain state260 Proceedings of NAACL-HLT 2016, pages 260–270, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics of-the-art NER performance with the LSTM-CRF model in Dutch, German, and Spanish, and v"
N16-1030,D15-1176,1,0.106945,"Missing"
N16-1030,D15-1104,0,0.870265,"ude them in our models. We did not perform any dataset preprocessing, apart from replacing every digit with a zero in the English NER dataset. 3 English (D=0.2), German, Spanish and Dutch (D=0.3) 266 5.3 Results Table 1 presents our comparisons with other models for named entity recognition in English. To make the comparison between our model and others fair, we report the scores of other models with and without the use of external labeled data such as gazetteers and knowledge bases. Our models do not use gazetteers or any external labeled resources. The best score reported on this task is by Luo et al. (2015). They obtained a F1 of 91.2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011). Their model uses a lot of hand-engineered features including spelling features, WordNet clusters, Brown clusters, POS tags, chunks tags, as well as stemming and external knowledge bases like Freebase and Wikipedia. Our LSTM-CRF model outperforms all other systems, including the ones using external labeled data like gazetteers. Our StackLSTM model also outperforms all previous models that do not incorporate external features, apart from the one presented by Chiu and Nichols (2015). Tables 2"
N16-1030,W04-0308,0,0.0378214,"). This allows the Stack-LSTM to work like a stack that maintains a “summary embedding” of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity. 263 Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the StackLSTM model since in this paper we merely use the same architecture through a new transition-based algorithm presented in the following Section. 3.1 Chunking Algorithm We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed. The transition inventory contains the following transitions: The SHIFT transition moves a word from the buffer to the stack, the OUT transition moves a word from the buffer directly into the output stack while the REDUCE(y) transition pops all items from the top of the stack creating a “chunk,” labels this with label y, and pushes a representation of this chunk onto the output stack. The a"
N16-1030,W14-1609,0,0.0526639,"; we hypothesize that the LSTM-CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs; however, the Stack-LSTM model consumes the words one by one and it just relies on the word representations when it chunks words. 5.4 Network architectures Our models had several components that we could tweak to understand their impact on the overall performance. We explored the impact that the CRF, the character-level representations, pretraining of our Model Collobert et al. (2011)* Lin and Wu (2009) Lin and Wu (2009)* Huang et al. (2015)* Passos et al. (2014) Passos et al. (2014)* Luo et al. (2015)* + gaz Luo et al. (2015)* + gaz + linking Chiu and Nichols (2015) Chiu and Nichols (2015)* LSTM-CRF (no char) LSTM-CRF S-LSTM (no char) S-LSTM F1 89.59 83.78 90.90 90.10 90.05 90.90 89.9 91.2 90.69 90.77 90.20 90.94 87.96 90.33 ing character-level word embeddings resulted in an increase of about +0.74. For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5. Model LSTM LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF S-LSTM S-LSTM S-LSTM S-LSTM S-LSTM Table 1: English NER results (CoNLL-2003 te"
N16-1030,W09-1119,0,0.489621,"a named entity, I-label if it is inside a named entity but not the first token within the named entity, or O otherwise. However, we decided to use the IOBES tagging scheme, a variant of IOB commonly used for named entity recognition, which encodes information about singleton entities (S) and explicitly marks the end of named entities (E). Using this scheme, tagging a word as I-label with high-confidence narrows down the choices for the subsequent word to I-label or E-label, however, the IOB scheme is only capable of determining that the subsequent word cannot be the interior of another label. Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance marginally. However, we did not observe a significant improvement over the IOB tagging scheme. 3 Transition-Based Chunking Model As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representations of the multi-token names (e.g., the name Mark Watney is composed into a single representation). This m"
N16-1030,W15-3904,0,0.31074,"Missing"
N16-1030,W02-2024,0,0.52193,"g is of dimension 20. We experimented with different dropout rates and reported the scores using the best dropout rate for each language.3 It is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al., 2016). 5.2 Data Sets We test our model on different datasets for named entity recognition. To demonstrate our model’s ability to generalize to different languages, we present results on the CoNLL-2002 and CoNLL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) that contain independent named entity labels for English, Spanish, German and Dutch. All datasets contain four different types of named entities: locations, persons, organizations, and miscellaneous entities that do not belong in any of the three previous categories. Although POS tags were made available for all datasets, we did not include them in our models. We did not perform any dataset preprocessing, apart from replacing every digit with a zero in the English NER dataset. 3 English (D=0.2), German, Spanish and Dutch (D=0.3) 266 5.3 Results Table 1 pr"
N16-1030,P10-1040,0,0.0470759,"Missing"
N16-1030,J11-1005,0,0.0353525,"ropout rate to 0.5. Using higher rates negatively impacted our results, while smaller rates led to longer training time. The stack-LSTM model uses two layers each of dimension 100 for each stack. The embeddings of the actions used in the composition functions have 16 dimensions each, and the output embedding is of dimension 20. We experimented with different dropout rates and reported the scores using the best dropout rate for each language.3 It is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al., 2016). 5.2 Data Sets We test our model on different datasets for named entity recognition. To demonstrate our model’s ability to generalize to different languages, we present results on the CoNLL-2002 and CoNLL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) that contain independent named entity labels for English, Spanish, German and Dutch. All datasets contain four different types of named entities: locations, persons, organizations, and miscellaneous entities that do not belong in any of the three previous categorie"
N16-1030,P15-1109,0,0.122812,"he output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers. Language independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recognition by co-training character-level (word-internal) and token-level (context) features. Eisens"
N16-1077,E14-1060,0,0.356302,"roach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflect"
N16-1077,N15-1107,0,0.591763,"on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected for"
N16-1077,D15-1041,1,0.717699,"out of characters. These  character vectors are parameters that are learned by our model, exactly as other character vectors. Regarding the second difference, to provide the model the ability to learn the transformation of semantics from input to output, we apply an affine transformation on the encoded vector e: e ← Wtrans e + btrans (5) where, Wtrans , btrans are the transformation parameters. Also, in the encoder we use a bidirectional LSTM (Graves et al., 2005) instead of a uni-directional LSTM, as it has been shown to capture the sequence information more effectively (Ling et al., 2015; Ballesteros et al., 2015; Bahdanau et al., 2015). Our resultant inflection generation model is shown in Figure 3. 637 4.1 Supervised Learning The parameters of our model are the set of character vectors, the transformation parameters (Wtrans , btrans ), and the parameters of the encoder and decoder LSTMs (§3.2). We use negative loglikelihood of the output character sequence as the loss function: XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) (6) t=1 We minimize the loss using stochastic updates with AdaDelta (Zeiler, 2012). This is our purely supervised model for inflection generation and we evaluate it in two different"
N16-1077,D13-1174,1,0.807869,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,N13-1140,1,0.855557,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,E03-1009,0,0.0140071,"phology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al.,"
N16-1077,P11-1004,0,0.0372597,"ses and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed tra"
N16-1077,N15-1140,0,0.0257116,"Missing"
N16-1077,Q15-1031,0,0.0314936,", back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer. Our model obtains state-"
N16-1077,N16-1080,0,0.0507045,"Missing"
N16-1077,D11-1057,0,0.0342392,"004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection"
N16-1077,N13-1138,0,0.784674,"tions. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wik"
N16-1077,P02-1001,0,0.31819,"al. (2015), and Nicolai et al. (2015), by DDN13, AFH14, AFH15, and NCK15 respectively. These models perform inflection generation as string transduction and largely consist of three major components: (1) Character alignment of word forms in a table; (2) Extraction of string transformation rules; (3) Application of rules to new root forms. The first step is learning character alignments across inflected forms in a table. Figure 2 (a) shows alignment between three word forms of Kalb. Different models use different heuristic algorithms for alignments such as edit distance, dynamic edit distance (Eisner, 2002; Oncina and Sebban, 2006), and longest subsequence alignment (Bergroth et al., 2000). Aligning characters across word forms provide spans of characters that have changed and spans that remain unchanged. These spans are used to extract rules for inflection generation for different inflection types as shown in Figure 2 (b)–(d). By applying the extracted rules to new root forms, inflected words can be generated. DDN13 use a semi-Markov model (Sarawagi and Cohen, 2004) to predict what rules should be applied, using character n-grams (n = 1 to 4) as features. AFH14 and AFH15 use substring features"
N16-1077,E12-1068,0,0.158263,"cted forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impra"
N16-1077,H05-1085,0,0.0322115,"n from the back to the front vowels. 7 Related Work Similar to the encoder in our framework, Rastogi et al. (2016) extract sub-word features using a forwardbackward LSTM from a word, and use them in a traditional weighted FST to generate inflected forms. Neural encoder-decoder models of string transduction have also been used for sub-word level transformations like grapheme-to-phoneme conversion (Yao and Zweig, 2015; Rao et al., 2015). Generation of inflectional morphology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonologic"
N16-1077,D11-1125,0,0.036259,"ed corpus. We use this language model to make predictions about the next character in the sequence given the previous characters, in following two settings. Output Reranking. In the first setting, we first train the inflection generation model using the supervised setting as described in §4.1. While making predictions for inflections, we use beam search to generate possible output character sequences and rerank them using the language model probability along with other easily extractable features as described in Table 2. We use pairwise ranking optimization (PRO) to learn the reranking model (Hopkins and May, 2011). The reranker is trained on the beam output of dev set and evaluated on test set. Language Model Interpolation. In the second setting, we interpolate the probability of observing the next character according to the language model with the probability according to our inflection genferent (equ. 5), and observed consistently worse results. 638 root forms 2764 2027 4055 6400 7249 11200 6957 Infl. 8 27 57 28 53 9 48 Table 3: The number of root forms and types of inflections across datasets. eration model. Thus, the loss function becomes: 1 XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) t=1 Z − λlog"
N16-1077,W14-2804,0,0.0911554,"ection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table cont"
N16-1077,J94-3001,0,0.687857,"and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological process"
N16-1077,D15-1176,1,0.0464737,"Missing"
N16-1077,P07-1017,0,0.372308,"m Kalb (calf) when it is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they ca"
N16-1077,Q15-1012,0,0.0435906,"nization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer."
N16-1077,N15-1093,0,0.72903,"state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected form of a given root word"
N16-1077,W04-0409,0,0.110421,"Missing"
N16-1077,J96-1003,0,0.0654284,"al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ("
N16-1077,N09-1024,0,0.0607111,"vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to seq"
N16-1077,N16-1076,0,0.151754,"Missing"
N16-1077,P08-1084,0,0.0451158,"haracter vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural netw"
N16-1077,P08-1059,0,0.285332,"is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1"
N16-1077,W04-0109,0,0.0383169,"(Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large numb"
N16-1077,P00-1027,0,0.147447,"cation of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, ba"
N16-1077,D14-1179,0,\N,Missing
N16-1087,W13-2322,0,0.598809,"new method is trained statistically from AMRannotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-tostring transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr 1 Introduction We consider natural language generation from the Abstract Meaning Representation (AMR; Banarescu et al., 2013). AMR encodes the meaning of a sentence as a rooted, directed, acyclic graph, where concepts are nodes, and edges are relationships among the concepts. Because AMR models propositional meaning1 while abstracting away from surface syntactic realizations, and is designed with human annotation in mind, it suggests a separation of (i) engineering the application-specific propositions that need to be 1 In essence, AMR handles semantic roles, entity types, within-sentence coreference, discourse connectives, modality, negation, and some other phenomena. communicated about from (ii) general-purpose re"
N16-1087,W11-2832,0,0.013904,"y be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We ha"
N16-1087,C10-1012,0,0.0119508,"much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language gene"
N16-1087,P06-1130,0,0.0471353,"Missing"
N16-1087,W02-1001,0,0.145668,"ac and c. For i > c+1, li contains all words between ai−1 and ai , and for i = c + 1, li contains all words between c and ai . The tables for lex , left lex , and right lex are populated using the segmented basic rules. For each basic rule extracted from the training corpus and segmented according to the previous paragraph, f → c is added to lex , and Aki → hli , ri i is added to left lex for i ≤ c and right lex for i > c. The permutation ki is known during extraction in Eq. 8. The parameters ψ are trained using AdaGrad (Duchi et al., 2011) with the perceptron loss function (Rosenblatt, 1957; Collins, 2002) for 10 iterations over the basic rules. The features g are listed in Table 2. 7 Tokens 210,000 29,000 30,000 5,000 Table 3: Train/dev./test/MT09 split. Table 2: Synthetic rule model features. POS is the most common part-of-speech tag sequence for c, “dist” is the string “dist”, and side is “L” if i < c, “R” otherwise. + denotes string concatenation. l1 X1 r1 . . . c . . . lm Xm rm Sentences 10,000 1,400 1,400 204 Abstract Rules Like the synthetic rules, the abstract rules RA (G) generalize the basic rules. However, abstract rules 737 are much simpler generalizations which use partof-speech (P"
N16-1087,P10-4002,1,0.897274,"lly or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we describe our approach in §4. The transducer’s rules are extracted from the limited AMR corpus and learned general731 Proceedings of N"
N16-1087,P14-1134,1,0.394856,"ed by defining b(·) and e(·) recursively, bottom up: b(i) = min(aj , e(i) = max(a0j , Inducing Basic Rules min b(j)) max e(j)) j∈children(i) j∈children(i) The basic rules, denoted RB , are extracted from the training AMR data using an algorithm similar to extracting tree transucers from tree-string aligned parallel corpora (Galley et al., 2004). Informally, the rules are extracted from a sentence w = hw1 , . . . , wn i with AMR graph G as follows: Also define functions ˜b and e˜, from fragment indices to integers, as: 1. The AMR graph and the sentence are aligned; we use the JAMR aligner from Flanigan et al. (2014), which aligns non-overlapping subgraphs of the graph to spans of words. The subgraphs that JAMR aligns are called fragments. In JAMR’s aligner, all fragments are trees. For fragment i, let Ci = children(root(i)) − nodes(i), which is the children of the fragment’s root concept that are not included in the fragment. Let fi be the TI representation for fragment i.5 If Ci is empty, then the rule extracted for fragment i is: 2. G is replaced by its spanning tree by deleting relations that use a variable in the AMR annotation. 3. In the spanning tree, for each node i, we keep track of the word indi"
N16-1087,N04-1035,0,0.0173377,"e child nodes of a node. We consider a node aligned if it belongs to an aligned fragment. Let the span of an aligned node i be denoted by endpoints ai and a0i ; for unaligned nodes, ai = ∞ and a0i = −∞ (depicted with superscripts in Fig. 2). The node alignments are propagated by defining b(·) and e(·) recursively, bottom up: b(i) = min(aj , e(i) = max(a0j , Inducing Basic Rules min b(j)) max e(j)) j∈children(i) j∈children(i) The basic rules, denoted RB , are extracted from the training AMR data using an algorithm similar to extracting tree transucers from tree-string aligned parallel corpora (Galley et al., 2004). Informally, the rules are extracted from a sentence w = hw1 , . . . , wn i with AMR graph G as follows: Also define functions ˜b and e˜, from fragment indices to integers, as: 1. The AMR graph and the sentence are aligned; we use the JAMR aligner from Flanigan et al. (2014), which aligns non-overlapping subgraphs of the graph to spans of words. The subgraphs that JAMR aligns are called fragments. In JAMR’s aligner, all fragments are trees. For fragment i, let Ci = children(root(i)) − nodes(i), which is the children of the fragment’s root concept that are not included in the fragment. Let fi"
N16-1087,J99-4004,0,0.0965541,"rule extraction from an AMRannotated sentence. 736 where the max is over c ∈ 0 . . . m, k1 , . . . , km is any permutation of 1, . . . , m, and Ri ∈ left lex (Ai ) for i < c and Ri ∈ right lex (Ai ) for i > c. ∗ is used to denote the concept position.  is the empty string. The best solution to Eq. 10 is found exactly by brute force search over concept position c ∈ [0, m + 1] and the permutation k1 , . . . , km . With fixed concept position and permutation, each Ri for the arg max is found independently. To obtain the exact K-best solutions, we use dynamic programming with a K-best semiring (Goodman, 1999) to keep track of the K best sequences for each concept position and permutation, and take the best K sequences over all values of c and k· . The synthetic rule model’s parameters are estimated using basic rules extracted from the training data. Basic rules are put into the form of Eq. 9 by Feature name POS + Ai + “dist” POS + Ai + side POS + Ai + side + “dist” POS + Ai + Ri + side c + Ai + “dist” c + Ai + side c + Ai + side + “dist” c + POS + Ai + side + “dist” Value |c − i| 1.0 |c − i| 1.0 |c − i| 1.0 |c − i| |c − i| Split Train Dev. Test MT09 segmenting the RHS into the form (11) by choosin"
N16-1087,N04-1014,0,0.181127,"nd leaves underspecified many important details—including tense, number, definiteness, whether a concept should be referred to nominally or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we descri"
N16-1087,P13-2121,0,0.0197701,"ules is returned. 8 Handwritten Rules We have handwritten rules for dates, conjunctions, multiple sentences, and the concept have-org-role91. We also create pass-through rules for concepts by removing sense tags and quotes (for string literals). 9 Experiments We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset. We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set. Statistics for this dataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement ("
N16-1087,D07-1028,0,0.0567956,"Missing"
N16-1087,2006.amta-papers.8,0,0.0387566,"ual formatting of the TI representation in Fig. 1 is: (X want-01 (ARG0 (X boy)) (ARG1 (X ride-01 (ARG0 (X bicycle (mod (X red))))))) To ease notation, we use the function sort[] to lexicographically sort edge labels in a TI representation. Using this function, an equivalent way of representing the TI representation in Eq. 1, if the Li are unsorted, is: (X C sort[(L1 T1 ) . . . (Lm Tm )]) The TI representation is converted into a word sequence using a tree-to-string transducer. The tree transducer formalism we use is one-state extended linear, non-deleting tree-to-string (1-xRLNs) transducers (Huang et al., 2006; Graehl and Knight, 2004).3 Definition 1. (From Huang et al., 2006.) A 1xRLNs transducer is a tuple (N, Σ, W, R) where N 2 If there are duplicate child edge labels, then the conversion process is ambiguous and any of the conversions can be used. The ordering ambiguity will be handled later in the treetransducer rules. 3 Multiple states would be useful for modeling dependencies in the output, but we do not use them here. 732 boy ride-01 ARG0 X bicycle mod X red The boy wants to ride the red bicycle . Figure 1: The generation pipeline. An AMR graph (top), with a deleted re-entrancy (dashed), is"
N16-1087,C12-1083,0,0.0452424,"ataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation. 11 erence Bleu for the LCD2014T12 test set, and fourreference Bleu for the MT09 set. We report ablation experiments for different sources of rules. When ablating handwritten rules, we do not ablate passthrough rules. The full system achieves 22.1 Bleu on the test set, and 21.2 on MT09. Removing the synthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing t"
N16-1087,P07-2045,1,0.010277,"e referred to nominally or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we describe our approach in §4. The transducer’s rules are extracted from the limited AMR corpus and learned general"
N16-1087,P98-1116,0,0.174995,"ody of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, tree-transducer approach can be easily extended with rules and"
N16-1087,A00-2023,0,0.165467,"and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, tree-transducer approach can be easily extended with rules and features from othe"
N16-1087,W05-1510,0,0.0374539,"nthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing the basic and abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation"
N16-1087,P03-1021,0,0.0208502,"transduction of G0 : ! e=E 4 arg max score(d; θ) d∈D(G0 ,T ) (6) If fi is just a single concept with no children, then m = 0 and fi = (X C). Eq. 6 is solved approximately using the cdec decoder for machine translation (Dyer et al., 2010). The score of the transduction is a linear function (with coefficients θ) of a vector of features including the output sequence’s language model logprobability and features associated with the rules in the derivation (denoted f ; Table 1): X score(d; θ) = θLM log(pLM (E(d))) + θ > f (r) r∈d The feature weights are trained on a development dataset using MERT (Och, 2003). In the next four sections, we describe the rules extracted and generalized from the training corpus. 5 1, . . . , F . Let nodes : {1, . . . , F } → 2{1,...,N } and root : {1, . . . , F } → {1, . . . , N } be functions that return the nodes in a fragment and the root of a fragment, respectively, and let children : {1, . . . , N } → 2{1,...,N } return the child nodes of a node. We consider a node aligned if it belongs to an aligned fragment. Let the span of an aligned node i be denoted by endpoints ai and a0i ; for unaligned nodes, ai = ∞ and a0i = −∞ (depicted with superscripts in Fig. 2). Th"
N16-1087,J05-1004,0,0.00451229,"s statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, t"
N16-1087,P02-1040,0,0.126115,"have-org-role91. We also create pass-through rules for concepts by removing sense tags and quotes (for string literals). 9 Experiments We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset. We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set. Statistics for this dataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation."
N16-1087,2007.mtsummit-ucnlg.4,0,0.0605298,"abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been"
N16-1087,N06-1056,0,0.0309,"thetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation. 11 erence Bleu for the LCD2014T12 test set, and fourreference Bleu for the MT09 set. We report ablation experiments for different sources of rules. When ablating handwritten rules, we do not ablate passthrough rules. The full system achieves 22.1 Bleu on the test set, and 21.2 on MT09. Removing the synthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing the basic and abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic a"
N16-1087,C98-1112,0,\N,Missing
N16-1102,W11-1218,0,0.0140755,"s of parameters with vast potential for over-fitting. Table 1 shows the statistics of the training sets.6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. We used ‘devset1 2’ and ‘devset 3’ as the development and test sets, respectively, and in both cases used only the first reference for evaluation. For Romanian and Estonian, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.7 The RussianEnglish data was taken from a web derived corpus (Antonova and Misyurev, 2011). The dataset is split into three parts using the same technique as for the Europarl sets. During the preprocessing stage we lower-cased and tokenized the data, and excluded sentences longer than 30 words. For the Europarl 5 As the alignment cells are normalised using the softmax and thus take values in [0,1], the trace term is bounded above by min(I, J) which occurs when the two alignment matrices are transposes of each other, representing perfect one-to-one alignments in both directions 880 We could share some parameters, e.g., the word embedding matrices, however we found this didn’t make m"
N16-1102,J93-2003,0,0.177161,"er-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus on the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend to different parts of the source as it generates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993). In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged ("
N16-1102,N13-1073,1,0.930705,"to different parts of the source as it generates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993). In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above"
N16-1102,P08-1112,0,0.0515496,"Missing"
N16-1102,W11-2123,0,0.0299588,"els and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora. configuration Sutskever encdec Attentional +align +align+glofer +align+glofer-pre +align+sym +align+sym+glofer-pre perplexity data, we also removed sentences containing headings and other meeting formalities.8 0 2 4 6 8 epochs Figure 3: Perplexity with training epochs on ro-en translation, comparing several model variants. embedding, 512 hidden, and 256 alignment dimensions. For each model, we also report the number of its parameters. Models are trained end-to-end using stochastic"
N16-1102,D13-1176,0,0.721739,"ation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting. 1 Introduction Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). This is despite the neural approaches using an overall simpler model, with fewer assumptions about the learning and prediction problem. Broadly, neural approaches are based around the notion of an encoder-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus on the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend"
N16-1102,N03-1017,0,0.101355,"ranslate as several words. Compared to the fertility model in IBM 3–5 (Brown et al., 1993), ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions. 3.4 Bilingual Symmetry So far we have considered a conditional model of the target given the source, modelling p(t|s). However it is well established for latent variable translation models that the alignments improve if p(s|t) is 3 Modern decoders (Koehn et al., 2003) often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility within phrases. 4 The normal distribution is deficient, as it has support for all scalar values, despite fi being bounded above and below (0 ≤ fi ≤ J). This could be corrected by using a truncated normal, or various other choices of distribution. lang-pair Zh-En Ru-En Et-En Ro-En # tokens (K) 422 454 1639 1809 1411 1857 1782 1806 # types (K) 3.44 3.12 145 65 90 25 39 24 Table 1: Statistics of the trai"
N16-1102,2005.iwslt-1.8,0,0.0597971,"truncated normal, or various other choices of distribution. lang-pair Zh-En Ru-En Et-En Ro-En # tokens (K) 422 454 1639 1809 1411 1857 1782 1806 # types (K) 3.44 3.12 145 65 90 25 39 24 Table 1: Statistics of the training sets, showing in each cell the count for the source language (left) and target language (right). Figure 2: Symmetric training with trace bonus, computed as matrix multiplication, − tr(αs←t αs→t > ). Dark shading indicates higher values. also modelled and the inferences of both directional models are combined – evidenced by the symmetrisation heuristics used in most decoders (Koehn et al., 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008). The rationale is that both models make somewhat independent errors, so an ensemble stands to gain from variance reduction. We propose a method for joint training of two directional models as pictured in Figure 2. Training twinned models involves optimising L = − log p(t|s) − log p(s|t) + γB where, as before, we consider only a single sentence pair, for simplicity of notation. This corresponds to a pseudo-likelihood objective, with the B linking the two models.5 The B component considers the a"
N16-1102,P07-2045,1,0.0319222,"rained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora. configuration Sutskever encdec Attentional +align +align+glofer +align+glofer-pre +align+sym +align+sym+glofer-pre perplexity data, we also removed sentences containing headings and other meeting formalities.8 0 2 4 6 8 epochs Figure 3: Perplexity with training epochs on ro-en translation, comp"
N16-1102,W04-3250,0,0.0357109,"15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features i"
N16-1102,2005.mtsummit-papers.11,0,0.140774,". This serves to demonstrate the robustness and generalisation of our model on sparse data – something that has not yet been established for neural models with millions of parameters with vast potential for over-fitting. Table 1 shows the statistics of the training sets.6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. We used ‘devset1 2’ and ‘devset 3’ as the development and test sets, respectively, and in both cases used only the first reference for evaluation. For Romanian and Estonian, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.7 The RussianEnglish data was taken from a web derived corpus (Antonova and Misyurev, 2011). The dataset is split into three parts using the same technique as for the Europarl sets. During the preprocessing stage we lower-cased and tokenized the data, and excluded sentences longer than 30 words. For the Europarl 5 As the alignment cells are normalised using the softmax and thus take values in [0,1], the trace term is bounded above by min(I, J) which occurs when the two alignment matrices are transposes"
N16-1102,N15-1063,0,0.0126112,"iance reduction. We propose a method for joint training of two directional models as pictured in Figure 2. Training twinned models involves optimising L = − log p(t|s) − log p(s|t) + γB where, as before, we consider only a single sentence pair, for simplicity of notation. This corresponds to a pseudo-likelihood objective, with the B linking the two models.5 The B component considers the alignment (attention) matrices, αs→t ∈ RJ×I and αt←s ∈ RI×J , and attempts to make these close to one another for both translation directions (see Fig. 2). To achieve this, we use a ‘trace bonus’, inspired by (Levinboim et al., 2015), formulated as B = − tr(αs←t > αs→t ) = XX j s←t s→t αi,j αj,i . i 4 Experiments Datasets. We conducted our experiments with four language pairs, translating between English ↔ Romanian, Estonian, Russian and Chinese. These languages were chosen to represent a range of translation difficulties, including languages with significant morphological complexity (Estonian, Russian). We focus on a (simulated) low resource setting, where only a limited amount of training data is available. This serves to demonstrate the robustness and generalisation of our model on sparse data – something that has not"
N16-1102,N06-1014,0,0.529688,"to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics RNN Attentional Decoder Beginnings are difficult START Aller Anfang ist schwer STOP Figure 1: Attentional model of translation (Bahdanau et al., 2015). The encoder is shown below the decod"
N16-1102,D15-1166,0,0.338515,"Missing"
N16-1102,W15-5003,0,0.0228087,"baseline. All other results are for the attentional model with a single-layer LSTM as encoder and two-layer LSTM as decoder, using 512 8 9 E.g., (The sitting was closed at 10.20pm). https://github.com/clab/cnn/ 881 #param (M) 8.7 15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development"
N16-1102,J03-1002,0,0.129597,"sitional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics RNN Attentional Decoder Beginnings are difficult START Aller Anfang ist schwer STOP Figure 1: Attentional model of translation (Bahdanau et al., 2015"
N16-1102,P03-1021,0,0.0154081,"015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: rel"
N16-1102,P02-1040,0,0.0951203,"sed at 10.20pm). https://github.com/clab/cnn/ 881 #param (M) 8.7 15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented"
N16-1102,P16-1008,0,0.0479383,"ion mechanism to be more local, by constraining attention to a text span, whose words’ representations are averaged. Similar in spirit to our work, recent research has proposed different ways of leveraging the attention history to incorporate alignment structural biases. (Luong et al., 2015) made use of the attention vector of the previous position when generating the attention vector for the next position. Feng et al. (2016) added another recurrent structure for the attention mechanism to enhance its memorization capabilities and capture long-range dependencies between the attention vectors. Tu et al. (2016) proposed a coverage vector to keep track of the attention history, hence refining future attentions. Finally, Cheng et al. (2015) proposed a similar agreement-based joint training for bidirectional attention-based neural machine translation, and showed significant improvements in BLEU for the large data French↔English translation. 6 Conclusion We have shown that the attentional model of translation does not capture many well known properties of traditional word-based translation models, and proposed several ways of imposing these as structural biases on the model. We show improvements across"
N16-1102,C96-2141,0,0.94621,"ord based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Ling"
N16-1102,W14-4012,0,\N,Missing
N16-1161,P14-2131,0,0.011684,"e tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360 words—constitute 10–"
N16-1161,P15-1033,1,0.113206,"Missing"
N16-1161,E14-1049,1,0.21174,") that polyglot phonetic feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages c"
N16-1161,D14-1012,0,0.0184753,"Missing"
N16-1161,D15-1127,0,0.0128799,"feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from"
N16-1161,D13-1196,0,0.0422633,"at (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360"
N16-1161,L16-1529,1,0.829863,"learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguistic matrix and 5 This matrix is described in Littell et al. (2016) and is available at https://github.com/dmort27/panphon/. 1364 manually examined aligned dimensions in the phone vectors from §5.3 (trained on six languages). In the maximally-correlated columns—corresponding to linguistic features long, consonant, nasalized—we examined phones with highest coefficients. These &gt; were: [5:, U:, i:, O:, E:] for long; [v, ñ, dZ, d, f, j, &gt; ts, N] for consonant; and [˜O, ˜E, A˜, œ] ˜ for nasalized. Clearly, the learned representation discover standard phonological features. Moreover, these top-ranked sounds are not grouped by a single language, e.g., &gt; /dZ/ is pres"
N16-1161,N15-1028,0,0.0136804,"ions are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from a finite inventor"
N16-1161,W11-2124,0,0.0106161,"gful related groupings across languages. 6 Related Work Multilingual language models. Interpolation of monolingual LMs is an alternative to obtain a multilingual model (Harbeck et al., 1997; Weng et al., 1997). However, interpolated models still require a trained model per language, and do not allow parameter sharing at training time. Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003). Adaptations have been proposed to apply language models in bilingual settings in machine translation (Niehues et al., 2011) and code switching (Adel et al., 2013). These approaches, however, require adaptation to every pair of languages, and an adapted model cannot be applied to more than two languages. Independently, Ammar et al. (2016) used a different polyglot architecture for multilingual dependency parsing. This work has also confirmed the utility of polyglot architectures in leveraging multilinguality. Multimodal neural language models. Multimodal language modeling is integrating image/video modalities in text LMs. Our work is inspired by the neural multimodal LMs (Kiros and Salakhutdinov, 2013; Kiros et al."
N16-1161,qian-etal-2010-python,0,0.0593655,"Missing"
N16-1161,D13-1170,0,0.00194071,"Missing"
N16-1161,P15-2021,1,0.840243,"Missing"
N16-1161,D15-1243,1,0.588955,"with hand-crafted features. We found that using both feature sets added no value, suggesting that learned phone vectors are capturing information that is equivalent to the hand-engineered vectors. 5.5 Qualitative analysis of vectors Phone vectors learned by Polyglot LMs are mere sequences of real numbers. An interesting question is whether these vectors capture linguistic (phonological) qualities of phones they are encoding. To analyze to what extent our vectors capture linguistic properties of phones, we use the QVEC—a tool to quantify and interpret linguistic content of vector space models (Tsvetkov et al., 2015). The tool aligns dimensions in a matrix of learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguisti"
N16-1161,P10-1040,0,0.0233197,"nd /U/ in “bit” and “book.” Only through linguistic analysis does it become evident that (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for"
N16-1161,P15-1130,0,0.00692841,"Missing"
N16-1161,P15-1113,0,0.0360795,"Missing"
N16-1161,P13-2037,0,\N,Missing
N16-1174,P14-1062,0,0.152187,"ication is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the r"
N16-1174,D14-1002,1,0.0625345,"ig. 1, which is a short Yelp review where the task is to predict the rating on a scale from 1–5. Intuitively, the first and third sentence have stronger information in assisting the prediction of the rating; within these sentences, the word delicious, a-m-a-z-i-n-g contributes more in implying the positive attitude contained in this review. Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014). The key difference to previous work is that our system uses context to discover when a sequence of tokens is relevant rather than simply filtering for (sequences of) tokens, taken out of context. To evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets (§3). Our model outperforms previous approaches by a significant margin. 2 Hierarchical Attention Networks The overall architecture of the Hierarchical Attention Network (HAN) is shown in Fig. 2. It consists of several parts: a word sequence encoder, a word-level attention la"
N16-1174,D14-1181,0,0.347192,"8), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation. Our primary contribution is a new neural architecture (§2), the Hierarchical Attention Network (HAN) that is designed to capture two"
N16-1174,P15-1107,0,0.642353,"Missing"
N16-1174,D15-1106,0,0.0757692,"nd Internet. explore the structure of a sentence and use a treestructured LSTMs for classification. There are also some works that combine LSTM and CNN structure to for sentence classification (Lai et al., 2015; Zhou et al., 2015). Tang et al. (2015) use hierarchical structure in sentiment classification. They first use a CNN or LSTM to get a sentence vector and then a bi-directional gated recurrent neural network to compose the sentence vectors to get a document vectors. There are some other works that use hierarchical structure in sequence generation (Li et al., 2015) and language modeling (Lin et al., 2015). The attention mechanism was proposed by (Bahdanau et al., 2014) in machine translation. The encoder decoder framework is used and an attention mechanism is used to select the reference words in original language for words in foreign language before translation. Xu et al. (2015) uses the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions. Further uses of the attention mechanism include parsing (Vinyals et al., 2014), natural language question answering (Sukhbaatar et al., 2015; 1487 Kumar et al., 2015; Hermann et al., 201"
N16-1174,P11-1015,0,0.312116,"cocktails. ||next time I in Phoenix, I will go back here. ||Highly recommend. Figure 1: A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word delicious, a-m-a-z-i-n-g contributes the most in defining sentiment of the two sentences. Introduction Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have bee"
N16-1174,P14-5010,0,0.0519213,"words (average and maximum per document). LSTM takes the whole document as a single sequence and the average of the hidden states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by (Tang et al., 2015). They also explore the hierarchical structure: a CNN or LSTM provides a sentence vector, and then a gated recurrent neural network (GRNN) combines the sentence vectors from a document level vector representation for classification. 3.3 Model configuration and training We split documents into sentences and tokenize each sentence using Stanford’s CoreNLP (Manning et al., 2014). We only retain words appearing more than 5 times in building the vocabulary and replace the words that appear 5 times with a special UNK token. We obtain the word embedding by training an unsupervised word2vec (Mikolov et al., 2013) model on the training and validation splits and then use the word embedding to initialize We . The hyper parameters of the models are tuned on the validation set. In our experiments, we set the word embedding dimension to be 200 and the GRU dimension to be 50. In this case a combination of forward and backward GRU gives us 100 dimensions for word/sentence annotat"
N16-1174,D13-1170,0,0.0565051,"ing sentences. Note that this happens in a multiclass setting, that is, detection happens before the selection of the topic! 4 Related Work Kim (2014) use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision (LeCun et al., 1998), albeit with NLP interpretations. Johnson and Zhang (2014) explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, Zhang et al. (2015) apply a character-level CNN for text classification and achieve competitive results. Socher et al. (2013) use recursive neural networks for text classification. Tai et al. (2015) GT: 4 Prediction: 4 pork belly = delicious . scallops ? i do n’t . even . like . scallops , and these were a-m-a-z-i-n-g . fun and tasty cocktails . next time i ’m in phoenix , i will go back here . highly recommend . GT: 0 Prediction: 0 terrible value . ordered pasta entree . . $ 16.95 good taste but size was an appetizer size . . no salad , no bread no vegetable . this was . our and tasty cocktails . our second visit . i will not go back . Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1."
N16-1174,P15-1150,0,0.146221,"ction happens before the selection of the topic! 4 Related Work Kim (2014) use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision (LeCun et al., 1998), albeit with NLP interpretations. Johnson and Zhang (2014) explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, Zhang et al. (2015) apply a character-level CNN for text classification and achieve competitive results. Socher et al. (2013) use recursive neural networks for text classification. Tai et al. (2015) GT: 4 Prediction: 4 pork belly = delicious . scallops ? i do n’t . even . like . scallops , and these were a-m-a-z-i-n-g . fun and tasty cocktails . next time i ’m in phoenix , i will go back here . highly recommend . GT: 0 Prediction: 0 terrible value . ordered pasta entree . . $ 16.95 good taste but size was an appetizer size . . no salad , no bread no vegetable . this was . our and tasty cocktails . our second visit . i will not go back . Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1. GT: 1 Prediction: 1 why does zebras have stripes ? what is the purpose or"
N16-1174,P14-1146,0,0.12484,"(Mikolov et al., 2013) is used as feature set. 3.2.2 SVMs SVMs-based methods are reported in (Tang et al., 2015), including SVM+Unigrams, Bigrams, Text Features, AverageSG, SSWE. In detail, Unigrams and Bigrams uses bag-of-unigrams and bagof-bigrams as features respectively. Text Features are constructed according to (Kiritchenko et al., 2014), including word and character n-grams, sentiment lexicon features etc. AverageSG constructs 200-dimensional word vectors using word2vec and the average word embeddings of each document are used. SSWE uses sentiment specific word embeddings according to (Tang et al., 2014). 3.2.3 Neural Network methods The neural network based methods are reported in (Tang et al., 2015) and (Zhang et al., 2015). CNN-word Word based CNN models like that of (Kim, 2014) are used. CNN-char Character level CNN models are reported in (Zhang et al., 2015). Data set Yelp 2013 Yelp 2014 Yelp 2015 IMDB review Yahoo Answer Amazon review classes documents average #s max #s average #w max #w vocabulary 5 5 5 10 10 5 335,018 1,125,457 1,569,264 348,415 1,450,000 3,650,000 8.9 9.2 9.0 14.0 6.4 4.9 151 151 151 148 515 99 151.6 156.9 151.9 325.6 108.4 91.9 1184 1199 1199 2802 4002 596 211,245 4"
N16-1174,D15-1167,0,0.706504,"l approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation. Our primary contribution is a new neural architecture (§2), the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since documents"
N16-1174,P12-2018,0,0.260108,"ops, and these were a-m-a-z-i-n-g . ||fun and tasty cocktails. ||next time I in Phoenix, I will go back here. ||Highly recommend. Figure 1: A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word delicious, a-m-a-z-i-n-g contributes the most in defining sentiment of the two sentences. Introduction Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–bas"
N16-1174,D15-1176,1,\N,Missing
N16-1174,N15-1011,0,\N,Missing
N18-1130,J92-1002,0,0.743957,"g et al., 2011; Ling et al., 2015; Kim et al., 2016). Unsupervised morphology has also been shown to improve the representations used by a log-bilinear LM (Botha and Blunsom, 2014). Jozefowicz et al. (2016) explore many interesting such architectures, and compare with fully character-based models. 1442 While these models allow for the elegant encoding of novel word forms they lack an open vocabulary. Open-vocabulary hybrid models alleviate this problem, extending the benefits of character-level representations to the generation. Such hybrid models with open vocabularies have been around since Brown et al. (1992). More recently, Chung et al. (2016) and Hwang and Sung (2016) describe methods of modelling sentences at both the word and character levels, using mechanisms to allow both a word-internal model that captures shortrange dependencies and a word-external model to capture longer-range dependencies. These models have been successfully applied to machine translation by Luong and Manning (2016), who use a character-level model to predict translations of out of vocabulary words. Our work falls in this category—we combine multiple representation levels while maintaining the ability to generate any cha"
N18-1130,N13-1140,1,0.705653,"arly challenging due to the vast set of potential word forms and the sparsity with which they appear in corpora. Traditional closed vocabulary models are unable to produce word forms unseen in training data and unable to generalize sub-word patterns found in data. The most straightforward solution is to treat language as a sequence of characters (Sutskever et al., 2011). However, models that operate at two levels—a character level and a word level— have better performance (Chung et al., 2016). Another solution is to use morphological information, which has shown benefits in non-neural models (Chahuneau et al., 2013). In this paper, we present a model that combines these approaches in a fully neural framework. Our model incorporates explicit morphological knowledge (e.g. from a finite-state morphological analyzer/generator) into a neural language model, combining it with existing word- and characterlevel modelling techniques, in order to create a model capable of successfully modelling morphologically complex languages. In particular, our model achieves three desirable properties. First, it conditions on all available (intrasentential) context, allowing it, in principle, to capture long-range dependencies"
N18-1130,P10-4002,1,0.764207,"wer than 25 times in the training corpus. Arrows indicate line wrapping. 4.1 As an extrinsic evaluation we test whether our language model improves machine translation between Turkish and English. While we could transform our model into a source-conditioned translation model, we choose here to focus on testing our model as an external unconditional language model, leaving the conditional version for future work. Since neural machine translation systems struggle with low-resource languages (Koehn and Knowles, 2017), we choose to introduce the score of our LM as an additional feature to a cdec (Dyer et al., 2010) hierarchical MT system. We train on the WMT 2016 Turkish–English data set, and perform n-best reranking after re-tuning weights with the new feature. The results, shown in Table 4 demonstrate small but significant gains in both directions, particularly into Turkish, where modelling productive morphology should be more important. 4.2 Lang. Pair TR-EN Machine Translation Morphological Disambiguation Our model is a joint model over words and the latent processes giving rise to those words (i.e., which generation process was selected and, for the EN-TR System Baseline Morph. Input Baseline Morph."
N18-1130,W17-3204,0,0.0335844,"forms well on were seen hundreds or thousands of times in the training corpus. Words in bold were seen fewer than 25 times in the training corpus. Arrows indicate line wrapping. 4.1 As an extrinsic evaluation we test whether our language model improves machine translation between Turkish and English. While we could transform our model into a source-conditioned translation model, we choose here to focus on testing our model as an external unconditional language model, leaving the conditional version for future work. Since neural machine translation systems struggle with low-resource languages (Koehn and Knowles, 2017), we choose to introduce the score of our LM as an additional feature to a cdec (Dyer et al., 2010) hierarchical MT system. We train on the WMT 2016 Turkish–English data set, and perform n-best reranking after re-tuning weights with the new feature. The results, shown in Table 4 demonstrate small but significant gains in both directions, particularly into Turkish, where modelling productive morphology should be more important. 4.2 Lang. Pair TR-EN Machine Translation Morphological Disambiguation Our model is a joint model over words and the latent processes giving rise to those words (i.e., wh"
N18-1130,P16-1057,0,0.0619416,"Missing"
N18-1130,D16-1124,1,0.768662,"epresented as a sequence of characters. 2.1 Word generation mixture model In typical RNNLMs the probability of the ith word in a sentence, wi given the preceding words is computed by using an RNN to encode the context followed by a softmax: p(wi |w<i ) = p(wi |hi = ϕRNN (w1 , . . . , wi−1 )) = softmax(Whi + b) where ϕRNN is an RNN that reads a sequence of words and returns a fixed sized vector encoding, W is a weight matrix, and b is a bias. In this work, we will use a mixture model over M different models for generating words in place of the single softmax over words (Miyamoto and Cho, 2016; Neubig and Dyer, 2016): p(wi |hi ) = = M X mi =1 M X mi =1 p(wi , mi |hi ) p(mi |hi )p(wi |hi , mi ), where mi ∈ [1, M ] indicates the model used to generate word wi . To ensure tractability for training and inference, we assume that mi is conditionally independent of all m<i , given the sequence of word forms w<i . We use three (M = 3) component models: (1) directly sampling a word from a finite vocabulary (mi = WORD), (2) generating a word as a sequence of characters (mi = CHARS), and (3) generating as a sequence of (abstract) morphemes which are then stitched together using a handwritten morphological transducer"
N18-1130,C16-1018,1,0.846285,"sentence likelihoods using this model is intractable, but posterior inference over mi and ai is feasible since the normalization factors cancel and therefore do not need to be computed. For our experiments we use the data set of Yuret and Türe (2006) who manually disambiguated from among the possible forms identified by an FST. We significantly out-perform the simple baseline of randomly guessing, and our results are competitive with Yatbaz and Yuret (2009), although they evaluated on a different dataset so they are not directly comparable. Furthermore, we also compare to a supervised model (Shen et al., 2016). While unsupervised techniques can’t hope to exceed supervised accuracies, this comparison provides insight into the difficulty of the problem. See Table 5 for results. 5 Related Work Purely Character-based or Subword-based LMs have a rich history going all the way back to Markov (1906)’s work modelling Russian character-by-character with his namesake models. More recently Sutskever et al. (2011) were the first to apply RNNs to character-level language modelling, leveraging their ability to handle the longrange dependencies required to model language at the character level. It is also possibl"
N18-1130,N06-1042,0,0.0537358,"ective (Besag, 1975). Y LPL = p(wi |w−i ) i = YX i m p(mi = m |w−i )p(wi |m, w−i ) We note that although this model has a very different semantics from the directed one, the PL training objective is identical to the directed model’s, the only difference is that features are based both on the past and future, rather than only the past. Similarly to training, evaluating sentence likelihoods using this model is intractable, but posterior inference over mi and ai is feasible since the normalization factors cancel and therefore do not need to be computed. For our experiments we use the data set of Yuret and Türe (2006) who manually disambiguated from among the possible forms identified by an FST. We significantly out-perform the simple baseline of randomly guessing, and our results are competitive with Yatbaz and Yuret (2009), although they evaluated on a different dataset so they are not directly comparable. Furthermore, we also compare to a supervised model (Shen et al., 2016). While unsupervised techniques can’t hope to exceed supervised accuracies, this comparison provides insight into the difficulty of the problem. See Table 5 for results. 5 Related Work Purely Character-based or Subword-based LMs have"
N18-1130,D15-1176,1,0.907096,"n machine translation and morphological disambiguation tasks. 2 Multi-level RNNLMs Recurrent neural network language models are composed of three parts: (a) an encoder, which turns a context word into a vector, (b) a recurrent backbone that turns a sequence of word vectors that represent the ordered sequence of context vectors into a single vector, and (c) a generator, which assigns a probability to each word that could follow the given context. RNNLMs often use the same process for (a) and (c), but there is no reason why these processes cannot be decoupled. For example, Kim et al. (2016) and Ling et al. (2015) compose character-level representations for their word encoder, but generate words using a softmax whose probabilities rely on inner products between the current context vector and type-specific word embeddings. In our model both the word generator (§2.1) and the word encoder (§2.2) compute representations that leverage three different views of words: frequent words have their own parameters, words that can be analyzed/generated by an analyzer are represented in terms of sequences of abstract morphemes, and all words are represented as a sequence of characters. 2.1 Word generation mixture mod"
N18-1130,P16-1100,0,0.0451799,"Missing"
N19-1114,P17-2021,0,0.0614883,"ork and the generative model) to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often em"
N19-1114,N19-1116,0,0.263685,"on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of the one billion word corpus. PRPN† is the model from Shen et al. (2018), whose hyperparameters were tuned by us. RNNG‡ is tr"
N19-1114,P06-1109,0,0.234657,"lso shows the F1 scores for grammar induction. Note that we induce latent trees directly from words on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of the one billi"
N19-1114,P15-1030,0,0.197267,"cting inductive bias. Specifically we employ amortized variational inference (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014) with a structured inference network. Variational inference lets us tractably optimize a lower bound on the log marginal likelihood, while employing a structured inference network encourages non-trivial structure. In particular, a con1105 Proceedings of NAACL-HLT 2019, pages 1105–1117 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ditional random field (CRF) constituency parser (Finkel et al., 2008; Durrett and Klein, 2015), which makes significant independence assumptions, acts as a guide on the generative model to learn meaningful trees through regularizing the posterior (Ganchev et al., 2010). We experiment with URNNGs on English and Chinese and observe that they perform well as language models compared to their supervised counterparts and standard neural LMs. In terms of grammar induction, they are competitive with recently-proposed neural architectures that discover tree-like structures through gated attention (Shen et al., 2018). Our results, along with other recent work on joint language modeling/structur"
N19-1114,K16-1002,0,0.280211,"pout of 0.5. We share word embeddings between the generative model and the inference network, and also tie weights between the input/output word embeddings (Press and Wolf, 2016). Optimization of the model itself required standard techniques for avoiding posterior collapse in VAEs.12 We warm-up the ELBO objective by linearly annealing (per batch) the weight on the conditional prior log pθ (z |x<z ) and the entropy H[qφ (z |x)] from 0 to 1 over the first two epochs (see equation (1) for definition of log pθ (z |x<z )). This is analogous to KL-annealing in VAEs with continuous latent variables (Bowman et al., 2016; Sønderby et al., 2016). We train for 18 epochs (enough for convergence for all models) with a batch size of 16 and K = 8 samples for the Monte Carlo gradient estimators. The generative model is optimized with SGD with learning rate equal to 1, 12 Posterior collapse in our context means that qφ (z |x) always produced trivial (always left or right branching) trees. 1109 except for the affine layer that produces a distribution over the actions, which has learning rate 0.1. Gradients of the generative model are clipped at 5. The inference network is optimized with Adam (Kingma and Ba, 2015) with"
N19-1114,P15-1033,1,0.8078,"te a sentence of length T , and z ∈ ZT to denote an unlabeled binary parse tree over a sequence of length T , represented as a a binary vector of length 2T − 1. Here 0 and 1 correspond to SHIFT and REDUCE actions, explained below.1 Figure 1 presents an overview of our approach. 2.1 Generative Model An RNNG defines a joint probability distribution pθ (x, z) over sentences x and parse trees z. We consider a simplified version of the original RNNG (Dyer et al., 2016) by ignoring constituent labels and only considering binary trees. The RNNG utilizes an RNN to parameterize a stack data structure (Dyer et al., 2015) of partiallycompleted constituents to incrementally build the parse tree while generating terminals. Using the current stack representation, the model samples an action (SHIFT or REDUCE): SHIFT generates a terminal symbol, i.e. word, and shifts it onto the stack,2 REDUCE pops the last two elements off the stack, composes them, and shifts the composed Figure 1: Overview of our approach. The inference network qφ (z |x) (left) is a CRF parser which produces a distribution over binary trees (shown in dotted box). Bij are random variables for existence of a constituent spanning i-th and j-th words"
N19-1114,W17-4303,0,0.0207434,"to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoenco"
N19-1114,N16-1024,1,0.100545,"of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms. 1 Introduction Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017), better encode syntactic properties of language (Kuncoro et al., 2018), and correlate with electrophysiological responses in the human brain (Hale et al., 2018). However, these all require annotated syntactic trees for training. In this work, we explore unsuperv"
N19-1114,P15-2142,0,0.114578,"ten qφ (z |x) by dividing span scores sij by a temperature term 2.0 before feeding it to the CRF. 15 Using the code from https://github.com/yikangshen/ PRPN, we tuned model size, initialization, dropout, learning rate, and use of batch normalization. 16 RNNG is trained to maximize log pθ (x, z) while URNNG is trained to maximize (a lower bound on) the language modeling objective log pθ (x). 1110 PTB KN 5-gram (Dyer et al., 2016) RNNLM (Dyer et al., 2016) Original RNNG (Dyer et al., 2016) Stack-only RNNG (Kuncoro et al., 2017) Gated-Attention RNNG (Kuncoro et al., 2017) Generative Dep. Parser (Buys and Blunsom, 2015) RNNLM (Buys and Blunsom, 2018) Sup. Syntactic NLM (Buys and Blunsom, 2018) Unsup. Syntactic NLM (Buys and Blunsom, 2018) PRPN† (Shen et al., 2018) This work: RNNLM URNNG RNNG RNNG → URNNG Figure 2: Perplexity of the different models grouped by sentence length on PTB. modeling of syntax helps generalization even with richly-parameterized neural models. Encouraged by these observations, we also experiment with a hybrid approach where we train a supervised RNNG first and continue fine-tuning the model (including the inference network) on the URNNG objective (RNNG → URNNG in Table 1).17 This appr"
N19-1114,N18-1086,0,0.206657,"2 each epoch after the first epoch at which validation performance does not improve, but this learning rate decay is not triggered for the first eight epochs to ensure adequate training. We use the same hyperparameters/training setup for both PTB and CTB. For experiments on (the subset of) the one billion word corpus, we use a smaller dropout rate of 0.1. The baseline RNNLM also uses the smaller dropout rate. All models are trained with an end-of-sentence token, but for perplexity calculation these tokens are not counted to be comparable to prior work (Dyer et al., 2016; Kuncoro et al., 2017; Buys and Blunsom, 2018). To be more precise, the inference network does not make use of the end-of-sentence token to produce parse trees, but the generative model is trained to generate the end-of-sentence token after the final REDUCE operation. 3.3 Baselines We compare the unsupervised RNNG (URNNG) against several baselines: (1) RNNLM, a standard RNN language model whose size is the same as URNNG’s stack LSTM; (2) Parsing Reading Predict Network (PRPN) (Shen et al., 2018), a neural language model that uses gated attention layers to embed soft tree-like structures into a neural network (and among the current state-o"
N19-1114,P17-2012,0,0.0739657,") to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heurist"
N19-1114,D17-1171,0,0.057174,"tama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion It is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while Shi et al. (2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provi"
N19-1114,D14-1082,0,0.0565262,"lso test our approach on a subset of the one billion word 10 https://github.com/clab/rnng Both versions of the PTB data can be obtained from http: //demo.clab.cs.cmu.edu/cdyer/ptb-lm.tar.gz. 11 corpus (Chelba et al., 2013). We randomly sample 1M sentences for training and 2K sentences for validation/test, and limit the vocabulary to 30K word types. While still a subset of the full corpus (which has 30M sentences), this dataset is two orders of magnitude larger than PTB. Experiments on Chinese utilize version 5.1 of the Chinese Penn Treebank (CTB) (Xue et al., 2005), with the same splits as in Chen and Manning (2014). Singleton words are replaced with a single hUNKi token, resulting in a vocabulary of 17,489 word types. 3.2 Training and Hyperparameters The stack LSTM has two layers with input/hidden size equal to 650 and dropout of 0.5. The tree LSTM also has 650 units. The inference network uses a one-layer bidirectional LSTM with 256 hidden units, and the MLP (to produce span scores sij for i ≤ j) has a single hidden layer with a ReLU nonlinearity followed by layer normalization (Ba et al., 2016) and dropout of 0.5. We share word embeddings between the generative model and the inference network, and als"
N19-1114,W14-4012,0,0.0576477,"Missing"
N19-1114,P08-1109,0,0.0709613,"Missing"
N19-1114,W06-1673,0,0.0185025,"018) observe a similar phenomenon in the context of learning latent trees for classification tasks. However Li et al. (2019) find that it is possible use a transition-based parser as the inference network for dependency grammar induction, if the inference network is constrained via posterior regularization (Ganchev et al., 2010) based on universal syntactic rules (Naseem et al., 2010). K 1 X ∇θ log pθ (x, z(k) ), K k=1 with samples z(1) , . . . , z(K) from qφ (z |x). Sampling uses the intermediate values calculated during the inside algorithm to sample split points recursively (Goodman, 1998; Finkel et al., 2006), as shown in Algorithm 2. The gradient with respect to φ involves two parts. The entropy term H[qφ (z |x)] can be calculated exactly in O(T 3 ), again using the intermediate values from the inside algorithm (see Algorithm 3).8 Since each step of this dynamic program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An estimator for the gradient with respect to Eqφ (z |x) [log pθ (x, z)] is obtained via the score function gradient estimator (Glynn, 1987; Williams, 1992), ∇φ Eqφ (z |x) [log pθ (x, z)] = Eqφ (z |x) [log pθ (x, z)∇φ log qφ (z |x)] ≈ K"
N19-1114,D18-1037,0,0.0314417,"e URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 20"
N19-1114,P18-1254,1,0.850713,"es from words through attention mechanisms. 1 Introduction Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017), better encode syntactic properties of language (Kuncoro et al., 2018), and correlate with electrophysiological responses in the human brain (Hale et al., 2018). However, these all require annotated syntactic trees for training. In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximizeP the log marginal likelihood log pθ (x) = log z pθ (x, z). Successful"
N19-1114,N19-1115,0,0.0703663,"duce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and sem"
N19-1114,W18-5452,0,0.0592617,"a subset (1M sentences) of the one billion word corpus. PRPN† is the model from Shen et al. (2018), whose hyperparameters were tuned by us. RNNG‡ is trained on predicted parse trees from Kitaev and Klein (2018). tree from qφ (z |x) through the Viterbi inside (i.e. CKY) algorithm. We calculate unlabeled F1 using evalb, which ignores punctuation and discards trivial spans (width-one and sentence spans).20 Since we compare F1 against the original, nonbinarized trees (per convention), F1 scores of models using oracle binarized trees constitute the upper bounds. We confirm the replication study of Htut et al. (2018) and find that PRPN is a strong model for grammar induction. URNNG performs on par with PRPN on English but PRPN does better on Chinese; both outperform right branching baselines. Table 3 further analyzes the learned trees and shows the F1 score of URNNG trees against 20 Available at https://nlp.cs.nyu.edu/evalb/. We evaluate with COLLINS.prm parameter file and LABELED option equal to 0. We observe that the setup for grammar induction varies widely across different papers: lexicalized vs. unlexicalized; use of punctuation vs. not; separation of train/test sets; counting sentence-level spans fo"
N19-1114,P99-1010,0,0.350123,"are copied from Table 1 of Drozdov et al. (2019). other trees (left), and the recall of URNNG/PRPN trees against ground truth constituents (right). We find that trees induced by URNNG and PRPN are quite different; URNNG is more sensitive to SBAR and VP, while PRPN is better at identifying NP. While left as future work, this naturally suggests a hybrid approach wherein the intersection of constituents from URNNG and PRPN is used to create a corpus of partially annotated trees, which can be used to guide another model, e.g. via posterior regularization (Ganchev et al., 2010) or semisupervision (Hwa, 1999). Finally, Table 4 compares our results using the same evaluation setup as in Drozdov et al. (2019), which differs considerably from our setup. 4.3 Distributional Metrics Table 5 shows some standard metrics related to the learned generative model/inference network. The “reconstruction” perplexity based on Eqφ (z |x) [log pθ (x |z)] is much lower than regular perplexity, and further, the Kullback-Leibler divergence between the conditional prior and the variational posterior, given by   qφ (z |x) Eqφ (z |x) log , pθ (z |x<z ) PPL Recon. PPL KL Prior Entropy Post. Entropy Unif. Entropy RNNG PTB"
N19-1114,W00-1306,0,0.12894,"alculated exactly in O(T 3 ), again using the intermediate values from the inside algorithm (see Algorithm 3).8 Since each step of this dynamic program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An estimator for the gradient with respect to Eqφ (z |x) [log pθ (x, z)] is obtained via the score function gradient estimator (Glynn, 1987; Williams, 1992), ∇φ Eqφ (z |x) [log pθ (x, z)] = Eqφ (z |x) [log pθ (x, z)∇φ log qφ (z |x)] ≈ K 1 X log pθ (x, z(k) )∇φ log qφ (z(k) |x). K k=1 8 We adapt the algorithm for calculating tree entropy in PCFGs from Hwa (2000) to the CRF case. 9 ∇φ H[qφ (z |x)] can also be computed using the insideoutside algorithm and a second-order expectation semiring (Li and Eisner, 2009), which has the same asymptotic runtime complexity but generally better constants. 1108 Algorithm 2 Top-down sampling a tree from qφ (z |x) Algorithm 3 Calculating the tree entropy H[qφ (z |x)] 1: procedure S AMPLE(β) . β from running I NSIDE(s) 2: B=0 . binary matrix representation of tree 3: Q = [(1, T )] . queue of constituents 4: while Q is not empty do 5: (i, j) = pop(Q) P 6: τ = j−1 k=i β[i, k] · β[k + 1, j] 7: for k := i to j − 1 do . ge"
N19-1114,Q18-1016,0,0.212843,"uce latent trees directly from words on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of the one billion word corpus. PRPN† is the model from Shen et al. (2018), whose hyp"
N19-1114,N07-1018,0,0.163783,"ar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximizeP the log marginal likelihood log pθ (x) = log z pθ (x, z). Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives (Klein and Manning, 2002) or priors (Johnson et al., 2007). These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions. Like RNN language models, RNNGs make no independence assumptions. Instead they encode structural bias through operations that compose linguistic constituents. The lack of independence assumptions contributes to the strong language modeling performance of RNNGs, b"
N19-1114,P18-1249,0,0.441047,"riational autoencoders) learn posterior distributions that are close to the variational family (Cremer et al., 2018). We can use this to our advantage with an inference network that injects inductive bias. We propose to do this by using a context-free model for the inference network, in particular, a neural CRF parser (Durrett and Klein, 2015). This choice 1107 can seen as a form of posterior regularization that limits posterior flexibility of the overly powerful RNNG generative model.6,7 The parameterization of span scores is similar to recent works (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018): we add position embeddings to word embeddings and run a bidirectional LSTM over the input representations to → − → − obtain the forward [ h 1 , . . . , h T ] and backward ← − ← − [ h 1 , . . . , h T ] hidden states. The score sij ∈ R for a constituent spanning xi to xj is given by, → − → − ← − ← − sij = MLP([ h j+1 − h i ; h i−1 − h j ]). Letting B be the binary matrix representation of a tree (Bij = 1 means there is a constituent spanning xi and xj ), the CRF parser defines a distribution over binary trees via the Gibbs distribution, X  1 qφ (B |x) = exp Bij sij , ZT (x) i≤j where ZT (x)"
N19-1114,P02-1017,0,0.736041,"mars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximizeP the log marginal likelihood log pθ (x) = log z pθ (x, z). Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives (Klein and Manning, 2002) or priors (Johnson et al., 2007). These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions. Like RNN language models, RNNGs make no independence assumptions. Instead they encode structural bias through operations that compose linguistic constituents. The lack of independence assumptions contributes to the strong language"
N19-1114,E17-1117,1,0.883284,"Missing"
N19-1114,P18-1132,1,0.848696,"induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms. 1 Introduction Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017), better encode syntactic properties of language (Kuncoro et al., 2018), and correlate with electrophysiological responses in the human brain (Hale et al., 2018). However, these all require annotated syntactic trees for training. In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model pθ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequ"
N19-1114,D09-1005,0,0.0454807,"c program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An estimator for the gradient with respect to Eqφ (z |x) [log pθ (x, z)] is obtained via the score function gradient estimator (Glynn, 1987; Williams, 1992), ∇φ Eqφ (z |x) [log pθ (x, z)] = Eqφ (z |x) [log pθ (x, z)∇φ log qφ (z |x)] ≈ K 1 X log pθ (x, z(k) )∇φ log qφ (z(k) |x). K k=1 8 We adapt the algorithm for calculating tree entropy in PCFGs from Hwa (2000) to the CRF case. 9 ∇φ H[qφ (z |x)] can also be computed using the insideoutside algorithm and a second-order expectation semiring (Li and Eisner, 2009), which has the same asymptotic runtime complexity but generally better constants. 1108 Algorithm 2 Top-down sampling a tree from qφ (z |x) Algorithm 3 Calculating the tree entropy H[qφ (z |x)] 1: procedure S AMPLE(β) . β from running I NSIDE(s) 2: B=0 . binary matrix representation of tree 3: Q = [(1, T )] . queue of constituents 4: while Q is not empty do 5: (i, j) = pop(Q) P 6: τ = j−1 k=i β[i, k] · β[k + 1, j] 7: for k := i to j − 1 do . get distribution over splits 8: wk = (β[i, k] · β[k + 1, j])/τ 9: k ∼ Cat([wi , . . . , wj−1 ]) . sample a split point 10: Bi,k = 1, Bk+1,j = 1 . update B"
N19-1114,D18-1184,0,0.0207635,"dov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have"
N19-1114,Q18-1005,0,0.0290815,"milar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 20"
N19-1114,W18-2903,0,0.037053,"terate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion"
N19-1114,J93-2004,0,0.0658527,"estimator is unbiased but typically suffers from high variance. To reduce variance, we use a control variate derived from an average of the other samples’ joint likelihoods (Mnih and Rezende, 2016), yielding the following estimator, K 1 X (log pθ (x, z(k) ) − r(k) )∇φ log qφ (z(k) |x), K k=1 1 P (j) where r(k) = K−1 j6=k log pθ (x, z ). This control variate worked better than alternatives such as estimates of baselines from an auxiliary network (Mnih and Gregor, 2014; Deng et al., 2018) or a language model (Yin et al., 2018). 3 Experimental Setup 3.1 Data For English we use the Penn Treebank (Marcus et al., 1993, PTB) with splits and preprocessing from Dyer et al. (2016) which retains punctuation and replaces singleton words with Berkeley parser’s mapping rules, resulting in a vocabulary of 23,815 word types.10 Notably this is much larger than the standard PTB LM setup from Mikolov et al. (2010) which uses 10K types.11 Also different from the LM setup, we model each sentence separately instead of carrying information across sentence boundaries, as the RNNG is a generative model of sentences. Hence our perplexity numbers are not comparable to the PTB LM results (Melis et al., 2018; Merity et al., 2018"
N19-1114,D18-1151,0,0.0283266,"rior pθ (z |x<z ), and uniform entropy is the entropy of the uniform distribution over binary trees. is highly nonzero. (See equation (1) for definitions of log pθ (x |z) and log pθ (z |x<z )). This indicates that the latent space is being used in a meaningful way and that there is no posterior collapse (Bowman et al., 2016). As expected, the entropy of the variational posterior is much lower than the entropy of the conditional prior, but there is still some uncertainty in the posterior. 4.4 Syntactic Evaluation We perform a syntactic evaluation of the different models based on the setup from Marvin and Linzen (2018): the model is given two minimally different sentences, one grammatical and one ungrammatical, and must identify the grammatical sentence by assigning it higher probability.21 Table 6 shows the accuracy results. Overall the supervised RNNG significantly outperforms the other models, indicating opportunities for further work in unsupervised modeling. While the URNNG does slightly outperform an RNNLM, the distribution of errors made from both models are similar, and thus it is not clear whether the outperformance is simply due to better perplexity or learning different structural biases. 4.5 Lim"
N19-1114,P05-1010,0,0.0754416,"es gated attention layers to embed soft tree-like structures into a neural network (and among the current state-of-the-art in grammar induction from words on the full corpus); (3) RNNG with trivial trees (left branching, right branching, random); (4) supervised RNNG trained on unlabeled, binarized gold trees.13 Note that the supervised RNNG also trains a discriminative parser qφ (z |x) (alongside the generative model pθ (x, z)) in order to sample parse forests for perplexity evaluation (i.e. importance sampling). This discriminative parser has the same ar13 We use right branching binarization—Matsuzaki et al. (2005) find that differences between various binarization schemes have marginal impact. Our supervised RNNG therefore differs the original RNNG, which trains on nonbinarized trees and does not ignore constituent labels. PTB PPL F1 Model RNNLM PRPN (default) PRPN (tuned) Left Branching Trees Right Branching Trees Random Trees URNNG CTB PPL F1 93.2 126.2 96.7 100.9 93.3 113.2 90.6 – 32.9 41.2 10.3 34.8 17.0 40.7 201.3 290.9 216.0 223.6 203.5 209.1 195.7 – 32.9 36.1 12.4 20.6 17.4 29.1 RNNG RNNG → URNNG 88.7 85.9 68.1 67.7 193.1 181.1 52.3 51.9 Oracle Binary Trees – 82.5 – 88.6 Table 1: Language modeli"
N19-1114,D10-1120,0,0.0336572,"looks at the entire sentence. However we found that under this setup, the inference network degenerated into a local minimum whereby it always generated left-branching trees despite various optimization strategies. Williams et al. (2018) observe a similar phenomenon in the context of learning latent trees for classification tasks. However Li et al. (2019) find that it is possible use a transition-based parser as the inference network for dependency grammar induction, if the inference network is constrained via posterior regularization (Ganchev et al., 2010) based on universal syntactic rules (Naseem et al., 2010). K 1 X ∇θ log pθ (x, z(k) ), K k=1 with samples z(1) , . . . , z(K) from qφ (z |x). Sampling uses the intermediate values calculated during the inside algorithm to sample split points recursively (Goodman, 1998; Finkel et al., 2006), as shown in Algorithm 2. The gradient with respect to φ involves two parts. The entropy term H[qφ (z |x)] can be calculated exactly in O(T 3 ), again using the intermediate values from the inside algorithm (see Algorithm 3).8 Since each step of this dynamic program is differentiable, we can obtain the gradient ∇φ H[qφ (z |x)] using automatic differentation.9 An e"
N19-1114,D18-1108,0,0.0153213,"2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al."
N19-1114,P14-1100,0,0.0571071,"d parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of"
N19-1114,P18-1173,0,0.0209789,"PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al.,"
N19-1114,P11-1108,0,0.0560271,"es. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent l"
N19-1114,P17-1105,0,0.0250359,"rs for the inference network and the generative model) to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees"
N19-1114,P07-1049,0,0.119971,"al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also r"
N19-1114,D18-1492,0,0.0535325,"inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion It is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while Shi et al. (2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provide useful inductive biases and improves performance. Finally, in modeling child language acquisition, the complex interaction of the parser and the grammatical knowledge being acquired is the object of much investigation (Trueswell and Gleitman, 2007); our work shows that apparently grammatical constraints can emerge from the interaction of a constrained parser and a more ge"
N19-1114,P04-1062,0,0.193764,"mmar Induction Table 1 also shows the F1 scores for grammar induction. Note that we induce latent trees directly from words on the full dataset.19 For RNNG/URNNG we obtain the highest scoring 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. 18 To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. 19 Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags (Klein and Manning, 2002; Smith and Eisner, 2004; Bod, 2006). However more recent works do train directly words (Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019). PPL 169.3 113.4 102.4 101.2 100.9 138.6 100.7 107.6 125.2 96.7 93.2 90.6 88.7 85.9 1M Sentences PPL † 77.7 77.4 71.8 72.9 72.0 PRPN (Shen et al., 2018) RNNLM URNNG RNNG‡ RNNG‡ → URNNG Table 2: (Top) Comparison of this work as a language model against prior works on sentence-level PTB with preprocessing from Dyer et al. (2016). Note that previous versions of RNNG differ from ours in terms of parameterization and model size. (Bottom) Results on a subset (1M sentences) of t"
N19-1114,D13-1204,0,0.052756,"enerally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctuation (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013; Parikh et al., 2014), as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning l"
N19-1114,P17-1076,0,0.0846814,"l inference (i.e. variational autoencoders) learn posterior distributions that are close to the variational family (Cremer et al., 2018). We can use this to our advantage with an inference network that injects inductive bias. We propose to do this by using a context-free model for the inference network, in particular, a neural CRF parser (Durrett and Klein, 2015). This choice 1107 can seen as a form of posterior regularization that limits posterior flexibility of the overly powerful RNNG generative model.6,7 The parameterization of span scores is similar to recent works (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018): we add position embeddings to word embeddings and run a bidirectional LSTM over the input representations to → − → − obtain the forward [ h 1 , . . . , h T ] and backward ← − ← − [ h 1 , . . . , h T ] hidden states. The score sij ∈ R for a constituent spanning xi to xj is given by, → − → − ← − ← − sij = MLP([ h j+1 − h i ; h i−1 − h j ]). Letting B be the binary matrix representation of a tree (Bij = 1 means there is a constituent spanning xi and xj ), the CRF parser defines a distribution over binary trees via the Gibbs distribution, X  1 qφ (B |x) = exp Bij sij ,"
N19-1114,D18-1548,0,0.0362,"2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VAEs (Johnson et al., 2016; Krishnan et al., 2017), which have been used previously for unsupervised (Cai et al., 2017; Drozdov et al., 2019; Li et al., 2019) and semi-supervised (Yin et al., 2018; Corro and Titov, 2019) parsing. 6 Conclusion It is an open question as to whether explicit modeling of syntax significantly helps neural models. Strubell et al. (2018) find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while Shi et al. (2018) observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provide useful inductive biases and improves performance. Finally, in modeling child language acquisition, the complex interaction of the parser and the grammatical knowledge being acquired is the object of much investigation (Trueswell and Gleitman, 2"
N19-1114,P15-1150,0,0.244109,"Missing"
N19-1114,W18-2704,0,0.0354662,"such as depth-bounded PCFGs (Jin et al., 2018) and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen et al., 2018) and Ordered Neurons (Shen et al., 2019) induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in Table 4). trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Liu and Lapata, 2018; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; Liu et al., 2018), policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019), or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018). The variational approximation uses amortized inference (Kingma and Welling, 2014; Mnih and Gregor, 2014; Rezende et al., 2014), in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders (Ammar et al., 2014) and structured VA"
N19-1114,P16-1218,0,0.034818,"h amortized variational inference (i.e. variational autoencoders) learn posterior distributions that are close to the variational family (Cremer et al., 2018). We can use this to our advantage with an inference network that injects inductive bias. We propose to do this by using a context-free model for the inference network, in particular, a neural CRF parser (Durrett and Klein, 2015). This choice 1107 can seen as a form of posterior regularization that limits posterior flexibility of the overly powerful RNNG generative model.6,7 The parameterization of span scores is similar to recent works (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018): we add position embeddings to word embeddings and run a bidirectional LSTM over the input representations to → − → − obtain the forward [ h 1 , . . . , h T ] and backward ← − ← − [ h 1 , . . . , h T ] hidden states. The score sij ∈ R for a constituent spanning xi to xj is given by, → − → − ← − ← − sij = MLP([ h j+1 − h i ; h i−1 − h j ]). Letting B be the binary matrix representation of a tree (Bij = 1 means there is a constituent spanning xi and xj ), the CRF parser defines a distribution over binary trees via the Gibbs distribution, X  1 qφ (B"
N19-1114,D18-1509,0,0.0369447,"llapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency 22 Many prior works that induce trees directly from words often employ additional heuristics based on punctu"
N19-1114,Q18-1019,0,0.0811044,"Missing"
N19-1114,D18-1356,1,0.894549,"Missing"
N19-1114,P17-1041,0,0.0319167,"various optimization strategies (e.g. separate optimizers for the inference network and the generative model) to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed.22 5 Related Work There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional (Emami and Jelinek, 2005; Buys and Blunsom, 2015; Dyer et al., 2016) and conditional (Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; Gu et al., 2018) cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep network (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018, 2019). In contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization ov"
N19-1114,P18-1070,0,0.078418,"] 11: − log wu ) · wu 12: return H[1, T ] . return tree entropy H[qφ (z |x)] The above estimator is unbiased but typically suffers from high variance. To reduce variance, we use a control variate derived from an average of the other samples’ joint likelihoods (Mnih and Rezende, 2016), yielding the following estimator, K 1 X (log pθ (x, z(k) ) − r(k) )∇φ log qφ (z(k) |x), K k=1 1 P (j) where r(k) = K−1 j6=k log pθ (x, z ). This control variate worked better than alternatives such as estimates of baselines from an auxiliary network (Mnih and Gregor, 2014; Deng et al., 2018) or a language model (Yin et al., 2018). 3 Experimental Setup 3.1 Data For English we use the Penn Treebank (Marcus et al., 1993, PTB) with splits and preprocessing from Dyer et al. (2016) which retains punctuation and replaces singleton words with Berkeley parser’s mapping rules, resulting in a vocabulary of 23,815 word types.10 Notably this is much larger than the standard PTB LM setup from Mikolov et al. (2010) which uses 10K types.11 Also different from the LM setup, we model each sentence separately instead of carrying information across sentence boundaries, as the RNNG is a generative model of sentences. Hence our perplexity"
N19-1171,P16-1231,0,0.540019,"d to those with 1724 Proceedings of NAACL-HLT 2019, pages 1724–1733 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics the more common locally normalized parametrization. We posit that this difference is due to label bias (Bottou, 1991) arising from the interaction of approximate search and search-aware optimization in locally normalized models. A commonly understood source of label bias in locally normalized sequence models is an effect of conditioning only on partial input (for example, only the history of the input) at each step during decoding (Andor et al., 2016; Lafferty et al., 2001; Wiseman and Rush, 2016). We discus another potential source of label bias arising from approximate search with locally normalized models that may be present even with access to the full input at each step. To this end, we train search-aware globally and locally normalized models in an end-to-end (sub)-differentiable manner using a continuous relaxation to the discontinuous beam search procedure introduced by Goyal et al. (2017b). This approach requires initialization with a suitable globally normalized model to work in practice. Hence, we also propose an initialization"
N19-1171,N15-1027,0,0.02122,"model it is: &quot; n # X log s(x, y1:i−1 , yi ) − log ZG (x) i=1 3.2.1 Self Normalization One way to find a locally normalized model that is parametrized like a globally normalized model is to ensure that the local normalizer at each step, log ZL,i (x, y1:i−1 ), is 0. With the local normalizer being zero it is straightforward to see that the log probability of a sequence under a locally normalized model can easily be interpreted as log probability of the sequence under a globally normalized model with the global log-normalizer, log ZG (x) = 0. This training technique is called self-normalization (Andreas and Klein, 2015) because the resulting models’ unnormalized score at each step lies on a probability simplex. A common technique for training self-normalized models is L2-regularization of local log normalizer which encourages learning a model with log Z = 0 and was found to be effective for learning a language model by Devlin et al. (2014)2 . The L2regularized cross entropy objective is given by: min θ X x,y∗ ∈D − n X i=1 log p(yi∗ |x, y1:i−1 ) +λ · (log ZL,i (x, y1:i−1 ))2 In Table 1, we report the mean and variance of the local log normalizer on the two different tasks using L2-regularization (L2) based se"
N19-1171,P84-1044,0,0.311862,"Missing"
N19-1171,hockenmaier-steedman-2002-acquiring,0,0.034186,"entropy trained models and self-normalized models to study the effects of search-aware optimization and global normalization. We follow Goyal et al. (2017b) and use the decomposable Hamming loss approximation with search-aware optimization for both the tasks and decode via soft beam search decoding method which involves continuous beam search with soft backpointers for the LSTM Beam search dynamics as described in Section 3, but using identifiable backpointers and labels (using MAP estimates of soft backpointers and labels) to decode. CCG supertagging We used the standard splits of CCG bank (Hockenmaier and Steedman, 2002) for training, development, and testing. The label space of supertags is 1,284 and the labels are correlated with each other based on their syntactic relations. The distribution of supertag labels in the training data exhibits a long tail distribution. This task is sensitive to the long range sequential decisions because it encodes rich syntactic information about the sentence. Hence, this task is ideal to analyze the effects of label bias and search effects. We perform minor preprocessing on the data similar to the preprocessing in Vaswani et al. (2016). For experiments related to search awar"
N19-1171,2014.iwslt-evaluation.1,0,0.10811,"Missing"
N19-1171,P04-1015,0,0.488966,"zation Src sent-length → pretrain-beam locally-normalized globally-normalized 0-20 29.36 32.35 33.21 20-30 25.73 26.95 28.08 30-40 24.71 25.39 26.75 40+ 24.50 25.2 26.41 Table 6: BLEU scores with different length inputs on dev set Reported on Self-normalized initialization. The header specifies the range of length of the input sentences that globally normalized models perform better on all the length ranges but especially so on long sentences. 5 Related Work Much of the existing work on search-aware training of globally normalized neural sequence models uses some mechanism like early updates (Collins and Roark, 2004) that relies on explicitly tracking if the gold sequence falls off the beam and is not end-to-end continuous. Andor et al. (2016) describe a method for training globally normalized neural feedforward models, which involves optimizing a CRF-based likelihood where the normalizer is approximated by the sum of the scores of the final beam elements. They describe label bias arising out of conditioning on partial input and hence focused on the scenario in which locally normalized models can be less expressive than globally normalized models, whereas we also consider another source of label bias whic"
N19-1171,P14-1129,0,0.111054,"Missing"
N19-1171,D15-1044,0,0.0477999,"arm-starting with pre-trained models, we also propose a novel initialization strategy based on self-normalization for pretraining globally normalized models. We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global normalization under different conditions while using search-aware training. 1 Introduction Neural encoder-decoder models have been tremendously successful at a variety of NLP tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Dyer et al., 2016, 2015), summarization (Rush et al., 2015), dialog generation (Serban et al., 2015), and image captioning (Xu et al., 2015). With these models, the target sequence is generated in a left-to-right step-wise manner with the predictions at every step being conditioned on the input sequence and the whole prediction history. This long-distance memory precludes exact search for the maximally scoring sequence according to the model and therefore, approximate algorithms like greedy search or beam search are necessary in practice during decoding. In this scenario, it is natural to resort to search-aware learning techniques for these models whi"
N19-1171,P15-1033,1,0.853784,"Missing"
N19-1171,J07-4003,0,0.261515,"ware locally normalized sequence models that involve projecting the scores of items in the vocabulary onto a probability simplex at each step and globally normalized/unnormalized sequence models that involve scoring sequences without explicit normalization at each step. When conditioned on the the full input sequence and the entire prediction history, both locally normalized and globally normalized conditional models should have same expressive power under a highcapacity neural parametrization in theory, as they can both model same set of distributions over all finite length output sequences (Smith and Johnson, 2007). However, locally normalized models are constrained in how they respond to search errors during training since the scores at each decoding step must sum to one. To let a search-aware training setup have the most flexibility, abandoning this constraint may be useful for easier optimization. In this paper, we demonstrate that the interaction between approximate inference and nonconvex parameter optimization results in more robust training and better performance for models with global normalization compared to those with 1724 Proceedings of NAACL-HLT 2019, pages 1724–1733 c Minneapolis, Minnesot"
N19-1171,N16-1024,1,0.818841,"e our training approach is sensitive to warm-starting with pre-trained models, we also propose a novel initialization strategy based on self-normalization for pretraining globally normalized models. We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global normalization under different conditions while using search-aware training. 1 Introduction Neural encoder-decoder models have been tremendously successful at a variety of NLP tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Dyer et al., 2016, 2015), summarization (Rush et al., 2015), dialog generation (Serban et al., 2015), and image captioning (Xu et al., 2015). With these models, the target sequence is generated in a left-to-right step-wise manner with the predictions at every step being conditioned on the input sequence and the whole prediction history. This long-distance memory precludes exact search for the maximally scoring sequence according to the model and therefore, approximate algorithms like greedy search or beam search are necessary in practice during decoding. In this scenario, it is natural to resort to search-awar"
N19-1171,P17-2058,1,0.870083,"Missing"
N19-1171,N16-1027,0,0.0944075,"standard splits of CCG bank (Hockenmaier and Steedman, 2002) for training, development, and testing. The label space of supertags is 1,284 and the labels are correlated with each other based on their syntactic relations. The distribution of supertag labels in the training data exhibits a long tail distribution. This task is sensitive to the long range sequential decisions because it encodes rich syntactic information about the sentence. Hence, this task is ideal to analyze the effects of label bias and search effects. We perform minor preprocessing on the data similar to the preprocessing in Vaswani et al. (2016). For experiments related to search aware optimization, we report results with beam size of 5.3 4.1.1 Tagging model for ablation study We changed the standard sequence-to-sequence model to be more suitable for the tagging task. This change also lets us perform controlled experiments pertaining to the amount of input sequence information available to the decoder at each time step. In a standard encoder-decoder model with attention, the initial hidden state of the decoder is often some function of the final encoder state so that the decoder’s predictions can be conditioned on the full input. For"
N19-1171,D16-1137,0,0.661753,"LT 2019, pages 1724–1733 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics the more common locally normalized parametrization. We posit that this difference is due to label bias (Bottou, 1991) arising from the interaction of approximate search and search-aware optimization in locally normalized models. A commonly understood source of label bias in locally normalized sequence models is an effect of conditioning only on partial input (for example, only the history of the input) at each step during decoding (Andor et al., 2016; Lafferty et al., 2001; Wiseman and Rush, 2016). We discus another potential source of label bias arising from approximate search with locally normalized models that may be present even with access to the full input at each step. To this end, we train search-aware globally and locally normalized models in an end-to-end (sub)-differentiable manner using a continuous relaxation to the discontinuous beam search procedure introduced by Goyal et al. (2017b). This approach requires initialization with a suitable globally normalized model to work in practice. Hence, we also propose an initialization strategy based upon self-normalization for pre-"
P07-2045,N03-2002,0,0.152204,"nfusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input."
P07-2045,koen-2004-pharaoh,0,0.148177,"to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of amb"
P07-2045,D07-1091,1,0.158367,"Missing"
P07-2045,N03-1017,1,0.161374,"informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 07aec_2@williams.edu. 8 evh4@cornell.edu 2 Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is co"
P07-2045,P03-1021,0,0.176468,"ent data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customiz"
P07-2045,J03-1002,0,0.164868,"L 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online pre"
P07-2045,P02-1040,0,0.148118,"d language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Mo"
P07-2045,N07-1062,1,0.152186,"up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more"
P07-2045,D08-1076,0,\N,Missing
P07-2045,2006.iwslt-evaluation.8,1,\N,Missing
P09-1019,J07-2003,0,0.308769,"soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair H = hV, Ei consisting of a vertex set V and a set of hyperedges E ⊆ V ∗ × V. Each hyperedge e ∈ E connects a head vertex h(e) with a sequence of tail vertices T (e) = {v1 , ..., vn }. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arit"
P09-1019,C08-5001,0,0.0762935,"ew, CA 94043, USA {shankarkumar,wmach,och}@google.com Abstract number of translation alternatives relative to N best lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MB"
P09-1019,N04-1022,1,0.945384,"nts from MBR decoding on several language pairs. 1 Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu Introduction Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N -best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher Rally World Championship its future in X1 X2 X1 X2 Suzuki X1 X2 X1 X2 soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Sin"
P09-1019,D08-1076,1,0.459184,"ce improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algo"
P09-1019,P08-1023,0,0.0627745,"ill soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair H = hV, Ei consisting of a vertex set V and a set of hyperedges E ⊆ V ∗ × V. Each hyperedge e ∈ E connects a head vertex h(e) with a sequence of tail vertices T (e) = {v1 , ..., vn }. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice). Each hyperedge"
P09-1019,J04-4002,1,0.491531,"the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N -best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher Rally World Championship its future in X1 X2 X1 X2 Suzuki X1 X2 X1 X2 soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number o"
P09-1019,P03-1021,1,0.18736,"rom MERT and MBR as well as performance improvements from MBR decoding on several language pairs. 1 Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu Introduction Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N -best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher Rally World Championship its future in X1 X2 X1 X2 Suzuki X1 X2 X1 X2 soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL"
P09-1019,2001.mtsummit-papers.68,0,0.157746,"llmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects the hypothesis in the hypergraph with the maximum expected approximate corpus BLEU score (Tr"
P09-1019,D08-1065,1,0.301213,"R decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects"
P09-1019,P08-1025,0,0.0280132,"Missing"
P09-1019,W06-3119,0,0.0621183,"nnounces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair H = hV, Ei consisting of a vertex set V and a set of hyperedges E ⊆ V ∗ × V. Each hyperedge e ∈ E connects a head vertex h(e) with a sequence of tail vertices T (e) = {v1 , ..., vn }. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice"
P09-1019,D08-1022,0,\N,Missing
P09-1019,P02-1040,0,\N,Missing
P09-1019,P06-1121,0,\N,Missing
P09-1088,N06-2013,0,0.00554308,"), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty"
P09-1088,N07-1018,0,0.0346801,"× 4.2 A Gibbs sampler for derivations Markov chain Monte Carlo sampling allows us to perform inference for the model described in 4.1 without restricting the infinite space of possible translation rules. To do this we need a method for sampling a derivation for a given sentence pair from p(d|d− ). One possible approach would be to first build a packed chart representation of the derivation forest, calculate the inside probabilities of all cells in this chart, and then sample derivations top-down according to their inside probabilities (analogous to monolingual parse tree sampling described in Johnson et al. (2007)). A problem with this approach is that building the derivation forest would take O(|f |3 |e|3 ) time, which would be impractical for long sentences. Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample. After a period of burn in, the derivations produced by the sampler will be drawn from the posterior distribution, p(d|x). The advantage of this algorithm is that we only store the current derivation for each training sentence pair (together these constitute the state of the sampler), but nev"
P09-1088,N03-1017,0,0.592202,"nous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchrono"
P09-1088,P07-2045,1,0.0237661,"iero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the"
P09-1088,J93-2003,0,0.0378426,"lel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. 1 Introduction The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and c"
P09-1088,W08-0336,0,0.019233,", containing fewer spurious off-diagonal alignments, than the heuristic (see Figure 5), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent"
P09-1088,W02-1018,0,0.720719,".ed.ac.uk Trevor Cohn∗ tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted"
P09-1088,W07-0403,0,0.847523,"tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context f"
P09-1088,W06-1606,0,0.0297598,"siderably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; Related work Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state of an unsupervised word-based aligner such 2 We include grammar based transducers, such as Chiang (2007) and Marcu et al. (2006), in our definition of phrasebased models. 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f |3 |e|3 )) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine tra"
P09-1088,J07-2003,0,0.855119,"eld of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable. Word-based models are incapable of learning translational equivalences between non-compositional phrasal units, while the algorithms used for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification. A 1 f and e are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, c Suntec, Singapore, 2-7 August 2009. 2009 ACL a"
P09-1088,P08-2007,0,0.0824084,"m aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihoo"
P09-1088,J03-1002,0,0.0161441,"that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus. Each process maintains a set of rule counts for the entire corpus and communicates the changes it has made to its section of the corpus only after sampling every sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model an"
P09-1088,W06-3105,0,0.305426,"er polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints fr"
P09-1088,P03-1021,0,0.0148685,"nd distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty passes through the data. It is clear from the training curves that the distributed approximation tracks the corpus probability of the correct sampler sufficiently closely. Th"
P09-1088,D08-1033,0,0.492948,"Missing"
P09-1088,2001.mtsummit-papers.68,0,0.0114687,"ard alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase tables and Hiero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models"
P09-1088,J97-3002,0,0.890151,", 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a nov"
P09-1088,N04-1035,0,0.0196258,"thout resorting to heuristic restrictions on the model. Initial experiments suggest that this model performs well on languages for which the monotone bias of existing alignment and heuristic phrase extraction approaches fail. These results open the way for the development of more sophisticated models employing grammars capable of capturing a wide range of translation phenomena. In future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as Hiero grammars or tree substitution grammars of the form used by Galley et al. (2004). (2007) who also observed very little empirical difference between the sampler and its distributed approximation. Tables 3 and 4 show the result on the two NIST corpora when running the distributed sampler on a single 8-core machine.5 These scores tally with our initial hypothesis: that the hierarchical structure of our model suits languages that exhibit less monotone reordering. Figure 5 shows the projected alignment of a headline from the thousandth sample on the NIST Chinese data set. The effect of the grammar based alignment can clearly be seen. Where the combination of GIZA++ and the heu"
P09-1088,C08-1136,0,0.571081,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P07-1094,0,0.0150239,"pler to permute the internal structure of the trees more easily. ... ... ... ... Figure 4: Rule insert/delete sampler. A pair of adjacent nodes in a ternary rule can be re-parented as a binary rule, or vice-versa. 4.3 Hyperparameter Inference Our model is parameterised by a vector of hyperparameters, α = (αR , αN , αP , αPE , αPF , αnull ), which control the sparsity assumption over various model parameters. We could optimise each concentration parameter on the training corpus by hand, however this would be quite an onerous task. Instead we perform inference over the hyperparameters following Goldwater and Griffiths (2007) by defining a vague gamma prior on each concentration parameter, αx ∼ Gamma(10−4 , 104 ). This hyper-prior is relatively benign, allowing the model to consider a wide range of values for the hyperparameter. We sample a new value for each αx using a log-normal distribution with mean αx and variance 0.3, which is then accepted into the distribution p(αx |d, α− ) using the MetropolisHastings algorithm. Unlike the Gibbs updates, this calculation cannot be distributed over a cluster (see Section 4.4) and thus is very costly. Therefore for small corpora we re-sample the hyperparameter after every p"
P09-1088,P08-1012,0,0.818829,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P06-1085,0,0.00708198,"with a geometric distribution in which a string of length k will be more probable than its segmentations. We define P1null as the string probability of the non-null part of the rule:  1 E null 2 P0 (e) if |f |= 0 P1 (z → he, f i) = 1 F 2 P0 (f ) if |e |= 0 The terminal translation phrase pair distribution is a hierarchical Dirichlet Process in which each phrase are independently distributed according to DPs:4 F P1P (z → he, f i) = φE z (e) × φz (f ) PE φE , P0E ) z ∼ DP(α 4 This prior is similar to one used by DeNero et al. (2008), who used the expected table count approximation presented in Goldwater et al. (2006). However, Goldwater et al. (2006) contains two major errors: omitting P0 , and using the truncated Taylor series expansion (Antoniak, 1974) which fails for small αP0 values common in these models. In this work we track table counts directly. 785 ... ... ... ... ... ... ... ... ... Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a solid line is a null alignment. ... ... ... ... the Split/Join operator in Figure 3. In order for this opera"
P09-1088,P02-1040,0,\N,Missing
P09-1088,2005.iwslt-1.1,0,\N,Missing
P09-4007,P05-1032,1,0.805694,"ssor architectures and distributed computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2 ; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optim"
P09-4007,J07-2003,0,0.827653,"lation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was suppo"
P09-4007,P03-2041,0,0.0474557,"ize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). 2 The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable as the SRILM native bridge code. 3 The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. 26 • Variational Decoding: spurious ambiguity causes the probability of an output string among to be split among many derivations. The goodness of a string is measured by the total probability of its derivations, which means that finding the best output string is computatio"
P09-4007,P06-1121,0,0.410414,"grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Resear"
P09-4007,W05-1506,0,0.0961856,"t this demonstration description paper. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, c Suntec, Singapore, 3 August 2009. 2009 ACL and AFNLP System google lium dcu joshua uka limsi uedin rwth cmu-statxfer BLEU-4 31.14 26.89 26.86 26.52 25.96 25.51 25.44 24.89 23.65 • Pruning: We incorporate beam- and cubepruning (Chiang, 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. Table 1: BLEU scores for top primary systems on the WMT-09 French-English Task from CallisonBurch et al. (2009), who also provide human evaluation results. 2.1 • Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations (e.g. the references) may not be contained due to pruning or inherent modeling deficiency. We implement an efficient dynamic programming algorithm (Li and Khudanpur, 2009) for finding the oracle translations, which are most similar to the desired translations"
P09-4007,P09-1067,1,0.853412,"tand-alone tool that does not rely on the rest of the toolkit. Scalability: Joshua, especially the decoder, is scalable to large models and data sets. For example, the parsing and pruning algorithms are implemented with dynamic programming strategies and efficient data structures. We also utilize suffixarray grammar extraction, parallel/distributed decoding, and bloom filter language models. Joshua offers state-of-the-art quality, having been ranked 4th out of 16 systems in the FrenchEnglish task of the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. We describe Joshua (Li et al., 2009a)1 , an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interest"
P09-4007,P07-2045,1,0.0131579,"lkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone. 1 Please cite Li et al. (2009a) if you use Joshua in your research, and not this demonstration description paper. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, c Suntec, Singapore, 3 August 2009. 2009 ACL and AFNLP System google lium dcu joshua uka limsi uedi"
P09-4007,P06-1077,0,0.259566,"rsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Research Projects Agency’s GA"
P09-4007,D07-1104,0,0.0157455,"d computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2 ; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using th"
P09-4007,W08-0402,1,0.798672,"Missing"
P09-4007,P03-1021,0,0.0153466,"guage suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). 2 The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable"
P09-4007,P05-1034,0,0.0603092,"hronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the D"
P09-4007,N09-2003,1,0.88482,"Missing"
P09-4007,P07-1065,0,0.0575308,"Missing"
P09-4007,W09-0424,1,0.2272,"tand-alone tool that does not rely on the rest of the toolkit. Scalability: Joshua, especially the decoder, is scalable to large models and data sets. For example, the parsing and pruning algorithms are implemented with dynamic programming strategies and efficient data structures. We also utilize suffixarray grammar extraction, parallel/distributed decoding, and bloom filter language models. Joshua offers state-of-the-art quality, having been ranked 4th out of 16 systems in the FrenchEnglish task of the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. We describe Joshua (Li et al., 2009a)1 , an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interest"
P09-4007,W09-0401,1,\N,Missing
P09-4007,D08-1076,0,\N,Missing
P10-4002,W05-1506,0,0.0175505,"mentation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Tab"
P10-4002,P07-1019,0,0.10005,"and phrase-based models, these are strictly arranged in a monotone, leftbranching structure. ing models need not be explicitly represented as FSTs—the state space can be inferred. Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). 4 5 Rescoring with weighted FSTs Semiring framework Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings. A semiring is a 5-tuple (K, ⊕, ⊗, 0, 1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ⊕ and ⊗, and their identities 0 and 1. Multiplication and addition must be associative. Multiplication must distribute over addition, and v ⊗ 0 The design of cdec separates the creation of a transl"
P10-4002,W06-3601,0,0.0172305,"s avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 20"
P10-4002,N03-1017,0,0.465271,", the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization"
P10-4002,P07-2045,1,0.0302608,"versity of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then"
P10-4002,P09-1019,1,0.765356,"ng et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLE"
P10-4002,D07-1104,1,0.804043,"ms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpot JVM, version 1.6.0 12. A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to"
P10-4002,D09-1005,0,0.00908811,"oal a 1 shell 100 a 1 2 1 little 101 1 little 1 small Goal ll se hou 1 small small she sma NN little ll JJ 010 110 1 a little 1 house 1 shell Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines (small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distortion limit of 1 (right). must equal 0. Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). Since semirings are such a useful abstraction, cdec has been designed to facilitate implementation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring"
P10-4002,W09-0424,1,0.545644,"alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unif"
P10-4002,D08-1023,1,0.337071,"T03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. When used with sequential tagging models, this pipeline is identi"
P10-4002,P08-1024,1,0.647817,"nd alignment algorithms then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, th"
P10-4002,E09-1061,1,0.355868,"red forest: 1- or k-best derivations, feature expectations, or intersection with a target language reference (sentence or lattice). The last option generates an alignment forest, from which a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source"
P10-4002,J03-1006,0,0.00679634,"a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated. In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al., 2003). In word-based models, a single"
P10-4002,J93-2003,0,0.0420135,"st translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative tra"
P10-4002,P03-1021,0,0.088916,"d Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply usin"
P10-4002,P02-1040,0,0.103814,"whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure"
P10-4002,N09-1025,0,0.0233546,"s then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly sep"
P10-4002,J07-2003,0,0.969957,"ation techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on performance. We show experimentally that cdec uses less memory and time than comparabl"
P10-4002,N03-1028,0,0.299229,"ities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on p"
P10-4002,2006.amta-papers.25,0,0.0163455,"of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec conf"
P10-4002,N10-1128,1,0.770622,"nce pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-spec"
P10-4002,P08-1115,1,0.407306,"entation of all the derivations of the sentence pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kin"
P10-4002,N10-1033,1,\N,Missing
P11-1042,H05-1009,0,0.0582652,"si like like one of ”. that that is but since since hsi when , how , not 6 θk 3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems,"
P11-1042,N10-1083,0,0.158266,"Missing"
P11-1042,P06-1009,0,0.395676,"tion scores that are precomputed by looking at the full training data: Dice’s coefficient (discretized), which we use to measure association strength between pairs of source and target word types across sentence pairs (Dice, 1945), IBM Model 1 forward and reverse probabilities, and the geometric mean of the Model 1 forward and reverse probabilities. Finally, we also cluster the source and target vocabularies (Och, 1999) and include class pair indicator features, which can learn generalizations that, e.g., “nouns tend to translate into nouns but not modal verbs.” Positional features. Following Blunsom and Cohn (2006), we include features indicating closeness to the alignment matrix diagonal, aj j h(aj , j, m, n) = m − n . We also conjoin this feature with the source word class type indicator to enable the model to learn that certain word types are more or less likely to favor a location on the diagonal (e.g. Urdu’s sentence-final verbs). Source features. Some words are functional elements that fulfill purely grammatical roles and should not be the “source” of a translation. For example, Romance languages require a preposition in the formation of what could be a noun-noun compound in English, thus, it may"
P11-1042,P08-1024,0,0.0215045,"t, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM M"
P11-1042,D08-1023,0,0.0170163,"t, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM M"
P11-1042,bojar-prokopova-2006-czech,0,0.300645,"Missing"
P11-1042,J93-2003,0,0.180904,"r model yields better alignments than generative baselines in a number of language pairs. 1 Introduction Word alignment is an important subtask in statistical machine translation which is typically solved in one of two ways. The more common approach uses a generative translation model that relates bilingual string pairs using a latent alignment variable to designate which source words (or phrases) generate which target words. The parameters in these models can be learned straightforwardly from parallel sentences using EM, and standard inference techniques can recover most probable alignments (Brown et al., 1993). This approach is attractive because it only requires parallel training data. An alternative to the generative approach uses a discriminatively trained 409 alignment model to predict word alignments in the parallel corpus. Discriminative models are attractive because they can incorporate arbitrary, overlapping features, meaning that errors observed in the predictions made by the model can be addressed by engineering new and better features. Unfortunately, both approaches are problematic, but in different ways. In the case of discriminative alignment models, manual alignment data is required f"
P11-1042,J07-2003,0,0.0848504,"n test sets.10 While neither a decrease in the average singleton fertility nor an increase in the number of rules induced guarantees better alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experim"
P11-1042,P11-2031,1,0.446686,"Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relatively free word order. For this language pair, we evaluate alignment error rate using the manual alignment corpus described by Bojar and Prokopov´a (2006). Table 2 summarizes the results. Chinese-English. Chinese-English poses a different set of problems"
P11-1042,E09-1020,0,0.0365102,"Missing"
P11-1042,P10-1147,0,0.0270736,"0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith a"
P11-1042,N10-1128,1,0.745719,"se problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fe"
P11-1042,P10-4002,1,0.451451,"Missing"
P11-1042,E09-1037,1,0.831167,"as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM Model 3) useful for translation and alignment modeling. To be truly general, it must be possible to utilize such features. Unfortunately, features like this that depend on global properties of the alignment vector, a, make 417 the inference problem NP-hard, and approximations are necessary. Fortunately, there is much recent work on approximate inference techniques for incorporating nonlocal features (Blunsom et al., 2008b; Gimpel and Smith, 2009; Cromi`eres and Kurohashi, 2009; Weiss and Taskar, 2010), suggesting that this problem too can be solved using established techniques. 8 Conclusion We have introduced a globally normalized, loglinear lexical translation model that can be trained discriminatively using only parallel sentences, which we apply to the problem of word alignment. Our approach addresses two important shortcomings of previous work: (1) that local normalization of generative models constrains the features that can be used, and (2) that previous discriminatively trained word alignment models required supervised alignme"
P11-1042,N06-2013,0,0.0188684,"ring new and better features. Unfortunately, both approaches are problematic, but in different ways. In the case of discriminative alignment models, manual alignment data is required for training, which is problematic for at least three reasons. Manual alignments are notoriously difficult to create and are available only for a handful of language pairs. Second, manual alignments impose a commitment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data (Habash and Sadat, 2006) or may be constrained by requirements of other processing components, such parsers. Third, the “correct” alignment annotation for different tasks may vary: for example, relatively denser or sparser alignments may be optimal for different approaches to (downstream) translation model induction (Lopez, 2008; Fraser, 2007). Generative models have a different limitation: the joint probability of a particular setting of the random variables must factorize according to steps in a process that successively “generates” the values of the variables. At each step, the probability of some value being gene"
P11-1042,P09-1104,0,0.020558,"3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastiv"
P11-1042,N03-1017,0,0.0190572,"d confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation up and running. Future work will explore the scalability characteristics and limits of the model. 5.1 Methodology For each language pair, we train two log-linear translation models as described above (§3), once with English as the source and once with English as the target language. For a baseline, we use the Giza++ toolkit (Och and Ney, 2003) to learn Model 4, again in both directions. We symmetrize the alignments from both model types using the grow-diag-final-and heuristic (Koehn et al., 2003) producing, in total, six alignment sets. We evaluate them both intrinsically and in terms of their performance in a translation system. Since we only have gold alignments for CzechEnglish (Bojar and Prokopov´a, 2006), we can report alignment error rate (AER; Och and Ney, 2003) only for this pair. However, we offer two further measures that we believe are suggestive and that do not require gold alignments. One is the average alignment “fertility” of source words that occur only a single time in the training data (so-called hapax legomena). This assesses the impact of a typical alignment proble"
P11-1042,P09-1019,1,0.827278,"xtracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech word"
P11-1042,N09-1069,0,0.0123084,"depicted graph is determined by the features that we use (§4). the gradient of the unregularized objective (Tsuruoka et al., 2009). This method is quite attractive since it is only necessary to represent the active features, meaning impractically large feature spaces can be searched provided the regularization strength is sufficiently high. Additionally, not only has this technique been shown to be very effective for optimizing convex objectives, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective of the generative variant). To choose the regularization strength β and the initial learning rate η0 ,3 we trained several models on a 10,000-sentence-pair subset of the FrenchEnglish Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). For the remainder of the experiments, we use the values we obtained, β = 0.4 and η0 = 0.3. 3.2 Inference with WFSAs We now describe how to use"
P11-1042,D09-1106,0,0.0217753,"ed guarantees better alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” m"
P11-1042,J10-3002,0,0.0438443,"92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation techn"
P11-1042,C08-1064,0,0.0291766,"only for a handful of language pairs. Second, manual alignments impose a commitment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data (Habash and Sadat, 2006) or may be constrained by requirements of other processing components, such parsers. Third, the “correct” alignment annotation for different tasks may vary: for example, relatively denser or sparser alignments may be optimal for different approaches to (downstream) translation model induction (Lopez, 2008; Fraser, 2007). Generative models have a different limitation: the joint probability of a particular setting of the random variables must factorize according to steps in a process that successively “generates” the values of the variables. At each step, the probability of some value being generated may depend only on the generation history (or a subset thereof), and the possible values a variable will take must form a locally normalized conditional probability distribution (CPD). While these locally normalized CPDs may be paProceedings of the 49th Annual Meeting of the Association for Computat"
P11-1042,W03-0301,0,0.0383228,"s, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective of the generative variant). To choose the regularization strength β and the initial learning rate η0 ,3 we trained several models on a 10,000-sentence-pair subset of the FrenchEnglish Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). For the remainder of the experiments, we use the values we obtained, β = 0.4 and η0 = 0.3. 3.2 Inference with WFSAs We now describe how to use weighted finite-state automata (WFSAs) to compute the quantities neces3.1 Parameter Learning sary for training. We begin by describing the ideal To learn the parameters of our model, we select the WFSA representing the full translation search space, θ ∗ that minimizes the `1 regularized conditional log- which we call the discriminative neighborhood, and likelihood of a set of training data T : then discuss strategies for reducing its size in the X X X"
P11-1042,H05-1011,0,0.332958,"alignment variable a = ha1 , a2 , . . . , an i ∈ [0, m]n , where aj = 0 represents a special null token. X p(t |s, n) = p(t, a |s, n) 2 3 Model In this section, we develop a conditional model p(t |s) that, given a source language sentence s with length m = |s|, assigns probabilities to a target sentence t with length n, where each word tj is an element in the finite target vocabulary Ω. We begin by using the chain rule to factor this probability into two components, a translation model and a length model. p(t |s) = p(t, n |s) = p(t |s, n) × p(n |s) |{z } |{z } translation model length model 1 Moore (2005) likewise uses this example to motivate the need for models that support arbitrary, overlapping features. 410 a So far, our model is identical to that of (Brown et al., 1993); however, we part ways here. Rather than using the chain rule to further decompose this probability and motivate opportunities to make independence assumptions, we use a log-linear model with parameters θ ∈ Rk and feature vector function H that maps each tuple ha, s, t, ni into Rk to model p(t, a |s, n) directly: pθ (t, a |s, n) = Zθ (s, n) = exp θ &gt; H(t, a, s, n) , where Zθ (s, n) X X exp θ &gt; H(t0 , a0 , s, n) t0 ∈Ωn a0"
P11-1042,J03-1002,0,0.00681142,"translation. 9 http://statmt.org/wmt10 414 der in Chinese and Urdu, morphological complexity in Czech, and a non-alphabetic writing system in Chinese), and confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation up and running. Future work will explore the scalability characteristics and limits of the model. 5.1 Methodology For each language pair, we train two log-linear translation models as described above (§3), once with English as the source and once with English as the target language. For a baseline, we use the Giza++ toolkit (Och and Ney, 2003) to learn Model 4, again in both directions. We symmetrize the alignments from both model types using the grow-diag-final-and heuristic (Koehn et al., 2003) producing, in total, six alignment sets. We evaluate them both intrinsically and in terms of their performance in a translation system. Since we only have gold alignments for CzechEnglish (Bojar and Prokopov´a, 2006), we can report alignment error rate (AER; Och and Ney, 2003) only for this pair. However, we offer two further measures that we believe are suggestive and that do not require gold alignments. One is the average alignment “fert"
P11-1042,E99-1010,0,0.034305,"phic feature was computed after first applying a heuristic Romanization, which made the orthographic forms somewhat comparable. 413 regardless of length). We also include “global” association scores that are precomputed by looking at the full training data: Dice’s coefficient (discretized), which we use to measure association strength between pairs of source and target word types across sentence pairs (Dice, 1945), IBM Model 1 forward and reverse probabilities, and the geometric mean of the Model 1 forward and reverse probabilities. Finally, we also cluster the source and target vocabularies (Och, 1999) and include class pair indicator features, which can learn generalizations that, e.g., “nouns tend to translate into nouns but not modal verbs.” Positional features. Following Blunsom and Cohn (2006), we include features indicating closeness to the alignment matrix diagonal, aj j h(aj , j, m, n) = m − n . We also conjoin this feature with the source word class type indicator to enable the model to learn that certain word types are more or less likely to favor a location on the diagonal (e.g. Urdu’s sentence-final verbs). Source features. Some words are functional elements that fulfill purely"
P11-1042,P02-1040,0,0.108705,"nments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relativel"
P11-1042,D10-1052,1,0.841806,"?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is glob"
P11-1042,P05-1044,1,0.911055,"= 0.01) on the translation distributions and use the empirical Bayes (EB) method to infer a point estimate, using variational inference. Table 1: Comparison of alternative definitions Ωs (arrows indicate whether higher or lower is better). P Ωs time (s) ↓ AER ↓ s |Ωs |↓ =Ω 22.4 86.0M 0.0 co-occ. 8.9 0.68M 0.0 Model 1 0.2 0.38M 6.2 EB-Model 1 1.0 0.15M 2.9 Table 1 compares the average per-sentence time required to run the inference algorithm described 5 Future work will explore alternative formulations of the discriminative neighborhood with the goal of further improving inference efficiency. Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. above under these four different definitions of Ωs on a 10,000 sentence subset of the Hansards FrenchEnglish corpus that includes manual word alignments. While our constructions guarantee that all references are reachable even in the reduced neighborhoods, not all alignments between source and target are possible. The last column is the oracle AER. Although EB variant of Model 1 neighborhood is slightly more expensive to"
P11-1042,2006.amta-papers.25,0,0.0389463,"guage model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relatively free word order. For this language pair, we evaluate alignment erro"
P11-1042,takezawa-etal-2002-toward,0,0.0213892,"word translates as itself (for example, a name or a date, which occurs in languages that share the same alphabet) in position j, but then is translated again (as something else) in position j − 1 or j + 1. 5 Experiments We now turn to an empirical assessment of our model. Using various datasets, we evaluate the performance of the models’ intrinsic quality and theirtheir alignments’ contribution to a standard machine translation system. We make use of parallel corpora from languages with very different typologies: a small (0.8M words) Chinese-English corpus from the tourism and travel domain (Takezawa et al., 2002), a corpus of Czech-English news commentary (3.1M words),9 and an Urdu-English corpus (2M words) provided by NIST for the 2009 Open MT Evaluation. These pairs were selected since each poses different alignment challenges (word or8 This is of course what makes history-based language model integration an inference challenge in translation. 9 http://statmt.org/wmt10 414 der in Chinese and Urdu, morphological complexity in Czech, and a non-alphabetic writing system in Chinese), and confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation u"
P11-1042,H05-1010,0,0.0995513,"is but since since hsi when , how , not 6 θk 3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in"
P11-1042,P09-1054,0,0.228503,"me using dynamic programming. For example, when the graph has a sequential structure, exact inference can be carried out using the familiar forwardbackward algorithm (Lafferty et al., 2001). Although our features look at more structure than this, they are designed to keep treewidth low, meaning exact inference is still possible with dynamic programming. Figure 1 gives a graphical representation of our model as well as the more familiar generative (directed) variants. The edge set in the depicted graph is determined by the features that we use (§4). the gradient of the unregularized objective (Tsuruoka et al., 2009). This method is quite attractive since it is only necessary to represent the active features, meaning impractically large feature spaces can be searched provided the regularization strength is sufficiently high. Additionally, not only has this technique been shown to be very effective for optimizing convex objectives, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective o"
P11-1042,2008.amta-papers.18,1,0.874421,"er alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate trainin"
P11-1042,C96-2141,0,0.939204,"rization and only depends on hood, the set Ωn ×[0, m]n , such that every path from 2 One way to understand expressiveness is in terms of indethe start state to goal yields a pair ht0 , ai with weight pendence assumptions, of course. Research in graphical models has done much to relate independence assumptions to the complexity of inference algorithms (Koller and Friedman, 2009). 411 3 For the other free parameters of the algorithm, we use the default values recommended by Tsuruoka et al. (2009). s s n n s a1 a2 a3 ... an t1 t2 t3 ... tn a1 s s a2 s t1 Fully directed model (Brown et al., 1993; Vogel et al., 1996; Berg-Kirkpatrick et al., 2010) s a3 ... t3 ... s t2 s an s tn Our model Figure 1: A graphical representation of a conventional generative lexical translation model (left) and our model with an undirected translation model. For clarity, the observed node s (representing the full source sentence) is drawn in multiple locations. The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value). H(t0 , a, s, n). With our feature set (§4), number of states in this WFSA is O(m × n) since at each target index j, there is a different state for each possible index of th"
P11-2031,W05-0909,1,0.0850933,"m is the average of all mi s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find differ"
P11-2031,J08-1003,0,0.0192515,"Missing"
P11-2031,W08-0304,0,0.0315931,"ther uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental"
P11-2031,N10-1080,0,0.00491755,"eous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system"
P11-2031,D08-1024,0,0.049401,"initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development 2 This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. 3 Online subgradient techniques such as MIRA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics data and held-out test data, independently of any experimental manipulation. Thus, when trying to determine whether the difference between two measurements is significant, it is necessary to control for variance due to noisy parameter estimates. This can be done by replication of the optimization proce"
P11-2031,J07-2003,0,0.277048,"e MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT imple"
P11-2031,M93-1008,0,0.427905,"rocessing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to impro"
P11-2031,N10-1031,1,0.375888,"i s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise"
P11-2031,P10-4002,1,0.432465,"er to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 ra"
P11-2031,N09-1046,1,0.489862,"ebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 random restart points per iteration, drawn uniformly from the default ranges for each feature, and, at each iteration, 200-best lists were extracted with the current weight vector (Bertoldi et al., 2009). The cdec MERT implementation performs inference over the decoder search space which is structured as a hypergraph (Kumar et al., 2009). Rather than"
P11-2031,W09-0439,0,0.0241928,"been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize B"
P11-2031,P07-2045,1,0.0508851,"periments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Mo"
P11-2031,W04-3250,0,0.348961,"ine result paired with a low experimental result could lead to a useful experimental manipulation being incorrectly identified as useless. We now turn to the question of how to reduce the probability falling into this trap. 3 Related Work The use of statistical hypothesis testing has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instab"
P11-2031,P09-1019,1,0.208024,"egmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 random restart points per iteration, drawn uniformly from the default ranges for each feature, and, at each iteration, 200-best lists were extracted with the current weight vector (Bertoldi et al., 2009). The cdec MERT implementation performs inference over the decoder search space which is structured as a hypergraph (Kumar et al., 2009). Rather than using restart points, in addition to optimizing each feature independently, it optimizes in 5 random directions per iteration by constructing a search vector by uniformly sampling each element of the vector from (−1, 1) and then renormalizing so it has length 1. For all systems, the initial weight vector was manually initialized so as to yield reasonable translations. 4 http://statmt.org/wmt11/ System Avg ssel sdev BTEC Chinese-English (n = 300) System A 48.4 1.6 0.2 BLEU ↑ System B 49.9 1.5 0.1 System A 63.3 0.9 MET ↑ System B 63.8 0.9 System A 30.2 1.1 TER ↓ System B 28.7 1.0 W"
P11-2031,C08-1074,0,0.00521439,"8). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pa"
P11-2031,P03-1021,0,0.949219,"proves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately. 1 2 Introduction The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003). In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets. However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters.1 While the existence of optimizer instability is an acknowledged problem, it is only infrequently discussed in relation to the reliability of experimental results, and, to our knowledge, there has yet to be a systematic study of its effects on 1 We hypothesize that the convention of “trusting”"
P11-2031,P02-1040,0,0.112437,"for the ith optimization run, and m is the average of all mi s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, dif"
P11-2031,W05-0908,0,0.060395,"has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significanc"
P11-2031,2006.amta-papers.25,0,0.190763,"over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise due to this variable can depend"
P11-2031,zhang-etal-2004-interpreting,0,0.00388873,"ired with a low experimental result could lead to a useful experimental manipulation being incorrectly identified as useless. We now turn to the question of how to reduce the probability falling into this trap. 3 Related Work The use of statistical hypothesis testing has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our c"
P12-1002,P08-1024,0,0.0377061,"sizes on small development sets. Our software is freely available as a part of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to larg"
P12-1002,D08-1024,0,0.736009,"Missing"
P12-1002,N09-1025,0,0.49132,"Missing"
P12-1002,P05-1033,0,0.0405714,"8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n-best reranking. In contrast to our work, Duh et al. (2010) did not incorporate multitask learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools. 3 Local Features for Synchronous CFGs The work described in this paper is based on the SMT framework of hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG. Examples are rules like (1)-(3) shown in Figure 1. Local features are designed to be readable directly off the rule at decoding time. We use three rule templates in our work: Rule identifiers: These features identify each rule by a unique identifier. Such features correspond to the relative frequencies of rewrites rules used in standard models. Rule n-grams: These features identify n-grams of consecutive items in a rule. We use bigrams on source-sides of ru"
P12-1002,J07-2003,0,0.750409,"s on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n-best reranking. In contrast to our work, Duh et al. (2010) did not incorporate multitask learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools. 3 Local Features for Synchronous CFGs The work described in this paper is based on the SMT framework of hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG. Examples are rules like (1)-(3) shown in Figure 1. Local features are designed to be readable directly off the rule at decoding time. We use three rule templates in our work: Rule identifiers: These features identify each rule by a unique identifier. Such features correspond to the relative frequencies of rewrites rules used in standard models. Rule n-grams: These features identify n-grams of consecutive items in a rule. We use bigrams on source-sides of rules. Such featu"
P12-1002,W02-1001,0,0.0653264,"i)+ where (a)+ = max(0, a) , w ∈ IRD is a weight vector, and h·, ·i denotes the standard vector dot product. Instantiating SGD to the following stochastic 2 Similar “monolingual parse features” have been used in Dyer et al. (2011). 13 subgradient leads to the perceptron algorithm for pairwise ranking3 (Shen and Joshi, 2005): ( −¯ xj if hw, x ¯j i ≤ 0, ∇lj (w) = 0 else. Our baseline algorithm 1 (SDG) scales pairwise ranking to large scale scenarios. The algorithm takes an average over the final weight updates of each epoch instead of keeping a record of all weight updates for final averaging (Collins, 2002) or for voting (Freund and Schapire, 1999). Algorithm 1 SGD: int I, T , float η Initialize w0,0,0 ← 0. for epochs t ← 0 . . . T − 1: do for all i ∈ {0 . . . I − 1}: do Decode ith input with wt,i,0 . for all pairs xj , j ∈ {0 . . . P − 1}: do wt,i,j+1 ← wt,i,j − η∇lj (wt,i,j ) end for wt,i+1,0 ← wt,i,P end for wt+1,0,0 ← wt,I,0 end for return 1 T T P wt,0,0 t=1 While stochastic learning exhibits a runtime behavior that is linear in sample size (Bottou, 2004), very large datasets can make sequential processing infeasible. Algorithm 2 (MixSGD) addresses this problem by parallelization in the fram"
P12-1002,P08-2010,0,0.0552385,"scriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative"
P12-1002,W10-1757,0,0.280326,"ms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n-best reranking. In contrast to our work, Duh et al. (2010) did not incorporate multitask learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools. 3 Local Features for Synchronous CFGs The work described in this paper is based on the SMT framework of hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG. Ex"
P12-1002,P10-4002,1,0.613628,"rithm 4 performs feature selection based on a choice of meta-parameter of K features instead of by thresholding a regularization meta-parameter λ, however, these techniques are equivalent and can be transformed into each other. 5 Experiments 5.1 Data, Systems, Experiment Settings The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in Table 1. The translation direction is German-to-English. The SMT framework used in our experiments is hierarchical phrase-based translation (Chiang, 2007). We use the cdec decoder5 (Dyer et al., 2010) and induce SCFG grammars from two sets of symmetrized alignments using the method described by Chiang (2007). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal"
P12-1002,W11-2139,1,0.852474,"a feature vector x ∈ IRD where preference pairs for training are prepared by sorting translations according to smoothed sentence-wise BLEU score (Liang et al., 2006a) against the reference. For a (1) (2) (1) preference pair xj = (xj , xj ) where xj is pre(2) (1) (2) ferred over xj , and x ¯j = xj − xj , we consider the following hinge loss-type objective function: lj (w) = (− hw, x ¯j i)+ where (a)+ = max(0, a) , w ∈ IRD is a weight vector, and h·, ·i denotes the standard vector dot product. Instantiating SGD to the following stochastic 2 Similar “monolingual parse features” have been used in Dyer et al. (2011). 13 subgradient leads to the perceptron algorithm for pairwise ranking3 (Shen and Joshi, 2005): ( −¯ xj if hw, x ¯j i ≤ 0, ∇lj (w) = 0 else. Our baseline algorithm 1 (SDG) scales pairwise ranking to large scale scenarios. The algorithm takes an average over the final weight updates of each epoch instead of keeping a record of all weight updates for final averaging (Collins, 2002) or for voting (Freund and Schapire, 1999). Algorithm 1 SGD: int I, T , float η Initialize w0,0,0 ← 0. for epochs t ← 0 . . . T − 1: do for all i ∈ {0 . . . I − 1}: do Decode ith input with wt,i,0 . for all pairs xj ,"
P12-1002,N09-1046,1,0.762844,"e transformed into each other. 5 Experiments 5.1 Data, Systems, Experiment Settings The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in Table 1. The translation direction is German-to-English. The SMT framework used in our experiments is hierarchical phrase-based translation (Chiang, 2007). We use the cdec decoder5 (Dyer et al., 2010) and induce SCFG grammars from two sets of symmetrized alignments using the method described by Chiang (2007). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit of 200. 15 translate each individual sentence into separate files (so-called per-sentence grammars) (Lopez, 2007). When decoding,"
P12-1002,N12-1023,0,0.418187,"include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n"
P12-1002,2009.iwslt-papers.3,0,0.0529315,"overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million fe"
P12-1002,W11-2123,0,0.112348,"Missing"
P12-1002,D11-1125,0,0.55458,"ork. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have"
P12-1002,N03-1017,0,0.0233465,"atures are the 3 templates described in Section 3. All feature weights were tuned together using algorithms 1-4. If not indicated otherwise, the perceptron was run for 10 epochs with learning rate η = 0.0001, started at zero weight vector, using deduplicated 100-best lists. The results on the news-commentary (nc) data show that training on the development set does not benefit from adding large feature sets – BLEU result differences between tuning 12 default features 8 negative log relative frequency p(e|f ); log count(f ); log count(e, f ); lexical translation probability p(f |e) and p(e|f ) (Koehn et al., 2003); indicator variable on singleton phrase e; indicator variable on singleton phrase pair f, e; word penalty; language model weight; OOV count of language model; number of untranslated words; Hiero glue rules (Chiang, 2007). Alg. 1 4 Tuning set Features #Feats devtest-ep test-ep † Tuning set test-crawl10 dev-crawl 15.39 † test-crawl11 14.43† dev-ep default 12 25.62 26.42 dev-ep +id,ng,shape 300k 27.84 28.37 dev-crawl 17.84 16.834 train-ep +id,ng,shape 100k 28.0 @9 28.62 train-ep 19.121 17.331 Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl"
P12-1002,P09-1019,1,0.699806,"ers (id), rule n-gram (ng), and rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the same feature group is indicated by raised algorithm number. † indicates statistically significant differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs chosen on the devtest set. pergraph as is done in the cdec implementation of MIRA. We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hypergraph-MERT (Kumar et al., 2009) both of which depend on hypergraph sampling. In contrast, the perceptron is deterministic when started from a zero-vector of weights and achieves favorable 28.0 BLEU on the news-commentary test set. Since we are interested in relative improvements over a stable baseline, we restrict our attention in all following experiments to the perceptron.7 Table 2 shows the results of the experimental comparison of the 4 algorithms of Section 4. The 7 Absolute improvements would be possible, e.g., by using larger language models or by adding news data to the ep training set when evaluating on crawl test"
P12-1002,P79-1022,0,0.29814,"Missing"
P12-1002,N06-1014,0,0.47942,"ted Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning s"
P12-1002,D07-1104,0,0.0967957,"unds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit of 200. 15 translate each individual sentence into separate files (so-called per-sentence grammars) (Lopez, 2007). When decoding, cdec loads the appropriate file immediately prior to translation of the sentence. The computational overhead is minimal compared to the expense of decoding. Also, deploying disk space instead of memory fits perfectly into the MapReduce framework we are working in. Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima’an, 2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well. 3-gram (news-commentary) and 5-gram (Europarl) language models are trai"
P12-1002,N10-1069,0,0.0955785,"do for all i ∈ {0 . . . S − 1}: do Decode ith input with wz,t,i,0 . for all pairs xj , j ∈ {0 . . . P − 1}: do wz,t,i,j+1 ← wz,t,i,j − η∇lj (wz,t,i,j ) end for wz,t,i+1,0 ← wz,t,i,P end for wz,t+1,0,0 ← wz,t,S,0 end for end for Collect final weights from each   machine, return 1 Z Z P z=1 1 T T P wz,t,0,0 . t=1 3 Other loss functions lead to stochastic versions of SVMs (Collobert and Bengio, 2004; Shalev-Shwartz et al., 2007; Chapelle and Keerthi, 2010). Algorithm 2 is a variant of the SimuParallelSGD algorithm of Zinkevich et al. (2010) or equivalently of the parameter mixing algorithm of McDonald et al. (2010). The key idea of algorithm 2 is to partition the data into disjoint shards, then train SGD on each shard in parallel, and after training mix the final parameters from each shard by averaging. The algorithm requires no communication between machines until the end. McDonald et al. (2010) also present an iterative mixing algorithm where weights are mixed from each shard after training a single epoch of the perceptron in parallel on each shard. The mixed weight vector is re-sent to each shard to start another epoch of training in parallel on each shard. This algorithm corresponds to our algorithm"
P12-1002,P00-1056,0,0.0184572,"ttings The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in Table 1. The translation direction is German-to-English. The SMT framework used in our experiments is hierarchical phrase-based translation (Chiang, 2007). We use the cdec decoder5 (Dyer et al., 2010) and induce SCFG grammars from two sets of symmetrized alignments using the method described by Chiang (2007). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit of 200. 15 translate each individual sentence into separate files (so-called per-sentence grammars) (Lopez, 2007). When decoding, cdec loads the appropriate file immediately prior to translation of the sentence. T"
P12-1002,P02-1038,0,0.177134,"re sets of various sizes on small development sets. Our software is freely available as a part of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been"
P12-1002,P03-1021,0,0.120216,"MT that can be read off from rules at runtime, and present a learning algorithm that applies `1 /`2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets. 1 Introduction The standard SMT training pipeline combines scores from large count-based translation models and language models with a few other features and tunes these using the well-understood line-search technique for error minimization of Och (2003). If only a handful of dense features need to be tuned, minimum error rate training can be done on small tuning sets and is hard to beat in terms of accuracy and efficiency. In contrast, the promise of largescale discriminative training for SMT is to scale to arbitrary types and numbers of features and to provide sufficient statistical support by parameter estimation on large sample sizes. Features may be lexicalized and sparse, non-local and overlapping, or One possible reason why discriminative SMT has mostly been content with small tuning sets lies in the particular design of the features t"
P12-1002,2001.mtsummit-papers.68,0,0.0170286,"Missing"
P12-1002,W05-0908,1,0.875969,"Missing"
P12-1002,N04-1023,0,0.148515,"ly available as a part of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regul"
P12-1002,P06-1091,0,0.368282,"ly expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who"
P12-1002,E12-1083,1,0.861222,"Missing"
P12-1002,2006.iwslt-evaluation.14,0,0.099557,"art of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. Howev"
P12-1002,D07-1080,0,0.504754,"t numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, B"
P12-1002,P02-1040,0,\N,Missing
P12-1002,P06-1096,0,\N,Missing
P13-1018,2005.iwslt-1.8,0,0.0958689,"Missing"
P13-1018,C10-2010,0,0.0480398,"Missing"
P13-1018,J93-2003,0,0.0313653,"Missing"
P13-1018,W06-1008,0,0.429388,"Missing"
P13-1018,P11-2008,0,0.0238097,"Missing"
P13-1018,W12-3153,0,0.264353,"Missing"
P13-1018,N03-1017,0,0.0156335,"Missing"
P13-1018,P08-1113,0,0.141099,"Missing"
P13-1018,P03-1021,0,0.0199252,"Missing"
P13-1018,P02-1040,0,0.105883,"Missing"
P13-1018,W12-3152,0,0.0549925,"Missing"
P13-1018,J03-3002,0,0.404661,"Missing"
P13-1018,N10-1063,0,0.0989051,"Missing"
P13-1018,N12-1079,0,0.229942,"Missing"
P13-1018,C10-1124,0,0.0203108,"Missing"
P13-1018,2005.mtsummit-papers.11,0,0.147745,"Missing"
P13-1018,C96-2141,0,0.295961,"Missing"
P13-1018,I08-2120,0,0.398573,"Missing"
P13-1018,2005.eamt-1.37,0,0.138002,"Missing"
P13-1018,N12-1006,0,0.0533017,"Missing"
P13-1018,N03-1031,0,\N,Missing
P13-2136,W09-0420,0,0.0223645,"Missing"
P13-2136,P10-1156,0,0.0609848,"Missing"
P13-2136,J92-4003,0,0.468129,"score when using bilingual word clusters instead of monolingual clusters. 1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surpris"
P13-2136,2012.eamt-1.60,0,0.0204639,"zer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3 . The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5 , which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora are of different sizes and hence the monolingual German data from every parallel corpus is different. We treat the F1 score Inference Figure 1 shows th"
P13-2136,E03-1009,0,0.0459483,"nstead of monolingual clusters. 1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence mode"
P13-2136,D11-1005,0,0.0536264,"Missing"
P13-2136,P08-1068,0,0.0546467,"olingual clusters. 1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence model to define the mo"
P13-2136,D09-1092,0,0.0706998,"Missing"
P13-2136,N13-1073,1,0.762293,"evelopment set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5 , which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora are of different sizes and hence the monolingual German data from every parallel corpus is different. We treat the F1 score Inference Figure 1 shows the factor graph representation of our clustering models. Finding the optimal clustering under both the monolingual and bilingual objectives is a computationally hard combinatorial optimization problem (Och, 1995). We use a greedy hil"
P13-2136,E12-1064,1,0.874828,"Missing"
P13-2136,E99-1010,0,0.0664772,"for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence model to define the monolingual clustering objective. Let ci denote the word class of word wi . Our objective assumes that the probability of a word sequence w = hw1 , w2 , . . . , wM i is p(w) = M Y i=1 p(ci |ci−1 ) × p(wi |ci ), (2.1"
P13-2136,D09-1015,0,0.0153308,"2010). We therefore chose to focus our evaluation on the latter problem. For our evaluation, we use our word clusters as an input to a named entity recognizer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3 . The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5 , which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora"
P13-2136,P98-2193,0,0.0481045,"Missing"
P13-2136,D07-1014,0,0.0619852,"Missing"
P13-2136,P08-1084,0,0.028793,"Missing"
P13-2136,N12-1052,0,0.185105,"Missing"
P13-2136,P10-1040,0,0.734145,"1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence model to define the monolingual clustering o"
P13-2136,C98-2188,0,\N,Missing
P14-1024,W07-0103,0,0.295543,"eses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using"
P14-1024,W06-1670,0,0.0180673,"Missing"
P14-1024,levin-etal-2014-resources,1,0.819369,"selection of the training samples. Thus, we trust that annotator judgments were not biased towards the cases that the system is trained to process. Multilingual test sets We collect and annotate metaphoric and literal test sentences in four languages. Thus, we compile eight test datasets, four for SVO relations, and four for AN relations. Each dataset has an equal number of metaphors and non-metaphors, i.e., the datasets are balanced. English (EN) and Russian (RU) datasets have been compiled by our team and are publicly available. Spanish (ES) and Farsi (FA) datasets are published elsewhere (Levin et al., 2014). Table 1 lists test set sizes. EN RU ES FA SVO AN 222 240 220 44 200 200 120 320 5 5.1 Experiments English experiments Our task, as defined in Section 2, is to classify SVO and AN relations as either metaphoric or literal. We first conduct a 10-fold cross-validation experiment on the training set defined in Section 4.1. We represent each candidate relation using the features described in Section 3.2, and evaluate performance of the three feature categories and their combinations. This is done by computing an accuracy in the 10-fold cross validation. Experimental results are given in Table 2,"
P14-1024,D10-1004,0,0.013166,"ets are associated with the supersense noun.body. Therefore, the value of the feature noun.body is 4/38 ≈ 0.11. 3.3 4 Cross-lingual feature projection Datasets In this section we describe a training and testing dataset as well a data collection procedure. 4.1 English training sets To train an SVO metaphor classifier, we employ the TroFi (Trope Finder) dataset.17 TroFi includes 3,737 manually annotated English sentences from the Wall Street Journal (Birke and Sarkar, 2007). Each sentence contains either literal or metaphorical use for one of 50 English verbs. First, we use a dependency parser (Martins et al., 2010) to extract subject-verb-object (SVO) relations. Then, we filter extracted relations to eliminate parsing-related errors, and relations with verbs which are not in the TroFi verb list. After filtering, there are 953 metaphorical and 656 literal SVO relations which we use as a training set. In the case of AN relations, we construct and make publicly available a training set containing 884 metaphorical AN pairs and 884 pairs with literal meaning. It was collected by two annotators using public resources (collections of metaphors on the web). At least one additional person carefully examined and"
P14-1024,E14-1049,1,0.391097,"metaphorical and 656 literal SVO relations which we use as a training set. In the case of AN relations, we construct and make publicly available a training set containing 884 metaphorical AN pairs and 884 pairs with literal meaning. It was collected by two annotators using public resources (collections of metaphors on the web). At least one additional person carefully examined and culled the collected metaphors, by removing duplicates, weak metaphors, and metaphorical phrases (such as Vector space word representations. We employ 64-dimensional vector-space word representations constructed by Faruqui and Dyer (2014).14 Vector construction algorithm is a variation on traditional latent semantic analysis (Deerwester et al., 1990) that uses multilingual information to produce representations in which synonymous words have similar vectors. The vectors were curacy during cross-validation. 12 For the full taxonomy see http://www.sfs. uni-tuebingen.de/lsd/adjectives.shtml 13 http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz 14 http://www.cs.cmu.edu/˜mfaruqui/soft. html 15 http://www.statmt.org/wmt11/ http://www.babylon.com 17 http://www.cs.sfu.ca/˜anoop/students/ jbirke/ 16 251 drowning students) whose in"
P14-1024,J04-1002,0,0.0944408,"ly or not. Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-"
P14-1024,D11-1006,0,0.0129329,"l et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in othe"
P14-1024,W06-3506,0,0.227977,"cond, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for R"
P14-1024,W97-0802,0,0.298022,"membership in different supersenses are represented by feature vectors, where each element corresponds to one supersense. For example, the word head (when used as a noun) participates in 33 synsets, three of which are related to the supersense noun.body. The value of the feature corresponding to this supersense is 3/33 ≈ 0.09. Supersenses of adjectives. WordNet lacks coarse-grained semantic categories for adjectives. To divide adjectives into groups, Tsvetkov et al. (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). For example, the top-level classes in GermaNet include: adj.feeling (e.g., willing, pleasant, cheerful); adj.substance (e.g., dry, ripe, creamy); adj.spatial (e.g., adjacent, gigantic).12 For each adjective type in WordNet, they produce a vector with a classifier posterior probabilities corresponding to degrees of membership of this word in one of the 13 semantic classes,13 similar to the feature vectors we build for nouns and verbs. For example, for a word calm the top-2 categories (with the first and second highest degrees of membership) are adj.behavior and adj.feeling. For languages othe"
P14-1024,W13-0907,0,0.748517,"may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that"
P14-1024,P12-1092,0,0.00373356,"babilities of classifier predictions. We binarize these posteriors into abstractconcrete (or imageable-unimageable) boolean indicators using pre-defined thresholds.11 Perfor• Vector space word representations. Vector space word representations learned using unsupervised algorithms are often effective features in supervised learning methods (Turian et al., 2010). In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al., 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). In a recent study, Mikolov et al. (2013) reveal an interesting cross-lingual property of distributed word representations: there is a strong similarity between the vector spaces across languages that can be easily captured by linear mapping. Thus, vector space models can also be seen as vectors of (latent) semantic concepts, that preserve their “meaning” across languages. 3 Classification using Random Forests 8 See Theorem 1.2 in (Breiman, 2001) for details. In our experiments, random forests model slightly outperformed logistic regression and SV"
P14-1024,N13-1076,1,0.7392,"lkit to train our classifiers (Pedregosa et al., 2011). Supersenses are particularly attractive features for metaphor detection: coarse sense taxonomies can be viewed as semantic concepts, and since concept mapping is a process in which metaphors are born, we expect different supersense co-occurrences in metaphoric and literal combinations. In “drinks gasoline”, for example, mapping to supersenses would yield a pair &lt;verb.consumption, noun.substance>, contrasted with &lt;verb.consumption, noun.food> for “drinks juice”. In addition, this coarse semantic categorization is preserved in translation (Schneider et al., 2013), which makes supersense features suitable for cross-lingual approaches such as ours. 3.2 Feature extraction Abstractness and imageability. The MRC psycholinguistic database is a large dictionary listing linguistic and psycholinguistic attributes obtained experimentally (Wilson, 1988).10 It includes, among other data, 4,295 words rated by the degrees of abstractness and 1,156 words rated by the imageability. Similarly to Tsvetkov et al. (2013), we use a logistic regression classifier to propagate abstractness and imageability scores from MRC ratings to all words for which we have vector space"
P14-1024,N13-1118,0,0.140905,"tive component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors ar"
P14-1024,P13-1117,0,0.0225815,"e and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide support for the hypothesis that metaphors are concepWe show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets"
P14-1024,shutova-teufel-2010-metaphor,0,0.121436,"Missing"
P14-1024,C10-1113,0,0.331036,"6 0.4 EN (area = 0.92) ES (area = 0.73) FA (area = 0.83) RU (area = 0.8) 0.2 0.0 0.0 0.2 0.4 0.6 False Positive Rate 0.8 1.0 (b) AN Figure 2: Cross-lingual experiment: ROC curves for classifiers trained on the English data using a combination of all features, and applied to SVO and AN metaphoric and literal relations in four test languages: English, Russian, Spanish, and Farsi. 6 For a historic overview and a survey of common approaches to metaphor detection, we refer the reader to recent reviews by Shutova et al. (Shutova, 2010; Shutova et al., 2013). Here we focus only on recent approaches. Shutova et al. (2010) proposed a bottom-up method: one starts from a set of seed metaphors and seeks phrases where verbs and/or nouns belong to the same cluster as verbs or nouns in seed examples. Turney et al. (2011) show how abstractness scores could be used to detect metaphorical AN phrases. Neuman et al. (2013) describe a Concrete Category Overlap algorithm, where co-occurrence statistics and Turney’s abstractness scores are used to determine WordNet supersenses that correspond to literal usage of a given adjective or verb. For example, given an adjective, we can learn that it modifies concrete nouns that usua"
P14-1024,J13-2003,0,0.144988,"ard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2013; Kozhenikov and Titov, 2013), we provide"
P14-1024,P10-1071,0,0.0435314,"ine translation, dialog systems, sentiment analysis, and text analytics, etc.) would have access to a potentially useful high-level bit of information about whether something is to be understood literally or not. Second, scientific hypotheses about metaphoric language could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions:"
P14-1024,W13-0909,0,0.256984,"the scope of metaphor identification by including nominal metaphoric relations as well as explore techniques for incorporating contextual features, which can play a key role in identifying certain kinds of metaphors. Second, cross-lingual model transfer can be improved with more careful cross-lingual feature projection. Broadwell et al. (2013) argue that metaphors are highly imageable words that do not belong to a discussion topic. To implement this idea, they extend MRC imageability scores to all dictionary words using links among WordNet supersenses (mostly hypernym and hyponym relations). Strzalkowski et al. (2013) carry out experiments in a specific (government-related) domain for four languages: English, Spanish, Farsi, and Russian. Strzalkowski et al. (2013) explain the algorithm only for English and say that is the same for Spanish, Farsi, and Russian. Because they heavily rely on WordNet and availability of imageability scores, their approach may not be applicable to low-resource languages. Hovy et al. (2013) applied tree kernels to metaphor detection. Their method also employs WordNet supersenses, but it is not clear from the description whether WordNet is essential or can be replaced with some ot"
P14-1024,Q13-1001,0,0.0499428,"Missing"
P14-1024,W13-0906,1,0.94438,"tance>, contrasted with &lt;verb.consumption, noun.food> for “drinks juice”. In addition, this coarse semantic categorization is preserved in translation (Schneider et al., 2013), which makes supersense features suitable for cross-lingual approaches such as ours. 3.2 Feature extraction Abstractness and imageability. The MRC psycholinguistic database is a large dictionary listing linguistic and psycholinguistic attributes obtained experimentally (Wilson, 1988).10 It includes, among other data, 4,295 words rated by the degrees of abstractness and 1,156 words rated by the imageability. Similarly to Tsvetkov et al. (2013), we use a logistic regression classifier to propagate abstractness and imageability scores from MRC ratings to all words for which we have vector space representations. More specifically, we calculate the degree of abstractness and imageability of all English items that have a vector space representation, using vector elements as features. We train two separate classifiers for abstractness and imageability on a seed set of words from the MRC database. Degrees of abstractness and imageability are posterior probabilities of classifier predictions. We binarize these posteriors into abstractconcr"
P14-1024,tsvetkov-etal-2014-augmenting-english,1,0.424069,"tions. Supersenses of nouns and verbs. A lexical item can belong to several synsets, which are associated with different supersenses. Degrees of membership in different supersenses are represented by feature vectors, where each element corresponds to one supersense. For example, the word head (when used as a noun) participates in 33 synsets, three of which are related to the supersense noun.body. The value of the feature corresponding to this supersense is 3/33 ≈ 0.09. Supersenses of adjectives. WordNet lacks coarse-grained semantic categories for adjectives. To divide adjectives into groups, Tsvetkov et al. (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). For example, the top-level classes in GermaNet include: adj.feeling (e.g., willing, pleasant, cheerful); adj.substance (e.g., dry, ripe, creamy); adj.spatial (e.g., adjacent, gigantic).12 For each adjective type in WordNet, they produce a vector with a classifier posterior probabilities corresponding to degrees of membership of this word in one of the 13 semantic classes,13 similar to the feature vectors we build for nouns and verbs. For example, f"
P14-1024,P10-1040,0,0.00568192,"sh items that have a vector space representation, using vector elements as features. We train two separate classifiers for abstractness and imageability on a seed set of words from the MRC database. Degrees of abstractness and imageability are posterior probabilities of classifier predictions. We binarize these posteriors into abstractconcrete (or imageable-unimageable) boolean indicators using pre-defined thresholds.11 Perfor• Vector space word representations. Vector space word representations learned using unsupervised algorithms are often effective features in supervised learning methods (Turian et al., 2010). In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al., 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). In a recent study, Mikolov et al. (2013) reveal an interesting cross-lingual property of distributed word representations: there is a strong similarity between the vector spaces across languages that can be easily captured by linear mapping. Thus, vector space models can also be seen as vectors"
P14-1024,D11-1063,0,0.173781,"could be tested more easily at a larger scale with automation. However, metaphor detection is a hard problem. On one hand, there is a subjective component: humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language (Shutova, 2010). On the other, metaphors can be domain- and contextdependent.1 Previous work has focused on metaphor identification in English, using both extensive manuallycreated linguistic resources (Mason, 2004; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Turney et al., 2011; Broadwell et al., 2013) and corpus-based approaches (Birke and Sarkar, 2007; Shutova et al., 2013; Neuman et al., 2013; Shutova and Sun, 2013; Hovy et al., 2013). We build on this foundation and also extend metaphor detection into other languages in which few resources may exist. Our work makes the following contributions: (1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses;2 (2) we create new metaphor-annotated corpora for Russian and English;3 (3) using a paradigm of model"
P14-1024,E12-1004,0,\N,Missing
P14-1024,W07-0104,0,\N,Missing
P14-1134,W11-0103,0,0.0422234,"Missing"
P14-1134,W06-1655,0,0.0374261,"Missing"
P14-1134,W13-2322,0,0.53377,"gorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints. Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source system, JAMR, is available at: http://github.com/jflanigan/jamr 1 Introduction Semantic parsing is the problem of mapping natural language strings into meaning representations. Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. Nodes represent concepts, and labeled directed edges represent the relationships between them–see Figure 1 for an example AMR graph. The formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications—in particular, machine translation—in mind. In this paper we"
P14-1134,P13-2131,0,0.425377,"alism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications—in particular, machine translation—in mind. In this paper we introduce JAMR, the first published system for automatic AMR parsing. The system is based on a statistical model whose parameters are trained discriminatively using annotated sentences in the AMR Bank corpus (Banarescu et al., 2013). We evaluate using the Smatch score (Cai and Knight, 2013), establishing a baseline for future work. The core of JAMR is a two-part algorithm that first identifies concepts using a semi-Markov model and then identifies the relations that obtain between these by searching for the maximum spanning connected subgraph (MSCG) from an edge-labeled, directed graph representing all possible relations between the identified concepts. To solve the latter problem, we introduce an apparently novel O(|V |2 log |V |) algorithm that is similar to the maximum spanning tree (MST) algorithms that are widely used for dependency parsing (McDonald et al., 2005). Our MSCG"
P14-1134,1992.tmi-1.20,1,0.743247,"Missing"
P14-1134,P13-1091,0,0.183068,"Missing"
P14-1134,W02-1001,0,0.542547,"Missing"
P14-1134,S12-1029,1,0.839695,"Missing"
P14-1134,J14-1002,1,0.256753,"Missing"
P14-1134,de-marneffe-etal-2006-generating,0,0.0716777,"Missing"
P14-1134,dorr-etal-1998-thematic,0,0.405692,"Missing"
P14-1134,J02-3001,0,0.064329,"Missing"
P14-1134,C12-1083,0,0.303236,"Missing"
P14-1134,P03-1054,0,0.0983673,"Missing"
P14-1134,D10-1119,0,0.0230971,"Missing"
P14-1134,P11-1060,0,0.0809238,"Missing"
P14-1134,P09-1039,1,0.766485,"Missing"
P14-1134,D11-1022,1,0.524395,"Missing"
P14-1134,P13-2109,1,0.858632,"Missing"
P14-1134,E06-1011,0,0.178173,"Missing"
P14-1134,H05-1066,0,0.221071,"Missing"
P14-1134,J08-2005,0,0.0599703,"Missing"
P14-1134,W09-1119,0,0.154302,"Missing"
P14-1134,W06-1616,0,0.00889884,"Missing"
P14-1134,D07-1071,0,0.0579116,"Missing"
P14-2134,P98-2127,0,0.0302714,"that are shaped by the context in which it is uttered. In a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation. 1 In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in cont"
P14-2134,W11-2503,0,0.0139341,"ical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and Introduction The vast textual resources used in NLP – newswire, web text, parliamentary proceedings – can encourage a view"
P14-2134,P12-1015,0,0.0172109,"to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and Introduction The vast textual resources used in NLP – newswire, web text, parliamentary proceedings – can encourage a view of language as a dis"
P14-2134,E14-1011,0,0.0267058,"iamentary proceedings – can encourage a view of language as a disembodied phenomenon. The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics ... Main Alabama Alaska Arizona Arkansas W h X o Figure 1: Model. Illustrated are the input dimensions that fire for a single sampl"
P14-2134,N10-1013,0,0.0373229,"ich it is uttered. In a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation. 1 In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn r"
P14-2134,D10-1124,1,0.869174,"Missing"
P14-2134,D13-1115,0,0.027385,"Missing"
P14-2134,P11-1137,1,0.872534,"social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics ... Main Alabama Alaska Arizona Arkansas W h X o Figure 1: Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item #2) spoken in Alaska, along with a single output. Parameter"
P14-2134,D12-1137,0,0.0173599,"Missing"
P14-2134,D11-1014,0,0.00987513,"e of tokens around a target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued repre829 be in the same vector space and can therefore be compared to each other; with individual models (each with different initializations), word vectors across different states may not be directly compared. month, day of week, or other demographic variables of the speaker. Let |C |denote the sum of the cardinalities of all variables in C (i.e., 51 states, including the District of Columbia). Rather than using a single embeddin"
P14-2134,N10-1011,0,0.0193515,"xtends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and Introduction The vast textual resources used in NLP – newswire, web text, parliamentary proceedings – c"
P14-2134,P13-1045,0,0.0613431,"titative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation. 1 In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that v"
P14-2134,P10-1040,0,0.0103477,"as the bag or sequence of tokens around a target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued repre829 be in the same vector space and can therefore be compared to each other; with individual models (each with different initializations), word vectors across different states may not be directly compared. month, day of week, or other demographic variables of the speaker. Let |C |denote the sum of the cardinalities of all variables in C (i.e., 51 states, including the District of Columbia). Rather than us"
P14-2134,P11-1096,0,0.0632653,"Missing"
P14-2134,C98-2122,0,\N,Missing
P14-5004,N09-1003,0,0.385497,"Missing"
P14-5004,P12-1015,0,0.088831,"n given similarity rankings by humans. These differ from WS-353 in that it contains only nouns whereas the former contains all kinds of words. The sixth benchmark is the MTurk-2875 (Radinsky et al., 2011) dataset that constitutes 287 pairs of words and is different from the previous benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk (AMT). Similar in spirit is the MTruk-7716 (Halawi et al., 2012) dataset that contains 771 word pairs whose similarity was crowdsourced from AMT. Another, AMT created dataset is the MEN7 benchmark (Bruni et al., 2012) that consists of 3000 word pairs, randomly selected from words that occur at least 700 times in the freely available ukWaC and Wackypedia8 corpora combined. The next two benchmarks were created to put emphasis on different kinds of word types. To specifically emphasize on verbs, Yang and Powers (2006) created a new benchmark YP-130 of 130 verb pairs with human similarity judgements. Since, most of the earlier discussed datasets contain word pairs that are relatively more frequent in a corpus, Luong et al. (2013) create a new benchMultilingual Benchmarks. As is the case with most NLP problems,"
P14-5004,E14-1049,1,0.186072,"Missing"
P14-5004,D13-1170,0,0.00745772,"tendencies and meanings. With an overwhelming number of techniques to obtain word vector representations the task of comparison and choosing the vectors best suitable for a particular task becomes difficult. This is • Submission of user vectors for exhaustive offline evaluation and leader board ranking • Publicly available repository of word vectors with performance details Availability of such an evaluation system will help in enabling better consistency and uniformity in evaluation of word vector representations as well as provide an easy to use interface for endusers in a similar spirit to Socher et al. (2013a), a website for text classification.2 Apart from the online demo version, we also provide a software that can be run in an offline mode on the command line. Both the online and offline tools will be kept updated with continuous addition of new relevant tasks and vectors. 1 2 www.wordvectors.org/suite.php www.etcml.com 19 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 19–24, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics 2 mark (Rare-Word)9 that contains rare-words by sampling words"
P14-5004,P10-1040,0,0.764387,"ord vectors in R2 • Evaluation and comparison of the available open-source vectors on the suite Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using co-occurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010), it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). A number of approaches that use the internal representations from models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsets (Mikolov et al., 2013) to arrive at vector representations have also been shown to likewise capture co-occurrence tendencies and meanings. With an overwhelming number of techniques to obtain word vector representations the task of comparison and choosing the vectors best suitable for a particular task becomes difficult. This is • Submission of user vectors for exhaustive offline evaluation and leader board ranking • Publicly available"
P14-5004,P12-1092,0,0.541252,"These images can then be downloaded and used. We have 3 http://www.cs.technion.ac.il/˜gabr/ resources/data/wordsim353/ 4 http://alfonseca.org/eng/research/ wordsim353.html 5 http://tx.technion.ac.il/˜kirar/ Datasets.html 6 http://www2.mta.ac.il/˜gideon/ mturk771.html 7 http://clic.cimec.unitn.it/˜elia. bruni/MEN.html 8 http://wacky.sslmit.unibo.it/doku. php?id=corpora 9 http://www-nlp.stanford.edu/˜lmthang/ morphoNLM/ 10 http://homepage.tudelft.nl/19j49/ t-SNE_files/tsne_python.zip 20 Figure 1: Antonyms (red) and synonyms (green) of beautiful represented by Faruqui and Dyer (2014) (left) and Huang et al. (2012) (right). Metaoptimize. These word embeddings 11 have been trained in (Turian et al., 2010) using a neural network language model and were shown to be useful for named entity recognition (NER) and phrase chunking. included two datasets by default which exhibit different properties of the language: • Antonyms and synonyms of beautiful • Common male-female nouns and pronouns SENNA. It is a software12 which outputs a host of predictions: part-of-speech (POS) tags, chunking, NER etc (Collobert et al., 2011). The software uses neural word embeddings trained over Wikipedia data for over 2 months. In"
P14-5004,J06-3003,0,0.0110359,"evaluation benchmarks • Evaluation of user computed word vectors • Visualizing word vectors in R2 • Evaluation and comparison of the available open-source vectors on the suite Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using co-occurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010), it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). A number of approaches that use the internal representations from models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsets (Mikolov et al., 2013) to arrive at vector representations have also been shown to likewise capture co-occurrence tendencies and meanings. With an overwhelming number of techniques to obtain word vector representations the task of comparison and choosing the vectors best suitable for a particular task becomes difficult. This is • Submission of user"
P14-5004,W13-3512,0,0.0264642,"ty was crowdsourced from AMT. Another, AMT created dataset is the MEN7 benchmark (Bruni et al., 2012) that consists of 3000 word pairs, randomly selected from words that occur at least 700 times in the freely available ukWaC and Wackypedia8 corpora combined. The next two benchmarks were created to put emphasis on different kinds of word types. To specifically emphasize on verbs, Yang and Powers (2006) created a new benchmark YP-130 of 130 verb pairs with human similarity judgements. Since, most of the earlier discussed datasets contain word pairs that are relatively more frequent in a corpus, Luong et al. (2013) create a new benchMultilingual Benchmarks. As is the case with most NLP problems, the lexical semantics evaluation benchmarks for languages other than English have been limited. Currently, we provide a link to some of these evaluation benchmarks from our website and in future will expand the website to encompass vector evaluation for other languages. 3 Visualization The existing benchmarks provide ways of vector evaluation in a quantitative setting. To get an idea of what kind of information the vectors encode it is important to see how these vectors represent words in n-dimensional space, wh"
P14-5021,diaz-de-ilarraza-etal-2004-abar,0,0.067333,"Missing"
P14-5021,J93-2004,0,0.0477748,"Missing"
P14-5021,N03-4009,0,0.0449942,"units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency annotation interface that has text as the exclu7 Conclusion While the creation of high-quality, highly specified, syntactically annotated corpora is a goal that is out of reach for most languages and genres, GFL-Web facilitates a rapid annotation workflow within a simple framework for dependency syntax. More information on FUDG/GFL is available at http://www.ark.cs.cmu.edu/FUDG/, and the source code fo"
P14-5021,W13-2307,1,0.566479,"Missing"
P14-5021,E12-2021,0,0.0872351,"Missing"
P14-5021,P13-4001,0,0.0377836,"Missing"
P14-5021,P13-1023,0,0.0264748,"013). These in turn require Graphviz (Ellson et al., 2002), which is freely available. Flask provides a built-in server, but can also be deployed in Apache, via WSGI or CGI, etc. 6 Certain elements of the FUDG/GFL framework can be found in other annotation systems, such as the PASSAGE syntactic representation (Vilnat et al., 2010), which allows for grouping of words into units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency annotation interface that has te"
P14-5021,N13-1049,0,0.0175252,"UDG/GFL annotation. The simple interface provides instantaneous feedback on the wellformedness of a GFL annotation, and by wrapping Schneider et al.’s notation parsing and rendering software, gives a user-friendly visualization of the annotated sentence. The tool itself is lightweight, multi-user, and easily deployed with few software dependencies. Sentences are assigned to annotators via an administrative interface, which also records progress and provides for a text dump of 1 These can be especially effective when some details of the syntax can be predicted automatically with high accuracy (Alkuhlani et al., 2013). 121 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121–126, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics (a) @Bryan_wright11 i lost all my contacts , smh . (b) Texas Rangers are in the World Series ! Rangers !!!!!!!!! http://fb.me/D2LsXBJx Go Figure 1: FUDG annotation graphs for two tweets. • Multiword units may be joined to form composite lexical nodes (e.g., World_Series in figure 1b). These nodes are not annotated with any internal syntactic structure. • Tokens that are used i"
P14-5021,W13-2322,1,0.771883,"GFL (Schneider et al., 2013). These in turn require Graphviz (Ellson et al., 2002), which is freely available. Flask provides a built-in server, but can also be deployed in Apache, via WSGI or CGI, etc. 6 Certain elements of the FUDG/GFL framework can be found in other annotation systems, such as the PASSAGE syntactic representation (Vilnat et al., 2010), which allows for grouping of words into units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency anno"
P14-5021,2005.eamt-1.11,0,0.0485334,"Missing"
P14-5021,P11-2008,1,0.874453,"Missing"
P14-5021,P09-2056,0,0.0238252,"ies et al., 2012), the Penn Arabic Treebank (Maamouri et al., 2004), and the Prague dependency treebanks (Hajiˇc, 1998; ˇ Cmejrek et al., 2005), have relied on expert linguists to produce carefully-controlled annotated data. Because this process is costly, such annotation projects have been undertaken for only a handful of important languages. Therefore, developing syntactic resources for less-studied, lowerresource, or politically less important languages and genres will require alternative methods. To address this, simplified annotation schemes that trade cost for detail have been proposed (Habash and Roth, 2009).1 Although GFL offers a number of conveniences to annotators, the text-based UI is limiting: the existing tools require constant switching between a text editor and executing commands, and there are no tools for managing a large-scale annotation effort. Additionally, user interface research has found marked preferences for and better performance with graphical tools relative to text-based interfaces—particularly for less computer-savvy users (Staggers and Kobus, 2000). In this paper, we present the GFL-Web tool, a web-based interface for FUDG/GFL annotation. The simple interface provides inst"
P14-5021,vilnat-etal-2010-passage,0,\N,Missing
P15-1033,C14-1076,1,0.893251,"Missing"
P15-1033,P14-2131,0,0.0270437,"ds—both those that are OOV in both the very limited parsing data but present in the pretraining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our par"
P15-1033,P82-1020,0,0.861444,"Missing"
P15-1033,D12-1133,0,0.0254088,"Missing"
P15-1033,P08-1067,0,0.025115,"Missing"
P15-1033,D14-1081,0,0.0362549,"Missing"
P15-1033,D14-1082,0,0.815907,"Transition-Based Dependency Parsing with Stack Long Short-Term Memory Chris Dyer♣♠ Miguel Ballesteros♦♠ Wang Ling♠ Austin Matthews♠ Noah A. Smith♠ ♣ Marianas Labs ♦ NLP Group, Pompeu Fabra University ♠ Carnegie Mellon University chris@marianaslabs.com, miguel.ballesteros@upf.edu, {lingwang,austinma,nasmith}@cs.cmu.edu Abstract decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al., 2014; Ballesteros and Bohnet, 2014) and more recently using neural networks (Chen and Manning, 2014; Stenetorp, 2013). We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser’s state: (i) unbounded look-ah"
P15-1033,N15-1142,1,0.133656,"aining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our parser to maximize the conditional log-likelihood (Eq. 1) of treebank parses given sentenc"
P15-1033,C14-1078,0,0.0487974,"Missing"
P15-1033,D10-1004,1,0.291438,"Missing"
P15-1033,P13-1104,0,0.0353977,"Missing"
P15-1033,de-marneffe-etal-2006-generating,0,0.0341911,"Missing"
P15-1033,W03-3017,0,0.0269603,"step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses thre"
P15-1033,W04-0308,0,0.20895,"ructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses three stack LSTMs:"
P15-1033,P13-2111,0,0.0261337,"ter points (i.e., the hTOP ), a continuous-space “summary” of the contents of the current stack configuration is available. We refer to this value as the “stack summary.” it = σ(Wix xt + Wih ht−1 + Wic ct−1 + bi ) ft = σ(Wf x xt + Wf h ht−1 + Wf c ct−1 + bf ) ct = ft ct−1 + What does the stack summary look like? Intuitively, elements near the top of the stack will it tanh(Wcx xt + Wch ht−1 + bc ), 1 Ours is not the first deviation from a strict left-toright order: previous variations include bidirectional LSTMs (Graves and Schmidhuber, 2005) and multidimensional LSTMs (Graves et al., 2007). 2 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. where σ is the component-wise logistic sigmoid function, and is the component-wise (Hadamard) product. The value ht of the LSTM at each time step is controlled by a third gate (ot ) that is applied to the result of the application of a nonlinearity to the 335 P TO y0 P P TO TO y1 y0 y1 pop ; x1 y0 y1 y2 ; x1 x2 push ; x1 Figure 1: A stack LSTM extends a conventional left-to-right LSTM with the a"
P15-1033,N07-1050,0,0.0449813,"Missing"
P15-1033,J08-4003,0,0.120327,"Missing"
P15-1033,P09-1040,0,0.0683306,"Missing"
P15-1033,P13-1014,0,0.0122661,"represent each input token, we concatenate three vectors: a learned vector representation for each word type (w); a fixed vector representa˜ LM ), and a tion from a neural language model (w learned representation (t) of the POS tag of the token, provided as auxiliary input to the parser. A Our parser is based on the arc-standard transition inventory (Nivre, 2004), given in Figure 3. 5 In general, A(S, B) is the complete set of parser actions discussed in §3.2, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT operation is obligatory (Sartorio et al., 2013). 337 Stackt (u, u), (v, v), S (u, u), (v, v), S S Buffert B B (u, u), B Action Stackt+1 (gr (u, v), u), S (gr (v, u), v), S (u, u), S REDUCE - RIGHT (r) REDUCE - LEFT (r) SHIFT Buffert+1 B B B Dependency r u→v r u←v — Figure 3: Parser transitions indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the win"
P15-1033,P14-6005,0,0.0326662,"Missing"
P15-1033,P04-1013,0,0.0390693,"Missing"
P15-1033,P13-1088,0,0.0213712,"ons. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree has a value computed as a f"
P15-1033,P13-1045,0,0.0166315,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,D13-1170,0,0.0024939,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,P07-1080,0,0.0306643,"Missing"
P15-1033,N03-1033,0,0.0859622,"Missing"
P15-1033,I05-3027,0,0.0128763,"Missing"
P15-1033,P15-1032,0,0.257466,"Missing"
P15-1033,W03-3023,0,0.0327577,"for prediction at each time step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing mo"
P15-1033,D08-1059,0,0.0189076,"Missing"
P15-1033,P11-2033,0,0.0327321,"Missing"
P15-1033,D14-1109,0,0.0354567,"Missing"
P15-1033,Q14-1017,0,\N,Missing
P15-1077,N09-1003,0,0.0332982,"Missing"
P15-1077,P10-4002,1,0.18642,"ic LDA. However our model was not able to capture the ‘space’ topics which LDA was able to identify. Also we visualize a part of the continuous space where the word embedding is performed. For this task we performed the Principal Component Analysis (PCA) over all the word vectors and plot the first two components as shown in Figure 3. We can see clear separations between some of the clusters of topics as depicted. The other topics would be separated in other dimensions. We run our experiments4 on two datasets 20NEWSGROUP 5 and NIPS 6 . All the datasets were tokenized and lowercased with cdec (Dyer et al., 2010). 5.1 Topic Coherence Quantitative Analysis Typically topic models are evaluated based on the likelihood of held-out documents. But in this case, it is not correct to compare perplexities with models which do topic modeling on words. Since our topics are continuous distributions, the probability of a word vector is given by its density w.r.t the normal distribution based on its topic assignment, instead of a probability mass from a discrete topic distribution. Moreover, (Chang et al., 2009) showed that higher likelihood of held-out documents doesn’t necessarily correspond to human perception o"
P15-1077,N13-1092,0,0.0243815,"Missing"
P15-1077,D14-1012,0,0.0069595,"topics relevant to our work: vector space word embeddings and LDA. 2.1 Vector Space Semantics According to the distributional hypothesis (Harris, 1954), words occurring in similar contexts tend to have similar meaning. This has given rise to data-driven learning of word vectors that capture lexical and semantic properties, which is now a technique of central importance in natural language processing. These word vectors can be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). Word vectors can either be constructed using low rank approximations of cooccurrence statistics (Deerwester et al., 1990) or using internal representations from neural network models of word sequences (Collobert and Weston, 2008). We use a recently popular and fast tool called word2vec1 , to generate skip-gram word embeddings from unlabeled corpus. In this model, a word is used as an input to a log-linear classifier with continuous projection layer and words within a certain window before and after the words are predicted. 1 Latent Dirichlet Allocation (LDA) 3 Gaussian LDA As with multinomia"
P15-1077,J06-3003,0,0.0069741,"ver the word embedding space. Background Before going to the details of our model we provide some background on two topics relevant to our work: vector space word embeddings and LDA. 2.1 Vector Space Semantics According to the distributional hypothesis (Harris, 1954), words occurring in similar contexts tend to have similar meaning. This has given rise to data-driven learning of word vectors that capture lexical and semantic properties, which is now a technique of central importance in natural language processing. These word vectors can be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). Word vectors can either be constructed using low rank approximations of cooccurrence statistics (Deerwester et al., 1990) or using internal representations from neural network models of word sequences (Collobert and Weston, 2008). We use a recently popular and fast tool called word2vec1 , to generate skip-gram word embeddings from unlabeled corpus. In this model, a word is used as an input to a log-linear classifier with continuous projection layer and words within a certai"
P15-1077,N13-1090,0,0.0142278,"eneral corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents. 1 How does this capture our preference for semantic coherence? Word embeddings have been shown to capture lexico-semantic regularities in language: words with similar syntactic and semantic properties are found to be close to each other in the embedding space (Agirre et al., 2009; Mikolov et al., 2013). Since Gaussian distributions capture a notion of centrality in space, and semantically related words are localized in space, our Gaussian LDA model encodes a prior preference for semantically coherent topics. Our model further has several advantages. Traditional LDA assumes a fixed vocabulary of word types. This modeling assumption drawback as it cannot handle out of vocabulary (OOV) words in “held out” documents. Zhai and Boyd-Graber (2013) proposed an approach to address this problem by drawing topics from a Dirichlet Process with a base distribution over all possible character strings (i."
P15-1077,D14-1113,0,0.0811812,"Missing"
P15-1077,N10-1013,0,0.0369739,"Missing"
P15-1077,N03-1033,0,0.0179074,"lect a subset of documents and replace words of those documents by its synonyms if they haven’t occurred in the corpus before. We obtain the synonym of a word using two existing resources and hence we create two such datasets. For the first set, we use the Paraphrase Database (Ganitkevitch et al., 2013) to get the lexical paraThe second set was obtained using WordNet (Miller, 1995), a large human annotated lexicon for English that groups words into sets of synonyms called synsets. To obtain the synonym of a word, we first label the words with their part-ofspeech using the Stanford POS tagger (Toutanova et al., 2003). Then we use the WordNet database 7 801 http://www.cis.upenn.edu/˜ccb/ppdb/ 1.2 our model. Using the topic distribution of a document as features, we try to classify the document into one of the 20 news groups it belongs to. If the document topic distribution is modeled well, then our model should be able to do a better job in the classification task. To infer the topic distribution of a document we follow the usual strategy of fixing the learnt topics during the training phase and then running Gibbs sampling on the test set (G-LDA (fix) in table 2). However infvoc is an online algorithm, so"
P15-1077,P10-1040,0,0.0177358,"ome background on two topics relevant to our work: vector space word embeddings and LDA. 2.1 Vector Space Semantics According to the distributional hypothesis (Harris, 1954), words occurring in similar contexts tend to have similar meaning. This has given rise to data-driven learning of word vectors that capture lexical and semantic properties, which is now a technique of central importance in natural language processing. These word vectors can be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). Word vectors can either be constructed using low rank approximations of cooccurrence statistics (Deerwester et al., 1990) or using internal representations from neural network models of word sequences (Collobert and Weston, 2008). We use a recently popular and fast tool called word2vec1 , to generate skip-gram word embeddings from unlabeled corpus. In this model, a word is used as an input to a log-linear classifier with continuous projection layer and words within a certain window before and after the words are predicted. 1 Latent Dirichlet Allocation (LDA) 3 Gaussian LDA"
P15-1077,P14-1006,0,\N,Missing
P15-1081,N13-1056,0,0.024112,"Missing"
P15-1081,D13-1109,0,0.0234412,"Missing"
P15-1081,C10-1011,0,0.0174295,"9 Training Evaluation Spanish 992 million (Gigaword) 1.1 million (Europarl) English 940 million (Gigaword) 1.0 million (Europarl) Table 1: Size of data in tokens used in Spanish/English decipherment experiment cipher dependency bigrams and build a plaintext language model. We also use GIZA (Och and Ney, 2003) to align Europarl parallel data to build a dictionary for evaluating our decipherment. 4.2 Systems We implement a baseline system based on the work described in Dou and Knight (2013). The baseline system carries out decipherment on dependency bigrams. Therefore, we use the Bohnet parser (Bohnet, 2010) to parse the AFP section of both Spanish and English versions of the Gigaword corpus. Since not all dependency relations are shared across the two languages, we do not extract all dependency bigrams. Instead, we only use bigrams with dependency relations from the following list: • Verb / Subject • Verb / Object • Preposition / Object • Noun / Noun-Modifier We denote the system that uses our new method as DMRE (Dirichlet Multinomial Regression with Embedings). The system is the same as the baseline except that it uses a base distribution derived from word embeddings similarities. Word embeddin"
P15-1081,C12-1089,0,0.12694,"Missing"
P15-1081,P11-2071,0,0.0501802,"Missing"
P15-1081,P06-2065,1,0.644618,"eir context vectors are similar. Initially, the vectors contained only context ∗ Equal contribution 836 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 836–845, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics e, e0 range over the plaintext vocabulary. Given a plaintext bigram language model, the training objective is to learn P (f |e) that maximize P ({f n }N n=1 ). When formulated like this, one can directly apply EM to solve the problem (Knight et al., 2006). However, EM has time complexity O(N ·Ve2 ) and space complexity O(Vf ·Ve ), where Vf , Ve are the sizes of ciphertext and plaintext vocabularies respectively, and N is the number of cipher bigrams. This makes the EM approach unable to handle long ciphertexts with large vocabulary size. An alternative approach is Bayesian decipherment (Ravi and Knight, 2011). We assume that P (f |e) and P (e0 |e) are drawn from a Dirichet distribution with hyper-parameters αf,e and αe,e0 , that is: • We develop a new base-distribution technique that improves state-of-the art decipherment accuracy by a factor"
P15-1081,D12-1025,1,0.79622,"abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context into account, it requires high quality seed lexicons to learn a mapping between two vector spaces. In contrast, decipherment does not depend on any seed lexicon, but only looks at a limited n-gram context. In this work, we take advantage of both approaches and combine them in a joint inference process. More specifically, we extend previous work in large scale Bayesian decipherment by int"
P15-1081,P13-2109,0,0.0542467,"Missing"
P15-1081,D13-1173,1,0.864652,"swani,knight}@isi.edu Chris Dyer School of Computer Science Carnegie Mellon University cdyer@cs.cmu.edu Abstract words. Later extensions introduced more features (Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a), and used more abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context into account, it requires high quality se"
P15-1081,D14-1061,1,0.762408,"Missing"
P15-1081,W09-1117,0,0.0547052,"Missing"
P15-1081,P08-1088,0,0.0847689,"Missing"
P15-1081,P12-1017,0,0.412426,"alifornia {qdou,avaswani,knight}@isi.edu Chris Dyer School of Computer Science Carnegie Mellon University cdyer@cs.cmu.edu Abstract words. Later extensions introduced more features (Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a), and used more abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context into account, it re"
P15-1081,W13-2233,0,0.0621332,"Missing"
P15-1081,P95-1050,0,0.108409,"matically from parallel data. However, reliance on parallel data also limits the development and application of high-quality MT systems, as the amount of parallel data is far from adequate in low-density languages and domains. In general, it is easier to obtain non-parallel monolingual data. The ability to learn translations from monolingual data can alleviate obstacles caused by insufficient parallel data. Motivated by this idea, researchers have proposed different approaches to tackle this problem. They can be largely divided into two groups. The first group is based on the idea proposed by Rapp (1995), in which words are represented as context vectors, and two words are likely to be translations if their context vectors are similar. Initially, the vectors contained only context ∗ Equal contribution 836 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 836–845, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics e, e0 range over the plaintext vocabulary. Given a plaintext bigram language model, the training objective is to learn P (f |e) that"
P15-1081,P11-1002,1,0.954247,"niversity of Southern California {qdou,avaswani,knight}@isi.edu Chris Dyer School of Computer Science Carnegie Mellon University cdyer@cs.cmu.edu Abstract words. Later extensions introduced more features (Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a), and used more abstract representation such as word embeddings (Klementiev et al., 2012). Another promising approach to solve this problem is decipherment. It has drawn significant amounts of interest in the past few years (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2013; Ravi, 2013) and has been shown to improve end-to-end translation. Decipherment views a foreign language as a cipher for English and finds a translation table that converts foreign texts into sensible English. Both approaches have been shown to improve quality of MT systems for domain adaptation (Daum´e and Jagarlamudi, 2011; Dou and Knight, 2012; Irvine et al., 2013) and low density languages (Irvine and Callison-Burch, 2013a; Dou et al., 2014). Meanwhile, they have their own advantages and disadvantages. While context vectors can take larger context"
P15-1081,P13-1036,0,0.295978,"Missing"
P15-1081,1983.tc-1.13,0,0.0603901,"y relations between words. Throughout this paper, we use f to denote target language or ciphertext tokens, and e to denote source language or plaintext tokens. Given ciphertext f : f1 ...fn , the task of decipherment is to find a set of parameters P (fi |ei ) that convert f to sensible plaintext. The ciphertext f can either be full sentences (Ravi and Knight, 2011; Nuhn et al., 2012) or simply bigrams (Dou and Knight, 2013). Since using bigrams and their counts speeds up decipherment, in this work, we treat f as bigrams, n n N where f = {f n }N n=1 = {f1 , f2 }n=1 . Motivated by the idea from Weaver (1955), we model an observed cipher bigram f n with the following generative story: • First, a language model P (e) generates a sequence of two plaintext tokens en1 , en2 with probability P (en1 , en2 ). • Then, substitute en1 with f1n and en2 with f2n with probability P (f1n |en1 ) · P (f2n |en2 ). Based on the above generative story, the probability of any cipher bigram f n is: n P (f ) = X P (e1 e2 ) e1 e2 2 Y P (fin P (f |e) ∼ Dirichlet(αf,e ) P (e |e0 ) ∼ Dirichlet(αe,e0 ). The remainder of the generative story is the same as the noisy channel model for decipherment. In the next section, we des"
P15-1081,J03-1002,0,\N,Missing
P15-1144,J08-4004,0,0.041742,"Missing"
P15-1144,P98-1013,0,0.241048,"Missing"
P15-1144,P14-2131,0,0.0164479,"Missing"
P15-1144,E14-1049,1,0.0297678,"., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which con"
P15-1144,N15-1184,1,0.140197,"Missing"
P15-1144,P14-1046,0,0.0799848,"gularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporat"
P15-1144,N15-1004,0,0.157051,"rameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimizatio"
P15-1144,N04-1039,0,0.0582336,"rk To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V"
P15-1144,S13-1001,0,0.0655251,"Missing"
P15-1144,D14-1012,0,0.017442,"overcomplete representations A and also binarized representations B. Initial vectors are discussed in §A and tasks in §B. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. 4 Figure 2: Average performace across all tasks for sparse overcomplete vectors (A) produced by Glove initial vectors, as a function of the ratio of K to L. achieves a binary, sparse vector (B) by applying:  bi,j = 1 if xi,j > 0 0 otherwise (7) The second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through:   1 if xi,j ≥ M + −1 if xi,j ≤ M − ai,j = (8)  0 otherwise where M + (M − ) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability"
P15-1144,J15-4004,0,0.0321756,"Missing"
P15-1144,P12-1092,0,0.066485,"ia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word S"
P15-1144,W03-1018,0,0.0399485,"dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164"
P15-1144,D13-1147,0,0.0458503,"Missing"
P15-1144,D13-1196,0,0.025068,"d accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10"
P15-1144,Q13-1015,0,0.0245514,"Missing"
P15-1144,W14-2406,0,0.0387394,"Missing"
P15-1144,C02-1150,0,0.0787722,"e classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002). The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (8"
P15-1144,J93-2004,0,0.058184,"der three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validati"
P15-1144,D11-1139,1,0.497885,"Missing"
P15-1144,P14-1074,1,0.851886,"are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V466 V465 V128 V11 V413 V98 V131 V445 V199 V475 V208 V431 V299 V357 V149 V80 V247 V231 V42 V44 V376 V152 V74 V254 V141 V341 V349 V234"
P15-1144,P14-2089,0,0.0191967,"Missing"
P15-1144,C12-1118,0,0.72512,"is vectors. λ is a regularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can"
P15-1144,D14-1162,0,0.123387,"ymous reviewers for their feedback. This research was supported in part by the National Science Foundation through grant IIS-1251131 and the Defense Advanced Research Projects Agency through grant FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. A Initial Vector Representations (X) Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method. Glove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Conte"
P15-1144,D13-1170,0,0.0032486,"/code.google.com/p/word2vec http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6 http://cs.cmu.edu/˜mfaruqui/soft.html 1497 5 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many"
P15-1144,D13-1015,0,0.0389009,"Missing"
P15-1144,C98-1013,0,\N,Missing
P15-1144,P14-1009,0,\N,Missing
P15-2021,W12-4410,1,0.853119,"anguage OOVs. We therefore run the borrowing system on OOVs and non-OOV words that occur less than 3 times in the training corpus. We list in table 4 sizes of translated lexicons that we integrate in translation tables. Loan OOVs in SW– EN Loan OOVs in RO – EN 4K 5,050 347 8K 4,219 271 14K 3,577 216 Table 4: Sizes of translated lexicons extracted using pivoting via borrowing and integrated in translation models. Transliteration-augmented setups. In addition to the standard baselines, we evaluate transliteration-augmented setups, where we replace the borrowing model by a transliteration model (Ammar et al., 2012). The model is a linear-chain CRF where we label each source character with a sequence of target characters. The features are label unigrams and bigrams, separately or conjoined with a moving window of source characters. We employ the Swahili–Arabic and Romanian–French transliteration systems that were used as baselines in (Tsvetkov et al., 2015). As in the borrowing system, transliteration outputs are filtered to contain only target language lexicons. We list in table 5 sizes of obtained translated lexicons. www.cdec-decoder.org 128 Translit. OOVs in SW– EN Translit. OOVs in RO – EN 4K 49 906"
P15-2021,N06-1003,0,0.113461,"Missing"
P15-2021,2012.eamt-1.60,0,0.0341247,"e borrowing system only minimally overgenerates the set of output candidates given an input. If the borrowing system encounters an input word that was not borrowed from the target donor language, it usually (but not always) produces an empty output. 3 We set n and k to 5, we did not experiment with other values. 127 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus was crawled from the Global Voices project website8 . To simulate resource-poor scenario for the Romanian–English language pair, we sample a parallel corpus of same size from the transcribed TED talks (Cettolo et al., 2012). To evalu4 For Arabic and French we use the GlobalPhone pronunciation dictionaries (Schultz et al., 2013) (we manually convert them to IPA). For Swahili and Romanian we automatically construct pronunciation dictionaries using the Omniglot grapheme-to-IPA conversion rules at www.omniglot.com. 5 We assume that while parallel data is limited in the recipient language, monolingual data is available. 6 code.google.com/p/word2vec 7 github.com/mfaruqui/eacl14-cca 8 sw.globalvoicesonline.org ate translation improvement on corpora of different sizes we conduct experiments with sub-sampled 4K, 8K, and"
P15-2021,D13-1174,1,0.856016,", the borrowing system produces the n-best list of plausible donors; for each donor we then extract the k-best list of its translations.3 Then, we pair the OOV with the resulting n × k translation candidates. The translation candidates are noisy: some of the generated donors may be erroneous, the errors are then propagated in translation. To allow the low-resource system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hypotheses, we integrate the acquired translation candidates as synthetic phrases (Tsvetkov et al., 2013; Chahuneau et al., 2013). Synthetic phrases is a strategy of integrating translated phrases directly in the MT translation model, rather than via pre- or post-processing MT inputs and outputs. Synthetic phrases are phrasal translations that are not directly extractable from the training data, generated by auxiliary translation and postediting processes (for example, extracted from a borrowing model). An important advantage of synthetic phrases is that they are recall-oriented, allowing the system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hy"
P15-2021,2014.amta-researchers.24,0,0.0138589,"lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- and partially-assimilated foreign words, called bo"
P15-2021,P10-1048,0,0.0197761,"., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work f"
P15-2021,P10-4002,1,0.81688,"different sizes we conduct experiments with sub-sampled 4K, 8K, and 14K parallel sentences from the training corpora (the smaller the training corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via"
P15-2021,E14-1049,1,0.806793,"e monolingual corpora, 100-dimensional word vector representations for donor and recipient language vocabularies.5 Then, we employ canonical correlation analysis (CCA) with small donor–loanword dictionaries (training sets in the borrowing models) to project the word embeddings into 50-dimensional vectors with maximized correlation between their dimensions. The semantic feature annotating the synthetic translation candidates is cosine distance between the resulting donor and loanword vectors. We use the word2vec tool (Mikolov et al., 2013) to train monolingual vectors,6 and the CCA-based tool (Faruqui and Dyer, 2014) for projecting word vectors.7 the language-pair and reduced only to a small set of plausible changes that the donor word can undergo in the process of assimilation in the recipient language. Thus, the borrowing system only minimally overgenerates the set of output candidates given an input. If the borrowing system encounters an input word that was not borrowed from the target donor language, it usually (but not always) produces an empty output. 3 We set n and k to 5, we did not experiment with other values. 127 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus wa"
P15-2021,P08-2015,0,0.0264291,"erroneous and disfluent translations. All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs. These are often named entities and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010;"
P15-2021,P08-1088,0,0.0392908,"is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an"
P15-2021,A00-1002,0,0.100731,"Missing"
P15-2021,W11-2123,0,0.0148201,". Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement that can be achieved if we correctly transla"
P15-2021,P08-1045,0,0.030263,"rce language, producing erroneous and disfluent translations. All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs. These are often named entities and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani"
P15-2021,N03-2016,0,0.294531,"iques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four mai"
P15-2021,N01-1020,0,0.0698545,"ing transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be"
P15-2021,D09-1040,0,0.0250133,"n low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typo"
P15-2021,P03-1021,0,0.0122395,"4K, 8K, and 14K parallel sentences from the training corpora (the smaller the training corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potentia"
P15-2021,P02-1040,0,0.0938134,"n in table 3. SW– EN Sentences Tokens Types dev 1,552 33,446 7,008 test 1,732 35,057 7,180 RO – EN dev 2,687 24,754 5,141 test 2,265 19,659 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement that can be achieved if we correctly translate all OOVs that were borrowed from another language? What is the overall improvement that can be achieved if we correctly translate all OOVs? We answer this question by defining “upper bound” experi"
P15-2021,P95-1050,0,0.22103,"ies and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate tra"
P15-2021,P13-1109,0,0.0138213,"ios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant reso"
P15-2021,P14-1064,0,0.0586032,"rimarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. T"
P15-2021,W13-2234,1,0.847929,"anslation. For each OOV, the borrowing system produces the n-best list of plausible donors; for each donor we then extract the k-best list of its translations.3 Then, we pair the OOV with the resulting n × k translation candidates. The translation candidates are noisy: some of the generated donors may be erroneous, the errors are then propagated in translation. To allow the low-resource system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hypotheses, we integrate the acquired translation candidates as synthetic phrases (Tsvetkov et al., 2013; Chahuneau et al., 2013). Synthetic phrases is a strategy of integrating translated phrases directly in the MT translation model, rather than via pre- or post-processing MT inputs and outputs. Synthetic phrases are phrasal translations that are not directly extractable from the training data, generated by auxiliary translation and postediting processes (for example, extracted from a borrowing model). An important advantage of synthetic phrases is that they are recall-oriented, allowing the system to leverage good translations that are missing in the default phrase inventory, while being stabl"
P15-2021,N15-1062,1,0.688874,"al improvement (up to +1.6 BLEU) in Swahili–Arabic–English translation, and a small but statistically significant improvement (+0.2 BLEU) in Romanian–French–English. Figure 2: To improve a resource-poor Swahili–English SMT system, we extract translation candidates for OOV Swahili words borrowed from Arabic using the Swahili-to-Arabic borrowing system and Arabic–English resource-rich SMT. bridge between resource-rich and resource-limited languages; we use this observation in our work. Transliteration and cognate discovery models perform poorly in the task of loanword generation/identification (Tsvetkov et al., 2015). The main reason is that the recipient language, in which borrowed words are fully or partially assimilated, may have very different morpho-phonological properties from the donor language (e.g., ‘orange’ and ‘sugar’ are not perceived as foreign by native speakers, but these are English words borrowed from Arabic l. ' PAK (nArnj)1 and QºË@ (Alskr), respectively). Therefore, morpho-phonological loanword adaptation is more complex than is typically captured by transliteration or cognate models. We employ a discriminative cross-lingual model of lexical borrowing to identify plausible donors giv"
P15-2021,D12-1027,0,0.0178563,". Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Peripheral Partially assimilated Fully assimilated Core Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- an"
P15-2021,N15-1176,0,0.0267247,"peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting 125 LEXICON a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal,"
P15-2021,W02-0505,0,\N,Missing
P15-2036,S07-1018,0,0.0348331,"nce relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually select corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentence"
P15-2036,P98-1013,0,0.803968,"TIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annotation sources: ‡ t pan tici par ant r cip nce a"
P15-2036,W04-0817,0,0.0210726,"ING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually"
P15-2036,bonial-etal-2014-propbank,0,0.057978,"Missing"
P15-2036,W13-5503,0,0.0575005,"Missing"
P15-2036,J14-1002,1,0.781256,"ant, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annot"
P15-2036,S12-1029,1,0.891611,"model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at mo"
P15-2036,P11-1144,1,0.598328,"sk.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs an"
P15-2036,N12-1086,1,0.904091,"Missing"
P15-2036,P07-1033,0,0.0592643,"termined by frame identification). We use the heuristic procedure described by (Das et al., 2014) for extracting candidate argument spans for the predicate; call this spans(x, p, f ). spans always includes a special span denoting an empty or nonovert role, denoted ∅. For each candidate argument a ∈ spans(x, p, f ) and each role r, a binary feature vector φ (a,x, p, f ,r) is extracted. We use the feature extractors from (Das et al., 2014) as a baseline, adding additional ones in our experiments (§3.2– §3.4). Each a is given a real-valued score by a linear model: Domain Adaptation and Exemplars Daumé (2007) proposed a feature augmentation approach that is now widely used in supervised domain adaptation scenarios. We use a variant of this approach. Let Dex denote the exemplars training data, and Dft denote the full text training data. For every feature φ (a,x, p, f ,r) in the base model, we add a new feature φft (⋅) that fires only if φ (⋅) fires and x ∈ Dft . The intuition is that each base feature contributes both a “general” weight and a “domain-specific” weight to the model; thus, it can exhibit a general preference for specific roles, but this general preference can be fine-tuned for the dom"
P15-2036,S15-1005,0,0.0135716,"mance upon combining the best approaches. Both use full-text and exemplars for training; the first uses PropBank SRL as guide features, and the second adds hierarchy features. The best result is the 221 0.2 Acknowledgments 100 50 0 Test Examples 150 0.4 over, the techniques discussed here could be further explored using semi-automatic mappings between lexical resources (such as UBY; Gurevych et al., 2012), and correspondingly, this task could be used to extrinsically validate those mappings. Ours is not the only study to show benefit from heterogeneous annotations for semantic analysis tasks. Feizabadi and Padó (2015), for example, successfully applied similar techniques for SRL of implicit arguments.9 Ultimately, given the diversity of semantic resources, we expect that learning from heterogeneous annotations in different corpora will be necessary to build automatic semantic analyzers that are both accurate and robust. 0 200 400 600 800 1000 1200 1400 Frame Element, ordered by test set frequency 0.8 (a) Frequency of each role appearing in the test set. The authors are grateful to Dipanjan Das for his assistance, and to anonymous reviewers for their helpful feedback. This research has been supported by the"
P15-2036,W03-1007,0,0.023046,"annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annot"
P15-2036,J02-3001,0,0.941676,"A0 want ING is evoked by want, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We"
P15-2036,E12-1059,0,0.0207678,"Missing"
P15-2036,P14-1136,0,0.167275,"ts of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs and (as of recently) eventive noun and adjective predicates. An example with PB annotations is shown in figure 2. We use the mode"
P15-2036,N06-2015,0,0.136256,"Missing"
P15-2036,S12-1016,0,0.0167416,"d T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel"
P15-2036,N13-1013,0,0.0347516,"r the domain. Regularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 200"
P15-2036,D14-1108,1,0.685009,"ularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonal"
P15-2036,C04-1179,0,0.0362007,"dicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train"
P15-2036,W04-0803,0,0.0508177,"espect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the docum"
P15-2036,D08-1017,1,0.640396,"case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of overt arguments must not overlap. Beam search, with a beam size of 100, is used to find this argmax.5 3.2 Hierarchy Features 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide features: one encodes the"
P15-2036,P09-1003,0,0.0182029,"ED.Wrongdoer, and so forth. Subframe: This indicates a subevent within a complex event. E.g., the C RIMINAL _ PROCESS frame groups together subframes A RREST, A RRAIGN MENT and T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been"
P15-2036,P08-1108,0,0.0186663,"Missing"
P15-2036,J05-1004,0,0.717357,"Missing"
P15-2036,P15-2067,0,0.0221741,"Missing"
P15-2036,J08-2005,0,0.587921,"Missing"
P15-2036,Q15-1003,0,0.23587,"domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of"
P15-2036,C98-1013,0,\N,Missing
P15-2076,W02-1001,0,0.376393,"Missing"
P15-2076,N15-1184,1,0.732372,".3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some ve"
P15-2076,P15-1144,1,0.438728,".3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some ve"
P15-2076,P13-1174,0,0.0165144,"ets and records a number of relations among these synsets or their members. For a word we look up its synset for all possible part of speech (POS) tags that it can assume. For example, film will have S YNSET.F ILM .V.01 and S YNSET.F ILM .N.01 as features as it can be both a verb and a noun. In addition to synsets, we include the hyponym (for ex. H YPO .C OLLAGE F ILM .N.01), hypernym (for ex. H YPER :S HEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector. Connotation. Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories"
P15-2076,N09-1003,0,0.447409,"Missing"
P15-2076,N15-1004,0,0.0165613,"text (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distributional word vectors, linguistic Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove & LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge margin on SimLex. Linguistic vectors also perform better at RG-65. On sentiment analysis, linguistic vectors are competitive with Skip-Gram vectors and on the NP-bracketing task they outperf"
P15-2076,D14-1012,0,0.069217,"Missing"
P15-2076,P98-1013,0,0.303539,"entiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings sym1 Our vectors can be downloaded at: https:// github.com/mfaruqui/non-distributional 464 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 464–469, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Lexicon WordNet Supersense FrameNet Emotion Connotation Color Part of Speech Syn. & Ant. Union Vocabulary 10,794 71,836 9,462 6,468 76,134 1"
P15-2076,J15-4004,0,0.129279,"Missing"
P15-2076,P14-2131,0,0.0285485,"Missing"
P15-2076,D13-1196,0,0.064685,"of the words. 3 Sentiment Analysis. Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as features in an `2 -regularized logistic regression for classification. The classifier is tuned on the dev set and accuracy is reported on the test set. NP-Bracketing. Lazaridou et al. (2013) constructed a dataset from the Penn TreeBank (Marcus et al., 1993) of noun phrases (NP) of length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit left and right bracketing respectively. We append the word vectors of the three words in the NP in order and use them as features in an `2 -regularized logistic regression classifier. The dataset contains 2,227 noun phrases split into 10 folds. The cl"
P15-2076,W06-1670,0,0.0208086,"t contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector."
P15-2076,J93-2004,0,0.0533649,"h envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C OLOR .R ED is a feature evoked by the word blood. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word Part of Speech Tags. The Penn Treebank (Marcus et al., 1993) annotates naturally occurring text for linguistic structure. It contains syntactic parse trees and POS tags for every word in the corpus. We collect all the possible POS tags that a word is annotated with and use it as features in the linguistic vectors. For example, love has PTB.N OUN, 2 http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz 465 Word love hate ugly beauty refundable P OL .P OS 1 0 0 1 0 C OLOR .P INK 1 0 0 1 0 SS.N OUN .F EELING 1 1 0 0 0 PTB.V ERB 1 1 0 0 0 A NTO .FAIR 0 0 1 0 0 ··· C ON .N OUN .P OS 1 0 0 1 1 Table 2: Some linguistic word vectors. 1 indicates presence and"
P15-2076,tsvetkov-etal-2014-augmenting-english,1,0.527689,"nnotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C OLOR .R ED is a feature evoked by the word blood. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource"
P15-2076,E06-1011,0,0.0220642,"Missing"
P15-2076,W11-0611,0,0.0191767,"ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C OLOR .R ED is a feature evoked by the word blood. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word Part of Speech Tags. The Penn Treebank (Marcus et al., 1993) annotates naturally occurring text for linguistic structure. It contains syntactic parse trees and POS t"
P15-2076,P14-2089,0,0.0147205,"pedia with vector length 300 and context window of 5 words. 3.3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identif"
P15-2076,C12-1118,0,0.0326162,"al data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distributional word vectors, linguistic Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove & LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge margin on SimLex. Linguistic vectors also perform better at RG-65. On sentiment analysis, linguistic vectors are competitive with Skip-Gram vectors an"
P15-2076,I08-2105,0,0.0498544,"t connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C O"
P15-2076,J08-4003,0,0.0366018,"Missing"
P15-2076,D14-1162,0,0.111111,"ic vectors comparable to publicly available distributional word vectors, we perform singular value decompostion (SVD) on the linguistic matrix to obtain word vectors of lower dimensionality. If L ∈ {0, 1}N ×D is the linguistic matrix with N word types and D linguistic features, then we can obtain U ∈ RN ×K from the SVD of L as follows: L = UΣV&gt; , with K being the desired length of the lower dimensional space. We compare both sparse and dense linguistic vectors to three widely used distributional word vector models. The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively. We used latent semantic analysis (LSA) to obtain word vectors from the SVD decomposition of a word-word cooccurrence matrix (Turney and Pantel, 2010). These were trained on 1 billion words of Wikipedia with vector length 300 and context window of 5 words. 3.3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using l"
P15-2076,W96-0213,0,0.17772,"Missing"
P15-2076,D13-1170,0,0.00443223,"Missing"
P15-2076,C98-1013,0,\N,Missing
P15-2076,P14-1046,0,\N,Missing
P15-2105,P11-1038,0,0.0195409,"ical variants in Twitter (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three"
P15-2105,C10-2042,0,0.0355775,"selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including ne"
P15-2105,P12-1092,0,0.0163423,"11010, share the first three nodes in the hierarchically 110. Sharing more tree nodes tends to translate into better similarity between words within the clusters. Thus, a word a 11001 cluster is simultaneously in clusters 1, 11, 110, 1100 and 11001, and a feature can be extracted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported re"
P15-2105,P14-2111,0,0.0188732,"r (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three words, in which c"
P15-2105,W12-3153,0,0.0173412,"tugal {luis.marujo,wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We i"
P15-2105,P13-1018,1,0.272005,"wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differen"
P15-2105,N15-1142,1,0.664689,"racted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported results using a TF-IDF, the default MAUI toolkit, and our own implementation of (Li et al., 2010) framework. In all cases the IDF component was computed over a collection of 52 million tweets. Results are reported on rows 1 and 2 in Table 1, respectively. The parameter k (col"
P15-2105,W08-1404,0,0.00839334,"e frequently used in many occasions as indicators of important information contained in documents. These can be used by human readers to search for their desired documents, but also in many Natural Language Processing (NLP) applications, such as Text Summarization (Pal et al., ¨ ur et al., 2005), 2013), Text Categorization (Ozg¨ Information Retrieval (Marujo et al., 2011a; Yang and Nyberg, 2015) and Question Answering (Liu and Nyberg, 2013). Many automatic frameworks for extracting keywords have been proposed (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Litvak and Last, 2008). These systems were built for more formal domains, such as news data or Web data, where the content is still produced in a controlled fashion. The emergence of social media environments, such as Twitter and Facebook, has created a framework for more casual data to be posted online. 1. Provide a annotated keyword annotated dataset consisting of 1827 tweets. These tweets are obtained from (Gimpel et al., 2011), and also contain POS annotations. 2. Improve a state-of-the-art keyword extraction system (Marujo et al., 2011b; Marujo et al., 2013) for this domain by learning additional features in a"
P15-2105,N13-1039,1,0.474503,"Missing"
P15-2105,D10-1036,0,0.0132353,"l et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised key"
P15-2105,D11-1141,0,0.0220653,"Missing"
P15-2105,marujo-etal-2012-supervised,1,0.940917,"ihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised keyword extraction framework (based on KEA) with additional features, such as POS tags to performed keyword extraction on Facebook posts. However, at that time Facebook status updates or posts did not contained either hashtags or user mentions. The size of Facebook posts is frequently longer than tweets and has less abbreviations since it is not limited by number of character as in tweets. 3 4 There are many methods that have been proposed for keyword extraction. TF-IDF is one of the simplest approaches for this end (Salt"
P15-2105,N10-1101,0,0.0485415,"ly 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work formation (e.g., retweet). The annotations of each annotator are combined by selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and fi"
P15-2105,W04-3252,0,\N,Missing
P15-2105,W11-2210,0,\N,Missing
P15-2105,P11-1039,0,\N,Missing
P15-2105,P11-2008,0,\N,Missing
P15-2105,C12-1047,0,\N,Missing
P16-1013,D13-1111,1,0.823343,"and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word e"
P16-1013,N07-1058,0,0.0202033,"1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this featur"
P16-1013,W06-1670,0,0.00983607,"t); dog is more prototypical than canine (because dog is more concrete); and dog is more prototypical than bull terrier (because dog is less specific). According to the theory, more prototypical words are acquired earlier. We use lexical semantic databases to operationalize insights from the prototype theory in the following semantic features; the features are computed on token level and averaged over paragraphs: • Relative frequency in a supersense was computed by marginalizing the word frequencies in the training corpus over coarse semantic categories defined in the WordNet (Fellbaum, 1998; Ciaramita and Altun, 2006). There are 41 supersense types: 26 for nouns and 15 for verbs, e.g., NOUN . ANIMAL and VERB . MOTION . For example, in NOUN . ANIMAL the relative frequency of human is 0.06, of dog is 0.01, of bird is 0.01, of cattle is 0.009, and of bumblebee is 0.0002. • Relative frequency in a synset was calculated similarly to the previous feature category, but word frequencies were marginalized over WordNet synsets (more fine-grained synonym sets). For example, in the synset {vet, warhorse, veteran, oldtimer, seasoned stager}, veteran is the most prototypical word, scoring 0.87. 3 Evaluation Benchmarks W"
P16-1013,P13-1004,0,0.0755168,"Missing"
P16-1013,W02-1001,0,0.118037,"ss curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for En"
P16-1013,P15-1033,1,0.791568,"scribed in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for English on the universal dependencies v1.1 treebank (Agi´c et al., 2015) with the standard development and test splits, reporting unlabeled attachment scores (UAS) on the test data. We remove all part-of-speech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the pretrained embeddings. • Shuffled baselines: the curriculum is defined by random shuffling the training data. We shuffled the data 10 times, and trained 10 word embeddings models, each model"
P16-1013,N16-1030,1,0.480643,"40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and spirituality as 1.07. 2 http://http://people.sutd.edu.sg/ ~yue_zhang/doc 133 # paragraphs 2,532,361 et al., 2016). The `2 -regularized logistic regression classifier is tuned on the development set and accuracy is reported on the test set. # tokens 100,872,713 # types 156,663 Table 1: Training data sizes. Named Entity Recognition (NER). Named entity recognition is the task of identifying proper names in a sentence, such as names of persons, locations etc. We use the recently proposed LSTMCRF NER model (Lample et al., 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al., 2001) to predict the output label sequence. We use the CoNLL 2003 English NER dataset (Tjong Kim Sang and De Meulder, 2003) to train our models and present results on the test set. Setup. 100-dimensional word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013).3 All training data was used, either shuffled or ordered by a curriculum. As described in §3,"
P16-1013,D13-1170,0,0.00455636,"most prototypical word, scoring 0.87. 3 Evaluation Benchmarks We evaluate the utility of the pretrained word embeddings as features in downstream NLP tasks. We choose the following off-the-shelf models that utilize pretrained word embeddings as features: • Age of acquisition (AoA) of words was extracted from the crowd-sourced database, containing over 50 thousand English words (Kuperman et al., 2012). For example, the AoA of run is 4.47 (years), of flee is 8.33, and of abscond is 13.36. If a word was not found in the database it was assigned the maximal age of 25. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use the average of the word vectors of a given sentence as a feature vector for classification (Faruqui et al., 2015; Sedoc • Concreteness ratings on the scale of 1–5 (1 is most abstract) for 40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and sp"
P16-1013,N10-1116,0,0.528653,"g♣ Brian MacWhinney♠ Chris Dyer♣♠ ♠ Carnegie Mellon University ♣ Google DeepMind {ytsvetko,mfaruqui,cdyer}@cs.cmu.edu, lingwang@google.com, macw@cmu.edu Abstract complexity (Bengio et al., 2009; Spitkovsky et al., 2010). In language modeling, this preference for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabulary from frequent to less frequent words (Bengio et al., 2009). In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010). These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938). Had different heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum using Bayesian optimization. A curriculum is defined to be the ordering of the training inst"
P16-1013,J93-2004,0,0.0565311,"ithout additional features. All models were learned under same conditions, across curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For depend"
P16-1013,D08-1020,0,0.0314538,"SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that"
P16-1013,D13-1100,0,0.0250457,"Missing"
P16-1013,P14-1024,1,0.818283,"s the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a paragraph. Assuming that a Wikipedia title is a proxy to a conventionalized concept, we counted the number of existing titles (from a database of over 4.5 million titles) in the paragraph. • Average sentence length • Verb-token ratio • Noun-token ratio • Parse tree depth •"
P16-1013,W12-2019,0,0.0531361,"Missing"
P16-1013,P05-1065,0,0.0298158,"P • Quadratic entropy (Rao, 1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 In"
P16-1013,D15-1251,0,0.0243606,"l., 2012, GP), providing convenient and powerful prior distribution on functions, and tree-structured Parzen estimators (Bergstra et al., 2011, TPE), tailored to handle conditional spaces. Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Moˇckus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al., 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison. Yogatama et al. (2015) found that the combination of EI as the acquisition function and TPE as the surrogate model performed favorably in Bayesian optimization of text representations; we follow this choice in our model. 2.2 are used in many contrasting fields, from ecology and biology (Rosenzweig, 1995; Magurran, 2013), to economics and social studies (Stirling, 2007). Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009; Jiang et al., 2014). Let pi and pj correspond to empirical frequencies of word types ti and tj"
P16-1013,J11-1005,0,0.033983,"ories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a p"
P16-1103,J93-2003,0,0.0531595,"φfwd = −0.495208 φrev = −0.455368 φfwd = −0.499118 φrev = −0.269879 φfwd = −3.718241 φuses_suf_n = 1.0 φfwd = −2.840721 φuses_end_s = 1.0 Table 1: A fragment of the word-to-character rules used in the compound generation system. beddings work well. The recurrent part of the neural network uses two-layer LSTM (Hochreiter and Schmidhuber, 1997) cells with the hidden layer size set to 10. The final MLP’s hidden layer size is also set to 10. The training data is processed such that each span of length two to four is considered one training example, and is labeled as positive if it is wellaligned (Brown et al., 1993) to a single German compound word. Since most spans do not translate as compounds, we are faced with an extreme class imbalance problem (a ratio of about 300:1). We therefore experiment with down sampling the negative training examples to have an equal number of positive and negative examples. 3.3 Training the Compound Generation Model As a translation model, there are two components to learning the translation system: learning the rule inventory and their features (§3.3.1) and learning the parameters of the generation model (§3.3.2). 3.3.1 Learning Word to Character Sequence Translation Rules"
P16-1103,E14-1061,0,0.0333313,"U ↑ METR ↑ TER ↓ Len Baseline 12.3 29.0 72.7 96.5 29.1 72.8 96.8 +Our Compounds 12.3 Baseline 11.4 29.9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having"
P16-1103,D13-1174,1,0.807908,"only does generating them make the output seem more natural, but they are contentrich. Since each compound has, by definition, at least two stems, they are intuitively (at least) doubly important for translation adequacy. Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling. In this paper we introduce a two-stage method (§2) to dynamically generate novel compound word forms given a source language input text and incorporate these as “synthetic rules” in a standard phrase-based translation system (Bhatia et al., 2014; Chahuneau et al., 2013; Tsvetkov et al., 2013). First, a binary classifier examines each source-language sentence and labels each span therein with whether that span could become a compound word when translated into the target language. Second, we transduce the identified phrase into the target language using a word-to-character translation model. This system makes a closed vocabulary assumption, albeit at the character (rather than word) level—thereby enabling new word forms to be generated. Training data for these models is extracted from automatically aligned and compound split parallel corpora (§3). We evaluate"
P16-1103,H05-1098,0,0.0129145,", and extract our generator’s top ten hypotheses for each of the postively identified spans. These English phrases are then added to a synthetic phrase table, along with their German compound translations, and two features: the compound generator’s score, and an indicator feature simply showing that the rule represents a synthetic compound. Table 5 shows some example rules of this form. The weights of these features are learned, along with the standard translation system weights, by the MIRA algorithm as part of the MT training procedure. The underlying translation system is a standard Hiero (Chiang et al., 2005) system using the cdec (Dyer et al., 2010) decoder, trained on all constrained-track WMT English–German data as of the 2014 translation task. Tokenization was done with cdec’s tokenize-anything script. The first character of each sentence was down cased if the unigram probability of the downcased version of the first word was higher than that of the original casing. Word alignment was performed using cdec’s fast_align tool, WMT2012 WMT2013* Table 3: Mean reciprocal rank, character error rate, and precision at K statistics of our baseline MT system and our compound generator. BLEU ↑ METR ↑ TER"
P16-1103,P11-2031,1,0.867193,"Missing"
P16-1103,P11-1004,0,0.0265227,"unds 12.3 Baseline 11.4 29.9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their"
P16-1103,N12-1017,0,0.0305634,"anslation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their constituent morphemes and translating German into English at the morpheme level. Their approach works excellently when translating out of a compounding language, but is unable to g"
P16-1103,P08-1115,0,0.0410213,"d Dropping We observe that in order to generate many compounds, including bananenmarkts from “market for bananas”, a system must be able to both reorder and drop source words at will. Implemented naïvely, however, these allowances may produce invalid interleavings of source words and SUF/END tokens. For example, if we (correctly) drop the word “for” from our example, we might feed the decoder the sequence “market SUF SUF bananas END. To disallow such bogus input sequences we disable all reordering inside the decoder, and instead encode all possible reorderings in the form of an input lattice (Dyer et al., 2008). Moreover, we allow the decoder to drop non-content words by skipping over them in the lattice. Each edge in our lattices contains a list of features, including the indices, lexical forms, and parts of speech of each word kept or dropped. Each possible sequence in the lattice also encodes features of the full path of source words kept, the full list of source words dropped, the parts of speech of the path and all dropped words, and the order of indices traversed. With these constraints in place we can train the compound generator as though it were a normal MT system with no decode-time reorde"
P16-1103,P10-4002,1,0.825787,"eses for each of the postively identified spans. These English phrases are then added to a synthetic phrase table, along with their German compound translations, and two features: the compound generator’s score, and an indicator feature simply showing that the rule represents a synthetic compound. Table 5 shows some example rules of this form. The weights of these features are learned, along with the standard translation system weights, by the MIRA algorithm as part of the MT training procedure. The underlying translation system is a standard Hiero (Chiang et al., 2005) system using the cdec (Dyer et al., 2010) decoder, trained on all constrained-track WMT English–German data as of the 2014 translation task. Tokenization was done with cdec’s tokenize-anything script. The first character of each sentence was down cased if the unigram probability of the downcased version of the first word was higher than that of the original casing. Word alignment was performed using cdec’s fast_align tool, WMT2012 WMT2013* Table 3: Mean reciprocal rank, character error rate, and precision at K statistics of our baseline MT system and our compound generator. BLEU ↑ METR ↑ TER ↓ Len 16.2 34.5 64.8 94.1 +Our Compounds 1"
P16-1103,N09-1046,1,0.873726,"ond, we build a compound generator that outputs hypothesis word forms, given a source phrase. We will detail each of these steps in turn. 3.1 Extracting Compounds from Bitext In order to learn to generate compound words we naturally require training data. Ideally we would like a large list of English phrases with their natural contexts and translations as German compounds. Of course virtually no such data exists, but it is possible to extract from parallel data, using a technique similar to that used by Tsvetkov and Wintner (2012). To this end, we take our tokenized bitext and pass it through Dyer (2009)’s German compound splitter. We then align the segmented variant using the fast_align tool in both the forward and reverse directions, which produces both word alignments and lexical translation tables, which give the probability of a compound part given an English phrase. We then symmetrize the produced pair of alignments with the intersection heuristic. This results in a sparse alignment in which each target word is aligned to either 0 or 1 source words. We then undo any splits performed by the compound splitter, resulting in a corpus where the only words aligned many-to-one are precisely we"
P16-1103,W10-1734,0,0.0219955,"ons that characterize natural language, it is particularly challenging in languages such as German and Finnish that have productive compounding processes. In such languages, expressing compositions of basic concepts can require an unbounded number of words. For example, English multiword phrases like market for bananas, market for pears, and market for plums are expressed in German with single compound words (respectively, as Bananenmarkt, Birnenmarkt, and Pflaumenmarkt). Second, while they are individually rare, compound words are, on the whole, frequent in native texts (Baroni et al., 2002; Fritzinger and Fraser, 2010). Third, compounds are crucial for translation quality. Not only does generating them make the output seem more natural, but they are contentrich. Since each compound has, by definition, at least two stems, they are intuitively (at least) doubly important for translation adequacy. Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling. In this paper we introduce a two-stage method (§2) to dynamically generate novel compound word forms given a source language input text and incorporate these as “synthetic rules” in a stan"
P16-1103,P13-2121,0,0.080794,"Missing"
P16-1103,N13-1056,0,0.0318055,"en Baseline 12.3 29.0 72.7 96.5 29.1 72.8 96.8 +Our Compounds 12.3 Baseline 11.4 29.9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the s"
P16-1103,W01-0504,0,0.0666443,"age, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their constituent morphemes and translating German into English at the morpheme level. Their approach works excellently when translating out of a compounding language, but is unable to generate novel compound words in the target language without some sort of post processing. Dynamic generation of compounds in a target language using such post processing has been examined in the past by Cap et al. (2014) and Clifton and Sarkar (2011). Both perform compound sp"
P16-1103,W11-2129,0,0.0234795,".9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their constituent morphemes and tr"
P16-1103,N03-1033,0,0.0156958,"non-compoundable using independent binary predictions. Rather than attempting to hand-engineer features to represent phrases, we use a bidirectional LSTM to learn a fixed-length vector representation hi,j that is computed by composing representations of the tokens (fi , fi+1 , . . . , fj ) in the input sentence. The probability that a span is compoundable is then modeled as: p(compoundable? |fi , fi+1 , . . . , fj ) =   σ w> tanh(Vhi,j + b) + a , where σ is the logistic sigmoid function, and w, V, b, and a are parameters. To represent tokens that are inputs to the LSTM, we run a POS tagger (Toutanova et al., 2003), and for each token concatenate a learned embedding of the tag and word. Figure 1 shows the architecture. p(is a compound) p(not a compound) Suppose we want to translate the sentence the market for bananas has collapsed . MLP hidden layer from English into German. In order to produce the following (good) translation, Backward LSTM der bananenmarkt ist abgestürzt . Forward LSTM a phrase-based translation system would need to contain a rule similar to market for bananas → bananenmarkt. While it is possible that such a rule would be learned from parallel corpora using standard rule extraction te"
P16-1103,W13-2234,1,0.709531,"em make the output seem more natural, but they are contentrich. Since each compound has, by definition, at least two stems, they are intuitively (at least) doubly important for translation adequacy. Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling. In this paper we introduce a two-stage method (§2) to dynamically generate novel compound word forms given a source language input text and incorporate these as “synthetic rules” in a standard phrase-based translation system (Bhatia et al., 2014; Chahuneau et al., 2013; Tsvetkov et al., 2013). First, a binary classifier examines each source-language sentence and labels each span therein with whether that span could become a compound word when translated into the target language. Second, we transduce the identified phrase into the target language using a word-to-character translation model. This system makes a closed vocabulary assumption, albeit at the character (rather than word) level—thereby enabling new word forms to be generated. Training data for these models is extracted from automatically aligned and compound split parallel corpora (§3). We evaluate our approach on both in"
P16-1103,W07-0705,0,0.0322208,"h handles this problem seamlessly. Stymne (2012) gives an excellent taxonomy of compound types in Germanic languages, and discusses many different strategies that have been used to split and merge them for the purposes of machine translation. She identifies several difficulties with the split-translate-merge approach and points out some key subtleties, such as handling of bound morphemes that never occur outside of 1091 compounds, that one must bear in mind when doing translation to or from compounding languages. The idea of using entirely character-based translation systems was introduced by Vilar et al. (2007). While their letter-level translation system alone did not outperform standard phrase-based MT on a Spanish–Catalan task, they demonstrated substantial BLEU gains when combining phraseand character-based translation models, particularly in low resource scenarios. 7 Conclusion In this paper we have presented a technique for generating compound words for target languages with open vocabularies by dynamically introducing synthetic translation options that allow spans of source text to translate as a single compound word. Our method for generating such synthetic rules decomposes into two steps. F"
P16-1103,W14-3315,1,\N,Missing
P16-1103,C14-1100,1,\N,Missing
P16-1157,W13-3520,0,0.0130875,"ector representations in a unified framework, and conduct experiments to compare the performance of existing 1668 models in a unbiased manner. We chose existing cross-lingual word vector models that can be trained on two languages at a given time. In recent work, Ammar et al. (2016) train multilingual word vectors using more than two languages; our comparison does not cover this setting. It is also worth noting that we compare here different crosslingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013). The paper does not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.11 However, the models that we included for comparison in our survey are representative of other cross-lingual models in terms of the form of crosslingual supervision required by them. For example, BilBOWA (Gouws et al.,"
P16-1157,P13-1133,0,0.0310622,"Missing"
P16-1157,W06-2920,0,0.0208404,"ce-side embeddings as lexical features. These embeddings can be replaced by target-side embeddings at test time. All models are trained for 5000 iterations with fixed word embeddings during training. Since our goal is to determine the utility of word embeddings in dependency parsing, we turn off other features that can capture distributional information like brown clusters, which were originally used in Guo et al. (2015). We use the universal dependency treebank (McDonald et al., 2013) version2.0 for our evaluation. For Chinese, we use the treebank released as part of the CoNLL-X shared task (Buchholz and Marsi, 2006). We first evaluate how useful the word embeddings are in cross-lingual model transfer of dependency parsers (Table 6). On an average, BiCCA does better than other models. BiSkip is a close second, with an average performance gap of less than 1 point. BiSkip outperforms BiCVM on German and French (over 2 point improvement), owing to word alignment information BiSkip’s model uses during training. It is not surprising that English-Chinese transfer scores are low, due to the significant difference in syntactic structure of the 10 github.com/jiangfeng1124/ acl15-clnndep l1 l2 BiSkip BiCVM BiCCA Bi"
P16-1157,D15-1131,0,0.0393742,"ng cross-lingual word vector models that can be trained on two languages at a given time. In recent work, Ammar et al. (2016) train multilingual word vectors using more than two languages; our comparison does not cover this setting. It is also worth noting that we compare here different crosslingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013). The paper does not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.11 However, the models that we included for comparison in our survey are representative of other cross-lingual models in terms of the form of crosslingual supervision required by them. For example, BilBOWA (Gouws et al., 2015) and crosslingual Auto-encoder (Chandar et al., 2014) are similar to BiCVM in this respect. Multi-view CCA (Rastogi et al., 2015) and deep CCA (Lu et al.,"
P16-1157,N13-1073,1,0.749116,"gs of size 200. We provide all models with parallel corpora, irrespective of their requirements. Whenever possible, we also report statistical significance of our results. 3 www.statmt.org/europarl/v7/{de, sv}-en.tgz 4 www.statmt.org/wmt15/ translation-task.html Parameter Selection BiSkip. All models were trained using a window size of 10 (tuned over {5, 10, 20}), and 30 negative samples (tuned over {10, 20, 30}). The cross-lingual weight was set to 4 (tuned over {1, 2, 4, 8}). The word alignments for training the model (available at github. com/lmthang/bivec) were generated using fast_align (Dyer et al., 2013). The number of training iterations was set to 5 (no tuning) and we set α = 1 and β = 1 (no tuning). BiCVM. We use the tool (available at github. com/karlmoritz/bicvm) released by Hermann and Blunsom (2014) to train all embeddings. We P train an additive model (that is, f (~x) = g(~x) = i xi ) with hinge loss margin set to 200 (no tuning), batch size of 50 (tuned over 50, 100, 1000) and noise parameter of 10 (tuned over {10, 20, 30}). All models are trained for 100 iterations (no tuning). BiCCA. First, monolingual word vectors are trained using the skip-gram model5 with negative sampling (Miko"
P16-1157,E14-1049,1,0.924174,"ity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while"
P16-1157,N15-1157,0,0.0143038,", 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks.1 First, we show that different"
P16-1157,P15-1119,0,0.498815,"els often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is"
P16-1157,J15-4004,0,0.129398,"Missing"
P16-1157,C12-1089,0,0.159635,"r different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014;"
P16-1157,2005.mtsummit-papers.11,0,0.0208035,"sh-French (en-fr), English-Swedish (en-sv) and EnglishChinese (en-zh). For en-de and en-sv we use the 1663 l1 l2 #sent #l1 -words #l2 -words en de fr sv zh 1.9 2.0 1.7 2.0 53 55 46 58 51 61 42 50 4.1 We follow the BestAvg parameter selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus. Corpus statistics for all languages is shown in Table 1. 4 Evaluation We measure the quality of the induced crosslingual word embeddings in terms of their performance, when used as features in the following tasks: • monolingual word similarity for English • Cross-lingual dictionary induction • Cross-lingual document clas"
P16-1157,N15-1028,0,0.060572,"do-bilingual document and P (t |s) ∝ exp(tT s). Note that t, s ∈ W ∪ V . Although BiVCD is designed to use comparable corpus, we provide it with parallel data in our experiments (to ensure comparability), and treat two aligned sentences as comparable. 3 Data We train cross-lingual embeddings for 4 language pairs: English-German (en-de), English-French (en-fr), English-Swedish (en-sv) and EnglishChinese (en-zh). For en-de and en-sv we use the 1663 l1 l2 #sent #l1 -words #l2 -words en de fr sv zh 1.9 2.0 1.7 2.0 53 55 46 58 51 61 42 50 4.1 We follow the BestAvg parameter selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh"
P16-1157,W15-1521,0,0.671634,", inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks.1 First, we show that different models can be viewed as instances of a more general framework for inducing"
P16-1157,N15-1058,0,0.0609469,"Missing"
P16-1157,D15-1036,0,0.0547762,"Missing"
P16-1157,P15-1165,0,0.0457057,"Missing"
P16-1157,N12-1052,0,0.0186424,"Missing"
P16-1157,I05-3027,0,0.050151,"er selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus. Corpus statistics for all languages is shown in Table 1. 4 Evaluation We measure the quality of the induced crosslingual word embeddings in terms of their performance, when used as features in the following tasks: • monolingual word similarity for English • Cross-lingual dictionary induction • Cross-lingual document classification • Cross-lingual syntactic dependency parsing The first two tasks intrinsically measure how much can monolingual and cross-lingual similarity benefit from cross-lingual training. The last two tasks measure the ability of cross"
P16-1157,D15-1243,1,0.603503,"e respective English side of each language is shown in column marked Mono. In all cases (except BiCCA on ensv), the bilingually trained vectors achieve better scores than the mono-lingually trained vectors. Overall, across all language pairs, BiCVM is the best performing model in terms of Spearman’s correlation, but its improvement over BiSkip and BiVCD is often insignificant. It is notable that 2 of the 3 top performing models, BiCVM and BiVCD, need sentence aligned and document-aligned corpus only, which are easier to obtain than parallel data with word alignments required by BiSkip. Q VEC. Tsvetkov et al. (2015) proposed an intrinsic evaluation metric for estimating the quality of English word vectors. The score produced by Q VEC measures how well a given set of word vectors is able to quantify linguistic properties 6 We implemented the code for performing the merging as we could not find a tool provided by the authors. en-de en-fr en-sv en-zh 0.29 0.30 0.28 0.28 0.34 0.35 0.32 0.34 0.37 0.39 0.34 0.39 0.30 0.31 0.27 0.30 0.32 0.36 0.32 0.31 avg. 0.29 0.34 0.37 0.30 0.33 Table 2: Word similarity score measured in Spearman’s correlation ratio for English on SimLex-999. The best score for each language"
P16-1157,N13-1011,0,0.0375489,"Missing"
P16-1157,D13-1168,0,0.0341321,"Missing"
P16-1157,P15-2118,0,0.408453,"Missing"
P16-1157,D13-1141,0,0.630951,"ing intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015"
P16-1157,D15-1245,0,\N,Missing
P17-1015,P13-2009,0,0.014434,"7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000 question and rational"
P17-1015,P02-1040,0,0.108663,"Missing"
P17-1015,D13-1160,0,0.104533,"1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000"
P17-1015,P15-1085,0,0.0287949,"Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000 question and rationale pairs, and propose"
P17-1015,D15-1202,0,0.38211,"n four parts, two inputs and two outputs: the description of the problem, which we will denote as the question, and the possible (multiple choice) answer options, denoted as options. Our goal is to generate the description of the rationale used to reach the correct answer, denoted as rationale and the correct option label. Problem 1 illustrates an example of an algebra problem, which must be translated into an expression (i.e., (27x + 17y)/(x + y) = 23) and then the desired quantity (x/y) solved for. Problem 2 is an example that could be solved by multi-step arithmetic operations proposed in (Roy and Roth, 2015). Finally, Problem 3 describes a problem that is solved by testing each of the options, which has not been addressed in the past. 2.1 Construction We first collect a set of 34,202 seed problems that consist of multiple option math questions covering a broad range of topics and difficulty levels. Examples of exams with such problems include the GMAT (Graduate Management Admission Test) and GRE (General Test). Many websites contain example math questions in such exams, where the answer is supported by a rationale. Next, we turned to crowdsourcing to generate new questions. We create a task where"
P17-1015,D16-1117,0,0.218012,"ill an unsolved problem, as each additional step adds complexity to the problem both during inference and decoding. Yet, this is the first result showing that it is possible to solve math problems in such a manner, and we believe this modeling approach and dataset will drive work on this problem. 7 Related Work Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math problem. Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016). We aim to generate natural language rationales, where the bindings between variables and the problem solving approach are mixed into a single generative model that attempts to solve the problem while explaining the approach taken. Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014; BLEU. We observe that the regular sequence to sequence model achieves a low BLEU score. In fact, due to the high perplexities the model generates very short rationales, which frequently consist of segments similar to “Answer s"
P17-1015,D14-1058,0,0.0514733,", as these cannot be copied from the input or output. 6.5 Discussion. While we show that our model can outperform the models built up to date, generating complex rationales as those shown in Figure 1 correctly is still an unsolved problem, as each additional step adds complexity to the problem both during inference and decoding. Yet, this is the first result showing that it is possible to solve math problems in such a manner, and we believe this modeling approach and dataset will drive work on this problem. 7 Related Work Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math problem. Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016). We aim to generate natural language rationales, where the bindings between variables and the problem solving approach are mixed into a single generative model that attempts to solve the problem while explaining the approach taken. Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014; BLEU. We observ"
P17-1015,P12-1051,0,0.01495,"0.025 130 2.99 div(m1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this en"
P17-1015,D13-1176,1,0.441142,"r capsule for bottle T ? (A) $ 0.25 (B) $ 0.12 (C) $ 0.05 (D) $ 0.03 (E) $ 0.002 m sub(m6,m3) 250 6.25 0.025 130 2.99 div(m1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the pr"
P17-1015,P14-1026,0,0.327097,"gth Figure 2: Distribution of examples per length. Table 1: Descriptive statistics of our dataset. define in the instructions that this is not allowed, as it will generate multiple copies of the same problem in the dataset if two or more Turkers copy from the same resource. These Turkers can be detected by checking the nearest neighbours within the collected datasets as problems obtained from online resources are frequently submitted by more than one Turker. Using this method, we obtained 70,318 additional questions. 2.2 17y)/(x + y) = 23 must be solved to obtain the answer. In previous work (Kushman et al., 2014), this could be done by feeding the equation into an expression solver to obtain x/y = 3/2. However, this would skip the intermediate steps 27x + 17y = 23x + 23y and 4x = 6y, which must also be generated in our problem. We propose a model that jointly learns to generate the text in the rationale, and to perform the math operations required to solve the problem. This is done by generating a program, containing both instructions that generate output and instructions that simply generate intermediate values used by following instructions. Statistics Descriptive statistics of the dataset is shown"
P17-1015,D16-1011,0,0.0607361,"Missing"
P17-1015,P16-1057,1,0.916495,"Missing"
P17-1137,N13-1140,1,0.778979,"neural language models have been widely explored (Sutskever et al., 2011; Mikolov et al., 2012; Graves, 2013, inter alia). Attempts to make them more aware of word-level dynamics, using models similar to our hierarchical formulation, have also been proposed (Chung et al., 2017). The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013). These, however, do not use distributed representations on RNNs to capture long-range dependencies. 8 Conclusion In this paper, we proposed a character-level language model with an adaptive cache which selectively assign word probability from past history or character-level decoding. And we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset. To further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages. We also show that our mod"
P17-1137,C00-1027,0,0.165129,"are typically replaced by a special token, called the unknown word token, hUNKi. Although fixedvocabulary language models have some important practical applications and are appealing models for study, they fail to capture two empirical facts about the distribution of words in natural languages. First, vocabularies keep growing as the number of documents in a corpus grows: new words are constantly being created (Heaps, 1978). Second, rare and newly created words often occur in “bursts”, i.e., once a new or rare word has been used once in a document, it is often repeated (Church and Gale, 1995; Church, 2000). The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters (Sutskever et al., 2011; Chung et al., 2017). Character-based models are quite successful at learning what (new) word forms look like (e.g., they learn a language’s orthographic conventions that tell us that sustinated is a plausible English word and bzoxqir is not) and, when based on models that learn long-range dependencies such as RNNs, they can also be good models of how words fit together to form sentences. However, existing character-sequen"
P17-1137,D15-1176,1,0.797601,"n, we describe our hierarchical character language model with a word cache. As is typical for RNN language models, our model uses the chain rule to decompose the problem into incremental predictions of the next word conditioned on the history: p(w) = |w| Y t=1 p(wt |w<t ). We make two modifications to the traditional RNN language model, which we describe in turn. First, we begin with a cache-less model we call the hierarchical character language model (HCLM; §2.1) which generates words as a sequence of characters and constructs a “word embedding” by encoding a character sequence with an LSTM (Ling et al., 2015). However, like conventional closedvocabulary, word-based models, it is based on an LSTM that conditions on words represented by fixed-length vectors.1 The HCLM has no mechanism to reuse words that it has previously generated, so new forms will 1 The HCLM is an adaptation of the hierarchical recurrent encoder-decoder of Sordoni et al. (2015) which was used to model dialog as a sequence of actions sentences which are themselves sequences of words. The original model was proposed to compose words into query sequences but we use it to compose characters into word sequences. only be repeated with"
P17-1137,C16-1165,0,0.018917,"Missing"
P17-1137,P06-1124,0,0.0147366,"ave et al., 2017). Open vocabulary neural language models have been widely explored (Sutskever et al., 2011; Mikolov et al., 2012; Graves, 2013, inter alia). Attempts to make them more aware of word-level dynamics, using models similar to our hierarchical formulation, have also been proposed (Chung et al., 2017). The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013). These, however, do not use distributed representations on RNNs to capture long-range dependencies. 8 Conclusion In this paper, we proposed a character-level language model with an adaptive cache which selectively assign word probability from past history or character-level decoding. And we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset. To further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologic"
P17-1191,P08-1037,0,0.503666,"heir senses. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions. 6 Conclusion In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embeddings for predi"
P17-1191,Q14-1043,0,0.601607,"mportance of each item in a sequence, it can also be applied to non-sequential items. 4 See Table 2 in §4 for detailed results. 2091 training set. At test time, we predict the candidate head with the highest probability according to the model in Eq. 3, i.e., kˆ = arg max p(hk is head = 1). k Figure 3: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP ‘with butter’ attaches to the noun ‘spaghetti’. In the bottom sentence, the PP ‘with chopsticks’ attaches to the verb ‘ate’. Note: This figure and caption have been reproduced from Belinkov et al. (2014). its direct dependent d in the prepositional phrase (PP), our goal is to predict the correct head word for the PP among an ordered list of candidate head words h. Each example in the train, validation, and test sets consists of an input tuple hh, p, di and an output index k to identify the correct head among the candidates in h. Note that the order of words that form each hh, p, di is the same as that in the corresponding original sentence. 3.2 Model Definition Both our proposed and baseline models for PP attachment use bidirectional RNN with LSTM cells (bi-LSTM) to encode the sequence t = hh"
P17-1191,C94-2195,0,0.771055,"convex combinations of their senses. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions. 6 Conclusion In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embed"
P17-1191,D14-1082,0,0.00954565,"s define a single vector for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word ‘pool’ has different meanings, but the same representation is typically used for both of them. Similarly, the fact that ‘pool’ and ‘lake’ are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space. Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Sch¨utze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings. In this work, we represent a word token in a given context by estimating a context-sensi"
P17-1191,D14-1110,0,0.043953,"Missing"
P17-1191,N15-1184,1,0.896681,"xplicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space. Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Sch¨utze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings. In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method"
P17-1191,P12-1092,0,0.0541338,"mation. Related Work This work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations. The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015; Arora et al., 2016, etc.). However, the target of all these approaches is obtaining multisense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which prop"
P17-1191,N15-1070,1,0.904646,"ed to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations. The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015; Arora et al., 2016, etc.). However, the target of all these approaches is obtaining multisense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system"
P17-1191,N15-1164,0,0.0199999,"o improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. Similarly, Johansson and Pina (2015) improved word embeddings by representing each sense of the word in a way that reflects the topology of the semantic network they belong to, and then representing the words as convex combinations of their senses. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al"
P17-1191,N16-1030,1,0.409477,"for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word ‘pool’ has different meanings, but the same representation is typically used for both of them. Similarly, the fact that ‘pool’ and ‘lake’ are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space. Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Sch¨utze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings. In this work, we represent a word token in a given context by estimating a context-sensitive probability distr"
P17-1191,P14-1130,0,0.026954,"Missing"
P17-1191,D14-1113,0,0.0593386,"Missing"
P17-1191,D14-1162,0,0.0848322,"Missing"
P17-1191,H94-1048,0,0.335356,"Missing"
P17-1191,N10-1013,0,0.0386834,"on with distributional information. Related Work This work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations. The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015; Arora et al., 2016, etc.). However, the target of all these approaches is obtaining multisense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (201"
P17-1191,H93-1054,0,0.640279,"wnstream task benefits from all the updates to related words which share one or more concept embeddings. 2089 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2089–2098 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1191 stractions. Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generalization and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word ‘pool’ is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections. 2.2 Figure 1: An example grounding for the word ‘pool’. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word ‘pool’ regardless of its context. Other WordNet senses for ‘pool’ were removed from the figure for simplic"
P17-1191,P15-1173,0,0.0797428,"Missing"
P17-1191,P14-2089,0,0.0232829,"ses on estimating token-level word embeddings as context sensitive distributions of concept em2095 Figure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of infrequent word types. The prepositions are shown in bold, LSTM-PP’s predictions in red and OntoLSTMPP’s predictions in green. Words that are not candidate heads or dependents are shown in brackets. beddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. Similarly, Johansson and Pina (2015) improved word embeddings by representing each"
P17-2058,P16-1231,0,0.0779065,"Missing"
P17-2058,D15-1044,0,0.0463382,"ding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation. 1 Taylor Berg-Kirkpatrick Carnegie Mellon University Pittsburgh, PA, USA tberg@cs.cmu.edu Introduction Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), dialogue generation (Serban et al., 2015), and image captioning (Xu et al., 2015). However, the standard cross-entropy training procedure for these models suffers from the well-known problem of exposure bias: because cross-entropy training always uses gold contexts, the states and contexts encountered during training do not match those encountered at test time. This issue has been addressed using several approaches that try to incorporate awareness of decoding choices into the training optimization. These include reinforcement learning (Ranzato et al., 2016; Bahdanau 366 Proceedings of the 5"
P17-2058,2014.iwslt-evaluation.1,0,0.400421,"Missing"
P17-2058,W03-0419,0,0.0826902,"Missing"
P17-2058,D16-1137,0,0.0405369,"Missing"
P17-2058,Q15-1035,0,0.0224919,"ty of selecting the ground truth token as an inverse sigmoid (Bengio et al., 2015) of epochs with a decay strength parameter k. We also tuned for different values of α and explore the effect of varying α exponentially (annealing) with the epochs. In table 1, we report results for the best performing configuration of decay parameter and the α parameter on the validation set. To account for variance across randomly started runs, we ran multiple random restarts (RR) for all the systems evaluated and always used the RR with the best validation set score to calculate test performance. Related Work Gormley et al. (2015)’s approximation-aware training is conceptually related, but focuses on variational decoding procedures. Hoang et al. (2017) also propose continuous relaxations of decoders, but are focused on developing better inference procedures. Grefenstette et al. (2015) successfully use a soft approximation to argmax in neural stack mechanisms. Finally, Ranzato et al. (2016) experiment with a similarly motivated objective that was not fully continuous, but found it performed worse than the standard training. 5 Comparison We report validation and test metrics for NER and MT tasks in Table 1, F1 and BLEU r"
P18-1132,D10-1066,0,0.025447,"The fox eats worms Stack 0 1 2 3 4 5 6 7 8 9 10 11 12 The (NP |The (NP |The |fox (NP The fox) (S |(NP The fox) (S |(NP The fox) |eats (S |(NP The fox) |(VP |eats (S |(NP The fox) |(VP |eats |worms (S |(NP The fox) |(VP |eats |(NP |worms (S |(NP The fox) |(VP |eats |(NP worms) (S |(NP The fox) |(VP eats (NP worms)) (S (NP The fox) (VP eats (NP worms))) Action GEN (The) NT SW (NP) GEN (fox) anticipatory representations, it is said, could explain the rapid, incremental processing that humans seem to exhibit (Marslen-Wilson, 1973; Tanenhaus et al., 1995); this line of thinking similarly motivates Charniak (2010), among others. While most work in this domain has been concerned with the parsing problem, our findings suggest that anticipatory mechanisms are also beneficial in capturing structural dependencies in language modeling. We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of Schuler et al. (2010). REDUCE 5 NT SW (S) Conclusion GEN (eats) NT SW (VP) GEN (worms) NT SW (NP) REDUCE REDUCE REDUCE N/A Figure"
P18-1132,D16-1257,0,0.420859,"that explicitly models syntactic structures, the recurrent neural network grammars (Dyer et al., 2016, RNNGs), considerably outperforms sequential LSTM language models for cases with multiple attractors (§3). We present experiments affirming that this gain is due to an explicit composition operator rather than the presence of predicted syntactic annotations. Rather surprisingly, syntactic LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plau"
P18-1132,E17-1117,1,0.823907,"Missing"
P18-1132,E17-2020,0,0.0449163,"Missing"
P18-1132,1997.iwpt-1.18,0,0.450127,"y of strings and phrase-structure trees, thereby imposing different biases on the learner. Earlier work in parsing has characterized the plausibility of top-down, left-corner, and bottom-up strategies as viable candidates of human sentence processing, especially in terms of memory constraints and human difficulties with center embedding constructions (Johnson-Laird, 1983; Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992, inter alia), along with neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we cast the three strategies as models of language generation (Manning and Carpenter, 1997), and focus on the empirical question: which generation order has the most appropriate bias in modeling non-local structural dependencies in English? These alternative orders organize the learning problem so as to yield intermediate states in generation that condition on different aspects of the grammatical structure. In number agreement, this amounts to making an agreement controller, such as the word flowers in Fig. 3, more or less salient. If it is more salient, the model should be better-able to inflect the main verb in agreement with this controller, without getting distracted by the attr"
P18-1132,N16-1024,1,0.564713,"in the number agreement task. Given the strong performance of word-based LSTM language models, are there are any substantial benefits, in terms of number agreement accuracy, to explicitly modeling hierarchical structures as an inductive bias? We discover that a 1426 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1426–1436 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics certain class of LSTM language models that explicitly models syntactic structures, the recurrent neural network grammars (Dyer et al., 2016, RNNGs), considerably outperforms sequential LSTM language models for cases with multiple attractors (§3). We present experiments affirming that this gain is due to an explicit composition operator rather than the presence of predicted syntactic annotations. Rather surprisingly, syntactic LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hyp"
P18-1132,J93-2004,0,0.0611748,"rather than vase. In some sense, the composition operator can be understood as injecting a structural recency bias into the model design, as subjects and verbs that are sequentially apart are encouraged to be close together in the RNNGs’ representation. Recurrent Neural Network Grammars Experiments Here we summarize the experimental settings of running RNNGs on the number agreement dataset and discuss the empirical findings. Experimental settings. We obtain phrasestructure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model8 trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.9 At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols. An example of the stack contents (i.e. the prefix) when predicting the verb is provided in Fig. 3(a). We similarly run a grid search over the same hyper-parameter range as the sequential 7 For a complete example of action sequences, we refer the reader to the example provided by Dyer et al. (2016)"
P18-1132,P17-2025,0,0.0870533,"Missing"
P18-1132,N18-1108,0,0.0498081,"ale language model, the primary difference is that we do not map infrequent word types to their POS tags and that we subsample to obtain 500 test instances of each number of attractor due to computation cost; both preprocessing were also done by Linzen et al. (2016). 4 The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/ tree/master/research/lm_1b. 5 This trend is also observed by comparing results with H=150 and H=250. While both models achieve near-identical performance for zero attractor, the model with H=250 persame finding as the recent work of Gulordava et al. (2018), who also find that LSTMs trained with language modeling objectives are able to learn number agreement well; here we additionally identify model capacity as one of the reasons for the discrepancy with the Linzen et al. (2016) results. While the pretrained large-scale language model of Jozefowicz et al. (2016) has certain advantages in terms of model capacity, more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the numbe"
P18-1132,N03-1014,0,0.0402388,"c LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plausibility (Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992) and neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we move away from the realm of parsing and evaluate the three strategies as models of generation instead, and address the following empirical question: which generation order is most appropriately biased to model structural depend"
P18-1132,P04-1013,0,0.258321,"Missing"
P18-1132,D17-1215,0,0.0146658,", more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set. Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infrequent instances that require deeper language understanding abilities (Rimell et al., 2009; Jia and Liang, 2017). In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1). Nevertheless, the fact that number agreement accuracy gets worse as the number of attractors increases is consistent with a sequential recency bias in LSTMs: under this conjecture, identifying the correct structural dependency becomes harder when there are more adjacent nouns of different number forms than the true subject. If the sequential recency conjecture is correct, then LSTMs would perform worse when the structural dependency is more distant in the s"
P18-1132,C92-1032,0,0.803938,"tructures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plausibility (Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992) and neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we move away from the realm of parsing and evaluate the three strategies as models of generation instead, and address the following empirical question: which generation order is most appropriately biased to model structural dependencies in English, as indicated by number agreement accuracy? Our key finding is that the top-down generation outperforms left-corner and bottom-up variants for difficult cases with multiple attractors. In theory, the three traversal strategies approximate the same chain rule tha"
P18-1132,D09-1085,1,0.694538,"rms of model capacity, more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set. Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infrequent instances that require deeper language understanding abilities (Rimell et al., 2009; Jia and Liang, 2017). In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1). Nevertheless, the fact that number agreement accuracy gets worse as the number of attractors increases is consistent with a sequential recency bias in LSTMs: under this conjecture, identifying the correct structural dependency becomes harder when there are more adjacent nouns of different number forms than the true subject. If the sequential recency conjecture is correct, then LSTMs would perform worse when the structural dependency is"
P18-1132,J10-1001,0,0.0137268,"humans seem to exhibit (Marslen-Wilson, 1973; Tanenhaus et al., 1995); this line of thinking similarly motivates Charniak (2010), among others. While most work in this domain has been concerned with the parsing problem, our findings suggest that anticipatory mechanisms are also beneficial in capturing structural dependencies in language modeling. We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of Schuler et al. (2010). REDUCE 5 NT SW (S) Conclusion GEN (eats) NT SW (VP) GEN (worms) NT SW (NP) REDUCE REDUCE REDUCE N/A Figure 6: Example Derivation for left-corner traversal. Each NT SW(X) action adds the open nonterminal symbol (X to the stack, followed by a deterministic swap operator that swaps the top two elements on the stack. Discussion. In Table 5, we focus on empirical results for cases where the structural dependencies matter the most, corresponding to cases with two, three, and four attractors. All three RNNG variants outperform the sequential LSTM language model baseline for these cases. Nevertheles"
P18-1132,P17-1099,0,0.01002,"-up variants in capturing long-distance structural dependencies. 1 Figure 1: An example of the number agreement task with two attractors and a subject-verb distance of five. Introduction Recurrent neural networks (RNNs) are remarkably effective models of sequential data. Recent years have witnessed the widespread adoption of recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997) in various NLP tasks, with state of the art results in language modeling (Melis et al., 2018) and conditional generation tasks like machine translation (Bahdanau et al., 2015) and text summarization (See et al., 2017). Here we revisit the question asked by Linzen et al. (2016): as RNNs model word sequences without explicit notions of hierarchical structure, to what extent are these models able to learn non-local syntactic dependencies in natural language? Identifying number agreement between subjects and verbs—especially in the presence of attractors—can be understood as a cognitivelymotivated probe that seeks to distinguish hierarchical theories from sequential ones, as models that rely on sequential cues like the most recent noun would favor the incorrect verb form. We provide an example of this task in"
P18-1132,E17-2060,0,0.0252701,"to resolve dependencies between word tokens. Second, by nature of modeling characters, non-local structural dependencies are sequentially further apart than in the wordbased language model. On the other hand, character LSTMs have the ability to exploit and share informative morphological cues, such as the fact that plural nouns in English tend to end with ‘s’. As demonstrated on the last row of Table 2, we find that the character LSTM language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts. This finding is consistent with that of Sennrich (2017), who find that character-level decoders in neural machine translation perform worse than subword models in capturing morphosyntactic agreement. To some extent, our finding demonstrates the limitations that character LSTMs face in learning structure from language modeling objectives, despite earlier evidence that character LSTM language models are able to implicitly acquire a lexicon (Le Godais et al., 2017). 3.1 RNNGs (Dyer et al., 2016) are language models that estimate the joint probability of string terminals and phrase-structure tree nonterminals. Here we use stack-only RNNGs that achieve"
P18-1132,C00-2137,0,0.311374,"cal results for cases where the structural dependencies matter the most, corresponding to cases with two, three, and four attractors. All three RNNG variants outperform the sequential LSTM language model baseline for these cases. Nevertheless, the top-down variant outperforms both left-corner and bottom-up strategies for difficult cases with three or more attractors, suggesting that the top-down strategy is most appropriately biased to model difficult number agreement dependencies in English. We run an approximate randomization test by stratifying the output and permuting within each stratum (Yeh, 2000) and find that, for four attractors, the performance difference between the top-down RNNG and the other variants is statistically significant at p &lt; 0.05. The success of the top-down traversal in the domain of number-agreement prediction is consistent with a classical view in parsing that argues top-down parsing is the most human-like parsing strategy since it is the most anticipatory. Only Given enough capacity, LSTMs trained on language modeling objectives are able to learn syntax-sensitive dependencies, as evidenced by accurate number agreement accuracy with multiple attractors. Despite thi"
P18-1254,P93-1005,0,0.196159,"ocessing can be found in naturalistic speech stimuli if ambiguity resolution is modeled as beam search. 2 w Prob(word) current state vector of “stack” LSTM NP v NP ry ng e se t g t ca do ea hu th cha … … (S x x NP Recurrent neural network grammars for incremental processing Recurrent neural network grammars (henceforth: RNNGs Kuncoro et al., 2017; Dyer et al., 2016) are probabilistic models that generate trees. The probability of a tree is decomposed via the chain rule in terms of derivational actionprobabilities that are conditioned upon previous actions i.e. they are history-based grammars (Black et al., 1993). In the vanilla version of RNNG, these steps follow a depth-first traversal of the developing phrase structure tree. This entails that daughters are announced bottom-up one by one as they are completed, rather than being predicted at the same time as the mother. Each step of this generative story depends on the state of a stack, depicted inside the gray box in Figure 1. This stack is “neuralized” such that each stack entry corresponds to a numerical vector. At each stage of derivation, a single vector summarizing the entire stack is available in the form of the final state of a neural sequenc"
P18-1254,C00-1017,0,0.189377,"such that some small number kf t of parser states are promoted directly into nextword. These states are required to come via a lexical action, but in the absence of fast tracking they quite possibly would have failed the thresholding step in line 5. Table 1 shows Penn Treebank accuracies for this word-synchronous beam search procedure, as applied to RNNG. As expected, accuracy goes up as the parser considers more and more analyses. Above k = 200, the RNNG+beam search combination outperforms a conditional model based on greedy decoding (88.9). This demonstration reemphasizes the point, made by Brants and Crocker (2000) among others, that cognitively-plausible incremental processing can be achieved without loss of parsing performance. They quantify unexpectedness and uncertainty, respectively, about alternative syntactic analyses at a given point within a sentence. Hale (2016) reviews their applicability across many different languages, psycholinguistic measurement techniques and grammatical models. Recent work proposes possible relationships between these two metrics, at the empirical as well as theoretical level (van Schijndel and Schuler, 2017; Cho et al., 2018). metric DISTANCE SURPRISAL ENTROPY 4 Comple"
P18-1254,W18-0103,0,0.029355,"Missing"
P18-1254,P10-2012,0,0.0237439,"sing that occurs during normal human language comprehension. 1 Introduction Computational psycholinguistics has “always been...the thing that computational linguistics stood the greatest chance of providing to humanity” (Kay, 2005). Within this broad area, cognitively-plausible parsing models are of particular interest. They are mechanistic computational models that, at some level, do the same task people do in the course of ordinary language comprehension. As such, they offer a way to gain insight into the operation of the human sentence processing mechanism (for a review see Hale, 2017). As Keller (2010) suggests, a promising place to look for such insights is at the intersection of (a) incremental processing, (b) broad coverage, and (c) neural signals from the human brain. jobrenn@umich.edu The contribution of the present paper is situated precisely at this intersection. It combines a probabilistic generative grammar (RNNG; Dyer et al., 2016) with a parsing procedure that uses this grammar to manage a collection of syntactic derivations as it advances from one word to the next (Stern et al., 2017, cf. Roark, 2004). Via well-known complexity metrics, the intermediate states of this procedure"
P18-1254,J13-4008,0,0.116368,"ffects including the P600, which is known to be associated with syntactic processing (e.g. Osterhout and Holcomb, 1992). Comparison with language models based on long short term memory networks (LSTM, e.g. Hochreiter and Schmidhuber, 1997; Mikolov, 2012; Graves, 2012) shows that these effects are specific to the RNNG. A further analysis pinpoints one of these effects to RNNGs’ syntactic composition mechanism. These positive findings reframe earlier null results regarding the syntaxsensitivity of human processing (Frank et al., 2015). They extend work with eyetracking (e.g. Roark et al., 2009; Demberg et al., 2013) and neuroimaging (Brennan et al., 2016; Bachrach, 2008) to higher temporal resolution.1 Perhaps most significantly, they establish a general correspondence between a computational model and electrophysiological responses to naturalistic language. Following this Introduction, section 2 presents recurrent neural network grammars, emphasizing their suitability for incremental parsing. Sections 3 then reviews a previously-proposed 1 Magnetoencephalography also offers high temporal resolution and as such this work fits into a tradition that includes Wehbe et al. (2014), van Schijndel et al. (2015)"
P18-1254,P03-1054,0,0.0372272,"ther than one. The balanced parentheses (NP and )NP are rather like instructions for some subsequent agent who might later perform the kind of syntactic composition that occurs online in RNNGs, albeit in an implicit manner. In all cases, these language models were trained on chapters 2–12 of Alice’s Adventures in Wonderland. This comprises 24941 words. The stimulus that participants saw during EEG data collection, for which the metrics in Table 2 are calculated, was chapter 1 of the same book, comprising 2169 words. RNNGs were trained to match the output trees provided by the Stanford parser (Klein and Manning, 2003). These trees conform to the Penn Treebank annotation standard but do not explicitly mark long-distance dependency or include any empty categories. They seem to adequately represent basic syntactic properties such 2731 as clausal embedding and direct objecthood; nevertheless we did not undertake any manual correction. During RNNG training, the first chapter was used as a development set, proceeding until the per-word perplexity over all parser actions on this set reached a minimum, 180. This performance was obtained with a RNNG whose state vector was 170 units wide. The corresponding LSTM lang"
P18-1254,P15-1033,1,0.86732,"depth-first traversal of the developing phrase structure tree. This entails that daughters are announced bottom-up one by one as they are completed, rather than being predicted at the same time as the mother. Each step of this generative story depends on the state of a stack, depicted inside the gray box in Figure 1. This stack is “neuralized” such that each stack entry corresponds to a numerical vector. At each stage of derivation, a single vector summarizing the entire stack is available in the form of the final state of a neural sequence model. This is implemented using the stack LSTMs of Dyer et al. (2015). These stack-summary vectors (central rectangle in Figure 1) allow RNNGs to be sensitive to aspects of the left context that would be masked by independence assumptions in a probabilistic context-free grammar. In the present paper, these stack-summaries serve as input to a multi-layer perceptron whose output is converted via softmax into a categorical distribution over three possible parser actions: open a new constituent, close off the latest constituent, or generate a word. A hard decision is made, and if the first or last option is selected, then the same vector-valued stack–summary is aga"
P18-1254,E17-1117,1,0.902822,"Missing"
P18-1254,N16-1024,1,0.577784,"c computational models that, at some level, do the same task people do in the course of ordinary language comprehension. As such, they offer a way to gain insight into the operation of the human sentence processing mechanism (for a review see Hale, 2017). As Keller (2010) suggests, a promising place to look for such insights is at the intersection of (a) incremental processing, (b) broad coverage, and (c) neural signals from the human brain. jobrenn@umich.edu The contribution of the present paper is situated precisely at this intersection. It combines a probabilistic generative grammar (RNNG; Dyer et al., 2016) with a parsing procedure that uses this grammar to manage a collection of syntactic derivations as it advances from one word to the next (Stern et al., 2017, cf. Roark, 2004). Via well-known complexity metrics, the intermediate states of this procedure yield quantitative predictions about language comprehension difficulty. Juxtaposing these predictions against data from human encephalography (EEG), we find that they reliably derive several amplitude effects including the P600, which is known to be associated with syntactic processing (e.g. Osterhout and Holcomb, 1992). Comparison with languag"
P18-1254,Q16-1037,0,0.0469011,"nset, using a non-parametric cluster-based permutation test to correct for multiple comparisons across electrodes and time-points (Maris and Oostenveld, 2007). 6 Language models for literary stimuli We compare the fit against EEG data for models that are trained on the same amount of textual data but differ in the explicitness of their syntactic representations. At the low end of this scale is the LSTM language model. Models of this type treat sentences as a sequence of words, leaving it up to backpropagation to decide whether or not to encode syntactic properties in a learned history vector (Linzen et al., 2016). We use SURPRISAL from the LSTM as a baseline. RNNGs are higher on this scale because they explicitly build a phrase structure tree using a symbolic stack. We consider as well a degraded version, RNNG−comp which lacks the composition mechanism shown in Figure 2. This degraded version replaces the stack with initial substrings of bracket expressions, following Choe and Charniak (2016); Vinyals et al. (2015). An example would be the length 7 string shown below (S (NP the hungry cat )N P (VP Here, vertical lines separate symbols whose vector encoding would be considered separately by RNNG−comp ."
P18-1254,P17-2025,0,0.0660464,"ord may not grow by much. Once there are enough parser states, another threshold called the word beam kword kicks in (line 15). This other threshold sets the number of analyses that are handed off to the next invocation of the algorithm. In the study reported here the word beam remains at the default setting suggested by Stern and colleagues, k/10. Stern et al. (2017) go on to offer a modification of the basic procedure called “fast tracking” which improves performance, particularly when the action beam k is small. Under fast tracking, an additional step is added between lines 4 and 5 of 2729 Fried et al. (2017) RNNG ppl unknown, −fast track this paper ppl=141, −fast track this paper ppl=141, kft = k/100 k=100 k=200 k=400 k=600 k=800 k=1000 k=2000 74.1 80.1 85.3 87.5 88.7 89.6 not reported 71.5 87.1 78.81 88.96 84.15 90.48 86.42 90.64 87.34 90.84 88.16 90.96 89.81 91.25 Table 1: Penn Treebank development section bracketing accuracies (F1) under Word-Synchronous beam search. These figures show that an incremental parser for RNNG can perform well on a standard benchmark. “ppl” indicates the perplexity of over both trees and strings for the trained model on the development set, averaged over words In al"
P18-1254,P82-1020,0,0.763638,"Missing"
P18-1254,D09-1034,0,0.557574,"several amplitude effects including the P600, which is known to be associated with syntactic processing (e.g. Osterhout and Holcomb, 1992). Comparison with language models based on long short term memory networks (LSTM, e.g. Hochreiter and Schmidhuber, 1997; Mikolov, 2012; Graves, 2012) shows that these effects are specific to the RNNG. A further analysis pinpoints one of these effects to RNNGs’ syntactic composition mechanism. These positive findings reframe earlier null results regarding the syntaxsensitivity of human processing (Frank et al., 2015). They extend work with eyetracking (e.g. Roark et al., 2009; Demberg et al., 2013) and neuroimaging (Brennan et al., 2016; Bachrach, 2008) to higher temporal resolution.1 Perhaps most significantly, they establish a general correspondence between a computational model and electrophysiological responses to naturalistic language. Following this Introduction, section 2 presents recurrent neural network grammars, emphasizing their suitability for incremental parsing. Sections 3 then reviews a previously-proposed 1 Magnetoencephalography also offers high temporal resolution and as such this work fits into a tradition that includes Wehbe et al. (2014), van"
P18-1254,W15-1109,0,0.0417584,"Missing"
P18-1254,D17-1178,0,0.370582,"sight into the operation of the human sentence processing mechanism (for a review see Hale, 2017). As Keller (2010) suggests, a promising place to look for such insights is at the intersection of (a) incremental processing, (b) broad coverage, and (c) neural signals from the human brain. jobrenn@umich.edu The contribution of the present paper is situated precisely at this intersection. It combines a probabilistic generative grammar (RNNG; Dyer et al., 2016) with a parsing procedure that uses this grammar to manage a collection of syntactic derivations as it advances from one word to the next (Stern et al., 2017, cf. Roark, 2004). Via well-known complexity metrics, the intermediate states of this procedure yield quantitative predictions about language comprehension difficulty. Juxtaposing these predictions against data from human encephalography (EEG), we find that they reliably derive several amplitude effects including the P600, which is known to be associated with syntactic processing (e.g. Osterhout and Holcomb, 1992). Comparison with language models based on long short term memory networks (LSTM, e.g. Hochreiter and Schmidhuber, 1997; Mikolov, 2012; Graves, 2012) shows that these effects are spe"
P18-1254,D14-1030,0,0.313061,"(e.g. Roark et al., 2009; Demberg et al., 2013) and neuroimaging (Brennan et al., 2016; Bachrach, 2008) to higher temporal resolution.1 Perhaps most significantly, they establish a general correspondence between a computational model and electrophysiological responses to naturalistic language. Following this Introduction, section 2 presents recurrent neural network grammars, emphasizing their suitability for incremental parsing. Sections 3 then reviews a previously-proposed 1 Magnetoencephalography also offers high temporal resolution and as such this work fits into a tradition that includes Wehbe et al. (2014), van Schijndel et al. (2015), Wingfield et al. (2017) and Brennan and Pylkk¨anen (2017). 2727 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2727–2736 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics e ph ras rd e oﬀ … … clo se rat ea wo ph ras ew ne an ge op en NP VP PP S Prob(nonterminal) u Prob(action) the hungry u v w NP Figure 2: RNNG composition function traverses daughter embeddings u, v and w, representing the entire tree with a single vector x. This Figure is reproduced from (Dyer"
P19-1228,P06-1109,0,0.752129,"Spitkovsky et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some"
P19-1228,N19-1116,0,0.167779,"e latent vector, we employ standard amortized inference using reparameterized samples from a variational 2369 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics posterior approximated from an inference network (Kingma and Welling, 2014; Rezende et al., 2014). On standard benchmarks for English and Chinese, the proposed approach is found to perform favorably against recent neural network-based approaches to grammar induction (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019). 2 Probabilistic Context-Free Grammars We consider context-free grammars (CFG) consisting of a 5-tuple G = (S, N , P, Σ, R) where S is the distinguished start symbol, N is a finite set of nonterminals, P is a finite set of preterminals,1 Σ is a finite set of terminal symbols, and R is a finite set of rules of the form, S → A, A∈N A → B C, A ∈ N , B, C ∈ N ∪ P T → w, Charniak, 1992).2 Successful approaches to unsupervised parsing have therefore modified the model/learning objective by guiding potentially unrelated rules to behave similarly. Recognizing that sharing among rul"
P19-1228,P15-1030,0,0.0253383,"odel trained on induced trees on classification tasks. Figure 2: Alignment of induced nonterminals ordered from top based on predicted frequency (therefore NT-04 is the most frequently-predicted nonterminal). For each nonterminal we visualize the proportion of correctly-predicted constituents that correspond to particular gold labels. For reference we also show the precision (i.e. probability of correctly predicting unlabeled constituents) in the rightmost column. tween the original model and the URNNG’s structured inference network, which is parameterized as a neural CRF constituency parser (Durrett and Klein, 2015; Liu et al., 2018).18 Model Analysis We analyze our best compound PCFG model in more detail. Since we induce a full set of nonterminals in our grammar, we can analyze the learned nonterminals to see if they can be aligned with linguistic constituent labels. Figure 2 visualizes the alignment between induced and gold labels, where for each nonterminal we show the empirical probability that a predicted constituent of this type will correspond to a particular linguistic constituent in the test set, conditioned on its being a correct constituent (for reference we also show the precision). We obser"
P19-1228,N16-1024,1,0.888677,"Missing"
P19-1228,P12-2004,0,0.587187,"Carroll and Charniak, 1992). While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered features (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG’s rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there are optimization benefits afforded by over-parameterized models (Arora et al., 2018; Xu et al., 2018; Du et al., 2019), and"
P19-1228,N19-1254,0,0.0148113,"ver, the 2019), though comparison is confounded by various factors such as preprocessing (e.g. we drop punctuation). A neural PCFG/HMM obtains 68.2 and 63.4 respectively. model fails to identify ((T-40 w5 ) (T-22 w6 )) as a constituent in this case (as well as well in the bottom right example). See appendix A.5 for more examples. It is possible that the model is utilizing the subtrees to capture broad template-like structures and then using z to fill them in, similar to recent works that also train models to separate “what to say” from “how to say it” (Wiseman et al., 2018; Peng et al., 2019; Chen et al., 2019a,b). Limitations We report on some negative results as well as important limitations of our work. While distributed representations promote parameter sharing, we were unable to obtain improvements through more factorized parameterizations that promote even greater parameter sharing. In particular, for rules of the type A → BC, we tried having the output embeddings be a function of the input embeddings (e.g. uBC = g([wB ; wC ]) where g is an MLP), but obtained worse results. For rules of the type T → w, we tried using a character-level CNN (dos Santos and Zadrozny, 2014; Kim et al., 2016) to o"
P19-1228,J98-4004,0,0.424064,"more expressive than PCFGs as each sentence has its own set of rule probabilities. However, it still assumes a tree-based generative process, making it possible to learn latent tree structures. Our motivation for the compound PCFG is based on the observation that for grammar induction, first-order context-free assumptions are generally made not because they represent an adequate model of natural language, but because they allow for tractable training.4 Higher-order PCFGs can introduce dependencies between children and ancestors/siblings through, for example, vertical/horizontal Markovization (Johnson, 1998; Klein and Manning, 2003). However such dependencies complicate training due to the rapid increase in the number of rules. Under this view, we can interpret the compound PCFG as a restricted version of some higher-order PCFG where a child can depend on its ancestors and siblings through a shared latent vector. We hypothesize that this dependence among siblings is especially useful in grammar induction from words, where (for example) if we know that watched is used as a verb 4 A piece of evidence for the misspecification of first-order PCFGs as a statistical model of natural language is that i"
P19-1228,W01-0713,0,0.697458,"hest (PC +) principal component values. plementations, training was significantly more expensive (both in terms of time and memory) than NLM-based grammar induction systems due to the O(|R||x|3 ) dynamic program, which makes our approach potentially difficult to scale. 6 Related Work Grammar induction has a long and rich history in natural language processing. Early work on grammar induction with pure unsupervised learning was mostly negative (Lari and Young, 1990; Carroll and Charniak, 1992; Charniak, 1993), though Pereira and Schabes (1992) reported some success on partially bracketed data. Clark (2001) and Klein and Manning (2002) were some of the first successful statistical approaches to grammar induction. In particular, the constituent-context model (CCM) of Klein and Manning (2002), which explicitly models both constituents and distituents, was the basis for much subsequent work (Klein and Manning, 2004; Huang et al., 2012; Golland et al., 2012). Other works have explored imposing inductive biases through Bayesian priors (Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), modified objectives (Smith and Eisner, 2004), and additional constraints on recursion depth (Noji et"
P19-1228,N07-1018,0,0.78419,"language data through direct methods, such as optimizing the log likelihood with the EM algorithm (Lari and Young, 1990; Carroll and Charniak, 1992). While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered features (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG’s rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there ar"
P19-1228,N09-1009,0,0.078737,"supervised initial grammar while the log marginal likelihood improves (Johnson et al., 2007). Similar observations have been made for part-of-speech induction with Hidden Markov Models (Merialdo, 1994). 2371 then the noun phrase is likely to be a movie. In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule probabilities (Kurihara and Sato, 2006; Johnson et al., 2007; Wang and Blunsom, 2013), the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by Cohen et al. (2009) and Cohen and Smith (2009), who also sample local rule probabilities from a logistic normal prior for training dependency models with valence (DMV) (Klein and Manning, 2004). Inference in Compound PCFGs The expressivity of compound PCFGs comes at a significant challenge in learning and inference. Letting θ = {EG , λ} be the parameters of the generative model, we would like to maximize the log marginal likelihood of the observed sentence log pθ (x). In the neural PCFG P the log marginal likelihood log pθ (x) = log t∈TG (x) pθ (t) can be obtained by summing out the latent tree structure using the inside algorithm (Baker,"
P19-1228,N19-1114,1,0.658695,"ploy standard amortized inference using reparameterized samples from a variational 2369 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics posterior approximated from an inference network (Kingma and Welling, 2014; Rezende et al., 2014). On standard benchmarks for English and Chinese, the proposed approach is found to perform favorably against recent neural network-based approaches to grammar induction (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019). 2 Probabilistic Context-Free Grammars We consider context-free grammars (CFG) consisting of a 5-tuple G = (S, N , P, Σ, R) where S is the distinguished start symbol, N is a finite set of nonterminals, P is a finite set of preterminals,1 Σ is a finite set of terminal symbols, and R is a finite set of rules of the form, S → A, A∈N A → B C, A ∈ N , B, C ∈ N ∪ P T → w, Charniak, 1992).2 Successful approaches to unsupervised parsing have therefore modified the model/learning objective by guiding potentially unrelated rules to behave similarly. Recognizing that sharing among rule types is benefici"
P19-1228,J94-2001,0,0.206064,"ful in grammar induction from words, where (for example) if we know that watched is used as a verb 4 A piece of evidence for the misspecification of first-order PCFGs as a statistical model of natural language is that if one pretrains a first-order PCFG on supervised data and continues training with the unsupervised objective (i.e. log marginal likelihood), the resulting grammar deviates significantly from the supervised initial grammar while the log marginal likelihood improves (Johnson et al., 2007). Similar observations have been made for part-of-speech induction with Hidden Markov Models (Merialdo, 1994). 2371 then the noun phrase is likely to be a movie. In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule probabilities (Kurihara and Sato, 2006; Johnson et al., 2007; Wang and Blunsom, 2013), the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by Cohen et al. (2009) and Cohen and Smith (2009), who also sample local rule probabilities from a logistic normal prior for training dependency models with valence (DMV) (Klein and Manning, 2004). Inference in Compound PCFGs Th"
P19-1228,D16-1004,0,0.0551085,"(2001) and Klein and Manning (2002) were some of the first successful statistical approaches to grammar induction. In particular, the constituent-context model (CCM) of Klein and Manning (2002), which explicitly models both constituents and distituents, was the basis for much subsequent work (Klein and Manning, 2004; Huang et al., 2012; Golland et al., 2012). Other works have explored imposing inductive biases through Bayesian priors (Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), modified objectives (Smith and Eisner, 2004), and additional constraints on recursion depth (Noji et al., 2016; Jin et al., 2018). While the framework of specifying the structure of a grammar and learning the parameters is common, other methods exist. Bod (2006) consider a nonparametric-style approach to unsupervised parsing by using random subsets of training subtrees to parse new sentences. Seginer (2007) utilize an incremental algorithm to unsupervised parsing which makes local decisions to create constituents based on a complex set of heuristics. Ponvert et al. (2011) induce parse trees through cascaded applications of finite state models. More recently, neural network-based approaches to grammar"
P19-1228,P18-1249,0,0.111453,"Missing"
P19-1228,P14-1100,0,0.0148972,"unctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some works do not explicitly separate out train/test sets (Reichart and Rappoport, 2010; Golland et al., 2012) while some do (Huang et al., 2012; Parikh et al., 2014; Htut 7 However, we used F1 against validation trees on PTB to select some hyperparameters (e.g. grammar size), as is sometimes done in grammar induction. Hence our PTB results are arguably not fully unsupervised in the strictest sense of the term. The hyperparameters of the PRPN/ON baselines are also tuned using validation F1 for fair comparison. et al., 2018). Maintaining train/test splits is less of an issue for unsupervised structure learning, however in this work we follow the latter and separate train/test data. (4) evaluation: for unlabeled F1 , almost all works ignore punctuation (eve"
P19-1228,P02-1017,0,0.943053,"ameters through optimization. Early work found that it was difficult to induce probabilistic context-free grammars (PCFG) from natural language data through direct methods, such as optimizing the log likelihood with the EM algorithm (Lari and Young, 1990; Carroll and Charniak, 1992). While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives (Klein and Manning, 2002), priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models (Kurihara and Sato, 2006; Johnson et al., 2007; Liang et al., 2007; Wang and Blunsom, 2013), and manually-engineered features (Huang et al., 2012; Golland et al., 2012) to encourage the desired structures to emerge. We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG’s rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful g"
P19-1228,N19-1263,0,0.0161336,"eed mostly PP. However, the 2019), though comparison is confounded by various factors such as preprocessing (e.g. we drop punctuation). A neural PCFG/HMM obtains 68.2 and 63.4 respectively. model fails to identify ((T-40 w5 ) (T-22 w6 )) as a constituent in this case (as well as well in the bottom right example). See appendix A.5 for more examples. It is possible that the model is utilizing the subtrees to capture broad template-like structures and then using z to fill them in, similar to recent works that also train models to separate “what to say” from “how to say it” (Wiseman et al., 2018; Peng et al., 2019; Chen et al., 2019a,b). Limitations We report on some negative results as well as important limitations of our work. While distributed representations promote parameter sharing, we were unable to obtain improvements through more factorized parameterizations that promote even greater parameter sharing. In particular, for rules of the type A → BC, we tried having the output embeddings be a function of the input embeddings (e.g. uBC = g([wB ; wC ]) where g is an MLP), but obtained worse results. For rules of the type T → w, we tried using a character-level CNN (dos Santos and Zadrozny, 2014; Kim"
P19-1228,P04-1061,0,0.915052,"ech induction with Hidden Markov Models (Merialdo, 1994). 2371 then the noun phrase is likely to be a movie. In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule probabilities (Kurihara and Sato, 2006; Johnson et al., 2007; Wang and Blunsom, 2013), the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by Cohen et al. (2009) and Cohen and Smith (2009), who also sample local rule probabilities from a logistic normal prior for training dependency models with valence (DMV) (Klein and Manning, 2004). Inference in Compound PCFGs The expressivity of compound PCFGs comes at a significant challenge in learning and inference. Letting θ = {EG , λ} be the parameters of the generative model, we would like to maximize the log marginal likelihood of the observed sentence log pθ (x). In the neural PCFG P the log marginal likelihood log pθ (x) = log t∈TG (x) pθ (t) can be obtained by summing out the latent tree structure using the inside algorithm (Baker, 1979), which is differentiable and thus amenable to gradientbased optimization. In the compound PCFG, the log marginal likelihood is given by Z X"
P19-1228,P07-1049,0,0.516964,"ingful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some works do not explicitly separate out train/test sets (Reichart and Rappoport, 2010; Golland et al., 2012) while some do (Huang et al., 2012; Parikh et al., 2014; Htut 7 However, we used F1 against validation trees on PTB to select some hyperparameters (e.g. grammar size), as is sometimes done in grammar induction. Hence our PTB results are arguably n"
P19-1228,P04-1062,0,0.0562063,"for grammar induction (Spitkovsky et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test"
P19-1228,P09-1009,0,0.11015,"et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al., 2009), while more recent works induce grammar directly from words (Spitkovsky et al., 2013; Shen et al., 2018); (2) use of punctuation: even within papers that induce a grammar directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents (Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013), some train with punctuation (Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019), while others discard punctuation altogether for training (Shen et al., 2018, 2019); (3) train/test data: some works do not explicitl"
P19-1228,D12-1063,0,0.027687,"f the LSTM and passing it through an affine layer. Model parameters are initialized with Xavier uniform initialization. For training we use Adam (Kingma and Ba, 2015) with β1 = 0.75, β2 = 0.999 and learning rate of 0.001, with a maximum gradient norm limit of 3. We train for 10 epochs with batch size equal to 4. We employ a curriculum learning strategy (Bengio et al., 2009) where we train only on sentences of length up to 30 in the first epoch, and increase this length limit by 1 each epoch. This slightly improved performance and similar strategies have used in the past for grammar induction (Spitkovsky et al., 2012). During training we perform early stopping based on validation perplexity.7 To mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB. Baselines and Evaluation We observe that even on PTB, there is enough variation in setups across prior work on grammar induction to render a meaningful comparison difficult. Some important dimensions along which prior works vary include, (1) lexicalization: earlier work on grammar induction generally assumed gold (or induced) partof-speech tags (Klein and Manning, 2004; Smith and Eisner, 2004; Bod, 2006; Snyder et al."
P19-1228,D18-1356,1,0.894462,"Missing"
P19-1337,P17-2021,0,0.0203852,"elinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical recurrence, thereby providing a shortcut to the gra"
P19-1337,J99-2004,0,0.176336,"0.77 (±0.02) 0.99 (±0.01) 0.93 (±0.02) 0.96 (±0.02) 0.94 (±0.03) 0.95 (±0.01) 0.95 (±0.03) 0.95 (±0.03) 0.93 (±0.02) 0.96 (±0.01) 0.96 (±0.02) 0.95 (±0.01) 0.96 0.93 0.94 0.82 0.85 0.88 0.85 0.82 0.78 0.79 0.86 0.93 (±0.01) 0.77 (±0.03) 0.63 (±0.02) 0.78 (±0.01) 0.83 (±0.02) 0.46 (±0.05) 0.82 (±0.02) 0.70 (±0.02) 0.96 0.91 0.87 0.91 0.93 (±0.06) 0.82 (±0.09) 0.88 (±0.05) 0.79 (±0.02) 0.28 (±0.05) 0.78 (±0.06) 0.53 (±0.04) 0.85 (±0.02) 0.98 0.81 0.90 0.88 Table 1: Replication of Marvin and Linzen (2018) results. M&L-Multi is the Marvin and Linzen (2018) LSTM trained on LM and CCG supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2007) losses with an interpolation factor of 0.5. We report our LSTM LM, small LSTM† , and RNNG† performance († smaller training data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences i"
P19-1337,P17-1080,0,0.02996,"times as much data as the DSA-LSTM, this finding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed"
P19-1337,P18-2003,0,0.0912624,"Association for Computational Linguistics faster to train. On targeted syntactic evaluations, it achieves better accuracy than: (i) a strong LSTM LM which, through careful hyperparameter tuning, performs much better than previously thought (§2); (ii) the teacher RNNG that exploits a hierarchical inductive bias but lacks scalability (§3); and (iii) a born-again network (Furlanello et al., 2018) that similarly learns from KD, albeit without a hierarchical bias from the teacher. We analyse the DSA-LSTM’s internal representation through the syntactic probe (Shi et al., 2016; Adi et al., 2017) of Blevins et al. (2018), and find that the learned representations encode hierarchical information to a large extent, despite the DSA-LSTM lacking direct access to syntactic annotation. While not directly comparable, on subject-verb agreement both the teacher RNNG and student DSA-LSTM outperform BERT (Devlin et al., 2019; Goldberg, 2019), which benefits from bidirectional information and is trained on 30 times as much data. Altogether, these findings suggest that structural biases continue to play an important role, even at massive data scales, in improving the linguistic competence of LMs. 2 Replication of Targeted"
P19-1337,W15-2108,1,0.890398,"Missing"
P19-1337,N16-1024,1,0.952594,"STMs (Hochreiter and Schmidhuber, 1997) have numerous practical applications, but it has also been shown that they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten"
P19-1337,K17-1003,0,0.0736648,"Missing"
P19-1337,P16-1078,0,0.0251677,"some form of syntactic structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a mu"
P19-1337,P17-2012,0,0.0147635,"enderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical recurrence, thereby providing a shortcut to the gradient propagation of lon"
P19-1337,P17-2025,0,0.0199916,"2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures"
P19-1337,N18-1108,0,0.304663,"ecoding (Kuncoro et al., 2018). 1 By modelling each sentence separately, our setup is consistent with that of Marvin and Linzen (2018) but differs from those with cross-sentential context (Mikolov et al., 2010). 2 While BERT (Devlin et al., 2019) achieves even better number agreement performance (Goldberg, 2019), the results are not directly comparable since BERT operates nonincrementally and was trained on 500 times as much data. The current state of the art among models trained on the Linzen et al. (2016) training set is the adaptive universal transformer model (Dehghani et al., 2019). 3473 Gulordava et al. (2018) test perplexity Simple In a sentential complement Short VP coordination Long VP coordination Across a prepositional phrase Across a subject relative clause Across an object relative clause Across an object relative clause (no that) In an object relative clause In an object relative clause (no that) Average of subject-verb agreement Simple In a sentential complement Across a relative clause Average of reflexive anaphora Simple Across a relative clause Average of negative polarity items Average of all constructions Marvin & Linzen models Ours M&L-LSTM M&L-Multi Our LSTM 78.65 61.10 53.73 (±0.16"
P19-1337,D16-1257,0,0.022929,"orical distribution defined by an affine transformation and a softmax: at ∼ softmax(Wa ht + ba ). • If at ∈ {GEN, NT}, the model samples a terminal x or a non-terminal n from each respective categorical distribution as the next input: x ∼ softmax(Wx ht + bx ), n ∼ softmax(Wn ht + bn ). • If at = REDUCE, the topmost stack elements going back to the last incomplete non-terminal are popped, and a composition function (here a bidirectional LSTM) is executed to represent the completed phrase on the stack. This recursive composition function constitutes a primary difference with the syntactic LM of Choe and Charniak (2016) that operates sequentially, and has been found to be crucial for achieving good number agreement (Kuncoro et al., 2018) and correlation with brain signals (Hale et al., 2018). The stack LSTM, composition function, lookup embeddings, and pairs of affine transformation weights and biases {W, b} are model parameters. 3.2 Experiments Here we outline the experimental settings and present our RNNG findings. Experimental settings. We implement the RNNG with DyNet and enable autobatching on GPU. Predicted phrase-structure trees for the training and validation sets of the Gulordava et al. (2018) Wikip"
P19-1337,P18-1254,1,0.940646,"es to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters e"
P19-1337,P04-1013,0,0.195863,"Missing"
P19-1337,J07-4004,1,0.60422,".93 (±0.02) 0.96 (±0.02) 0.94 (±0.03) 0.95 (±0.01) 0.95 (±0.03) 0.95 (±0.03) 0.93 (±0.02) 0.96 (±0.01) 0.96 (±0.02) 0.95 (±0.01) 0.96 0.93 0.94 0.82 0.85 0.88 0.85 0.82 0.78 0.79 0.86 0.93 (±0.01) 0.77 (±0.03) 0.63 (±0.02) 0.78 (±0.01) 0.83 (±0.02) 0.46 (±0.05) 0.82 (±0.02) 0.70 (±0.02) 0.96 0.91 0.87 0.91 0.93 (±0.06) 0.82 (±0.09) 0.88 (±0.05) 0.79 (±0.02) 0.28 (±0.05) 0.78 (±0.06) 0.53 (±0.04) 0.85 (±0.02) 0.98 0.81 0.90 0.88 Table 1: Replication of Marvin and Linzen (2018) results. M&L-Multi is the Marvin and Linzen (2018) LSTM trained on LM and CCG supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2007) losses with an interpolation factor of 0.5. We report our LSTM LM, small LSTM† , and RNNG† performance († smaller training data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences in a process reminiscent o"
P19-1337,N19-1419,0,0.0365748,"ding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed in this task. We use a linear classifier for the"
P19-1337,P18-1198,0,0.0312272,"the DSA-LSTM, this finding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed in this task. We use a"
P19-1337,P18-1031,0,0.024229,"ly, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge from a large training corpu"
P19-1337,N19-1423,0,0.170169,"at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge from a large training corpus. By learning from b"
P19-1337,P15-1033,1,0.812139,"aining data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences in a process reminiscent of shift-reduce parsing. At any given state, the decision over which action to take is parameterised by a stack LSTM (Dyer et al., 2015) encoding partially-completed constituents. Let ht be the stack LSTM hidden state at time t. The next action at ∈ {GEN, NT, REDUCE} is sampled according to a categorical distribution defined by an affine transformation and a softmax: at ∼ softmax(Wa ht + ba ). • If at ∈ {GEN, NT}, the model samples a terminal x or a non-terminal n from each respective categorical distribution as the next input: x ∼ softmax(Wx ht + bx ), n ∼ softmax(Wn ht + bn ). • If at = REDUCE, the topmost stack elements going back to the last incomplete non-terminal are popped, and a composition function (here a bidirection"
P19-1337,D16-1139,0,0.28832,"interthat syntactic regularities are inferred. In contrast, polate the distillation (left) and LM (right) losses: the interpolated target assigns a minimum probability of 0.5 to the correct label, but crucially con5 This procedure of training a student LSTM LM on string tains additional information about the plausibility samples from the RNNG with K ≈ 3, 000, 000 yields a high of every alternative based on the teacher RNNG’s validation perplexity of above 1,000, due to the enormity of the sample space and the use of discrete samples. predictions. Under this objective, the plural verbs 6 While Kim and Rush (2016) proposed a technique for sequence-level KD for machine translation through beam search, the same technique is not directly applicable to LM, which is an unconditional language generation problem. 7 Recall that `KD (x; θ) does not depend on the true next word x∗j . 8 We use the same pre-trained Berkeley parser to obtain training and validation trees in §3. 9 ˆ berk The resulting syntactic prefix y &lt;j (x) for approximating ∗ t(w |x &lt;j ) under the RNNG is obtained from a Berkeley parser that has access to yet unseen words x&gt;j . 3476 Figure 1: Example of the KD target (top), the standard LM targe"
P19-1337,N19-1114,1,0.836935,"esting that the means by which the DSA-LSTM achieves better syntactic competence is by tracking more hierarchical information during sequential processing. 5 Related Work Augmenting language models with syntactic information and structural inductive bias has been a long-standing area of research. To this end, syntactic language models estimate the joint probability of surface strings and some form of syntactic structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the"
P19-1337,E17-1117,1,0.884318,"Missing"
P19-1337,P18-1132,1,0.922299,"at they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such,"
P19-1337,P18-1129,0,0.0160896,"eralisations (and we have shown that it largely does in §3), every training instance provides the student LSTM with a wealth of information about all the possible legitimate continuations according to the predictions of the hierarchical teacher, thereby making it easier for the student to learn the appropriate hierarchical constraints and generalisations. Differences with other KD work. Our approach departs from the predominant view of distillation primarily as a means of compressing knowledge from a bigger teacher or an ensemble to a compact student (Ba and Caruana, 2014; Kim and Rush, 2016; Liu et al., 2018, inter alia) in two important ways. First, here the teacher and student models are different in character, and not just in size: we transfer knowledge from a teacher that models the joint probability of strings and phrasestructure trees through hierarchical operations, to a student that only models surface strings through sequential operations. This setup presents an interesting dynamic since the DSA-LSTM has to mimic the predictions of the RNNG, which conditions on syntactic annotation to guide hierarchical operations, even though the DSA-LSTM itself has no direct access to any syntactic ann"
P19-1337,D18-1151,0,0.402372,"training data it learns from. On targeted syntactic evaluations, we find that, while sequential LSTMs perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data. 1 Introduction Language models (LMs) based on sequential LSTMs (Hochreiter and Schmidhuber, 1997) have numerous practical applications, but it has also been shown that they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried"
P19-1337,P15-2084,0,0.0621342,"Missing"
P19-1337,W17-4707,0,0.0161857,"structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical"
P19-1337,N18-1202,0,0.0669526,"., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge fro"
P19-1337,N07-1051,0,0.207262,"Missing"
P19-1337,J01-2004,0,0.073207,"Missing"
P19-1337,D16-1159,0,0.185934,", Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics faster to train. On targeted syntactic evaluations, it achieves better accuracy than: (i) a strong LSTM LM which, through careful hyperparameter tuning, performs much better than previously thought (§2); (ii) the teacher RNNG that exploits a hierarchical inductive bias but lacks scalability (§3); and (iii) a born-again network (Furlanello et al., 2018) that similarly learns from KD, albeit without a hierarchical bias from the teacher. We analyse the DSA-LSTM’s internal representation through the syntactic probe (Shi et al., 2016; Adi et al., 2017) of Blevins et al. (2018), and find that the learned representations encode hierarchical information to a large extent, despite the DSA-LSTM lacking direct access to syntactic annotation. While not directly comparable, on subject-verb agreement both the teacher RNNG and student DSA-LSTM outperform BERT (Devlin et al., 2019; Goldberg, 2019), which benefits from bidirectional information and is trained on 30 times as much data. Altogether, these findings suggest that structural biases continue to play an important role, even at massive data scales, in improving the linguistic"
P19-1337,D17-1178,0,0.0710463,"Missing"
P19-1337,D18-1503,0,0.0447877,"Missing"
P19-1645,P09-1012,0,0.222391,"ber of efforts to annotate word boundaries. We evaluate against two corpora that have been manually segmented according different segmentation standards. unknown languages or domains. Thus, our experiments are designed to assess how well the models infers word segmentations of unsegmented inputs when they are trained and tuned to maximize the likelihood of the held-out text. DP/HDP Benchmarks Among the most effective existing word segmentation models are those based on hierarchical Dirichlet process (HDP) models (Goldwater et al., 2009; Teh et al., 2006) and hierarchical Pitman–Yor processes (Mochihashi et al., 2009). As a representative of these, we use a simple bigram HDP model: θ· ∼ DP(α0 , p0 ) Beijing University Corpus (PKU) The Beijing University Corpus was one of the corpora used for the International Chinese Word Segmentation Bakeoff (Emerson, 2005). Chinese Penn Treebank (CTB) We use the Penn Chinese Treebank Version 5.1 (Xue et al., 2005). It generally has a coarser segmentation than PKU (e.g., in CTB a full name, consisting of a given name and family name, is a single token), and it is a larger corpus. 5.3 Image Caption Dataset To assess whether jointly learning about meanings of words from non"
P19-1645,N09-1036,0,0.108255,"menting text (although we will see below that its performance is much less good on other datasets). Second, despite careful tuning, the HMLSTM of Chung et al. (2017) fails to discover good segments, although in their paper they show that when spaces are present between, HMLSTMs learn to switch between their internal models in response to them. Furthermore, the priors used in the DP/HDP models were tuned to maximize the likelihood assigned to the validation set by the inferred posterior predictive distribution, in contrast to previous papers which either set them subjectively or inferred them (Johnson and Goldwater, 2009). For example, the DP and HDP model with subjective priors obtained 53.8 and 72.3 F1 scores, respectively (Goldwater et al., 2009). However, when the hyperparameters are set to maximize held-out likelihood, this drops obtained 56.1 and 56.9. Another result on this dataset is the feature unigram model of Berg-Kirkpatrick et al. (2010), which obtains an 88.0 F1 score with hand-crafted features and by selecting the regularization strength to optimize segmentation performance. Once the features are removed, the model achieved a 71.5 F1 score when it is tuned on segmentation performance and only 11"
P19-1645,P17-1137,1,0.544214,"the induced segmentations, in both unconditional (sequence-only) contexts and when conditioning on a related image. First, we look at the segmentations induced by our model. We find that these correspond closely to human intuitions about word segments, competitive with the best existing models for unsupervised word discovery. Importantly, these segments are obtained in models whose hyperparameters are tuned to optimize validation (held-out) likelihood, whereas tuning the hyperparameters of our benchmark models using held-out likelihood produces poor segmentations. Second, we confirm findings (Kawakami et al., 2017; Mielke and Eisner, 2018) that show that word segmentation information leads to better language models compared to pure character models. However, in contrast to previous work, we realize this performance improvement without having to observe the segment boundaries. Thus, our model may be applied straightforwardly to Chinese, where word boundaries are not part of the orthography. 1 Since the lexical memory stores strings that appear in the training data, each sentence could, in principle, be generated as a single lexical unit, thus the model could fit the training data perfectly while general"
P19-1645,W01-0714,0,0.0168171,"l memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al., 2009; Wang et al., 2014) and phrases (Klein and Manning, 2001) have been investigated. Recently, speech recognition has benefited from techniques that enable the discovery of subword units (Chan et al., 2017; Wang et al., 2017); however, in that work, the optimally discovered character sequences look quite unlike orthographic words. In fact, the model proposed by Wang et al. (2017) is essentially our model without a lexicon or the expected length regularization, i.e., (−memory, −length), which we have shown performs quite poorly in terms of segmentation accuracy. Finally, some prior work has also sought to discover lexical units directly from speech base"
P19-1645,D09-1005,0,0.0419014,"rk (Liang and Klein, 2009; Berg-Kirkpatrick et al., 2010). This expectation is a differentiable function of the model parameters. Because of the linearity of the penalty across segments, it can be computed efficiently using the above dynamic programming algorithm under the expectation semiring (Eisner, 2002). This is particularly efficient since the expectation semiring jointly computes the expectation and marginal likelihood in a single forward pass. For more details about computing gradients of expectations under distributions over structured objects with dynamic programs and semirings, see Li and Eisner (2009). 4.1 Training Objective The model parameters are trained by minimizing the penalized log likelihood of a training corpus D of unsegmented sentences, X L= [− log p(x) + λR(x, β)]. x∈D 5 Datasets We evaluate our model on both English and Chinese segmentation. For both languages, we used standard datasets for word segmentation and language modeling. We also use MS-COCO to evaluate how the model can leverage conditioning context information. For all datasets, we used train, validation and test splits.2 Since our model assumes a closed character set, we removed validation and test samples which co"
P19-1645,P08-1084,0,0.0514809,"r language models than neural models. The neural model of Sun and Deng (2018) is similar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al., 2009; Wang et al., 2014) and phrases (Klein and Manning, 2001) have been investigated. Recently, speech recognition has benefited from techniques that enable the discovery of subword units (Chan et al., 2017; Wang et al., 2017); however, in that work, the optimally discovered character sequences look quite unlike orthographic words. In fact, the model proposed by Wang et al. (2017) is essentially our model without a lexicon or the expected length regularization, i.e., (−memory, −length), which we have shown performs quite poorly in terms of s"
P19-1645,D18-1531,0,0.130379,"antially non-local dependencies). Additionally, the features and grammars used in prior work reflect certain English-specific design considerations (e.g., syllable structure in the case of adaptor grammars and phonotactic equivalence classes in the feature unigram model), which make them questionable models if the goal is to ex6436 plore what models and biases enable word discovery in general. For Chinese, the best nonparametric models perform better at segmentation (Zhao and Kit, 2008; Mochihashi et al., 2009), but again they are weaker language models than neural models. The neural model of Sun and Deng (2018) is similar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al.,"
P19-1645,N09-1069,0,0.0408723,"ed by replacing the summation with a max operator in Eq. 3 and maintaining backpointers. 4 Expected length regularization When the lexical memory contains all the substrings in the training data, the model easily overfits by copying the longest continuation from the memory. To prevent overfitting, we introduce a regularizer that penalizes based on the expectation of the exponentiated (by a hyperparameter β) length of each segment: X X |s|β . R(x, β) = p(s |x) s:π(s)=x s∈s This can be understood as a regularizer based on the double exponential prior identified to be effective in previous work (Liang and Klein, 2009; Berg-Kirkpatrick et al., 2010). This expectation is a differentiable function of the model parameters. Because of the linearity of the penalty across segments, it can be computed efficiently using the above dynamic programming algorithm under the expectation semiring (Eisner, 2002). This is particularly efficient since the expectation semiring jointly computes the expectation and marginal likelihood in a single forward pass. For more details about computing gradients of expectations under distributions over structured objects with dynamic programs and semirings, see Li and Eisner (2009). 4.1"
P19-1645,J01-3002,0,0.142534,"esults using (1) the surprisal criterion of Elman (1990) and (2) a two-level hierarchical multiscale LSTM (Chung et al., 2017), which has been shown to predict boundaries in 6433 whitespace-containing character sequences at positions corresponding to word boundaries. As with all experiments in this paper, the BR-corpora for this experiment do not contain spaces. SNLM Model configurations and Evaluation LSTMs had 512 hidden units with parameters learned using the Adam update rule (Kingma and Ba, 2015). We evaluated our models with bits-percharacter (bpc) and segmentation accuracy (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009). Refer to Appendices C–F for details of model configurations and evaluation metrics. For the image caption dataset, we extend the model with a standard attention mechanism in the backbone LSTM (LSTMenc ) to incorporate image context. For every character-input, the model calculates attentions over image features and use them to predict the next characters. As for image representations, we use features from the last convolution layer of a pre-trained VGG19 model (Simonyan and Zisserman, 2014). 7 Results In this section, we first do a careful comparison of segmentation p"
P19-1645,P14-2122,0,0.013653,"ilar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al., 2009; Wang et al., 2014) and phrases (Klein and Manning, 2001) have been investigated. Recently, speech recognition has benefited from techniques that enable the discovery of subword units (Chan et al., 2017; Wang et al., 2017); however, in that work, the optimally discovered character sequences look quite unlike orthographic words. In fact, the model proposed by Wang et al. (2017) is essentially our model without a lexicon or the expected length regularization, i.e., (−memory, −length), which we have shown performs quite poorly in terms of segmentation accuracy. Finally, some prior work has also sought to discover l"
P19-1645,I08-1002,0,0.0496608,"igram word model; the adaptor grammar model is effectively phrasal unigram model; both are incapable of generalizing about substantially non-local dependencies). Additionally, the features and grammars used in prior work reflect certain English-specific design considerations (e.g., syllable structure in the case of adaptor grammars and phonotactic equivalence classes in the feature unigram model), which make them questionable models if the goal is to ex6436 plore what models and biases enable word discovery in general. For Chinese, the best nonparametric models perform better at segmentation (Zhao and Kit, 2008; Mochihashi et al., 2009), but again they are weaker language models than neural models. The neural model of Sun and Deng (2018) is similar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple"
Q14-1016,W13-1016,0,0.0856848,"Missing"
Q14-1016,I11-1130,0,0.0168818,"tic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Trad"
Q14-1016,W06-1620,0,0.105697,"rpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. Gimpel and Smith’s (2011) shallow, gappy language model allows arbitrary token groupings within a sentence, whereas our model imposes projectivity and nesting constraints (§3). Blunsom and Baldwin (2006) present a sequence model for HPSG supertagging, and evaluate performance on discontinuous MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures"
Q14-1016,J92-4003,0,0.037196,"ramming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 clusters 199 on the 21-million-word Yelp Academic Dataset15 (which is similar in genre to the annotated web reviews data) gives us a hard clustering of word types. To our tagger, we add features mapping the previous, current, and next token to Brown cluster IDs. The feature for the current token"
Q14-1016,N10-1029,0,0.148488,"units in the lexicon. As figure 1 illustrates, MWEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing c"
Q14-1016,W06-1670,0,0.0891639,"ng on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. G"
Q14-1016,W02-1001,0,0.0210912,"expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8 Hierarchical modeling based on our representations is left to future work. constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose asymptotic complexity is linear in the length of the input and (with a first-order Markov assumption) quadratic in the number of tags. Below, we review the structured perceptron and discuss our cost function, features, and experimental setup. 5.1 Cost-Augmented Structured Perceptron The structured perceptron’s (Collins, 2002) learning procedure, algorithm 1, generalizes the clas"
Q14-1016,W11-0809,0,0.0955959,"like their out-of-gap counterparts. Gappy, 2-level (8 tags). 8 tags are required to encode the 2-level scheme with gaps: {O o B b ¯ I¯ ı˜ I˜ ı}. Variants of the inside tag are marked for strength of the incoming link—this applies gap-externally (capitalized tags) and gap-internally (lowercase tags). If ¯ I or ˜ I immediately follows a gap, its diacritic reflects the strength of the gappy expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8 Hierarchical modeling based on our representations is left to future work. constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose a"
Q14-1016,P12-1022,0,0.521376,",y′ ) + cost(y,y′ ,x)) if yˆ ≠ y then w ← w + g(x,y) − g(x, yˆ ) w ← w +tg(x,y) −tg(x, yˆ ) end t ← t +1 end end Output: w − (w/t) Algorithm 1: Training with the averaged perceptron. (Adapted from Daumé, 2006, p. 19.) N experiments showed that a cost function penalizing all recall errors—i.e., with ρ⟦y∗ ≠ O ∧ y′ = O⟧ as the second term, as in Mohit et al.—tended to append additional tokens to high-confidence MWEs (such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary;11 the WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2"
Q14-1016,W09-2903,0,0.21043,"edParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinctio"
Q14-1016,I13-1168,0,0.0572292,"in termine the number of training iterations (epochs) M places to eat in Baltimore (because eat in, meaning by early stopping—that is, after each iteration, we use ‘eat at home,’ is listed in WordNet). The supervised the model to decode the held-out data, and when that approach has learned not to trust WordNet too much accuracy ceases to improve, use the previous model. due to this sort of ambiguity. Downstream applicaThe two hyperparameters are the number of iterations tions that currently use lexicon matching for MWE and the value of the recall cost hyperparameter (ρ). identification (e.g., Ghoneim and Diab, 2013) likely Both are tuned via cross-validation on train; we use stand to benefit from our statistical approach. the multiple of 50 that maximizes average link-based F1 . The chosen values are shown in table 3. Experi- 6.2 How best to exploit MWE lexicons (type-level information)? ments were managed with the ducttape tool.18 For statistical tagging (right portion of table 2), using 6 Results more preexisting (out-of-domain) lexicons generally We experimentally address the following questions improves recall; precision also improves a bit. A lexicon of MWEs occurring in the non-held-out to probe an"
Q14-1016,W11-2165,1,0.894392,"Missing"
Q14-1016,W13-3511,0,0.0198865,"nd one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 cluste"
Q14-1016,D11-1067,0,0.0185646,"Missing"
Q14-1016,hajic-etal-2012-announcing,0,0.022231,"Missing"
Q14-1016,D08-1104,0,0.0477308,"such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many va"
Q14-1016,P08-1068,0,0.0247046,"f which may be variables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: http"
Q14-1016,J93-2004,0,0.0472323,"valuate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide range of constructions, the annotator is not 1 2 http://www.ark.cs.cmu.edu/LexSem/ Because we use treebank data, syntactic parses are available to assist in post hoc analysis. Syntactic information was not shown to annotators. # of constituent tokens 2 3"
Q14-1016,W97-0311,0,0.106691,"MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagge"
Q14-1016,H93-1061,0,0.216655,"such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary;11 the WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2003); the named entities and other MWEs in the WSJ corpus on the English side of the CEDT (Hajiˇc et al., 2012); 10 The WordNet API in NLTK (Bird et al., 2009) was used for lemmatization. 11 http://en.wiktionary.org; data obtained from https://toolserver.org/~enwikt/definitions/ enwikt-defs-20130814-en.tsv.gz L OOKUP preexising lexicons none WordNet + SemCor 6 lexicons 10 lexicons entries 0 71k 420k 437k best con"
Q14-1016,N04-1043,0,0.0119357,"f word lemmas, some of which may be variables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) im"
Q14-1016,E12-1017,1,0.651813,"sion of the structured perceptron that is sensitive to different kinds of errors during training. When recall is the bigger obstacle, we can adopt the following cost function: given a sentence x, its gold labeling y∗ , and a candidate labeling y′ , ∗ ′ ∣y∗ ∣ cost(y ,y ,x) = ∑ c(y∗j ,y′j ) where j=1 ′ c(y∗ ,y′ ) = ⟦y∗ ≠ y ⟧ + ρ⟦y∗ ∈ {B, b} ∧ y′ ∈ {O, o}⟧ A single nonnegative hyperparameter, ρ, controls the tradeoff between recall and accuracy; higher ρ biases the model in favor of recall (possibly hurting accuracy and precision). This is a slight variant of the recall-oriented cost function of Mohit et al. (2012). The difference is that we only penalize beginning-of-expression recall errors. Preliminary 9 The 8-tag scheme licenses 42 tag bigrams: sequences such as B O and o ¯ ı are prohibited. There are also constraints on the allowed tags at the beginning and end of the sequence. 198 Input: data ⟨⟨x(n) ,y(n) ⟩⟩n=1 ; number of iterations M w←0 w←0 t ←1 for m = 1 to M do for n = 1 to N do ⟨x,y⟩ ← ⟨x(n) ,y(n) ⟩ yˆ ← argmaxy′ (w⊺ g(x,y′ ) + cost(y,y′ ,x)) if yˆ ≠ y then w ← w + g(x,y) − g(x, yˆ ) w ← w +tg(x,y) −tg(x, yˆ ) end t ← t +1 end end Output: w − (w/t) Algorithm 1: Training with the averaged per"
Q14-1016,W06-2405,0,0.0359021,"he sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking"
Q14-1016,C12-1127,0,0.308867,"WEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary as"
Q14-1016,N13-1039,1,0.165809,"Missing"
Q14-1016,W12-3311,0,0.0171944,"ction Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a"
Q14-1016,W12-3301,0,0.207661,"Missing"
Q14-1016,ramisch-etal-2010-mwetoolkit,0,0.0636479,"lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allows for MWEs containing gaps, and includes a strength distinction to separate highly idiomatic expressions from collocations. It is trained and evaluated on a corp"
Q14-1016,W95-0107,0,0.0576853,"O gappy, he was willing to budge a little on the price which means a lot to me . ¯ O ˜ ¯ ˜ ˜ 2-level O O O O B b ¯ ı I O O B I I I I O (O∣BI+ )+ ¯I ˜]+ )+ (O∣B[I (O∣B(o∣bi+ ∣I)∗ I+ )+ ¯I ˜])∗ [I ¯I ˜]+ )+ ¯ı ˜]+ ∣[I (O∣B(o∣b[ı Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications. score because F1 = F1↑ = F1↓ . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the"
Q14-1016,W09-1119,0,0.264965,"+ ¯I ˜])∗ [I ¯I ˜]+ )+ ¯ı ˜]+ ∣[I (O∣B(o∣b[ı Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications. score because F1 = F1↑ = F1↓ . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the standard contiguous chunking representation from Ramshaw and Marcus (1995) using the tags {O B I}. O is for tokens outside any chunk; B marks tokens beginning a chunk; and I marks o"
Q14-1016,D11-1141,0,0.0107976,"Missing"
Q14-1016,schneider-etal-2014-comprehensive,1,0.374841,"rpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a prespecified MWE lexicon (§2). The scheme applies to gappy (discontinuous) as well as contiguous expressions, and allows for a qualitative distinction of association strengths. In Schneider et al. (2014) we have applied this scheme to fully annotate a 55,000-word corpus of English web reviews (Bies et al., 2012a), a conversational genre in which colloquial idioms are highly salient. This article’s main contribution is to show that the representation— constrained according to linguistically motivated assumptions (§3)—can be transformed into a sequence tagging scheme that resembles standard approaches in named entity recognition and other text chunking tasks (§4). Along these lines, we develop a discriminative, structured model of MWEs in context (§5) and train, evaluate, and examine it on the"
Q14-1016,N03-1033,0,0.00694519,"Missing"
Q14-1016,C10-2144,0,0.0511466,"non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allo"
Q14-1016,D11-1077,0,0.0260984,"cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allows for MWEs containing gaps, and includes a strength di"
Q14-1016,W11-0807,0,0.0247747,"ppy expressions or the strong/weak distinction, we would expect it to do no better when trained with the full tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Tre"
Q14-1016,S12-1010,0,0.0570731,"Missing"
Q14-1016,P10-1040,0,0.00760394,"iables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percy"
Q14-1016,2005.eamt-1.11,0,0.0139397,"Missing"
Q14-1016,M95-1005,0,0.0417178,"on1 the price which means4 a43 lot43 to4 me4 . Subscripts denote strong MW groups and superscripts weak MW groups; unmarked tokens serve as single-word expressions. The MW groups are thus {budge, on}, {a, little}, {a, lot}, and {means, {a, lot}, to, me}. As should be evident from the grammar, the projectivity and gap-nesting constraints apply here just as in the 1-level scheme. 3.2 Evaluation Matching criteria. Given that most tokens do not belong to an MWE, to evaluate MWE identification we adopt a precision/recall-based measure from the coreference resolution literature. The MUC criterion (Vilain et al., 1995) measures precision and recall great gateways never1 before1 , so23 far23 as23 Hudson knew2 , seen1 by Europeans was annotated in another corpus. 4 This was violated 6 times in our annotated data: modifiers within gaps are sometimes collocated with the gappy expression, as in on12 a12 tight1 budget12 and have12 little1 doubt12 . 196 of links in terms of groups (units) implied by the transitive closure over those links.5 It can be defined as follows: Let a Ð b denote a link between two elements in the gold standard, and aÐb ˆ denote a link in the system prediction. Let the ∗ operator denote the"
Q14-1016,vincze-2012-light,0,0.02716,"set for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant a"
Q14-1016,J13-1009,0,\N,Missing
Q14-1016,W13-1021,0,\N,Missing
Q14-1031,D13-1106,0,0.0640405,"ational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and demonstrate that disc"
Q14-1031,D10-1066,0,0.0129665,"Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with in"
Q14-1031,N12-1047,0,0.0373472,"MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3 Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4 Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the `1 -renormalized variant of regularized MERT. 395 3 Discretization and Feature Induction In this section, we propose a feature induction technique based on discretization that produc"
Q14-1031,D08-1024,0,0.0852249,"arability remains an informative tool for thinking about modeling in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N ). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), A"
Q14-1031,J07-2003,0,0.150009,"ment of weights. We augment RLNR with a smooth damping term D(w, j), which has the shape of a bathtub curve with steepness γ: 1 2γ 2 (wj−1 + wj+1 ) − wj D(w, j) = tanh (12) 1 2 (wj−1 − wj+1 ) RMNR (w) = β |h|−1 X j=2 D(w, j)RLNR (w, j) (13) D is nearly zero while wj ∈ [wj−1 , wj+1 ] and nearly one otherwise. Briefly, the numerator measures how far wj is from the midpoint of wj−1 and wj+1 while the denominator scales that distance by the radius from the midpoint to the neighboring weight. Experimental Setup6 5 Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implem"
Q14-1031,P11-2031,1,0.952373,"Bojar et al., 2012). First, we lowercased and performed sentence-level deduplication of the data.7 Then, we uniformly sampled a training set of 1M sentences (sections 1 – 97) along with a weighttuning set (section 98), hyperparameter-tuning (section 99), and test set (section 99) from the paraweb domain contained of CzEng.8 Sentences less than 5 words were discarded due to noise. Evaluation: We quantify increases in translation quality using case-insensitive BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by averaging over multiple optimizer replicas (Clark et al., 2011).9 7 CzEng is distributed deduplicated at the document level, leading to very high sentence-level overlap. 8 The section splits recommended by Bojar et al. (2012). 9 MultEval 0.5.1: github.com/jhclark/multeval Bits Features Test BLEU 4 101 36.4 8 1302 36.6 12 12,910 36.8 Table 2: Translation quality for Cz→En system with varying bits for discretization. For all other experiments, we tune the number of bits on held-out data. Condition P log P Disc P Over. P LNR P MNR P MNR C Zh→En 20.8? (-2.7) 23.5† 23.4† (-0.1) 20.7? (-2.8) 23.1?† (-0.4) 23.8† (+0.3) 23.6† (±) Ar→En 44.3? (-3.6) 47.9† 47.2† (-"
Q14-1031,P10-4002,1,0.798396,"tance by the radius from the midpoint to the neighboring weight. Experimental Setup6 5 Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011). The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008), which is distributed with cdec. 6 All code at http://github.com/"
Q14-1031,N13-1025,1,0.860909,"in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N ). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are al"
Q14-1031,D13-1201,0,0.243368,"OLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3 Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4 Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the `1 -renormalized variant of regularized MERT. 395 3 Discretization and Feature Induction In this section, we propose a feature induction technique based on discretization that produces a feature set that is less prone to non-linearities (see §2.2"
Q14-1031,W12-3134,0,0.0145241,", it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3 Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4 Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the `1 -renormalized variant of regularized MERT. 395 3 Discretization and Feature Induction"
Q14-1031,W07-0719,0,0.0503606,"Missing"
Q14-1031,D09-1023,0,0.255479,"ence translations, we can always recover the reference. In practice, both of these conditions are typically violated to a certain degree. However, if we modify our feature set such that some lower-ranked higher-quality hypothesis can be separated from all higher-ranked lower-quality hypotheses, then we can improve translation quality. For this reason, we believe that separability remains an informative tool for thinking about modeling in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N ). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we"
Q14-1031,P12-1031,0,0.0761962,"for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured `p regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 20"
Q14-1031,H94-1052,0,0.0537078,"ns a shape that deviates from the log in several regions. Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence align"
Q14-1031,D13-1176,0,0.0251975,", 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and demonstrate that discretization can be used to recover"
Q14-1031,P13-1078,0,0.363753,"Missing"
Q14-1031,I13-1032,0,0.229139,"Missing"
Q14-1031,C08-1064,0,0.0230693,"syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011). The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008), which is distributed with cdec. 6 All code at http://github.com/jhclark/cdec 400 Bidirectional lexical log-probabilities, the coherent phrasal translation log-probability, target word count, glue rule count, source OOV count, target OOV count, and target language model logprobability. Note that these features may be simplified or removed as specified in each experimental condition. Zh→En Ar→En Cz→En Train 303K 5.4M 1M WeightTune 1664 1797 3000 HyperTune 1085 1056 2000 Test 1357 1313 2000 Table 1: Corpus statistics: number of parallel sentences. Chinese Resources: For the Chinese→English expe"
Q14-1031,P95-1037,0,0.0751733,"nic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference ("
Q14-1031,D13-1024,0,0.0303365,"n proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured `p regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 2011). 8 Conclusion In the absence of highly refined knowledge about a feature, discretization wit"
Q14-1031,W07-0710,0,0.65197,"log curve, especially for low probabilities, that lead to 10 We also keep a real-valued copy of the word penalty to help normalize the language model. 11 These features can single-out rules with c(s) = 1, c(s, t) = 1, subsuming separate low-count features Weight improvements 0.07 1.0in translation quality. 0.8 0.02 0.6 0.4 0.07 Weight For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. 0.02 0.04 0.06 0.08 0.10 Original probability feature value 0.2 0.08 0.00.0 0 50 0.2 1000.4 150 0.6200 0.8 250 300 1.0 Original raw count feature value Figure 7: Plots of weights learned for the discretized pcoherent (e|f ) (top) and c(f ) (bottom) for the Ar→En system with 4 bits and monotone neighbor regularization. p(e|f ) > 0.11 is omitted for exposition as values were constant after this point. The gray line fits a log curve to the weights. The system learns a shape that deviates from the log in several regions. Each non-monotonic"
Q14-1031,P02-1038,0,0.432968,"on of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers. 1 Alon Lavie† Introduction Linear models using log-transformed probabilities as features have emerged as the dominant model in MT systems. This practice can be traced back to the IBM noisy channel models (Brown et al., 1993), which decompose decoding into the product of a translation model (TM) and a language model (LM), motivated by Bayes’ Rule. When Och and Ney (2002) introduced a log-linear model for translation (a linear sum of log-space features), they noted that the noisy channel model was a special case of their model using log probabilities. This ∗ This work was conducted as part of the first author’s Ph.D. work at Carnegie Mellon University. The community has abandoned the original motivations for a linear interpolation of two logtransformed features. Is there empirical evidence that we should continue using this particular transformation? Do we have any reason to believe it is better than other non-linear transformations? To answer these, we explor"
Q14-1031,P03-1021,0,0.157504,"Missing"
Q14-1031,P02-1040,0,0.0899745,"on NIST MT 2005, and test on NIST MT 2008. Czech resources: We also construct a Czech→English system based on the CzEng 1.0 data (Bojar et al., 2012). First, we lowercased and performed sentence-level deduplication of the data.7 Then, we uniformly sampled a training set of 1M sentences (sections 1 – 97) along with a weighttuning set (section 98), hyperparameter-tuning (section 99), and test set (section 99) from the paraweb domain contained of CzEng.8 Sentences less than 5 words were discarded due to noise. Evaluation: We quantify increases in translation quality using case-insensitive BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by averaging over multiple optimizer replicas (Clark et al., 2011).9 7 CzEng is distributed deduplicated at the document level, leading to very high sentence-level overlap. 8 The section splits recommended by Bojar et al. (2012). 9 MultEval 0.5.1: github.com/jhclark/multeval Bits Features Test BLEU 4 101 36.4 8 1302 36.6 12 12,910 36.8 Table 2: Translation quality for Cz→En system with varying bits for discretization. For all other experiments, we tune the number of bits on held-out data. Condition P log P Disc P Over. P LNR P MNR P"
Q14-1031,C12-2104,0,0.0192198,"tion for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and de"
Q14-1031,P13-2004,0,0.0638803,"Missing"
Q14-1031,P13-2072,0,0.304047,"Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured `p regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater tha"
Q14-1031,N07-2047,0,0.433472,"to believe it is better than other non-linear transformations? To answer these, we explore the issue of non-linearity in models for MT. In the process, we will discuss the impact of linearity on feature engineering and develop a general mechanism for learning a class of non-linear transformations of real-valued features. Applying a non-linear transformation such as log to features is one way of achieving a non-linear response function, even if those features are aggregated in a linear model. Alternatively, we could achieve a non-linear response using a natively nonlinear model such as a SVM (Wang et al., 2007) or RankBoost (Sokolov et al., 2012). However, MT is a structured prediction problem, in which a full hypothesis is composed of partial hypotheses. MT decoders take advantage of the fact that the model 393 Transactions of the Association for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalab"
Q14-1031,P04-1081,0,0.0362688,"e conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ah"
Q14-1031,W04-3242,0,\N,Missing
Q14-1031,J93-2003,0,\N,Missing
Q14-1031,bojar-etal-2012-joy,0,\N,Missing
Q14-1031,D11-1125,0,\N,Missing
Q14-1031,N12-1023,0,\N,Missing
Q16-1031,W14-3902,0,0.0352708,"Missing"
Q16-1031,W06-2920,0,0.165241,"l., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a) has created an opportunity to develop a parser that is capable of parsing sentences in multiple languages, addressing these theoretical and practical concerns.3 A multilingual parser can potentially replace an array of language-specific monolingually-trained parsers 2 While our parser can be used to parse input with codeswitching, we have not evaluated this capability due to the lack of appropriate data. 3 Although multilingual dependency treebanks have been available for a decade via the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the treebank of each language was annotated independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxili"
Q16-1031,D14-1082,0,0.821854,"languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a dependency parser for a set of target languages Lt , given universal dependency annotations in a set of source languages Ls . Ideally, we would like to have training data in all target"
Q16-1031,D11-1005,1,0.955701,"language was annotated independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically"
Q16-1031,P15-2139,0,0.546315,"entions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing:"
Q16-1031,D15-1040,0,0.568546,"entions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing:"
Q16-1031,D12-1001,0,0.0806483,"ework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in"
Q16-1031,P15-1033,1,0.28604,"predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a dependency parser for a set of target languages Lt , given universal dependency annotations in a set of source languages Ls . Ideally, we would like to have training data in all target languages (i.e., Lt"
Q16-1031,P15-1119,0,0.573596,"parsing with M A LOPA, we start with a simple delexicalized model where the token representation only consists of learned embeddings of coarse POS tags, which are shared across all languages to enable model transfer. In the following subsections, we enhance the token representation in M A LOPA to include lexical embeddings, language embeddings, and fine-grained POS embeddings. 3.3 Lexical Embeddings Previous work has shown that sacrificing lexical features amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; Täckström et al., 2012; Tiedemann, 2015; Guo et al., 2015). Therefore, we extend the token representation in M A LOPA by concatenating learned embeddings of multilingual word clusters, and pretrained multilingual embeddings of word types. Multilingual Brown clusters. Before training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the ‘projected clusters’ method in Täckström et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster. Multilingual word embeddings. We"
Q16-1031,E89-1018,0,0.221127,"Missing"
Q16-1031,N03-1014,0,0.100262,"ers between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated se"
Q16-1031,P15-1162,0,0.195853,"efines a categorical distribution over possible POS tags. For parsing, we construct the token representation by further concatenating the embeddings of predicted POS tags. This token representation feeds into the stack-LSTM modules of the buffer and stack components of the transition-based parser. This multi-task learning setup enables us to predict both POS tags and dependency trees in the same model. We note that pretrained word embeddings, cluster embeddings and language embeddings are shared for tagging and parsing. Block dropout. We use an independently developed variant of word dropout (Iyyer et al., 2015), which we call block dropout. The token representation used for parsing includes the embedding of predicted POS tags, which may be incorrect. We introduce another modification which makes the parser more robust to incorrect POS tag predictions, by stochastically zeroing out the entire embedding of the POS tag. While training the parser, we replace the POS embedding vector e with another vector (of the same dimensionality) stochastically computed as: e0 = (1 − b)/µ × e, where b ∈ {0, 1} is a Bernoulli-distributed random variable with parameter µ which is initialized to 1.0 (i.e., always dropou"
Q16-1031,P12-3005,0,0.0572083,"Missing"
Q16-1031,P14-1126,0,0.0336688,"erson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped with language embed"
Q16-1031,D11-1006,0,0.536368,"ed independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recen"
Q16-1031,P12-1066,0,0.677145,"eebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a"
Q16-1031,P05-1013,0,0.0543947,"nsition-based Parsing with S-LSTMs This section briefly reviews Dyer et al.’s S-LSTM parser, which we modify in the following sections. The core parser can be understood as the sequential manipulation of three data structures: • a buffer (from which we read the token sequence), The parser uses the arc-standard transition system (Nivre, 2004).11 At each timestep t, a transition action is applied that alters these data structures according to Table 2. 11 In a preprocessing step, we transform nonprojective trees in the training treebanks to pseudo-projective trees using the “baseline” scheme in (Nivre and Nilsson, 2005). We evaluate against the original nonprojective test set. 433 Token Representations The vector representations of input tokens feed into the stack-LSTM modules of the buffer and the stack. 12 A stack-LSTM module is used to compute the vector representation for each data structure, as detailed in Dyer et al. (2015). 13 The total number of actions is 1+2× the number of unique dependency labels in the treebank used for training, but we only consider actions which meet the arc-standard preconditions in Fig. 2. Stackt u, v, S u, v, S S Buffert B B u, B Action REDUCE - RIGHT (r) REDUCE - LEFT (r) S"
Q16-1031,W04-0308,0,0.0841575,"action in every time step until a complete parse tree is produced. The following sections describe our extensions of the core parser. More details about the core parser can be found in Dyer et al. (2015). • a list of actions previously taken by the parser. 3.2 3.1 Transition-based Parsing with S-LSTMs This section briefly reviews Dyer et al.’s S-LSTM parser, which we modify in the following sections. The core parser can be understood as the sequential manipulation of three data structures: • a buffer (from which we read the token sequence), The parser uses the arc-standard transition system (Nivre, 2004).11 At each timestep t, a transition action is applied that alters these data structures according to Table 2. 11 In a preprocessing step, we transform nonprojective trees in the training treebanks to pseudo-projective trees using the “baseline” scheme in (Nivre and Nilsson, 2005). We evaluate against the original nonprojective test set. 433 Token Representations The vector representations of input tokens feed into the stack-LSTM modules of the buffer and the stack. 12 A stack-LSTM module is used to compute the vector representation for each data structure, as detailed in Dyer et al. (2015). 1"
Q16-1031,P15-2034,0,0.047877,"word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotat"
Q16-1031,petrov-etal-2012-universal,0,0.079337,"Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7),"
Q16-1031,D15-1039,0,0.0550973,"ntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped with language embeddings and fine-grained POS embeddings, on average outperforms monolingually-trained parsers for target languages with a treebank. This pattern of results is qui"
Q16-1031,P15-1165,0,0.11785,"Missing"
Q16-1031,N12-1052,0,0.735129,"accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly avai"
Q16-1031,Q13-1001,0,0.0298107,"OPA de 54.1 55.9 57.1 es 68.3 73.0 74.6 target language fr it pt 68.8 69.4 72.5 71.0 71.2 78.6 73.9 72.5 77.0 average sv 62.5 69.5 68.1 65.9 69.3 70.5 Table 8: Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then applied it to parse sentences in a target language. Cohen et al. (2011) and McDonald et al. (2011) trained unlexicalized parsers on treebanks of multiple source languages and applied the parser to different languages. Naseem et al. (2012), Täckström et al. (2013), and Zhang and Barzilay (2015) used language typology to improve model transfer. To add lexical information, Täckström et al. (2012) used multilingual word clusters, while Xiao and Guo (2014), Guo et al. (2015), Søgaard et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Duong et al. (2015b) used a neural network based model, sharing most of the parameters between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding la"
Q16-1031,W14-1614,0,0.110791,"nd Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped"
Q16-1031,W15-2137,0,0.0388388,"For multilingual parsing with M A LOPA, we start with a simple delexicalized model where the token representation only consists of learned embeddings of coarse POS tags, which are shared across all languages to enable model transfer. In the following subsections, we enhance the token representation in M A LOPA to include lexical embeddings, language embeddings, and fine-grained POS embeddings. 3.3 Lexical Embeddings Previous work has shown that sacrificing lexical features amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; Täckström et al., 2012; Tiedemann, 2015; Guo et al., 2015). Therefore, we extend the token representation in M A LOPA by concatenating learned embeddings of multilingual word clusters, and pretrained multilingual embeddings of word types. Multilingual Brown clusters. Before training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the ‘projected clusters’ method in Täckström et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster. Multilingual"
Q16-1031,P07-1080,0,0.0567686,"nguages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2"
Q16-1031,P16-2069,0,0.0633887,"Missing"
Q16-1031,W14-1613,0,0.0268074,"nt scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then applied it to parse sentences in a target language. Cohen et al. (2011) and McDonald et al. (2011) trained unlexicalized parsers on treebanks of multiple source languages and applied the parser to different languages. Naseem et al. (2012), Täckström et al. (2013), and Zhang and Barzilay (2015) used language typology to improve model transfer. To add lexical information, Täckström et al. (2012) used multilingual word clusters, while Xiao and Guo (2014), Guo et al. (2015), Søgaard et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Duong et al. (2015b) used a neural network based model, sharing most of the parameters between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers"
Q16-1031,H01-1035,0,0.0721877,"k parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single pa"
Q16-1031,I08-3008,0,0.044074,"train the parser on six other languages in the Google universal dependency treebanks version 2.029 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags. Our parser uses the same word embeddings and word clusters used in Guo et al. (2016), and does not use any typology information.30 The results in Table 8 show that, on average, our parser outperforms both baselines by more than 1 point in LAS, and gives the best LAS results in four (out of six) languages. 5 Related Work Our work builds on the model transfer approach, which was pioneered by Zeman and Resnik (2008) 29 https://github.com/ryanmcd/uni-dep-tb/ In preliminary experiments, we found language embeddings to hurt the performance of the parser for target languages without a treebank. 30 LAS Zhang and Barzilay (2015) Guo et al. (2016) M A LOPA de 54.1 55.9 57.1 es 68.3 73.0 74.6 target language fr it pt 68.8 69.4 72.5 71.0 71.2 78.6 73.9 72.5 77.0 average sv 62.5 69.5 68.1 65.9 69.3 70.5 Table 8: Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then appli"
Q16-1031,D15-1213,0,0.342662,"th its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to"
Q16-1031,D15-1127,0,\N,Missing
Q18-1023,P14-1035,0,0.0268979,"l require transferring text understanding capability from other supervised learning tasks. 7 Related Work This paper is the first large-scale question answering dataset on full-length books and movie scripts. However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset includes full length scripts. 8 Conclusion We have introduced a new dataset and a s"
Q18-1023,P09-1068,0,0.0848706,"s through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset includes full length scripts. 8 Conclusion We have introduced a new dataset and a set of tasks for training and evaluating reading comprehension systems, borne from an analysis of the limitations 326 Title: Jacob’s Ladder Question: What is the fatal injury that Jacob sustains which ultimately leads to his death ? Answer: A bayonete stabbing to his gut. Summary snippet: A terrified Jac"
Q18-1023,P16-1223,0,0.173691,"Missing"
Q18-1023,W11-2107,0,0.0187853,"e the design of architectures capable of modeling such relationships. This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents. The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts). At present, this task is intractable for existing neural models out of the box. We further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using BLEU-1, BLEU-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and ROUGE-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers. We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR).7 4 Baselines and Oracles In this section, we show that NarrativeQA presents a challenging problem for current approaches to reading comprehension by evaluating"
Q18-1023,D17-1200,0,0.016072,"iptive summaries, requiring models to “read between the lines”. We expect that understanding narratives as complex as those presented in NarrativeQA will require transferring text understanding capability from other supervised learning tasks. 7 Related Work This paper is the first large-scale question answering dataset on full-length books and movie scripts. However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to compr"
Q18-1023,N15-1113,0,0.0220503,"We expect that understanding narratives as complex as those presented in NarrativeQA will require transferring text understanding capability from other supervised learning tasks. 7 Related Work This paper is the first large-scale question answering dataset on full-length books and movie scripts. However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset i"
Q18-1023,N16-1180,0,0.0574426,"Missing"
Q18-1023,P16-1086,0,0.219579,"issing word) and are produced from either short abstractive summaries (CNN/Daily Mail) or from the next sentence in the document the context was taken from (CBT and BookTest). The tasks associated with these datasets are all selecting an answer from a set of options, which is explicitly provided for CBT and BookTest, and is implicit for CNN/Daily Mail, as the answers are always entities from the document. This significantly favors models that operate by pointing to a particular token (or type). Indeed, the most successful models on these datasets, such as the Attention Sum Reader (AS Reader) (Kadlec et al., 2016), exploit precisely this bias in the data. However, these models are inappropriate for answers requiring synthesis of a new answer. This bias towards answers that are shallowly salient is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event. Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) offer a different challenge. A large number of questions and answers are provided for a set of documents, where the ans"
Q18-1023,W04-1013,0,0.114151,"modeling such relationships. This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents. The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts). At present, this task is intractable for existing neural models out of the box. We further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using BLEU-1, BLEU-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and ROUGE-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers. We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR).7 4 Baselines and Oracles In this section, we show that NarrativeQA presents a challenging problem for current approaches to reading comprehension by evaluating several baselines based o"
Q18-1023,P02-1040,0,0.102406,"pe that NarrativeQA will motivate the design of architectures capable of modeling such relationships. This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents. The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts). At present, this task is intractable for existing neural models out of the box. We further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using BLEU-1, BLEU-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and ROUGE-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers. We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR).7 4 Baselines and Oracles In this section, we show that NarrativeQA presents a challenging problem for current approaches to"
Q18-1023,D14-1162,0,0.0896265,"Missing"
Q18-1023,D16-1264,0,0.282564,"that operate by pointing to a particular token (or type). Indeed, the most successful models on these datasets, such as the Attention Sum Reader (AS Reader) (Kadlec et al., 2016), exploit precisely this bias in the data. However, these models are inappropriate for answers requiring synthesis of a new answer. This bias towards answers that are shallowly salient is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event. Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) offer a different challenge. A large number of questions and answers are provided for a set of documents, where the answers are spans of the context document, i.e. contiguous sequences of words from the document. Although the answers are not Dataset Documents Questions Answers MCTest (Richardson et al., 2013) 660 short stories, grade school level 93K+220K news articles 2640 human generated, based on the document 387K+997K Cloze-form, based on highlights Cloze-form, from the 21st sentence Cloze-form, similar to CBT 108K human generated, based on the paragrap"
Q18-1023,D13-1020,0,0.710414,"the author (e.g. “muggle” in Harry Potter novels) but the reader need only appeal to the book itself to understand the meaning of these concepts, and their place in the narrative. This ability to form new concepts based on the contexts of a text is a crucial aspect of reading comprehension, and is in part tested as part of the question answering tasks we present. 2 318 ing comprehension models. We summarize the key features of a collection of popular recent datasets in Table 1. In this section, we briefly discuss the nature and limitations of these datasets and their associated tasks. MCTest (Richardson et al., 2013) is a collection of short stories, each with multiple questions. Each such question has set of possible answers, one of which is labelled as correct. While this could be used as a QA task, the MCTest corpus is in fact intended as an answer selection corpus. The data is human generated, and the answers can be phrases or sentences. The main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end-to-end training of models, due to its relatively small size. In contrast, CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al.,"
Q18-1023,P17-1018,0,0.0890611,"Missing"
S12-1011,D10-1115,0,0.0143673,"tion for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective semantics together with their comp"
S12-1011,W03-1022,0,0.212358,"|M| Figure 1: Plate diagram illustrating our model of noun and modifier semantic classes (designated N and M , respectively), a modifier-noun pair (m,n), and its context. N Parameterization and Inference Multi(ΨM Ni ) Multi(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British"
S12-1011,J02-2003,0,0.426222,"erence Multi(ΨM Ni ) Multi(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are"
S12-1011,P05-1004,0,0.123956,"valuations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional prefer"
S12-1011,J10-4007,0,0.0302055,"Missing"
S12-1011,D11-1129,0,0.0350219,"Missing"
S12-1011,N09-1036,0,0.0334129,"re treated as a bag of words and 1 We evaluate this hypothesis as well as its inverse. 70 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70–74, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2.2 αc Ψc αN αM ΨN ΨM |N| We use Gibbs sampling to estimate the distributions of N and M , integrating out the multinomial parameters Ψx (Griffiths and Steyvers, 2004). The Dirichlet parameters α are drawn independently from a Γ(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009). This “vague” prior encourages sparse draws from the Dirichlet distribution. The number of noun and adjective classes N and M was set to 50 each; other sizes (100,150) did not significantly alter results. |N| c N M n m k |D| Ψn Ψm |N| n α 3 m α include the words to the left and right of the noun, its siblings and governing verbs. We designate the vocabulary Vn for nouns, Vm for modifiers and Vc for context. We use zi to refer to the ith tuple in D and refer to variables within that tuple by subscripting them with i, e.g., ni and c3,i are the noun and the third context variable of zi . The lat"
S12-1011,J03-3005,0,0.414645,"d on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 4.1 Evaluation Supersense Tagging Supersense taggin"
S12-1011,P08-1028,0,0.04565,"latent semantic representation for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective seman"
S12-1011,P10-1045,0,0.0241587,"Missing"
S12-1011,P93-1024,0,0.564443,"ulti(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over ever"
S12-1011,P99-1014,0,0.0273177,"i ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 4"
S14-2027,J92-4003,0,0.0715867,"t at index i, the top classifier’s features included t’s POS tag, i, those two conjoined, and the depth of t in the syntactic dependency tree. 4 Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneff"
S14-2027,W08-1301,0,0.0702255,"Missing"
S14-2027,E14-1049,1,0.763349,"., nodes where there is at 178 least one outbound edge) are possible candidates to be “top”; the classifier probabilities are evaluated, and the highest-scoring node is chosen to be “top.” This is suboptimal, since some graphs have multiple tops (in PCEDT this is more common); but selection rules based on probability thresholds gave worse F1 performance on the dev set. For a given token t at index i, the top classifier’s features included t’s POS tag, i, those two conjoined, and the depth of t in the syntactic dependency tree. 4 Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent"
S14-2027,P14-1134,1,0.844279,"f length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision. Tree constraint: Enforces that the graph is a tree. Unsurprisingly, we found that enforcing a tree constraint hurt performance. Negative Results We followed a forward-selection process during feature engineering. For each potential feature, we tested the current feature set versus the current feature set plus the new potential feature. If the new feature did not improve performance, we did not add it. We list in table 2 some"
S14-2027,D08-1008,0,0.0230174,"se multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision."
S14-2027,P13-2109,1,0.846354,"es and constraints giving negative results. Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared DM PAS PCEDT Average LP 0.8446 0.9078 0.7681 0.8402 LR 0.8348 0.8851 0.7072 0.8090 LF 0.8397 0.8963 0.7364 0.8241 LM 0.0875 0.2604 0.0712 0.1397 Table 3: Labeled precision (LP), recall (LR), F1 (LF), and whole-sentence match (LM) on the held-out test data. between the three formalisms suggests that a multitask learning (Evgeniou and Pontil, 2004) framework could lead to gains. And finally, there is additional structure in the formalisms which could be exploited (such as the deterministic processes by which an original PCEDT tree annotation was converted into"
S14-2027,E06-1011,0,0.0537858,"ur scores. 6 Table 2: Features and constraints giving negative results. Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared DM PAS PCEDT Average LP 0.8446 0.9078 0.7681 0.8402 LR 0.8348 0.8851 0.7072 0.8090 LF 0.8397 0.8963 0.7364 0.8241 LM 0.0875 0.2604 0.0712 0.1397 Table 3: Labeled precision (LP), recall (LR), F1 (LF), and whole-sentence match (LM) on the held-out test data. between the three formalisms suggests that a multitask learning (Evgeniou and Pontil, 2004) framework could lead to gains. And finally, there is additional structure in the formalisms which could be exploited (such as the deterministic processes by which an original PCEDT tree annotat"
S14-2027,S14-2008,0,0.0475934,"Missing"
S16-1186,P13-2131,0,0.308975,"Missing"
S16-1186,W02-1001,0,0.141888,"ke predictions as follows: yˆ = arg max w · f (x, y 0 ) y 0 ∈Y(x) To train the model parameters w, a function of the training data is minimized with respect to w. This function is a sum of individual training examples’ losses L, plus a regularizer: X L(D; w) = L(xi , yi ; w) + λkwk2 (xi ,yi )∈D    C(xi ,yi ) }| { z     L(xi , yi ; w) = −  lim max w · f (xi , y) + α ·  min cost(yi , y 00 ) −cost(yi , y) 00 α→∞ y∈Y(xi ) + max y 0 ∈Y(xi )  y ∈Y(xi )  w · f (xi , y 0 ) + cost(yi , y 0 ) (1) Figure 1: Infinite ramp loss. Typical loss functions are the structured perceptron loss (Collins, 2002): L(xi , yi ; w) = −w · f (xi , yi ) + max w · f (xi , y) y∈Y(xi ) (2) and the structured SVM loss (Taskar et al., 2003; Tsochantaridis et al., 2004), which incorporates margin using a cost function:1 L(xi , yi ; w) = −w · f (xi , yi ) + max y∈Y(xi ) w · f (xi , y) + cost(yi , y)  (3) Both (2) and (3) are problematic if example i is unreachable, i.e., yi ∈ / Y(xi ), due to imperfect data or an imperfect definition of Y. In this case, the model is trying to learn an output it cannot produce. In some applications, the features f (xi , yi ) cannot even be computed for these examples. This proble"
S16-1186,P14-1134,1,0.736854,"ss function for structured prediction called infinite ramp, which is a generalization of the structured SVM to problems with unreachable training instances. 1 • Frame file lookup: for every word in the input sentence, if the lemma matches the name of a frame in the AMR frame files (with sense tag removed), we add the lemma concatenated with “-01” as a candidate concept fragment. • Lemma: for every word in the input sentence, we add the lemma of the word as a candidate concept fragment. Introduction Our entry to the SemEval 2016 Shared Task 8 is a set of improvements to the system presented in Flanigan et al. (2014). The improvements are: a novel training loss function for structured prediction, which we call “infinite ramp,” new sources for concepts, improved features, and improvements to the rule-based aligner in Flanigan et al. (2014). The overall architecture of the system and the decoding algorithms for concept identification and relation identification are unchanged from Flanigan et al. (2014), and we refer readers seeking a complete understanding of the system to that paper. 2 New Concept Fragment Sources and Features The concept identification stage relies on a function called clex in Section 3 o"
S16-1186,N12-1023,1,0.842131,"d to ramp loss (Collobert et al., 2006; Chapelle et al., 2009; Keshet and McAllester, 2011): L(xi , yi ; w) =  w · f (xi , y) − α · cost(yi , y) y∈Y(xi )  + max w · f (xi , y 0 ) + cost(yi , y 0 ) − max y 0 ∈Y(xi ) (5) The parameter α is often set to zero, and controls the “height” of the ramp, which is α + 1. Taking α → ∞ in Eq. 5 corresponds roughly to Eq. 1, hence the name “infinite ramp loss”. However, Eq. 1 also includes C(xi , yi ) term to make the limit well defined even when miny∈Y(xi ) cost(yi , y) 6= 0. Like infinite ramp loss, ramp loss also handles unreachable training examples (Gimpel and Smith, 2012), but we have found ramp loss to be more difficult to optimize than infinite ramp loss in practice due to local minima. Both loss functions are nonconvex. However, infinite ramp loss is convex if arg miny∈Y(xi ) cost(yi , y) is unique. the AMR annotation scheme between the production of the LDC2015E86 training data and the SemEval test set. During that time, there were changes to the concept senses and the concept frame files. Because the improvements in our parser were due to boosting recall in concept identification (and using the frame files to our advantage), our approach does not show as"
tsvetkov-etal-2014-augmenting-english,W10-0719,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,D08-1027,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,W06-1670,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,H93-1061,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,J12-3005,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P06-2072,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P14-1024,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,P12-2050,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,N13-1132,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,I08-2105,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,peters-peters-2000-treatment,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,E14-1049,1,\N,Missing
W08-0333,D07-1090,0,0.0879585,"ty hardware. We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates. On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical, optimally-parallelized version of current state-of-the-art single-core tools. 1 Introduction Like many other NLP problems, output quality of statistical machine translation (SMT) systems increases with the amount of training data. Brants et al. (2007) demonstrated that increasing the quantity of training data used for language modeling significantly improves the translation quality of an ArabicEnglish MT system, even with far less sophisticated backoff models. However, the steadily increasing quantities of training data do not come without cost. Figure 1 shows the relationship between the amount of parallel Arabic-English training data used and both the translation quality of a state-ofthe-art phrase-based SMT system and the time required to perform the training with the widely-used Moses toolkit on a commodity server.1 Building a model us"
W08-0333,J93-2003,0,0.0481858,"Missing"
W08-0333,P05-1032,0,0.0267713,"2 and HMM training using a “home-grown” parallelization scheme (Deng and Byrne, 2006). However, the tool relies on a cluster where all nodes have access to the same shared networked file storage, a restriction that MapReduce does not impose. There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, the standard channel probability P (f |e) cannot be computed, which is not a limitation of MapReduce.10 7 Conclusions We have shown that an important class of modelbuilding algorithms in statistical machine translation can be straightforwardly recast into the MapReduce framework, yielding a distributed solution that is cost-effective, scalable, robust, and exact (i.e., doesn’t resort to approximations). Alternative strategies for parallelizing these algorithms either impose significant demands on the developer, the hardware infrastructu"
W08-0333,N06-4004,0,0.0116113,"pproach suggested by Das et al. (2007), who show that a distributed database running in tandem with MapReduce can be used to provide the parameters for very large mixture models efficiently. Moreover, since the database is distributed across the same nodes as the MapReduce jobs, many of the same data locality benefits that Wolfe et al. (2007) sought to capitalize on will be available without abandoning the guarantees of the MapReduce paradigm. Although it does not use MapReduce, the MTTK tool suite implements distributed Model 1, 2 and HMM training using a “home-grown” parallelization scheme (Deng and Byrne, 2006). However, the tool relies on a cluster where all nodes have access to the same shared networked file storage, a restriction that MapReduce does not impose. There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, t"
W08-0333,N03-1017,0,0.0092073,"existing parallel programming models. The MapReduce abstraction shields the programmer from having to explicitly worry about system-level issues such as synchronization, data exchange, and fault tolerance (see Section 2 for details). The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics. This frees the programmer to focus on actual MT issues. In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today: a phrasebased translation model (Koehn et al., 2003) and word alignment models based on pairwise lexical translation trained using expectation maximization (Dempster et al., 1977). Currently, such models take days to construct using standard tools with publicly available training corpora; our MapReduce implementation cuts this time to hours. As an benefit to the community, it is our intention to release this code under an open source license. It is worthwhile to emphasize that we present 200 these results as a “sweet spot” in the complex design space of engineering decisions. In light of possible tradeoffs, we argue that our solution can be con"
W08-0333,W08-0207,1,0.806872,"ementation), and cheap (in terms of hardware costs). Faster running times could be achieved with more expensive hardware. Similarly, a custom implementation (e.g., in MPI) could extract finergrained parallelism and also yield faster running times. In our opinion, these are not worthwhile tradeoffs. In the first case, financial constraints are obvious. In the second case, the programmer must explicitly manage all the complexities that come with distributed processing (see above). In contrast, our algorithms were developed within a matter of weeks, as part of a “cloud computing” course project (Lin, 2008). Experimental results demonstrate that MapReduce provides nearly optimal scaling characteristics, while retaining a highlevel problem-focused abstraction. The remainder of the paper is structured as follows. In the next section we provide an overview of MapReduce. In Section 3 we describe several general solutions to computing maximum likelihood estimates for finite, discrete probability distributions. Sections 4 and 5 apply these techniques to estimate phrase translation models and perform EM for two word alignment models. Section 6 reviews relevant prior work, and Section 7 concludes. 2 Map"
W08-0333,D07-1104,0,0.0119919,"home-grown” parallelization scheme (Deng and Byrne, 2006). However, the tool relies on a cluster where all nodes have access to the same shared networked file storage, a restriction that MapReduce does not impose. There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, the standard channel probability P (f |e) cannot be computed, which is not a limitation of MapReduce.10 7 Conclusions We have shown that an important class of modelbuilding algorithms in statistical machine translation can be straightforwardly recast into the MapReduce framework, yielding a distributed solution that is cost-effective, scalable, robust, and exact (i.e., doesn’t resort to approximations). Alternative strategies for parallelizing these algorithms either impose significant demands on the developer, the hardware infrastructure, or both; o"
W08-0333,C00-2163,0,0.0242238,"se of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model (HMM), form a class of generative models that are based on a lexical translation model P (fj |ei ) where each word fj in the foreign sentence f1m is generated by precisely one word ei in the sentence el1 , independently of the other translation decisions (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000). Given these assumptions, we let the sentence translation probability be mediated by a latent alignment variable (am 1 in the equations below) that specifies the pairwise mapping between words in the source and target languages. Assuming a given sentence length m for f1m , the translation probability is defined as follows: P (f1m |el1 ) = X l P (f1m , am 1 |e1 ) am 1 = X l m P (am 1 |e1 , f1 ) am 1 m Y P (fj |eaj ) j=1 Once the model parameters have been estimated, the single-best word alignment is computed according to the following decision rule: a ˆm 1 = l m arg max P (am 1 |e1 , f1 ) am 1"
W08-0333,P02-1038,0,0.00803503,"solution to the problem, while providing an implementation that is both scalable and fault tolerant—in fact, transparently so since the runtime hides all these complexities from the researcher. From the graph it is clear that the overhead associated with the framework itself is quite low, especially for large quantities of data. We concede that it may be possible for a custom solution (e.g., with MPI) to achieve even faster running times, but we argue that devoting resources to developing such a solution would not be cost-effective. Next, we explore a class of models where the stan5 Following Och and Ney (2002), it is customary to combine both these probabilities as feature values in a log-linear model. 6 In our cluster, only 19 machines actually compute, and each has two single-core processors. 203 Word Alignment Although word-based translation models have been largely supplanted by models that make use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model"
W08-0333,J03-1002,0,0.00647585,"the total probability of generating a particular observation to be efficiently computed using dynamic programming.8 The HMM alignment model uses the forward-backward algorithm (Baum et al., 1970), which is also an instance of EM. Even with dynamic programming, this requires O(Slm) operations for Model 1, and O(Slm2 ) for the HMM model, where m and l are the average lengths of the foreign and English sentences in the training corpus, and S is the number of sentences. Figure 6 shows measurements of the average iteration run-time for Model 1 and the HMM alignment model as implemented in Giza++ (Och and Ney, 2003), a state-of-the-art C++ implementation of the IBM and HMM alignment models that is widely used. Five iterations are generally necessary to train the models, so the time to carry out full training of the models is approximately five times the per-iteration run-time. 3 hrs 60 min 20 min 3m20s 90 s 30 s 10 s 3s 10000 100000 1e+06 Corpus size (sentences) Figure 6: Per-iteration average run-times for Giza++ implementations of Model 1 and HMM training on corpora of various sizes. be associated with the given training instance. Reducers aggregate these partial counts to compute the total expected jo"
W08-0333,W99-0604,0,0.054748,"e to extract a consistent phrase corresponding to the foreign string la mesa or the English string the small. models in question: representing P (B|A = a) in the phrase model required at most 90k parameters, and in the lexical model, 128k parameters (i.e., the size of the vocabulary for language B). For the remainder of the experiments reported, we confine ourselves to the use of Method 3. 4 Phrase-Based Translation In phrase-based translation, the translation process is modeled by splitting the source sentence into phrases (a contiguous string of words) and translating the phrases as a unit (Och et al., 1999; Koehn et al., 2003). Phrases are extracted from a wordaligned parallel sentence according to the strategy proposed by Och et al. (1999), where every word in a phrase is aligned only to other words in the phrase, and not to any words outside the phrase bounds. Figure 4 shows an example aligned sentence and some of the consistent subphrases that may be extracted. dard tools work primarily in memory, but where the computational complexity of the models is greater. Moses training time MapReduce training (38 M/R) Optimal (Moses/38) 2 days 5 Time (seconds) 12 hrs 3 hrs 60 min 20 min 5 min 1.5 min"
W08-0333,C96-2141,0,0.0805825,"y models that make use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model (HMM), form a class of generative models that are based on a lexical translation model P (fj |ei ) where each word fj in the foreign sentence f1m is generated by precisely one word ei in the sentence el1 , independently of the other translation decisions (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000). Given these assumptions, we let the sentence translation probability be mediated by a latent alignment variable (am 1 in the equations below) that specifies the pairwise mapping between words in the source and target languages. Assuming a given sentence length m for f1m , the translation probability is defined as follows: P (f1m |el1 ) = X l P (f1m , am 1 |e1 ) am 1 = X l m P (am 1 |e1 , f1 ) am 1 m Y P (fj |eaj ) j=1 Once the model parameters have been estimated, the single-best word alignment is computed according to the following decision rule: a ˆm 1 = l m arg max P ("
W09-0424,P05-1032,1,0.752623,"s also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f |e) and reverse translation probability p(e|f ) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all rules from the training set. The current code requires suffix array rule extraction to be run as a pre-processing step to extract the rules needed to translate a particular test set. However, we are currently extending the decoder to directly access the suffix array. This will allow the decoder at runtime to efficiently extract exactly those rules needed to tra"
W09-0424,W08-0309,1,0.277708,"lable as a standalone application, Z-MERT, that can be used with other MT systems. (Software and documentation at: http://cs.jhu.edu/ ˜ozaidan/zmert.) 137 System Joshua Baseline Minimum Bayes Risk Rescoring Deterministic Annealing Variational Decoding of 21.2 million English sentences with half a billion words. We used SRILM to train a 5-gram language model using a vocabulary containing the 500,000 most frequent words in this corpus. Note that we did not use the English side of the parallel corpus as language model training data. To tune the system parameters we used News Test Set from WMT08 (Callison-Burch et al., 2008), which consists of 2,051 sentence pairs with 43 thousand English words and 46 thousand French words. This is in-domain data that was gathered from the same news sources as the WMT09 test set. 3.2 BLEU-4 25.92 26.16 25.98 26.52 Table 1: The uncased BLEU scores on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation. of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In t"
W09-0424,D07-1104,0,0.412715,"e. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of"
W09-0424,J07-2003,0,0.947277,"ovided as an abstract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and"
W09-0424,moore-2002-fast,0,0.0291243,"d the Olympic Committee. The crawl gathered approximately 40 million files, consisting of over 1TB of data. We converted pdf, doc, html, asp, php, etc. files into text, and preserved the directory structure of the web crawl. We wrote set of simple heuristics to transform French URLs onto English URLs, and considered matching documents to be translations of each other. This yielded 2 million French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section"
W09-0424,P03-2041,0,0.0958636,"For our submission, we used k = 20, which resulted in 1.5 million (out of 23 million) sentence pairs being selected for use as training data. There were 30,037,600 English words and 30,083,927 French words in the subsampled training corpus. 2.2 2.3 Decoding Algorithms2 Grammar formalism: Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Currently, it only handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs (e.g., (Galley et al., 2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). Chart parsing: Given a source sentence to decode, the decoder generates a one-best or k-best translations using a CKY algorithm. Specifically, the decoding algorithm maintains a chart, which contains an array of cells. Each cell in turn maintains a list of proven items. The parsing process starts with the axioms, and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translat"
W09-0424,J03-1002,0,0.00743813,"es on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation. of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In this system, we have used both deterministic annealing (for training) and variational decoding (for decoding). Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objecti"
W09-0424,P06-1121,0,0.562847,"ize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms des"
W09-0424,P03-1021,0,0.692054,"other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two direct"
W09-0424,P05-1034,0,0.103948,"stract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the es"
W09-0424,W05-1506,0,0.084474,"ls on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivations from the hypergraph. Parallel and distributed decoding: We also implement parallel decoding and a distributed language model by exploiting multi-core and multi-processor architectures and distributed computing techniques. More details on these two features are provided by Li and Khudanpur (2008b). 2.4 updates, each reflecting a greedy selection of the dimension giving the most gain. Each iteration also optimizes several random “intermediate initial” points in addition to the one surviving from the previous iteration, as an approximation to performing"
W09-0424,P06-2101,0,0.0477005,"raining (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations). In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness 4 C"
W09-0424,N03-1017,0,0.0293738,"oaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f |e) and reverse translation probability p(e|f ) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all ru"
W09-0424,P07-2045,1,0.0364964,"well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the to"
W09-0424,P07-1065,0,0.0441818,"distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring function in Java. This Java implementation is able to read the standard ARPA backoff n-gram models, and thus the decoder can be used independently from the SRILM toolkit.3 We also provide a native code bridge that allows the decoder to use the SRILM toolkit to read and score n-grams. This native implementation is more scalable than the basic Java LM implementation. We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). 2.5 WMT-09 Translation Task Results Minimum Error Rate Training Johsua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu. The optimization consists of a series of line-optimizations along the dimensions corresponding to the parameters. The search across a dimension uses the efficient method of Och (2003). Each iteration of our MERT implementation consists of multiple weight 3 This feature allows users to easily try the Joshua toolkit without installing the SRILM toolkit and compiling the n"
W09-0424,2005.mtsummit-papers.11,0,0.0944271,"llion French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section 2.1. For language model training, we used the monolingual news and blog data that was assembled by the University of Edinburgh and distributed as part of WMT09. This data consisted Language Models In addition to the distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring func"
W09-0424,N04-1022,0,0.0715322,"r training) and variational decoding (for decoding). Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct deri"
W09-0424,2008.amta-papers.12,1,0.948747,"dan Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD † Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD + Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany ? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN Abstract 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.1 We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieve"
W09-0424,W08-0402,1,0.946992,"dan Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD † Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD + Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany ? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN Abstract 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.1 We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieve"
W09-0424,W09-0424,1,0.108172,", and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. 2 More details on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivati"
W09-0424,P09-1067,1,0.881207,", and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. 2 More details on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivati"
W09-0424,P06-1077,0,0.253036,"new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007)"
W09-0424,D08-1076,0,\N,Missing
W09-0426,P01-1008,0,0.0294189,"ion baseline lattice baseline lattice BLEU TER 20.8 21.3 11.0 12.3 60.7 59.9 71.1 70.4 Table 1: Impact of compound segmentation lattices. To build the translation model for lattice system, we segmented the training data using the onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likely translations for each paraphrase were added to th"
W09-0426,N06-1003,0,0.022021,"ibed in the previous section. German Hungarian Condition baseline lattice baseline lattice BLEU TER 20.8 21.3 11.0 12.3 60.7 59.9 71.1 70.4 Table 1: Impact of compound segmentation lattices. To build the translation model for lattice system, we segmented the training data using the onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likel"
W09-0426,J07-2003,0,0.14396,"inning with a convention hierarchical phrase-based system, we found benefits for using word segmentation lattices as input, explicit generation of beginning and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 1 2 Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs. First, such a system makes use of lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to-English translation (Koehn et al., 2008). Additionally, since the decoder is based on a CKY parser, it can consider all licensed reorderings of the input in polynomial time, and German and Hungarian may require quite substantial reordering. Although such decoders and models have been common for several years, there have been no published results for these language pairs. The baseli"
W09-0426,P07-1090,1,0.847739,"ed to model the phrases being moved. The feature assesses the quality of a reordering by looking at the phrase alignment between pairs of Table 2: Impact of modeling sentence boundaries. 3.3 Dominance feature Source language paraphrases In order to deal with the sparsity associated with a rich source language morphology and limitedsize parallel corpora (bitexts), we experimented with a novel approach to paraphrasing out-ofvocabulary (OOV) source language phrases in 147 Source function words. In our experiments, we treated the 128 most frequent words in the corpus as function words, similar to Setiawan et al. (2007). Due to space constraints, we will discuss the details in another publication. As Table 3 reports, the use of this feature yields positive results. Source German Hungarian Condition baseline +dom baseline +dom BLEU TER 21.6 22.2 12.8 12.6 60.1 59.8 70.4 70.0 German Hungarian BLEU TER 22.2 22.6 12.6 12.8 59.8 59.4 70.0 69.8 Table 4: Performance of maximum derivation vs. MBR decoders. 4 Conclusion Table 5 summarizes the impact on the dev-test set of all features included in the University of Maryland system submission. Table 3: Impact of alignment dominance feature. Condition 3.5 Decoder Max-D"
W09-0426,J93-1003,0,0.094686,"e onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likely translations for each paraphrase were added to the grammar in order to provide mappings to English for OOV Hungarian phrases. This attempt at monolingually-derived sourceside paraphrasing did not yield improvements over baseline. Preliminary analysis suggests that"
W09-0426,P08-1115,1,0.844708,". 2.3 Word segmentation lattices Both German and Hungarian have a large number of compound words that are created by concatenating several morphemes to form a single orthographic token. To deal with productive compounding, we employ word segmentation lattices, which are word lattices that encode alternative possible segmentations of compound words. Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation. This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008). To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algor"
W09-0426,2006.amta-papers.25,0,0.027205,"from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002). We divided the 2008 workshop “news test” sets into two halves of approximately 1000 sentences each and designated one the dev set and the other the dev-test set. 2.2 3.1 Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning: − BLEU 2 Although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking, Yaser Al-Onaizan (personal communication) reports that it correlates well with the human evaluation metric HTER. In this paper, we report uncased TER and BLEU individually. 2.3 Word segmentation lattices Both German and Hungarian have a large number of compound words that are created by concatenating several morphemes to form a single orthographic token. To deal with productive compounding, we emplo"
W09-0426,E03-1076,0,0.0193663,"several morphemes to form a single orthographic token. To deal with productive compounding, we employ word segmentation lattices, which are word lattices that encode alternative possible segmentations of compound words. Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation. This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008). To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al., 2008). We reused the same features and weights to create the Hungarian lattices. For the test dat"
W09-0426,I08-1066,0,0.0257269,"first word, meaning that the exact computation must deferred, which makes pruning a challenge. In typical CKY decoders, the beginning and ends of the sentence (which often have special characteristics) are not conclusively determined until the whole sentence has been translated and the probabilities for the beginning and end sentence probabilities can be added. However, by this point it is often the case that a possibly better sentence beginning has been pruned away. To address this, we explicitly generate beginning and end sentence markers as part of the translation process, as suggested by Xiong et al. (2008). The results of doing this are shown in Table 2. Source German Hungarian Condition baseline +boundary baseline +boundary BLEU TER 21.3 21.6 12.3 12.8 59.9 60.1 70.4 70.4 3.4 Although our baseline hierarchical system permits long-range reordering, it lacks a mechanism to identify the most appropriate reordering for a specific sentence translation. For example, when the most appropriate reordering is a long-range one, our baseline system often also has to consider shorter-range reorderings as well. In the worst case, a shorter-range reordering has a high probability, causing the wrong reorderin"
W09-0426,W08-0318,0,0.0155767,"ing the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 1 2 Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs. First, such a system makes use of lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to-English translation (Koehn et al., 2008). Additionally, since the decoder is based on a CKY parser, it can consider all licensed reorderings of the input in polynomial time, and German and Hungarian may require quite substantial reordering. Although such decoders and models have been common for several years, there have been no published results for these language pairs. The baseline system translates lowercased and tokenized source sentences into lowercased target sentences. The features used were the rule translation relative frequency P (¯ e|f¯), the “lexical” translation probabilities Plex (¯ e|f¯) and Plex (f¯|¯ e), a rule coun"
W09-0426,N04-1022,0,0.0472478,"garian BLEU TER 22.2 22.6 12.6 12.8 59.8 59.4 70.0 69.8 Table 4: Performance of maximum derivation vs. MBR decoders. 4 Conclusion Table 5 summarizes the impact on the dev-test set of all features included in the University of Maryland system submission. Table 3: Impact of alignment dominance feature. Condition 3.5 Decoder Max-D MBR Max-D MBR Minimum Bayes risk decoding baseline +lattices +boundary +dom +MBR Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). This seeks the translation E of the input lattice F that has the least expected loss, measured by some loss function L: German Hungarian BLEU TER BLEU TER 20.8 21.3 21.6 22.2 22.6 60.7 59.9 60.1 59.8 59.4 11.0 12.3 12.8 12.6 12.8 71.1 70.4 70.4 70.0 69.8 Table 5: Summary of all features Acknowledgments 0 ˆ = arg min EP (E|F ) [L(E, E )] E E0 X = arg min P (E|F)L(E, E 0 ) 0 E (1) This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-2-001, and the Army Research Laboratory. Any opinions, findings, conclusions or recommen"
W09-0426,D07-1104,0,0.0211523,"rs, there have been no published results for these language pairs. The baseline system translates lowercased and tokenized source sentences into lowercased target sentences. The features used were the rule translation relative frequency P (¯ e|f¯), the “lexical” translation probabilities Plex (¯ e|f¯) and Plex (f¯|¯ e), a rule count, a target language word count, the target (English) language model P (eI1 ), and a “passthrough” penalty for passing a source language word to the target side.1 The rule feature values were computed online during decoding using the suffix array method described by Lopez (2007). Introduction For the shared translation task of the Fourth Workshop on Machine Translation (WMT09), we focused on two tasks: German to English and Hungarian to English translation. Despite belonging to different language families, German and Hungarian have three features in common that complicate translation into English: 1. productive nouns), compounding (especially Baseline system of 2. rich inflectional morphology, 3. widespread mid- to long-range word order differences with respect to English. 1 The “pass-through” penalty was necessary since the English language modeling data contained a"
W09-0426,C08-1064,0,0.012091,"and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 1 2 Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs. First, such a system makes use of lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to-English translation (Koehn et al., 2008). Additionally, since the decoder is based on a CKY parser, it can consider all licensed reorderings of the input in polynomial time, and German and Hungarian may require quite substantial reordering. Although such decoders and models have been common for several years, there have been no published results for these language pairs. The baseline system translates lowercased and tokenized source sentences into lowercased target sentences. The features used were the rule translation relative freque"
W09-0426,D08-1076,0,0.0484393,"mentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al., 2008). We reused the same features and weights to create the Hungarian lattices. For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low-probability segmentation paths (Sixtus and Ortmanns, 1999). We then concatenated the lattices in each sentence. Automatic evaluation metric TER Experimental variations Forest minimum error training Source To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the targe"
W09-0426,J03-1002,0,0.0041536,"omena are poorly addressed with conventional approaches to statistical machine Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 145–149, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 145 2.1 3 Training and development data This section describes the experimental variants explored. To construct the translation suffix arrays used to compute the translation grammar, we used the parallel training data provided. The preprocessed training data was filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) in both directions and symmetrized using the grow-diag-final-and heuristic. We trained a 5-gram language model from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002). We divided the 2008 workshop “news test” sets into two halves of approximately 1000 sentences each and designated one the dev set and the other the dev-test set. 2.2 3.1 Since the official evaluation criterion for WMT09 is human sentence ranking, we"
W09-0426,P03-1021,0,0.0131462,"r training algorithm to minimize WER (Macherey et al., 2008). We reused the same features and weights to create the Hungarian lattices. For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low-probability segmentation paths (Sixtus and Ortmanns, 1999). We then concatenated the lattices in each sentence. Automatic evaluation metric TER Experimental variations Forest minimum error training Source To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al., 2008). The loss function we used was the linear combination of TER and BLEU described in the previous section. German Hungarian Condition baseline lattice baseline lattice BLEU TER 20.8 21.3 11.0 12.3 60.7 59.9 71.1 70.4 Table 1: Impact of compound segmentation lattices. To build the translation model f"
W09-0426,P02-1040,0,0.0782984,"a 5-gram language model from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002). We divided the 2008 workshop “news test” sets into two halves of approximately 1000 sentences each and designated one the dev set and the other the dev-test set. 2.2 3.1 Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning: − BLEU 2 Although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking, Yaser Al-Onaizan (personal communication) reports that it correlates well with the human evaluation metric HTER. In this paper, we report uncased TER and BLEU individually. 2.3 Word segmentation lattices Both German and Hungarian have a large number of compound words that are created by concatenating several morphemes to form a single orthographic token. To deal with productive"
W09-0426,P99-1067,0,0.0414102,"data using the onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likely translations for each paraphrase were added to the grammar in order to provide mappings to English for OOV Hungarian phrases. This attempt at monolingually-derived sourceside paraphrasing did not yield improvements over baseline. Preliminary analysi"
W09-0426,C04-1051,0,\N,Missing
W09-0426,P07-1065,0,\N,Missing
W09-0426,P06-1002,0,\N,Missing
W09-0426,N03-1017,0,\N,Missing
W09-0426,P10-4002,1,\N,Missing
W09-0426,P07-1019,0,\N,Missing
W09-0426,N09-1046,1,\N,Missing
W09-0426,P09-1019,1,\N,Missing
W09-1114,P08-1024,1,0.253533,"r an advantage on an appropriately optimised model. 4 Minimum risk training In the previous section, we described how our sampler can be used to search for the best translation under a variety of decoding criteria (max derivation, translation, and minimum risk). However, there appeared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis tran"
W09-1114,J93-2003,0,0.0151602,"xpected risk training and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus"
W09-1114,D08-1033,0,0.0826863,"n reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effective for some proble"
W09-1114,W06-1673,0,0.0295365,"rementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. 109 This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001; and by the EuroMatrix project funded by the"
W09-1114,P01-1030,0,0.032787,"ns. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sampler is similar to the decoder of Germann et al. (2001), which starts with an approximate solution and then incrementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from it"
W09-1114,D07-1103,0,0.222812,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N07-1018,0,0.206505,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N03-1017,1,0.202563,"s for probabilistic inference: 1. It typically differs from the true model maximum. 2. It often requires additional approximations in search, leading to further error. 3. It introduces restrictions on models, such as use of only local features. 4. It provides no good solution to compute the normalization factor Z(f ) required by many probabilistic algorithms. In this work, we solve these problems using a Monte Carlo technique with none of the above drawbacks. Our technique is based on a novel Gibbs sampler that draws samples from the posterior distribution of a phrase-based translation model (Koehn et al., 2003) but operates in linear time with respect to the number of input words (Section 2). We show Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102–110, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics that it is effective for both decoding (Section 3) and minimum risk training (Section 4). 2 A Gibbs sampler for phrase-based translation models We begin by assuming a phrase-based translation model in which the input sentence, f , is segmented into phrases, which are sequences of adjacent words.1 Each foreign phrase is"
W09-1114,P07-2045,1,0.0245677,"en the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and the sampler may concentrate on a very narrow probability region. We optimised the scaling factor on a 200-sentence portion of the tuning set, finding that a multiplicative factor of 10 worked best for fr-en and a multiplicative factor of 6 for de-en. 3 The first experiment shows the effect of different initialisations and numbers of sampler iterations on max-derivation decoding performance of the sampler. The Moses decoder (Koehn et al., 2007) was used to generate the starting hypothesis, either in full DP max-derivation mode, or alternatively with restrictions on the features and reordering, or with zero weights to simulate a random initialisation, and the number of iterations varied from 100 to 200,000, with a 100 iteration burn-in in each case. Figure 3 shows the variation of model score with sampler iteration, for the different starting points, and for both language pairs. 3 We experimented with annealing, where the scale factor is gradually increased to sharpen the distribution while sampling. However, we found no improvements"
W09-1114,N04-1022,0,0.549837,"formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and"
W09-1114,W02-1018,0,0.133102,"ntroduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfo"
W09-1114,N06-1045,0,0.0117505,"led to a situation where entire classes of potentially useful features are not considered because they would be impractical to integrate into a DP based translation system. With the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the t"
W09-1114,C00-2163,0,0.042141,"g and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all o"
W09-1114,P03-1021,0,0.0606256,"nstructed a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train a 3-gram language model. 2 The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). 105 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1 -norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how peaked or flat the distribution is. If the scaling factor is too small, then the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and t"
W09-1114,P02-1040,0,0.0772047,"words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either running in max-derivation and max-translation mode. Using the Gibbs sampler in this way mak"
W09-1114,C96-2215,0,0.0376306,"Missing"
W09-1114,P06-2101,0,0.637386,"ared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis translation L= X X p(e|f )`eˆ(e) (3) hˆ e,f i∈D e This value can be trivially computed using equation (2). In this section, we are concerned with finding the parameters θ that minimise (3). Fortunately, with the log-linear parameterization of p(e|f ), L is differentiable with respect to"
W09-1114,N07-1062,0,0.101321,"of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common"
W09-1114,D07-1055,0,0.0381723,"th the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sample"
W09-1114,P08-1012,0,0.00643602,"framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effe"
W10-1707,N03-1017,0,0.00664368,"ith a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We u"
W10-1707,N04-1022,0,0.033198,"gmented the training data using the 1-best segmentation predicted by the segmentation model, and word aligned this with the English side. This version of the parallel corpus was concatenated with the original training parallel corpus. 3 Language Model RandLM SRILM TER 22.4 23.1 69.1 68.0 Table 1: Impact of language model on translation 3.2 Experimental variation Minimum Bayes risk decoding During minimum error rate training, the decoder employs a maximum derivation decision rule. However, upon exploration of alternative strategies, we have found benefits to using a minimum risk decision rule (Kumar and Byrne, 2004), wherein we want the translation E of the input F that has the least expected loss, again as measured by some loss function L: This section describes the experiments we performed in attempting to assess the challenges posed by current methods and our exploration of new ones. 3.1 BLEU Bloom filter language model Language models play a crucial role in translation performance, both in terms of quality, and in terms of practical aspects such as decoder memory usage and speed. Unfortunately, these two concerns tend to trade-off one another, as increasing to a higher-order more complex language mod"
W10-1707,J03-1002,0,0.00355772,"ch with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demon"
W10-1707,P06-1002,0,0.0129619,"overhead. The RandLM provides a median between the two extremes: reduced memory and (relatively) fast decoding at the price of somewhat decreased translation quality. Since we are using a relatively large beam of 1000 candidates for decoding, the time presented in Table 3 does not represent an accurate basis for comparison of cdec to other decoders, which should be done using the results presented in Dyer et al. (2010). We also tried one other grammar extraction configuration, which was with so-called ‘loose’ phrase extraction heuristics, which permit unaligned words at the edges of phrases (Ayan and Dorr, 2006). When decoded using the SRILM and MBR, this achieved the best performance for our system, with a BLEU score of 23.6 and TER of 67.7. 4 Conclusion We presented the University of Maryland hierarchical phrase-based system for the WMT2010 shared translation task. Using cdec, we experimented with a number of methods that are shown above to lead to improved German-to-English translation quality over our baseline according to BLEU and TER evaluation. These include methods to directly address German morphological complexity, such as appropriate feature functions, segmentation lattices, and a model fo"
W10-1707,P03-1021,0,0.0385487,"ng of all possible segmentations for every word consisting of more than 6 letters was created, and the paths were weighted by the posterior probability assigned by the segmentation model. Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer Viterbi envelope semiring training To optimize the feature weights for our model, we use Viterbi envelope semiring training (VEST), which is an implementation of the minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. VEST reinterprets MERT within a semiring framework, which is a useful mathematical abstraction for defining two general operations, addition (⊕) and multiplication (⊗) over a set of values. Formally, a semiring is a 5-tuple (K, ⊕, ⊗, 0, 1), where addition must be commu2 This algorithm is equivalent to the hypergraph MERT algorithm described by Kumar et al. (2009). 3 The reference segmentation lattices used for training are available in the cdec distribution. 73 (2009).4 To create the translation model for lattice input, we segmented the training d"
W10-1707,P96-1041,0,0.088436,"rl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We used the 2525 sentences in newstest2009 as our dev set on which we tuned the feature weights, and report results on the 2489 sentences of the news-test2010 test set. 2.2 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of pr"
W10-1707,P02-1040,0,0.0920472,"icted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 nicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized b"
W10-1707,J07-2003,0,0.0447776,"rchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 nicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method"
W10-1707,2006.amta-papers.25,0,0.0211012,"ls which significantly decrease space requirements, thus becoming amenable to being stored locally in memory, while only introducing a quantifiable number of false positives. In order to assess what the impact on translation quality would be, we trained a system identical to the one described above, except using a RandLM. Conveniently, it is possible to construct a RandLM directly from an existing SRILM, which is the route we followed in using the SRILM described in Section 2.1 to create our RandLM.5 Table 1 shows the comparison of SRILM and RandLM with respect to performance on BLEU and TER (Snover et al., 2006) on the test set. b = arg min EP (E|F ) [L(E, E 0 )] E E0 X = arg min P (E|F )L(E, E 0 ) 0 E E Using our system, we generate a unique 500best list of translations to approximate the posterior distribution P (E|F ) and the set of possible translations. Assuming H(E, F ) is the weight of the decoder’s current path, this can be written as: P (E|F ) ∝ exp αH(E, F ) where α is a free parameter which depends on the models feature functions and weights as well as pruning method employed, and thus needs to be separately empirically optimized on a held out development set. For this submission, we used"
W10-1707,W09-0426,1,0.791239,"hrase-based translation model, which is formally based on the notion of a synchronous context-free grammar (SCFG) (Chiang, 2007). These grammars contain pairs of CFG rules with aligned nonterminals, and by introducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to be well suited for German-English translation, as they are able to both exploit lexical information for and efficiently compute all possible reorderings using a CKY-based decoder (Dyer et al., 2009). Our system is implemented within cdec, an efficient and modular open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs (Dyer et al., 2010).1 cdec’s modular framework facilitates seamless integration of a translation model with different language models, pruning strategies and inference algorithms. As input, cdec expects a string, lattice, or context-free forest, and uses it to generate a hypergraph representation, which represents the full translation forest without any pruning. The forest can now be rescored, by intersecting"
W10-1707,P10-4002,1,0.884705,"ntroducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to be well suited for German-English translation, as they are able to both exploit lexical information for and efficiently compute all possible reorderings using a CKY-based decoder (Dyer et al., 2009). Our system is implemented within cdec, an efficient and modular open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs (Dyer et al., 2010).1 cdec’s modular framework facilitates seamless integration of a translation model with different language models, pruning strategies and inference algorithms. As input, cdec expects a string, lattice, or context-free forest, and uses it to generate a hypergraph representation, which represents the full translation forest without any pruning. The forest can now be rescored, by intersecting it with a language model for instance, to obtain output translations. The above capabilities of cdec allow us to perform the experiments described below, which would otherwise be quite cumbersome to carry o"
W10-1707,P07-1065,0,0.0232787,"ch as decoder memory usage and speed. Unfortunately, these two concerns tend to trade-off one another, as increasing to a higher-order more complex language model improves performance, but comes at the cost of increased size and difficulty in deployment. Ideally, the language model will be loaded into memory locally by the decoder, but given memory constraints, it is entirely possible that the only option is to resort to a remote language model server that needs to be queried, thus introducing significant decoding speed delays. One possible alternative is a randomized language model (RandLM) (Talbot and Osborne, 2007). Using Bloom filters, which are a randomized data structure for set representation, we can construct language models which significantly decrease space requirements, thus becoming amenable to being stored locally in memory, while only introducing a quantifiable number of false positives. In order to assess what the impact on translation quality would be, we trained a system identical to the one described above, except using a RandLM. Conveniently, it is possible to construct a RandLM directly from an existing SRILM, which is the route we followed in using the SRILM described in Section 2.1 to"
W10-1707,N09-1046,1,0.817241,"sults on the 2489 sentences of the news-test2010 test set. 2.2 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of productive compounding, where words are formed by the concatenation of several morphemes and the orthography does not delineate the morpheme boundaries, we utilize word segmentation lattices. These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). In order to construct diverse and accurate segmentation lattices, we built a maximum entropy model of compound word splitting which makes use of a small number of dense features, such as frequency of hypothesized morphemes as separate units in a monolingual corpus, number of predicted morphemes, and number of letters in a predicted morpheme. The feature weights are tuned to maximize conditional log-likelihood using a small amount of manually created reference lattices which encode linguistically plausible segmentations for a selected set of compound words.3 To create lattices for the dev and"
W10-1707,P07-1019,0,0.0279095,"The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 nicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method"
W10-1718,N09-1025,0,0.0465037,"Missing"
W10-1718,J07-2003,0,0.2479,"Missing"
W10-1718,P08-1115,0,0.0560574,"Missing"
W10-1718,J99-4004,0,0.114206,"Missing"
W10-1718,D09-1005,1,0.852808,"Missing"
W10-1718,W08-0402,1,0.900437,"Missing"
W10-1718,N09-2003,1,0.885137,"Missing"
W10-1718,W09-0424,1,0.907315,"Missing"
W10-1718,P09-1067,1,0.884344,"Missing"
W10-1718,D07-1104,0,0.0414902,"Missing"
W10-1718,P03-1021,0,0.11921,"Missing"
W10-1718,2006.amta-papers.25,0,0.0712727,"Missing"
W10-1718,W06-3119,0,0.208519,"Missing"
W10-1718,N04-1035,0,\N,Missing
W10-1718,W10-1726,1,\N,Missing
W10-1718,P02-1001,0,\N,Missing
W11-2139,2007.mtsummit-papers.3,0,0.0309223,"e with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion” of the first model about a translation hypoth"
W11-2139,P08-1024,0,0.69537,"we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion”"
W11-2139,J92-4003,0,0.294899,"Missing"
W11-2139,J93-2003,0,0.0182861,"ve use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Si"
W11-2139,W11-2103,0,0.0265339,"ogical analyzers). An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German. Aside from this, no further manually annotated data was used, and we suspect many of the improvements described here can be had in other language pairs. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the paralle"
W11-2139,P96-1041,0,0.137601,"g data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a"
W11-2139,D08-1024,0,0.0449065,"ormation in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we do not rely on linguistic source parses, but instead only make use of features that are directly computable from the source sentence and the parse structure being considered in the decoder. In particular, we take inspiration from the model of Klein and Manning (2002), which models constituency in terms of the contexts that rule productions occur in. Additionally, we make use of salient aspects of the spans being dominated by a nonterminal, such as the words at the beginning and end of t"
W11-2139,J07-2003,0,0.877323,"ments to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training. 1 Introduction We describe the German-English translation system submitted to the shared translation task in the Sixth Workshop on Machine Translation (WMT11) by the ARK research group at Carnegie Mellon University.1 The core translation system is a hierarchical phrase-based machine translation system (Chiang, 2007) that has been extended in several ways described in this paper. Some of our innovations focus on modeling. Since German and English word orders can diverge considerably, particularly in non-matrix clauses, we focused on feature engineering to improve the modeling of long-distance relationships, which are poorly captured in standard hierarchical phrasebased translation models. To do so, we developed features that assess the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2"
W11-2139,P10-1146,0,0.026468,"and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced"
W11-2139,P10-4002,1,0.856164,"ted data was used, and we suspect many of the improvements described here can be had in other language pairs. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the gr"
W11-2139,N09-1046,1,0.856464,"y higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sente"
W11-2139,W09-0439,0,0.105102,"the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2010), who showed that when training to optimize BLEU (Papineni et al., 2002), overfitting is reduced by supplementing a single human-generated reference translation with additional computer-generated references. We generated supplementary pseudo-references for our development set (which is translated into many languages, but once) by using MT output from a secondary Spanish-English translation system. Second, following Foster and Kuhn (2009), we used a secondary development set to select from among many optimization runs, which further improved generalization. We largely sought techniques that did not require language-specific resources (e.g., treebanks, POS 337 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337–343, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics annotations, morphological analyzers). An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German. Aside from this, no further manu"
W11-2139,P06-1121,0,0.0545557,"segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features”"
W11-2139,D09-1023,1,0.847272,"4). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. Howe"
W11-2139,W11-2123,0,0.0277226,"development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used"
W11-2139,D11-1125,0,0.0365813,"that MERT can not be used to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of"
W11-2139,P02-1017,0,0.0707841,"nder the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we do not rely on linguistic source parses, but instead only make use of features that are directly computable from the source sentence and the parse structure being considered in the decoder. In particular, we take inspiration from the model of Klein and Manning (2002), which models constituency in terms of the contexts that rule productions occur in. Additionally, we make use of salient aspects of the spans being dominated by a nonterminal, such as the words at the beginning and end of the span, and the length of the span. Importantly, the features do not rely on the target words being predicted, but only look at the structure of the translation derivation. As such, they can be understood as monolingual parse features.3 Table 1 lists the feature templates that were used. Template CTX :fi−1 , fj CTX :fi−1 , fj , x CTX :fi−1 , fj , x, (j − i) LU :fi−1 LB :fi"
W11-2139,N03-1017,0,0.162752,"ompounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many d"
W11-2139,N04-1022,0,0.0377714,") was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gi"
W11-2139,P09-1019,1,0.890376,"ds were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often po"
W11-2139,D09-1005,0,0.0209007,"ed to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary featu"
W11-2139,N09-1069,0,0.0173381,"a variable representing the unobserved synchronous parses giving rise to the pair of sentences hf, ei, and where R(θ) is a penalty that favors less complex models. Since we not only want to prevent over P fitting but also want a small model, we use R(θ) = k |θk |, the `1 norm, which forces many parameters to be exactly 0. Although L is not convex in θ (on account of the latent derivation variable), we make use of an online stochastic gradient descent algorithm that imposes an `1 penalty on the objective (Tsuruoka et al., 2009). Online algorithms are often effective for non-convex objectives (Liang and Klein, 2009). We selected 12,500 sentences randomly from the news-commentary portion of the training data to use to train the latent variable model. Using the standard rule extraction heuristics (Chiang, 2007), 9,967 of the sentence pairs could be derived.4 In addition to the parse features describe above, the standard phrase features (relative frequency and lexical translation probabilities), and a rule count feature were included. Training was run for 48 hours on a single machine, which resulted in 8 passes through the training data, instantiating over 8M unique features. The regularization strength λ w"
W11-2139,P06-1096,0,0.14682,"Missing"
W11-2139,C08-1064,0,0.0343443,"e a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was cr"
W11-2139,P08-1114,0,0.022175,"ture modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we"
W11-2139,P02-1038,0,0.0739731,"that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Park"
W11-2139,P03-1021,0,0.537349,"version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (K"
W11-2139,P02-1040,0,0.0824535,"his paper. Some of our innovations focus on modeling. Since German and English word orders can diverge considerably, particularly in non-matrix clauses, we focused on feature engineering to improve the modeling of long-distance relationships, which are poorly captured in standard hierarchical phrasebased translation models. To do so, we developed features that assess the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2010), who showed that when training to optimize BLEU (Papineni et al., 2002), overfitting is reduced by supplementing a single human-generated reference translation with additional computer-generated references. We generated supplementary pseudo-references for our development set (which is translated into many languages, but once) by using MT output from a secondary Spanish-English translation system. Second, following Foster and Kuhn (2009), we used a secondary development set to select from among many optimization runs, which further improved generalization. We largely sought techniques that did not require language-specific resources (e.g., treebanks, POS 337 Proce"
W11-2139,W04-3201,0,0.116735,"training corpus, it is replaced by a special unknown token. The SMALLCAPS prefixes prevent accidental feature collisions. 3.1 Two-phase discriminative learning The parse features just introduced are numerous and sparse, which means that MERT can not be used to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm"
W11-2139,P09-1054,0,0.0541831,"− X log hf,ei∈T where pθ (e, d |f) = X pθ (e, d |f) d exp θ&gt; h(f, e, d) Z(f) , where d is a variable representing the unobserved synchronous parses giving rise to the pair of sentences hf, ei, and where R(θ) is a penalty that favors less complex models. Since we not only want to prevent over P fitting but also want a small model, we use R(θ) = k |θk |, the `1 norm, which forces many parameters to be exactly 0. Although L is not convex in θ (on account of the latent derivation variable), we make use of an online stochastic gradient descent algorithm that imposes an `1 penalty on the objective (Tsuruoka et al., 2009). Online algorithms are often effective for non-convex objectives (Liang and Klein, 2009). We selected 12,500 sentences randomly from the news-commentary portion of the training data to use to train the latent variable model. Using the standard rule extraction heuristics (Chiang, 2007), 9,967 of the sentence pairs could be derived.4 In addition to the parse features describe above, the standard phrase features (relative frequency and lexical translation probabilities), and a rule count feature were included. Training was run for 48 hours on a single machine, which resulted in 8 passes through"
W11-2139,P10-1040,0,0.0101623,"prove long range reordering quality. To further support the modeling of larger spans, we incorporated a 7-gram class-based language model. Automatic word clusters are attractive because they can be learned for any language without supervised data, and, unlike part-of-speech annotations, each word is in only a single class, which simplifies inference. We performed Brown clustering (Brown et al., 1992) on 900k sentences from our language modeling data (including the news commentary corpus and a subset of Gigaword). We obtained 1,000 clusters using an implementation provided by Liang (2005),6 as Turian et al. (2010) found that relatively large numbers clusters gave better performance for information extraction tasks. We then replaced words with their clusters in our language modeling data and built a 7-gram LM with Witten-Bell smoothing (Witten and Bell, 1991).7 The last two rows of Ta6 http://www.cs.berkeley.edu/˜pliang/ software 7 The distributional assumptions made by the more commonly used Kneser-Ney estimator do not hold in the wordble 2 shows that in conjunction with the source parse features, a slight improvement comes from including the 7-gram LM. 4 Non-translating tokens When two languages share"
W11-2139,D07-1080,0,0.0804662,"algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion” of the first model abou"
W11-2208,N10-1083,0,0.327563,"vere problem of local maxima, and suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature engineering to incorporate knowledge and improve perfo"
W11-2208,D11-1005,1,0.858908,"sented in WFSTs. Das and Petrov (2011) also consider the problem of unsupervised bilingual POS induction. They make use of independent conventional HMM monolingual tagging models that are parameterized with feature-rich log-linear models (Berg-Kirkpatrick et al., 2010). However, training is constrained with tag dictionaries inferred using bilingual contexts derived from aligned parallel data. In this way, the complex inference and modeling challenges associated with a bilingual tagging model are avoided. Finally, multilingual POS induction has also been considered without using parallel data. Cohen et al. (2011) present a multilingual estimation technique for part-of-speech tagging (and grammar induction), where the lack of parallel data is compensated by the use of labeled data for some languages and unlabeled data for other languages. 3 Model Our model is a Markov random field whose random variables correspond to words in two parallel sentences and POS tags for those words. Let s = hs1 , . . . , sNs i and t = ht1 , . . . , tNt i denote the two word sequences; these correspond to Ns + Nt observed random variables.1 Let x and y denote the sequences of POS tags for s and t, respectively. These are the"
W11-2208,P11-1061,0,0.230006,"nd suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature engineering to incorporate knowledge and improve performance. We conjecture t"
W11-2208,erjavec-2004-multext,0,0.0739108,"Missing"
W11-2208,P07-1094,0,0.0711216,"task of unsupervised bilingual POS induction was originally suggested and explored by Snyder et al. (2008). Their work proposes a joint model over pairs of tag sequences and words that can be understood as a pair of hidden Markov models (HMMs) Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 64–71, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics in which aligned words share states (a fixed and observable word alignment is assumed). Figure 1 gives an example for a French-English sentence pair. Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. The hyperparameters of the prior distributions are inferred from data in an empirical Bayesian fashion. Why repeat x1/y1 X2/y2 that catastrophe ? x4/y5 x5/y6 catastrophe ? x3 Pourquoi répéter y3 y4 la même Figure 1: Bilingual Directed POS induction model When word alignments are monotonic (i.e., there are no crossing links in the alignment graph), the model of Snyder et al. is straightforward to construct. However, crossing alignment links pose a"
W11-2208,N06-1041,0,0.692952,"model. Their solution is to eliminate such alignment pairs (their algorithm for doing so is discussed below). Unfortunately, this is a potentially a serious loss of information. Crossing alignments often correspond to systematic word order differences between languages (e.g., SVO vs. SOV languages). As such, leaving them out prevents useful information about entire subsets of POS types from exploiting of bilingual context. In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. An example of such a monolingual model is shown in Figure 2. Both papers developed different approximations of the computationally expensive partition function. Haghighi and Klein (2006) approximated by ignoring all sentences of length greater than some maximum, and the “contrastive estimation” of Smith and Eisner (2005) approximates the partition function with a set 65 A N V V Economic discrepancies are growing Figure 2: Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples w"
W11-2208,2005.mtsummit-papers.11,0,0.106634,"Missing"
W11-2208,J93-2004,0,0.0361661,"Missing"
W11-2208,J03-1002,0,0.00473676,"Missing"
W11-2208,P05-1044,1,0.952248,"However, crossing alignment links pose a problem: they induce cycles in the tag sequence graph, which corresponds to an ill-defined probability model. Their solution is to eliminate such alignment pairs (their algorithm for doing so is discussed below). Unfortunately, this is a potentially a serious loss of information. Crossing alignments often correspond to systematic word order differences between languages (e.g., SVO vs. SOV languages). As such, leaving them out prevents useful information about entire subsets of POS types from exploiting of bilingual context. In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. An example of such a monolingual model is shown in Figure 2. Both papers developed different approximations of the computationally expensive partition function. Haghighi and Klein (2006) approximated by ignoring all sentences of length greater than some maximum, and the “contrastive estimation” of Smith and Eisner (2005) approximates the partition function with a set 65 A N V"
W11-2208,D08-1109,0,0.502551,"likelihood with joint MRFs suffers from a severe problem of local maxima, and suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature"
W11-2208,N03-1033,0,\N,Missing
W11-2208,W06-2920,0,\N,Missing
W11-2208,N09-1009,1,\N,Missing
W11-2208,H94-1020,0,\N,Missing
W12-4410,P10-4002,1,0.876198,"ce labeling model requires a runtime that is quadratic in the number of labels. Since our labels are character n-grams in the target language, we must cope with thousands of labels. To make the most of each inference call during training, we apply a mini-batch training algorithm which converges quickly. 66 Finally, we wish to consider some global features that would render exact inference intractable. We therefore use a reranking model (Collins, 2000). We demonstrate the performance benefits of these modifications on the Arabic-English transliteration task, using the open-source library cdec (Dyer et al., 2010)1 for learning and prediction. 2 Problem Description In the NEWS 2012 workshop, the task is to generate a list of ten transliterations in a specified target language for each named entity (in a known source language) in the test set. A training set is provided for each language pair. An entry in the training set comprises a named entity in the source language and one or more transliterations in the target language. Zhang et al. (2012) provides a detailed description of the shared task. 3 Approach 3.1 Character Alignment In order to extract source-target character mappings, we use m2m-aligner ("
W12-4410,N09-1046,1,0.852255,"possibility is to create multiple independent training inputs for each input x, one for each correct transliteration in Y ∗ (x). Using this approach, with K different transliterations, the CRF training objective will attempt to assign probability 1 K to each correct transliteration, and 0 to all others (modulo regularization). Alternatively, we can train the model to maximize the marginal probability assigned by the model to the set of correct labels Y ∗ = {y1 , . . . , yK }. That is, we assume a set of training data {(xj , Yj∗ )}`j=1 and replace the standard CRF objective with the following (Dyer, 2009):5 P P maxλ `j=1 log y∈Y ∗ pλ (y |xj ) − C||λ||22 (4) j This learning objective has more flexibility. It can maximize the likelihood of the training data by giving uniform probability to each reference transliteration for a given x, but it does not have to. In effect, we do not care how probability mass is distributed among the correct labels. Our hope is that if some transliterations are difficult to model—perhaps because they are incorrect—the model will be able to disregard them. To calculate the marginal probability for each xj , we represent Y ∗ (x) as a label lattice, which is supported"
W12-4410,I08-6006,0,0.276473,"raining objective that optimizes toward any of a set of possible correct labels (since more than one transliteration is often possible for a particular input), and a k-best reranking stage to incorporate nonlocal features. This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop. 1 Introduction Transliteration is the transformation of a piece of text from one language’s writing system into another. Since the transformation is mostly explained as local substitutions, deletions, and insertions, we treat word transliteration as a sequence labeling problem (Ganesh et al., 2008; Reddy and Waxmonsky, 2009), using linear-chain conditional random fields as our model (Lafferty et al., 2001; Sha and Pereira, 2003). We tailor this model to the transliteration task in several ways. First, for the Arabic-English task, each Arabic input is paired with multiple valid English transliteration outputs, any of which is judged to be correct. To effectively exploit these multiple references during learning, we use a training objective in which the model may favor some correct transliterations over the others. Computationally efficient inference is achieved by encoding the reference"
W12-4410,W05-1506,0,0.0350846,"n estimate of plen (|y || |x|) assuming a multinomial distribution with parameters estimated using transliteration pairs of the training set. The probabilistic model for each of the global features is trained using training data provided for the shared task. The reranking score is a linear combination of log pcrf (y |x), log pcharLM (y), log pclassLM (y) and log plen (|y x|). Linear coefficients are optimized using simulated annealing, optimizing accuracy of the 1-best transliteration in a development set. k-best lists are extracted from the CRF trellis using the lazy enumeration algorithm of Huang and Chiang (2005). 4 Experiments We tested on the NEWS 2012 Arabic-English dataset. The train, development, and test sets consist of 27,177, 1,292, and 1,296 source named entities, respectively, with an average 9.6 references per name in each case. Table 4 summarizes our results using the ACC score (Zhang et al., 2012) (i.e., word accuracy in top-1). “Basic CRF” is the model with mini-batch learning and represents multiple reference transliterations as independent training examples. We manually tuned the number of training examples and LBFGS iterations per mini-batch to five and eight, respectively. “CRF w/lat"
W12-4410,N07-1047,0,0.0465979,"1 for learning and prediction. 2 Problem Description In the NEWS 2012 workshop, the task is to generate a list of ten transliterations in a specified target language for each named entity (in a known source language) in the test set. A training set is provided for each language pair. An entry in the training set comprises a named entity in the source language and one or more transliterations in the target language. Zhang et al. (2012) provides a detailed description of the shared task. 3 Approach 3.1 Character Alignment In order to extract source-target character mappings, we use m2m-aligner (Jiampojamarn et al., 2007),2 which implements a forward-backward algorithm to sum over probabilities of possible character sequence mappings, and uses Expectation Maximization to learn mapping probabilities. We allow source characters to be deleted, but not target characters. Parameters -maxX and -maxY are tuned on a devevelopment set. Our running example is the Arabic name EAdl (in Buckwalter’s ASCII-based encoding of Arabic) with two English transliterations: ADEL and ’ADIL. The character alignment for the two pairs is shown in Fig. 1. 1 2 http://www.cdec-decoder.org http://code.google.com/p/m2m-aligner Proceedings o"
W12-4410,W09-3520,0,0.114285,"t optimizes toward any of a set of possible correct labels (since more than one transliteration is often possible for a particular input), and a k-best reranking stage to incorporate nonlocal features. This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop. 1 Introduction Transliteration is the transformation of a piece of text from one language’s writing system into another. Since the transformation is mostly explained as local substitutions, deletions, and insertions, we treat word transliteration as a sequence labeling problem (Ganesh et al., 2008; Reddy and Waxmonsky, 2009), using linear-chain conditional random fields as our model (Lafferty et al., 2001; Sha and Pereira, 2003). We tailor this model to the transliteration task in several ways. First, for the Arabic-English task, each Arabic input is paired with multiple valid English transliteration outputs, any of which is judged to be correct. To effectively exploit these multiple references during learning, we use a training objective in which the model may favor some correct transliterations over the others. Computationally efficient inference is achieved by encoding the references in a lattice. Second, infe"
W12-4410,N03-1028,0,0.0773347,"ble for a particular input), and a k-best reranking stage to incorporate nonlocal features. This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop. 1 Introduction Transliteration is the transformation of a piece of text from one language’s writing system into another. Since the transformation is mostly explained as local substitutions, deletions, and insertions, we treat word transliteration as a sequence labeling problem (Ganesh et al., 2008; Reddy and Waxmonsky, 2009), using linear-chain conditional random fields as our model (Lafferty et al., 2001; Sha and Pereira, 2003). We tailor this model to the transliteration task in several ways. First, for the Arabic-English task, each Arabic input is paired with multiple valid English transliteration outputs, any of which is judged to be correct. To effectively exploit these multiple references during learning, we use a training objective in which the model may favor some correct transliterations over the others. Computationally efficient inference is achieved by encoding the references in a lattice. Second, inference for our first-order sequence labeling model requires a runtime that is quadratic in the number of la"
W12-4410,W12-4401,0,\N,Missing
W13-1736,J92-4003,0,0.123486,"Missing"
W13-1736,U07-1006,0,0.0307986,"fy that language. This task has a clear empirical motivation. Nonnative speakers make different errors when they write English, depending on their native language (Lado, 1957; Swan and Smith, 2001); understanding the different types of errors is a prerequisite for correcting them (Leacock et al., 2010), and systems such as the one we describe here can shed interesting light on such errors. Tutoring applications can use our system to identify the native language of students and offer better-targeted advice. Forensic linguistic applications are sometimes required to determine the L1 of authors (Estival et al., 2007b; Estival et al., 2007a). Additionally, we believe that the task is interesting in and of itself, providing a better understanding of non-native language. We are thus equally interested in defining meaningful features whose contribution to the task can be linguistically interpreted. Briefly, our features draw heavily on prior work in general text classification and authorship identification, those used in identifying so-called translationese (Volansky et al., forthcoming), and a class of features that involves determining what minimal changes would be necessary to transform the essays into “s"
W13-1736,C90-2036,0,0.0232636,"rb and Masuda, 2008). Since English’s orthography is largely phonemic—even if it is irregular in many places, we expect leaners whose native phoneme contrasts are different from those of English to make characteristic spelling errors. For example, since Japanese and Korean lack a phonemic /l/-/r/ contrast, we expect native speakers of those languages to be more likely to make spelling errors that confuse l and r relative to native speakers of languages such as Spanish in which that pair is contrastive. To make this information available to our model, we use a noisy channel spelling corrector (Kernighan, 1990) to identify and correct misspelled words in the training and test data. From these corrections, we extract minimal edit features that show what insertions, deletions, substitutions and joinings (where two separate words are written merged into a single orthographic token) were made by the author of the essay. Restored tags We focus on three important token classes defined above: punctuation marks, function words and cohesive verbs. We first remove words in these classes from the texts, and then recover the most likely hidden tokens in a sequence of words, according to an n-gram language model"
W13-1736,P08-1068,0,0.0252399,"tions). To restore hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-clu"
W13-1736,P11-1132,1,0.750772,"e markers These are 40 function words (and short phrases) that have a strong discourse function in texts (however, because, in fact, etc.). Translators tend to spell out implicit utterances and render them explicitly in the target text (Blum-Kulka, 1986). We use the list of Volansky et al. (forthcoming). Cohesive verbs This is a list of manually compiled verbs that are used, like cohesive markers, to spell out implicit utterances (indicate, imply, contain, etc.). Function words Frequent tokens, which are mostly function words, have been used successfully for various text classification tasks. Koppel and Ordan (2011) define a list of 400 such words, of which we only use 100 (using the entire list was not significantly different). Note that pronouns are included in this list. Contextual function words To further capitalize on the ability of function words to discriminate, we define pairs consisting of a function word from the list mentioned above, along with the POS tag of its adjacent word. This feature captures patterns such as verbs and the preposition or particle immediately to their right, or nouns and the determiner that precedes them. We also define 3-grams consisting of one or two function words an"
W13-1736,N13-1039,1,0.79567,"Missing"
W13-1736,W13-1706,0,0.102937,"Missing"
W13-1736,N03-1033,0,0.010714,"etreault et al., 2013). The training data consists of 1000 essays from each native language. The essays are short, consisting of 10 to 20 sentences each. We used the provided splits of 900 documents for training and 100 for development. Each document is annotated with the author’s English proficiency level (low, medium, high) and an identification (1 to 8) of the essay prompt. All essays are tokenized and split into sentences. In table 1 we provide some statistics on the training corpora, listed by the authors’ proficiency level. All essays were tagged with the Stanford part-of-speech tagger (Toutanova et al., 2003). We did not parse the dataset. # Documents # Tokens # Types Low 1,069 245,130 13,110 Medium 5,366 1,819,407 37,393 High 3,456 1,388,260 28,329 Table 1: Training set statistics. 4 Model For our classification model we used the creg regression modeling framework to train a 11-class logistic regression classifier.1 We parameterize the classifier as a multiclass logistic regression: P exp j λ j h j (x, y) pλ (y |x) = , Zλ (x) where x are documents, h j (·) are real-valued feature functions of the document being classified, λ j are the corresponding weights, and y is one of the eleven L1 class lab"
W13-1736,W07-0602,0,0.263634,"Missing"
W13-1736,P10-1040,0,0.0107663,"hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-cluster http://www.ark.c"
W13-1736,U09-1008,0,0.0944702,"Missing"
W13-1736,D11-1148,0,0.0757912,"Missing"
W13-2205,P06-1121,0,0.0416165,"sh and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules were extracted from the corpus following the method of Hanneman et al. (2011). This decomposes each tree pair into a collection of SCFG rules by exhaustively identifying aligned subtrees to serve as rule left-hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of new nodes in the parse tree to subdivide regions of flat structure. Virtual nodes are similar to the A+B extended categories of SAMT (Zollmann and Venugopal, 2006), but with the added constraint that they may not conflict with the surrounding tree structure. Because the SCFG rules are labeled with nont"
W13-2205,W12-4410,1,0.923402,"for the 2013 Workshop on Machine Translation shared translation task: French–English, Russian–English, and English–Russian. Our French–English system (§3) showcased our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011). Our Russian–English and English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation"
W13-2205,J92-4003,0,0.07177,"e 500-best outputs of a syntactic system constructed similarly to the French–English system. 6 30.8 30.9 31.1 Russian side of the training corpus. An unpruned, modified Kneser-Ney smoothed 4-gram language model (Chen and Goodman, 1996) was estimated from all available Russian text (410 million words) using the KenLM toolkit (Heafield et al., 2013). A standard hierarchical phrase-based system was trained with rule shape indicator features, obtained by replacing terminals in translation rules by a generic symbol. MIRA training was performed to learn feature weights. Additionally, word clusters (Brown et al., 1992) were obtained for the complete monolingual Russian data. Then, an unsmoothed 7-gram language model was trained on these clusters and added as a feature to the translation system. Indicator features were also added for each cluster and bigram cluster occurence. These changes resulted in an improvement of more than a BLEU point on our held-out development set. Slavic languages like Russian have a large number of different inflected forms for each lemma, representing different cases, tenses, and aspects. Since our training data is rather limited relative to the number of inflected forms that are"
W13-2205,W11-1011,1,0.933305,"alized modules to create “synthetic translation options” that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context. 1 Introduction The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in three language pairs for the 2013 Workshop on Machine Translation shared translation task: French–English, Russian–English, and English–Russian. Our French–English system (§3) showcased our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011). Our Russian–English and English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastru"
W13-2205,P96-1041,0,0.103448,"Missing"
W13-2205,N13-1029,1,0.816688,"kens, contained a token of more than 30 characters, or had particularly unbalanced length ratios were also removed. After filtering, 30.9 million sentence pairs remained for rule extraction: 14.4 million from the clean data, and 16.5 million from the web data. 3.2 1 Selecting the stopping point still requires a measure of intuition. The label set size of 1814 chosen here roughly corresponds to the number of joint labels that would exist in the grammar if virtual nodes were not included. This equivalence has worked well in practice in both internal and published experiments on other data sets (Hanneman and Lavie, 2013). Preprocessing and Grammar Extraction Our French–English system uses parse trees in both the source and target languages, so tokeniza71 most common type. We conducted experiments with three different frequency cutoffs: 100, 200, and 500, with each increase decreasing the grammar size by 70–80 percent. Extracted rules each have 10 features associated with them. For an SCFG rule with source lefthand side `s , target left-hand side `t , source righthand side rs , and target right-hand side rt , they are: 3.3 • phrasal translation log relative frequencies log f (rs |rt ) and log f (rt |rs ); • la"
W13-2205,J07-2003,0,0.201588,"Missing"
W13-2205,W11-1015,1,0.856156,"hop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules were extracted from the corpus following the method of Hanneman et al. (2011). This decomposes each tree pair into a collection of SCFG rules by exhaustively identifying aligned subtrees to serve as rule left-hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of ne"
W13-2205,P11-2031,1,0.827764,"n relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomization algorithm. The closest is the variant with cutoff 200, which is generally judged to be sl"
W13-2205,P13-2121,0,0.0613657,"Missing"
W13-2205,W11-2107,1,0.844656,"f (rs |rt ) and log f (rt |rs ); • labeling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomiz"
W13-2205,W11-2123,0,0.0303697,"iterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by"
W13-2205,W12-3131,1,0.818679,"s in the various system pairs. 3 French-English Syntax System Our submission for French–English is a tree-totree translation system that demonstrates several innovations from group’s research on SCFG-based translation. 3.1 Data Selection We divided the French–English training data into two categories: clean data (Europarl, News Commentary, UN Documents) totaling 14.8 million sentence pairs, and web data (Common Crawl, Giga-FrEn) totaling 25.2 million sentence pairs. To reduce the volume of data used, we filtered non-parallel and other unhelpful segments according to the technique described by Denkowski et al. (2012). This procedure uses a lexical translation model learned from just the clean data, as well as source and target n-gram language models to compute the following feature scores: • French and English 4-gram log likelihood (normalized by length); • French–English and English–French lexical translation log likelihood (normalized by length); and, • Fractions of aligned words under the French– English and English–French models. We pooled previous years’ WMT news test sets to form a reference data set. We computed the same features. To filter the web data, we retained only sentence for which each fea"
W13-2205,P09-1019,1,0.851065,"based on a histogrambased similarity function that looks at what target labels correspond to a particular source label and vice versa. The number of clusters used is determined based on spikes in the distance between successive clustering iterations, or by the number of source, target, or joint labels remaining. Starting from a default grammar of 877 French, 2580 English, and 131,331 joint labels, we collapsed the label space for our WMT system down to 50 French, 54 English, and 1814 joint categories.1 the French–English system, using the hypergraph MERT algorithm and optimizing towards BLEU (Kumar et al., 2009). The remainder of the paper will focus on our primary innovations in the various system pairs. 3 French-English Syntax System Our submission for French–English is a tree-totree translation system that demonstrates several innovations from group’s research on SCFG-based translation. 3.1 Data Selection We divided the French–English training data into two categories: clean data (Europarl, News Commentary, UN Documents) totaling 14.8 million sentence pairs, and web data (Common Crawl, Giga-FrEn) totaling 25.2 million sentence pairs. To reduce the volume of data used, we filtered non-parallel and"
W13-2205,P10-4002,1,0.867841,"d English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria,"
W13-2205,C08-1064,0,0.0127826,"odels, we trained 4-gram Markov models using the target side of the bitext and any available monolingual data (including Gigaword for English). Additionally, we trained 7-gram language models using 600-class Brown clusters with Witten-Bell smoothing.2 5.2 Baseline System Our baseline Russian–English system is a hierarchical phrase-based translation model as implemented in cdec (Chiang, 2007; Dyer et al., 2010). SCFG translation rules that plausibly match each sentence in the development and deftest sets were extracted from the aligned parallel data using the suffix array indexing technique of Lopez (2008). A Russian morphological analyzer was used to lemmatize the training, development, and test data, and the “noisier channel” translation approach of Dyer (2007) was used in the Russian– English system to let unusually inflected surface forms back off to per-lemma translations. • transliterations of OOV Russian words (§5.3); • English target sides with varied function words (for example, given a phrase that translates into cat we procedure variants like the cat, a cat and of the cat); and, 2 73 http://www.ark.cs.cmu.edu/cdyer/ru-600/. 5.3 Synthetic Translations: Transliteration Table 2: Russian"
W13-2205,W11-2139,1,0.884244,"en incorporated in current statistical MT systems. For our Russian–English system, we additionally used a secondary “pseudo-reference” translation when tuning the parameters of our Russian– English system. This was created by automatically translating the Spanish translation of the provided development data into English. While the output of an MT system is not always perfectly grammatical, previous work has shown that secondary machine-generated references improve translation quality when only a single human reference is available when BLEU is used as an optimization criterion (Madnani, 2010; Dyer et al., 2011). We describe the CMU systems submitted to the 2013 WMT shared task in machine translation. We participated in three language pairs, French–English, Russian– English, and English–Russian. Our particular innovations include: a labelcoarsening scheme for syntactic tree-totree translation and the use of specialized modules to create “synthetic translation options” that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context. 1 Introduction The MT research group at Carnegie Mellon Uni"
W13-2205,D10-1004,0,0.0328156,"Missing"
W13-2205,N13-1073,1,0.782282,"om word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokeniz"
W13-2205,W07-0729,0,0.0220307,"trained 7-gram language models using 600-class Brown clusters with Witten-Bell smoothing.2 5.2 Baseline System Our baseline Russian–English system is a hierarchical phrase-based translation model as implemented in cdec (Chiang, 2007; Dyer et al., 2010). SCFG translation rules that plausibly match each sentence in the development and deftest sets were extracted from the aligned parallel data using the suffix array indexing technique of Lopez (2008). A Russian morphological analyzer was used to lemmatize the training, development, and test data, and the “noisier channel” translation approach of Dyer (2007) was used in the Russian– English system to let unusually inflected surface forms back off to per-lemma translations. • transliterations of OOV Russian words (§5.3); • English target sides with varied function words (for example, given a phrase that translates into cat we procedure variants like the cat, a cat and of the cat); and, 2 73 http://www.ark.cs.cmu.edu/cdyer/ru-600/. 5.3 Synthetic Translations: Transliteration Table 2: Russian-English summary. Analysis revealed that about one third of the unseen Russian tokens in the development set consisted of named entities which should be transli"
W13-2205,P02-1040,0,0.087928,"on log relative frequencies log f (rs |rt ) and log f (rt |rs ); • labeling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others accordin"
W13-2205,W12-3160,0,0.0121088,"n phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules w"
W13-2205,2006.amta-papers.25,0,0.0492601,"beling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomization algorithm. The closest is"
W13-2205,W13-2234,1,0.806649,"se pairs whose source sides match the test sentence are retained. • Abstract rules (whose RHS are all nonterminals) are globally pruned. Only the 4000 most frequently observed rules are retained. • Mixed rules (whose RHS are a mix of terminals and nonterminals) must match the test sentence, and there is an additional frequency cutoff. 4 Synthetic Translation Options Before discussing our Russian–English and English–Russian systems, we introduce the concept of synthetic translation options, which we use in these systems. We provide a brief overview here; for more detail, we refer the reader to Tsvetkov et al. (2013). In language pairs that are typologically similar, words and phrases map relatively directly from source to target languages, and the standard approach to learning phrase pairs by extraction from parallel data can be very effective. However, in language pairs in which individual source language words have many different possible translations (e.g., when the target language word could have many different inflections or could be surrounded by different function words that have no After this filtering, the number of completely lexical rules that match a given sentence is typically low, up to a f"
W13-2205,W06-3119,0,0.0265547,"hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of new nodes in the parse tree to subdivide regions of flat structure. Virtual nodes are similar to the A+B extended categories of SAMT (Zollmann and Venugopal, 2006), but with the added constraint that they may not conflict with the surrounding tree structure. Because the SCFG rules are labeled with nonterminals composed from both the source and target trees, the nonterminal inventory is quite large, leading to estimation difficulties. To deal with this, we automatically coarsening the nonterminal labels (Hanneman and Lavie, 2011). Labels are agglomeratively clustered based on a histogrambased similarity function that looks at what target labels correspond to a particular source label and vice versa. The number of clusters used is determined based on spik"
W13-2205,sharoff-etal-2008-designing,0,\N,Missing
W13-2234,W11-2103,0,0.0339276,"Missing"
W13-2234,P13-1110,0,0.02121,"Missing"
W13-2234,2012.eamt-1.60,0,0.0414271,"Missing"
W13-2234,P05-1045,0,0.0316055,"Missing"
W13-2234,2011.iwslt-evaluation.19,0,0.221029,"including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrates the synthesis of translation options into the decoder. While related in spirit, their method only supports a limited set of generative processes for producing the candidate set (lacking, for instance, the simple and effective phrase post-editing process we have used), and Conclusions and future work The contribution of this work is twofold."
W13-2234,E12-1068,0,0.0894067,"e need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing o"
W13-2234,N12-1047,0,0.012334,"matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics standard phrase-based translation approaches, and why this technique might be better than some alternative proposals that been made for generalizing beyond translation examples directly observable in the training data. In language pairs that ar"
W13-2234,P06-1121,0,0.169489,"Missing"
W13-2234,J07-2003,0,0.343534,"Missing"
W13-2234,P11-2031,1,0.83514,"say He is in the hospital while UK English speakers might prefer He is in hospital. 7 https://github.com/redpony/creg 8 Preliminary experiments indicated that the excess of N labels resulted in poor performance. 274 et al., 2007) to train a baseline phrase-based SMT system. Each configuration we compare has a different phrase table, with synthetic phrases generated with best-first or iterative strategies, from a phrase table with- or without-determiners, with variable number of translation features. To verify that system improvement is consistent, and is not a result of optimizer instability (Clark et al., 2011), we replicate each experimental setup three times, and then estimate the translation quality of the median MT system using the MultEval toolkit.9 The corpus is the same as in Section 4.3: the training part contains 112,527 sentences from Russian-English TED corpus, randomly sampled 3K sentences are used for tuning and a disjoint set of 2K sentences is used for test. We lowercase both sides, and use Stanford CoreNLP10 tools to tokenize the corpora. We employ SRILM toolkit (Stolcke, 2002) to linearly interpolate the target side of the training corpus with the WMT English corpus, optimizing towa"
W13-2234,N12-1023,0,0.011934,"t sentence is created by matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics standard phrase-based translation approaches, and why this technique might be better than some alternative proposals that been made for generalizing beyond translation examples directly observable in the training data."
W13-2234,P11-1004,0,0.0666999,"billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7"
W13-2234,C08-1022,0,0.0705118,"Missing"
W13-2234,D07-1091,0,0.036584,"of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrates the synthesis of translation options into the decoder. While related in spirit, their method only supports a limited set of generative processes for producing the candidate set (lacking, for instance, the simple and effective phrase post-editing process we have used), and Conclusions and future work The contribution of this work is twofold. First, we propose a new su"
W13-2234,P10-1147,0,0.0342768,"Missing"
W13-2234,W00-1308,0,0.176747,"Missing"
W13-2234,N03-1017,0,0.0089937,"usual. Doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier. 1 Introduction Phrase-based translation works as follows. A set of candidate translations for an input sentence is created by matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, Au"
W13-2234,P07-2045,1,0.013907,"Missing"
W13-2234,W00-0708,0,0.154151,"him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7 Related Work 8 Automated determiner prediction has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabr"
W13-2234,W07-0704,0,0.101819,"Missing"
W13-2234,P02-1040,0,0.0861695,"Missing"
W13-2234,N10-1018,0,0.0318527,"determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7 Related Work 8 Automated determiner prediction has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrat"
W13-2234,P06-1132,0,0.0145425,"eed to feed the three billion urban hundreds of them . we need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiabi"
W13-2234,N07-1007,0,0.0515896,"n urban hundreds of them . we need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject m"
W13-2234,D08-1033,0,\N,Missing
W13-2234,I08-1059,0,\N,Missing
W13-2307,2020.lrec-1.643,0,0.293721,"Missing"
W13-2307,P99-1010,0,0.418476,"fficiently mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Bo"
W13-2307,N06-1019,0,0.236476,"mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Both annotations and analys"
W13-2307,W07-2416,0,0.0563019,"ator labels are as in table 2. Per-annotator com (with lexical reconciliation) and inter-annotator softComPrec are aggregated over sentences by arithmetic mean. less burdensome, and the specialized annotations did prove complementary to each other.19 5.4 Treebank Comparison Though the annotators in our study were native speakers well acquainted with representations of English syntax, we sought to quantify their agreement with the expert treebankers who created the EWTB (the source of the Reviews sentences). We converted the EWTB’s constituent parses to dependencies via the PennConverter tool (Johansson and Nugues, 2007),20 then removed punctuation. Agreement with the converted treebank parses appears in the bottom two rows of table 3. Because the EWTB commits to a single analysis, precision scores are quite lopsided. Most of its attachments are consistent with our annotations (softComPrec &gt; 0.9), but these allow many additional analyses (hence the scores below 0.5). Annotator Specialization As an experiment in using underspecification for labor division, two of the annotators of Reviews data were assigned specific linguistic phenomena to focus on. Annotator “D” was tasked with the internal structure of base"
W13-2307,P06-2066,0,0.0538705,"nd compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced language"
W13-2307,W12-1706,0,0.033779,"When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced languages (to name two examples). Traditional syntactic annotation projects like the Penn Treebank (Marcus ∗ 2 A Dependency Grammar for Annotation Although depende"
W13-2307,J93-2004,0,0.0444543,"portion of the English Web Treebank 14 Malagasy is a VOS Austronesian language spoken by 15 million people, mostly in Madagascar. Kinyarwanda is an SVO Bantu language spoken by 12 million people mostly in Rwanda. All annotations were done by native speakers of English. The Kinyarwanda and Malagasy annotators had basic proficiency in these languages. 15 As a point of comparison, during the Penn Treebank project, annotators corrected the syntactic bracketings produced by a high-quality hand-written parser (Fidditch) and achieved a rate of only 375 tokens/hour using a specialized GUI interface (Marcus et al., 1993). 16 Included with the data and software release (footnote 1). 57 com thus reduces the commitment averages for each annotation—to a greater extent for annotator “A” (.96 in table 2 vs. .82 in table 3) because “A” marked more multiwords. An analysis fully compatible with both annotations exists for only 27/60 sentences; the finer-grained softComPrec measure (§4.2), however, offers insight into the balance between commitment and agreement. Qualitatively, we observe three leading causes of incompatibilities (disagreements): obvious annotator mistakes (such as the marked as a head); inconsistent h"
W13-2307,1993.iwpt-1.22,0,0.0410599,"lop algorithms to evaluate and compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and"
W13-2307,D07-1014,1,0.853574,"edges that are known to be incompatible with the annotation before searching for spanning trees. Our “upward-downward” method for constructing a graph of supported edges first enumerates a set of candidate top nodes for every fudge expression, then uses that information to infer a set of supported parents for every node.12 The supported edge graph then consists of vertices lexnodes(A) ∪ {root} and edges S 0 0 v∈lexnodes(A) {(v → v ) ∀ v ∈ suppParentsA (v)}. From this graph we can count all directed spanning trees in cubic time using Kirchhoff’s matrix tree theorem (Chaiken and Kleitman, 1978; Smith and Smith, 2007; Margoliash, 2010).13 If some lexical node has no supported parents, this reflects conflicting constraints in the annotation, and no spanning tree will be found. Promiscuity will tend to be higher for longer sentences. To control for this, we define a second quantity, the annotation’s commitment quotient (commitment being the opposite of promiscuity), 4.2 Inter-Annotator Agreement FUDG can encode flat groupings and coreference at the lexical level, as well as syntactic structure over lexical items. Inter-annotator agreement can be measured separately for each of these facets. Pilot annotator"
W13-2307,D08-1027,1,0.400347,"Missing"
W13-2307,N13-1039,1,0.775169,"Missing"
W13-2307,W13-2307,1,0.0512826,"Missing"
W13-3001,J99-1004,0,0.0889239,"the number of times N ⇒ δ occurs in derivation tree τ , then Y p(τ ) = p(δ |N )fτ (N ⇒δ) . (10) MLE. The arguably most standard technique for setting the parameters of a probability distribution is so that they maximize the likelihood of a sample of training data. In the naive parameterization, the maximum likelihood estimate (MLE) for each ERF is the empirical relative frequency parameter θˆδ|N of the rewrite N ⇒ δ in the training data (Abney, 1997): P ˜ τ f (τ )fπ(τ ) (N ⇒ δ) ERF θˆδ|N = P . P 0 ˜ τ f (τ ) (N ⇒δ 0 )∈π(G) fπ(τ ) (N ⇒ δ ) (N ⇒δ)∈H With mild assumptions to ensure consistency (Chi, 1999), the p(τ )’s form a proper probability distribution over all derivations in H.7 Because the derivation trees of the MG G stand in a bijection with the derivation trees of the MCFG π(G), stochastic MCFGs can be used to define a distribution on MG derivations. 3.3 Unfaithfulness to MGs While the naive parameterization with MLE estimation is simple, it is arguably a poor choice for parameterizing distributions on MGs. The problem is that, relative to the independence assumptions encoded in the MG formalism, each step of the MCFG derivation both conditions on and predicts “too much” structure. As"
W13-3001,J99-4004,0,0.0930444,"r the two distributions.11 4.3 Locally normalized log-linear models Even with our assumption of feature locality, findˆ remains challenging since the second term ing λ 10 There are several conditions under which this is true. It is trivially true if |Ω(G) |&lt; ∞. When Ω is infinite, the denominator may still be finite if features functions grow (super) linearly with the derivation size in the limiting case as the size tends to infinity. Then, if feature weights are negative, the denominator will either be equal to or bounded from above by an infinite geometric series with a finite sum. Refer to Goodman (1999) and references therein. 11 While the maximizing point cannot generally be solved for analytically, gradient based optimization techniques may be effectively used to find it (and it is both guaranteed to exist and guaranteed to be unique). 12 We say that the issue of computational tractability is only partially resolved because only certain operations — identifying the most probable derivation of a string — are truly efficient. Computing the model’s normalization function, while no longer NP-hard, still not practical. 8 in (13) is difficult to compute.13 In this section we suggest a parameteri"
W13-3001,P87-1015,0,0.766218,"Missing"
W14-0311,W12-3160,0,0.0178252,"licable to the wealth of scenarios that still require precise human-quality translation that MT is currently unable to deliver, including an everincreasing number of government, commercial, and community-driven projects. 72 Workshop on Humans and Computer-assisted Translation, pages 72–77, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics and all post-editing data, letting the system upweight translations with newly learned vocabulary and phrasing absent in the large monolingual text. Finally, the margin-infused relaxed algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012) is used to make an online parameter update after each sentence is post-edited, minimizing model error. This allows the system to continuously rescale weights for translation and language model features that adapt over time. Since true post-editing data is infeasible to collect during system development and internal testing, as standard MT pipelines require tens of thousands of sentences to be translated with low latency, a simulated post-editing paradigm (Hardt and Elming, 2010) can be used, wherein pregenerated reference translations act as a stand-in for actual post-editing. This approximat"
W14-0311,2010.amta-papers.21,0,0.0400339,"phrasing absent in the large monolingual text. Finally, the margin-infused relaxed algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012) is used to make an online parameter update after each sentence is post-edited, minimizing model error. This allows the system to continuously rescale weights for translation and language model features that adapt over time. Since true post-editing data is infeasible to collect during system development and internal testing, as standard MT pipelines require tens of thousands of sentences to be translated with low latency, a simulated post-editing paradigm (Hardt and Elming, 2010) can be used, wherein pregenerated reference translations act as a stand-in for actual post-editing. This approximation is effective for tuning and internal evaluation when real post-editing data is unavailable. In simulated post-editing tasks, decoding (for both the test corpus and each pass over the development corpus during optimization) begins with baseline models trained on standard bilingual and monolingual text. After each sentence is translated, the following take place in order: First, MIRA uses the new source–reference pair to update weights for the current models. Second, the source"
W14-0311,2013.mtsummit-papers.5,0,0.0165575,"new computeraided translation (CAT) tools that leverage adaptive machine translation. The CASMACAT7 project (Alabau et al., 2013) focuses on building state-of-the-art tools for computer-aided translation. This includes translation predictions backed by machine translation systems that incrementally update model parameters as users edit translations (Mart´ınez-G´omez et al., 2012; L´opez-Salcedo et al., 2012). The MateCat8 project (Cattelan, 2013) specifically aims to integrate machine translation (including online model adaptation and translation quality estimation) into a web-based CAT tool. Bertoldi et al. (2013) show improvements in translator productivity when using the MateCat tool with an adaptive MT system that uses cache-based translation and language models. ios, we compare a static Spanish–English MT system to a comparable adaptive system on a blind out-of-domain test. Competitive with the current state-of-the-art, both systems are trained on the 2012 NAACL WMT (Callison-Burch et al., 2012) constrained resources (2 million bilingual sentences) using the cdec toolkit (Dyer et al., 2010). Blind post-editing evaluation sets are drawn from the Web Inventory of Transcribed and Translated Talks (WIT"
W14-0311,W11-2123,0,0.0131309,"Single instances of the directional word alignment models are loaded into memory for force-aligning post-edited data. When a new user requests a translation, a new context is started. The following are loaded into memory: a table of all postedited data from the user, a user-specific dynamic language model, and a user-specific decoder (in this case an instance of MIRA that has a userspecific decoder and set of weights). Each user also requires an instance of the large static language model, though all users effectively share a single instance through the memory mapped implementation of KenLM (Heafield, 2011). When a new sentence is to be translated, the grammar extractor samples from the shared background data plus the user-specific post-editing data to generate a sentence-specific grammar incorporating data from all prior sentences translated by the same user. The sentence is then decoded using the user and time-specific grammar, current weights, and current dynamic language model. When a postedited sentence is available as feedback, the following happen in order: (1) the source-reference pair is used to update feature weights with MIRA, (2) the source-reference pair is force-aligned and added t"
W14-0311,W12-3102,0,0.0547918,"Missing"
W14-0311,2012.amta-wptp.3,1,0.736462,"focused, allowing for exact timing measurements. A pause button is available if the translator needs to take breaks. TransCenter can generate reports 5 Experiments In a preliminary experiment to evaluate the impact of adaptive MT in real-world post-editing scenar4 https://github.com/mjdenkowski/ transcenter-live 75 Baseline Adaptive HTER 19.26 17.01 made freely available for further analysis.6 TransCenter records all data necessary for more sophisticated editing time analysis (Koehn, 2012) as well as analysis of translator behavior, including pauses (used as an indicator of cognitive effort) (Lacruz et al., 2012). Rating 4.19 4.31 Table 1: Aggregate HTER scores and average translator self-ratings (5 point scale) of postediting effort for translations of TED talks from Spanish into English. 6 There has been a recent push for new computeraided translation (CAT) tools that leverage adaptive machine translation. The CASMACAT7 project (Alabau et al., 2013) focuses on building state-of-the-art tools for computer-aided translation. This includes translation predictions backed by machine translation systems that incrementally update model parameters as users edit translations (Mart´ınez-G´omez et al., 2012; L"
W14-0311,J07-2003,0,0.0464049,"ost-editing can then be deployed to serve real human translators without further modification. These extensions allow the MT system to generate improved translations that require significantly less effort to correct for later sentences in the document. This paradigm is now implemented in the freely available cdec (Dyer et al., 2010) machine translation toolkit as Realtime, part of the pycdec (Chahuneau et al., 2012) Python API. Standard MT systems use aggregate statistics from all training text to learn a single large translation grammar (in the case of cdec’s hierarchical phrase-based model (Chiang, 2007), a synchronous context-free grammar) consisting of rules annotated with feature scores. As an alternative, the bitext can be indexed using a suffix array (Lopez, 2008), a data structure allowing fast source-side lookups. When a new sentence is to be translated, training sentences that share spans of text with the input sentence are sampled from the suffix array. Statistics from the sample are used to learn a small, sentence-specific grammar on-thefly. The adaptive paradigm extends this approach to support online updates by also indexing the new bilingual sentences generated as a post-editor w"
W14-0311,2006.amta-papers.25,0,0.0683311,"nto English. Five students training to be professional translators post-edit machine translations of these excerpts using TransCenter. Translations are provided by either the static or fully adaptive system. Tasks are divided such that each user translates 2 excerpts with the static system and 2 with the adaptive system and each excerpt is post-edited either 2 or 3 times with each system. Users do not know which system is providing the translations. Using the data collected by TransCenter, we evaluate post-editing effort with the established human-targeted translation edit rate (HTER) metric (Snover et al., 2006). HTER computes an edit distance score between initial MT outputs and the “targeted” references created by human postediting, with lower scores being better. Results for the two systems are aggregated over all users and documents. Shown in Table 1, introducing an adaptive MT system results in a significant reduction in editing effort. We additionally average the user post-ratings for each translation by system to evaluate user perception of the adaptive system compared to the static baseline. Also shown in Table 1, we see a slight preference for the adaptive system. This data, as well as preci"
W14-0311,E14-1042,1,0.819691,"are freely available under an open source license. 1 2 Adaptive Machine Translation Traditional machine translation systems operate in batch mode: statistical translation models are estimated from large volumes of sentence-parallel bilingual text and then used to translate new text. Incorporating new data requires a full system rebuild, an expensive operation taking up to days of time. As such, MT systems in production scenarios typically remain static for large periods of time (months or even indefinitely). Recently, an adaptive MT paradigm has been introduced specifically for post-editing (Denkowski et al., 2014). Three major MT system components are extended to support online updates, allowing human posteditor feedback to be immediately incorporated: • An online translation model is updated to include new translations extracted from postediting data. • A dynamic language model is updated to include post-edited target language text. • An online update is made to the system’s feature weights after each sentence is postedited. Introduction This paper describes the end-to-end machine translation post-editing setup provided by cdec Realtime and TransCenter. As the quality of MT systems continues to improv"
W14-0311,P06-1124,0,0.011483,"ne translation toolkit (Dyer et al., 2010), Realtime1 provides an efficient implementation of the adaptive MT paradigm that can serve an arbitrary number of unique post-editors concurrently. A full Realtime tutorial, including stepby-step instructions for installing required software and building full adaptive systems, is availThe adaptive paradigm uses two language models. A standard (static) n-gram language model estimated on large monolingual text allows the system to prefer translations more similar to humangenerated text in the target language. A (dynamic) Bayesian n-gram language model (Teh, 2006) can be updated with observations of the post-edited output in a straightforward way. This smaller model exactly covers the training bitext 1 https://github.com/redpony/cdec/tree/ master/realtime 73 import rt # Start new Realtime translator using a Spanish--English # system and automatic, language-independent text normalization # (pre-tokenization and post-detokenization) translator = rt.RealtimeTranslator(’es-en.d’, tmpdir=’/tmp’, cache_size=5, norm=True) # Translate a sentence for user1 translation = translator.translate(’Muchas gracias Chris.’, ctx_name=’user1’) # Learn from user1’s post-ed"
W14-0311,P10-4002,1,0.574109,"added to the Bayesian language model. As sentences are translated, the models gain valuable context information, allowing them to adapt to the specific target document and translator. Context is reset at the start of each development or test corpus. Systems optimized with simulated post-editing can then be deployed to serve real human translators without further modification. These extensions allow the MT system to generate improved translations that require significantly less effort to correct for later sentences in the document. This paradigm is now implemented in the freely available cdec (Dyer et al., 2010) machine translation toolkit as Realtime, part of the pycdec (Chahuneau et al., 2012) Python API. Standard MT systems use aggregate statistics from all training text to learn a single large translation grammar (in the case of cdec’s hierarchical phrase-based model (Chiang, 2007), a synchronous context-free grammar) consisting of rules annotated with feature scores. As an alternative, the bitext can be indexed using a suffix array (Lopez, 2008), a data structure allowing fast source-side lookups. When a new sentence is to be translated, training sentences that share spans of text with the input"
W14-0311,2012.eamt-1.60,0,\N,Missing
W14-1615,J99-1004,0,0.168391,"istributions are constructed to build in additional inductive bias. 3.1 The first idea subsumes the complexity measure used by Baldridge, but accomplishes the goal naturally by letting the probabilities decrease as the category grows. The rate of decay is governed by the pterm parameter: the marginal probability of generating a terminal (atomic) category in each expansion. A higher pterm means a stronger emphasis on simplicity. The probability distribution over categories is guaranteed to be proper so long as pterm > 12 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). The second idea is a natural extension of the complexity concept and is particularly relevant when features are used. The original complexity measure treated all atoms uniformly, but e.g. we would expect NPexpl / N to be less likely than NP / N since it contains the more specialized, and thus rarer, atom NPexpl . We define the distribution patom (a) as the prior over atomic categories. Due to our weak, type-only supervision, we have to estimate patom from just the tag dictionary and raw corpus, without frequency data. Our goal is to estimate the number of each atom in the supertags that shou"
W14-1615,P11-1061,0,0.050044,"Missing"
W14-1615,D12-1075,1,0.875226,"es using FFBS. To sample a tagging for a sentence x, the strategy is to inductively compute, for each token xi starting with i = 0 and going “forward”, the probability of generating x0 , x1 , . . . , xi via any tag sequence that ends with yi = u: Emission Prior Means (φ0t ) For each supertag type t, φ0t is the mean distribution over words it emits. While Baldridge’s approach used a uniform emission initialization, treating all words as equally likely, we can, again, induce token-level corpus-specific information:5 To set φ0t , we use a variant and simplification of the procedure introduced by Garrette and Baldridge (2012) that takes advantage of our prior over categories PG . Assuming that C(w) is the count of word type w in the raw corpus, TD(w) is the set of supertags associated with word type w in the tag dictionary, and TD(t) is the set of known word types associated with supertag t, the count of word/tag pairs for known words (words appearing in the tag dictionary) is estimated by uniformly distributing a word’s (δ-smoothed) raw counts over its tag dictionary entries: ( C(w)+δ |TD (w) |if t ∈ TD (w) Cknown (t, w) = 0 otherwise For unknown words, we first use the idea of tag “openness” to estimate the like"
W14-1615,N13-1014,1,0.879438,"Missing"
W14-1615,P13-1057,1,0.870138,"Missing"
W14-1615,P08-1085,0,0.018326,"s work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for C"
W14-1615,C08-1008,1,0.200491,". Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combinatory Categorial Grammar (CCG) supertagger, which labels each word with one of a large (possibly unbounded) number of structured categories called supertags, is a considerable challenge. Despite the apparent complexity of the task, supertag sequences have regularities due to universal properties of the CCG formalism (§2) that can be used to reduce the complexity of the problem; previous work showed promising results by using these regularities to initialize an HMM that is then refined with EM (Baldridge, 2008). Here, we exploit CCG’s category structure to motivate a novel prior over HMM parameters for use in Bayesian learning (§3). This prior encourages (i) crosslinguistically common tag types, (ii) tag bigrams that can combine using CCG’s combinators, and (iii) sparse transition distributions. We also go beyond the use of these universals to show how additional, corpus-specific information can be automatically extracted from a combination of the tag dictionary and raw data, and how that information can be combined with the universal knowledge for integration into the model to improve the prior. We"
W14-1615,P07-1094,0,0.241725,"that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical t"
W14-1615,J99-2004,0,0.0338277,"ase, etc), or a complex category formed from the combination of two categories by one of two slash operators. In CCG, complex categories indicate a grammatical relationship between the two operands. For example, the category ( SNP )/ NP might describe a transitive verb, looking first to its right (indicated by /) for an object, then to its left () for a subject, to produce a sentence. Further, atomic categories may be augmented with features, such as Sdcl , to restrict the set of atoms with which they may unify. The task of assigning a category to each word in a text is called supertagging (Bangalore and Joshi, 1999). Because they are recursively defined, there is an infinite number of potential CCG categories (though in practice it is limited by the number of actual grammatical contexts). As a result, the number of supertags appearing in a corpus far exceeds the number of POS tags (see Table 1). Since supertags specify the grammatical context of a token, and high frequency words appear in many contexts, CCG grammars tend to have very high lexical ambiguity, with frequent word types associating with a large number of categories. This ambiguity has made type-supervised supertagger learning very difficult b"
W14-1615,J07-3004,0,0.183225,"5k 60k 9k ambiguity type token 3.75 13.11 56.98 296.18 96.58 323.37 178.88 426.13 dev tokens — 128k 59k 5k test tokens — 127k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were"
W14-1615,N10-1083,0,0.0462617,"Missing"
W14-1615,N09-1036,0,0.0282852,"ld be automatically extracted from the weak supervision that is available: the raw corpus and the tag dictionary. This allows us to combine the cross-linguistic properties of the CCG formalism with corpus- or language-specific information in the data into a single, unified Bayesian prior. Our model uses a relatively large number of parameters, e.g., pterm , pfw , pmod , patom , in the prior. Here, we fixed each to a single value (i.e., a “fully Bayesian” approach). Future work might explore sensitivity to these choices, or empirical Bayesian or maximum a posteriori inference for their values (Johnson and Goldwater, 2009). Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (20"
W14-1615,Q13-1007,0,0.0671236,"ere, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infinite HMM for truly unsupervised POS-tagger learning (Van Gael et al., 2008; Beal et al., 2001). While their model is not restricted to the standard set of POS tags, and may learn a more fine-grained set of labels, the induced labels are arbitrary and not grounded in any grammatical formalism. Bisk and Hockenmaier (2013) developed an approach to CCG grammar induction that does not use a tag dictionary. Like ours, their procedure learns from general properties of the CCG formalism. However, while our work is intended to produce categories that match those used in a particular training corpus, however complex they might be, their work produces categories in a simplified form of CCG in which N and S are the only atoms 148 In this work, as in most type-supervised work, the tag dictionary was automatically extracted from an existing tagged corpus. However, a tag dictionary could instead be automatically induced vi"
W14-1615,D07-1031,0,0.022783,"fic information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar fo"
W14-1615,P11-1087,0,0.0131501,"y from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical token is associated with"
W14-1615,D10-1083,0,0.016251,"show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Ba"
W14-1615,J93-2004,0,0.0456766,"ens — 128k 59k 5k test tokens — 127k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for developme"
W14-1615,bosco-etal-2000-building,0,0.500961,"the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were used for test; the remaining data, 457 sentences from CIVIL LAW and 548 from NEWSPAPER, plus the much smaller 132sentence JRC ACQUIS data, was used for the tag dictionary."
W14-1615,J94-2001,0,0.352076,"of the CCG formalism. Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computation"
W14-1615,P09-1057,0,0.0422068,"Missing"
W14-1615,P10-1051,1,0.81725,"supervision that is available: the raw corpus and the tag dictionary. This allows us to combine the cross-linguistic properties of the CCG formalism with corpus- or language-specific information in the data into a single, unified Bayesian prior. Our model uses a relatively large number of parameters, e.g., pterm , pfw , pmod , patom , in the prior. Here, we fixed each to a single value (i.e., a “fully Bayesian” approach). Future work might explore sensitivity to these choices, or empirical Bayesian or maximum a posteriori inference for their values (Johnson and Goldwater, 2009). Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infini"
W14-1615,P05-1044,1,0.751187,"e obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000"
W14-1615,C10-1122,0,0.194953,"k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were u"
W14-1615,D09-1071,0,0.0533631,"Missing"
W14-1615,W12-3127,0,0.0384305,"Missing"
W14-3315,W12-4410,1,0.859453,"Hindi–English system includes improved data cleaning of development data, a sophisticated linguistically-informed tokenization scheme, a transliteration module, a synthetic phrase generator that improves handling of function words, and a synthetic phrase generator that leverages source-side paraphrases. We will discuss each of these five in turn. 4.1 4.3 We used the 12,000 Hindi–English transliteration pairs from the ACL 2012 NEWS workshop on transliteration to train a linear-chained CRF tagger1 that labels each character in the Hindi token with a sequence of zero or more English characters (Ammar et al., 2012). At decoding, unseen Hindi tokens are fed to the transliterator, which produces the 100 most probable transliterations. We add a synthetic translation option for each candidate transliteration. In addition to this sophisticated transliteration scheme, we also employ a rule-based transliterator that specifically targets acronyms. In Hindi, many acronyms are spelled out phonetically, such as NSA being rendered as enese (en.es.e). We detected such words in the input segments and generated synthetic translation options both with and without periods (e.g. N.S.A. and NSA). Development Data Cleaning"
W14-3315,W11-1011,1,0.850261,"guage pairs for the 2014 Workshop on Machine Translation shared translation task: German–English and Hindi–English. Our systems showcase our multi-phase approach to translation, in which synthetic translation options supplement the default translation rule inventory that is extracted from word-aligned training data. In the German–English system, we used our compound splitter (Dyer, 2009) to reduce data sparsity, and we allowed the translator to back off to translating lemmas when it detected caseinflected OOVs. We also demonstrate our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011) as a contrastive German–English submission. In both the German–English and Hindi–English systems, we used an array of supplemental ideas to enhance translation quality, ranging from lemmatization and synthesis of inflected phrase pairs to novel reordering and rule preference features. Our primary German–English and Hindi– English systems were Hiero-based (Chiang, 2007), while our contrastive German–English system used cdec’s tree-to-tree SCFG formalism. Before submitting, we ran cdec’s implementation of MBR on 500-best lists from each of our systems. For both language pairs, we used the Nelde"
W14-3315,J92-4003,0,0.264159,"ead method to optimize the MBR parameters. In the German–English system, we ran MBR on 500 hypotheses, combining the output of the Hiero and tree-to-tree systems. The remainder of the paper will focus on our primary innovations in the two language pairs. 142 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142–149, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 3 Common System Improvements Both source and target surface-form LM used modified Kneser-Ney smoothing (Kneser and Ney, 1995), while the model over Brown clusters (Brown et al., 1992) used subtract-0.5 smoothing. A number of our techniques were used for both our German–English and Hindi–English primary submissions. These techniques each fall into one of three categories: those that create translation rules, those involving language models, or those that add translation features. A comparison of these techniques and their performance across the two language pairs can be found in Section 6. 3.1 3.3 In addition to the standard array of features, we added four new indicator feature templates, leading to a total of nearly 150,000 total features. The first set consists of target"
W14-3315,N13-1029,1,0.879212,"Missing"
W14-3315,D13-1174,1,0.829223,"ible translations, e.g., when the target language word has many different morphological inflections or is surrounded by different function words that have no direct counterpart in the source language. Therefore, when very large quantities of parallel data are not available, we can expect our phrasal inventory to be incomplete. Synthetic translation option generation seeks to fill these gaps using secondary generation processes that exploit existing phrase pairs to produce plausible phrase translation alternatives that are not directly extractable from the training data (Tsvetkov et al., 2013; Chahuneau et al., 2013). To generate synthetic phrases, we first remove function words from the source and target sides of existing non-gappy phrase pairs. We manually constructed English and Hindi lists of common function words, including articles, auxiliaries, pronouns, and adpositions. We then employ the SRILM hidden-ngram utility (Stolcke, 2002) to restore missing function words according to an ngram language model probability, and add the resulting synthetic phrases to our phrase table. Nominal Normalization Another facet of our system was normalization of Hindi nominals. The Hindi nominal system shows much mor"
W14-3315,W11-1015,1,0.855376,"ore suitable for input into constituency parsing. Importantly, we left We filtered tokenized training sentences by sen146 tence length, token length, and sentence length ratio. The final corpus for parsing and word alignment contained 3,897,805 lines, or approximately 86 percent of the total training resources released under the WMT constrained track. Word alignment was carried out using FastAlign (Dyer et al., 2013), while for parsing we used the Berkeley parser (Petrov et al., 2006). Given the parsed and aligned corpus, we extracted synchronous context-free grammar rules using the method of Hanneman et al. (2011). In addition to aligning subtrees that natively exist in the input trees, our grammar extractor also introduces “virtual nodes.” These are new and possibly overlapping constituents that subdivide regions of flat structure by combining two adjacent sibling nodes into a single nonterminal for the purposes of rule extraction. Virtual nodes are similar in spirit to the “A+B” extended categories of SAMT (Zollmann and Venugopal, 2006), and their nonterminal labels are constructed in the same way, but with the added restriction that they do not violate any existing syntactic structure in the parse t"
W14-3315,J07-2003,0,0.0746686,"2009) to reduce data sparsity, and we allowed the translator to back off to translating lemmas when it detected caseinflected OOVs. We also demonstrate our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011) as a contrastive German–English submission. In both the German–English and Hindi–English systems, we used an array of supplemental ideas to enhance translation quality, ranging from lemmatization and synthesis of inflected phrase pairs to novel reordering and rule preference features. Our primary German–English and Hindi– English systems were Hiero-based (Chiang, 2007), while our contrastive German–English system used cdec’s tree-to-tree SCFG formalism. Before submitting, we ran cdec’s implementation of MBR on 500-best lists from each of our systems. For both language pairs, we used the Nelder–Mead method to optimize the MBR parameters. In the German–English system, we ran MBR on 500 hypotheses, combining the output of the Hiero and tree-to-tree systems. The remainder of the paper will focus on our primary innovations in the two language pairs. 142 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142–149, c Baltimore, Maryland USA"
W14-3315,P11-2031,1,0.887607,"Missing"
W14-3315,W11-2123,0,0.0122222,"observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon"
W14-3315,W11-2107,1,0.826032,"inal test set. Another interesting result is that only one feature set, namely our rule shape features based on Brown clusters, helped on the test set in both language pairs. No feature hurt the BLEU score on the test set in both language pairs, meaning the majority of features helped in one language and hurt in the other. If we compare results on the tuning sets, however, some clearer patterns arise. Brown cluster language models, n-gram features, and our new rule shape features all helped. Furthermore, there were a few features, such as the Brown cluster language model and tuning to Meteor (Denkowski and Lavie, 2011), that helped substantially in one language pair while just barely hurting the other. In particular, the fact that tuning to Meteor instead of BLEU can actually help both BLEU and Meteor scores was rather unexpected. German–English Specific Improvements Our German–English system also had its own suite of tricks, including the use of “pseudoreferences” and special handling of morphologically inflected OOVs. 5.1 Pseudo-References The development sets provided have only a single reference, which is known to be sub-optimal for tuning of discriminative models. As such, we use the output of one or m"
W14-3315,J03-1002,0,0.00414058,"nthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like"
W14-3315,W12-3131,1,0.878781,"dec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in two language pairs for the 2014 Workshop on Machine Translation shared translation task: German–English and Hindi–English. Our systems showcase our multi-p"
W14-3315,P02-1040,0,0.0893725,"r extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in two language pairs for the 2014 Workshop on Machi"
W14-3315,W13-2212,0,0.0231279,"Meteor scores was rather unexpected. German–English Specific Improvements Our German–English system also had its own suite of tricks, including the use of “pseudoreferences” and special handling of morphologically inflected OOVs. 5.1 Pseudo-References The development sets provided have only a single reference, which is known to be sub-optimal for tuning of discriminative models. As such, we use the output of one or more of last year’s top performing systems as pseudo-references during tuning. We experimented with using just one pseudo-reference, taken from last year’s Spanish– English winner (Durrani et al., 2013), and with using four pseudo-references, including the output of last year’s winning Czech–English, French– English, and Russian–English systems (Pino et al., 2013). 5.2 Results 7 German–English Syntax System In addition to our primary German–English system, we also submitted a contrastive German– English system showcasing our group’s tree-totree syntax-based translation formalism. Morphological OOVs Examination of the output of our baseline systems lead us to conclude that the majority of our 145 System Baseline *Meteor Tuning Sentence Boundaries Double Aligners Manual Number Rules Brown Clus"
W14-3315,P06-1055,0,0.0580255,"must be parsed in addition to being word-aligned, we prepared separate copies of the training, tuning, and testing data that are more suitable for input into constituency parsing. Importantly, we left We filtered tokenized training sentences by sen146 tence length, token length, and sentence length ratio. The final corpus for parsing and word alignment contained 3,897,805 lines, or approximately 86 percent of the total training resources released under the WMT constrained track. Word alignment was carried out using FastAlign (Dyer et al., 2013), while for parsing we used the Berkeley parser (Petrov et al., 2006). Given the parsed and aligned corpus, we extracted synchronous context-free grammar rules using the method of Hanneman et al. (2011). In addition to aligning subtrees that natively exist in the input trees, our grammar extractor also introduces “virtual nodes.” These are new and possibly overlapping constituents that subdivide regions of flat structure by combining two adjacent sibling nodes into a single nonterminal for the purposes of rule extraction. Virtual nodes are similar in spirit to the “A+B” extended categories of SAMT (Zollmann and Venugopal, 2006), and their nonterminal labels are"
W14-3315,P10-4002,1,0.866575,"2 We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, German–English and Hindi–English. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create “synthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012)"
W14-3315,W13-2225,0,0.0352624,"Missing"
W14-3315,N13-1073,1,0.91499,", several modules to create “synthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginnin"
W14-3315,P14-1064,0,0.0199677,"e phrases, namely the target phrasal inventory, can also be represented in a graph form, where the distributional features can also be computed from the target monolingual data. Translation information is then propagated from the labeled phrases to the unlabeled phrases in the source graph, proportional to how similar the phrases are to each other on the source side, as well as how similar the translation candidates are to each other on the target side. The newly acquired translation distributions for the unlabeled phrases are written out to a secondary phrase table. For more information, see Saluja et al. (2014). 5 6 As we added each feature to our systems, we first ran a one-off experiment comparing our baseline system with and without each individual feature. The results of that set of experiments are shown in Table 1 for Hindi–English and Table 2 for German–English. Features marked with a * were not included in our final system submission. The most surprising result is the strength of our Hindi–English baseline system. With no extra bells or whistles, it is already half a BLEU point ahead of the second best system submitted to this shared task. We believe this is due to our filtering of the tuning"
W14-3315,W13-2234,1,0.830476,"ave many different possible translations, e.g., when the target language word has many different morphological inflections or is surrounded by different function words that have no direct counterpart in the source language. Therefore, when very large quantities of parallel data are not available, we can expect our phrasal inventory to be incomplete. Synthetic translation option generation seeks to fill these gaps using secondary generation processes that exploit existing phrase pairs to produce plausible phrase translation alternatives that are not directly extractable from the training data (Tsvetkov et al., 2013; Chahuneau et al., 2013). To generate synthetic phrases, we first remove function words from the source and target sides of existing non-gappy phrase pairs. We manually constructed English and Hindi lists of common function words, including articles, auxiliaries, pronouns, and adpositions. We then employ the SRILM hidden-ngram utility (Stolcke, 2002) to restore missing function words according to an ngram language model probability, and add the resulting synthetic phrases to our phrase table. Nominal Normalization Another facet of our system was normalization of Hindi nominals. The Hindi nomi"
W14-3315,W06-3119,0,0.0377712,"Missing"
W14-3315,N09-1046,1,\N,Missing
W14-3315,W12-3160,0,\N,Missing
W14-3356,W10-0710,0,0.132945,"Missing"
W14-3356,2005.iwslt-1.8,0,0.0947835,"Missing"
W14-3356,P05-1074,0,0.156435,"Missing"
W14-3356,P11-1061,0,0.0567147,"Missing"
W14-3356,P91-1023,0,0.724078,"Missing"
W14-3356,1992.tmi-1.9,0,0.36141,"Missing"
W14-3356,W12-3134,0,0.0268465,"Missing"
W14-3356,2012.amta-papers.7,0,0.0943115,"Missing"
W14-3356,W11-2123,0,0.0200324,"Missing"
W14-3356,W12-3153,0,0.0696685,"Missing"
W14-3356,P07-2045,1,0.0120714,"Missing"
W14-3356,P13-1018,1,0.81692,"Missing"
W14-3356,I11-1125,0,0.0609802,"Missing"
W14-3356,J05-4003,0,0.186489,"Missing"
W14-3356,P03-1058,0,0.0964326,"Missing"
W14-3356,P03-1021,0,0.0602179,"Missing"
W14-3356,P02-1040,0,0.108513,"Missing"
W14-3356,W12-3152,0,0.0723752,"Missing"
W14-3356,J03-3002,0,0.372304,"Missing"
W14-3356,N10-1113,0,0.0352934,"Missing"
W14-3356,N10-1063,0,0.0896412,"Missing"
W14-3356,P11-1122,0,0.0870956,"Missing"
W14-3356,J93-1004,0,\N,Missing
W14-3356,W13-2201,0,\N,Missing
W14-3909,C82-1023,0,0.643512,"Missing"
W14-3909,N13-1131,0,0.163962,"Missing"
W14-3909,D14-1098,0,0.0717799,"Missing"
W14-3909,W14-1303,0,0.0366131,"a: semi-supervised learning, word embeddings, and word lists. 1 Introduction Code switching (CS) occurs when a multilingual speaker uses more than one language in the same conversation or discourse. Automatic idenefication of the points at which code switching occurs is important for two reasons: (1) to help sociolinguists analyze the frequency, circumstances and motivations related to code switching (Gumperz, 1982), and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech. CS is pervasive in social media due to its informal nature (Lui and Baldwin, 2014). The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framewor"
W14-3909,Q14-1003,0,0.0518801,"Missing"
W14-3909,D13-1084,0,0.187942,"Missing"
W14-3909,W14-3907,0,0.0215797,"ker uses more than one language in the same conversation or discourse. Automatic idenefication of the points at which code switching occurs is important for two reasons: (1) to help sociolinguists analyze the frequency, circumstances and motivations related to code switching (Gumperz, 1982), and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech. CS is pervasive in social media due to its informal nature (Lui and Baldwin, 2014). The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framework; conditional random field autoencoders (Ammar et al., 2 Task Description The shared task training data consists of code– swit"
W14-3909,P10-1040,0,0.0720813,"ven by: p(y, x ˆ |x, φ) = |x| Y i=1 y:|y|=|x| ``semi (λ, θ) = cL2 ||λ||22 + RDirichlet (θ, α)+ X X cunlabeled × log p(y, x ˆ |x)+ hx,ˆ xi∈U clabeled × X y:|y|=|x| log p(y |x) hx,yi∈L We use block coordinate descent to optimize this objective. First, we use cem iterations of the expectation maximization algorithm to optimize the θ-block while the λ-block is fixed, then we optimize the λ-block with clbfgs iterations of L-BFGS (Liu et al., 1989) while the θ-block is fixed.4 4.2 Unsupervised Word Embeddings For many NLP tasks, using unsupervised word representations as features improves accuracy (Turian et al., 2010). We use word2vec (Mikolov et al., 2013) to train 100–dimensional word embeddings from a large Twitter corpus of about 20 million tweets extracted from the live stream, in multiple languages. We define an additional feature function θxˆi |yi × P|x| exp λ&gt; i=1 f (x, yi−1 , yi , i, φ) P P|x| 0 0 &gt; y0 exp λ i=1 f (x, yi−1 , yi , i, φ) where λ is a vector of CRF feature weights, f is a vector of local feature functions (we use the same features described in §3.2), and θxˆi |yi are categor4 An open source efficient c++ implementation of our method can be found at https://github.com/ldmt-muri/alignm"
W16-2360,D15-1146,0,0.0533517,"increasing. (Bahdanau et al., 2014) extended encoder-decoder structure that the decoder only focuses on parts of source sentence. (Luong et al., 2015) further proposed attention-based model that combine global, attending to all source words, and local, only focusing on a part of source words, attentional mechanism. Rather than using the embedding of each modality independently, Some works (Hardoon et al., 2004; Andrew et al., 2013; Ngiam et al., 2011; Srivastava and Salakhutdinov, 2014) focus on learning joint space of different modalities. In machine translation fields, (Zhang et al., 2014; Su et al., 2015) learned phrase-level bilingual representation using recursive auto-encoder. Beyond textual embedding, (Kiros et al., 2014) proposed CNN-LSTM encoder to project two modalities into the same space. Based on the jointly learning of multiple modalities or languages, we find it possible to evaluate the quality of the translations that if the space of the translated sentence is similar to the source sentence or the image, it may imply that the translated sentence is good. 3 ferred as a content-based measurement of the similarity between the currently translating target and the source words. We util"
W16-2360,Q14-1006,0,0.0352672,"as 1.0, multiplied by 0.7 after 12 epochs, (d) dropout rate was 0.8. Note that the same dropout mask and NMT parameters are shared by all LSTM threads in model 3. Figure 5: Bilingual auto-encoder to re-construct both English and German using only one of them. 4.2 BLEU 34.5 (0.7) 34.8 (0.6) 35.1 (0.8) 36.2 (0.8) 36.5 (0.8) Experimental Setup In the official WMT 2016 multimodal translation task dataset (Elliott et al., 2016), there are 29,000 parallel sentences from English to German for training, 1014 for validation and 1000 for testing. Each sentence describes an image from Flickr30k dataset (Young et al., 2014). We preprocessed all the descriptions into lower case with tokenization and German compound word splitting. Global visual features (fc7) are extracted with VGG-19 (Simonyan and Zisserman, 2014). For regional visual features, the region proposal network in RCNN (Girshick et al., 2014) first recognizes bounding boxes of objects in an image and then we computed 4096-dimensional fc7 features from these regions with VGG-19. The RPN of RCNN is pre-trained on ImageNet dataset 2 and then fine-tuned on MSCOCO dataset 3 with 80 ob1 https://glosbe.com/en/de/ http://image-net.org/ 3 http://mscoco.org/ 2"
W16-2360,P14-1011,0,0.0153211,"of source sentences increasing. (Bahdanau et al., 2014) extended encoder-decoder structure that the decoder only focuses on parts of source sentence. (Luong et al., 2015) further proposed attention-based model that combine global, attending to all source words, and local, only focusing on a part of source words, attentional mechanism. Rather than using the embedding of each modality independently, Some works (Hardoon et al., 2004; Andrew et al., 2013; Ngiam et al., 2011; Srivastava and Salakhutdinov, 2014) focus on learning joint space of different modalities. In machine translation fields, (Zhang et al., 2014; Su et al., 2015) learned phrase-level bilingual representation using recursive auto-encoder. Beyond textual embedding, (Kiros et al., 2014) proposed CNN-LSTM encoder to project two modalities into the same space. Based on the jointly learning of multiple modalities or languages, we find it possible to evaluate the quality of the translations that if the space of the translated sentence is similar to the source sentence or the image, it may imply that the translated sentence is good. 3 ferred as a content-based measurement of the similarity between the currently translating target and the sou"
W16-2360,W16-3210,0,0.382035,"Missing"
W16-2360,D13-1176,0,0.114526,"Missing"
W16-2360,P15-1001,0,\N,Missing
W16-2506,N09-1003,0,0.38845,"Missing"
W16-2506,W14-1618,0,0.0531837,"ty datasets. In this paper, we give a comprehensive analysis of the problems that are associated with the evaluation of word vector representations using word similarity tasks.1 We survey existing literature to construct a list of such problems and also summarize existing solutions to some of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence"
W16-2506,D14-1034,0,0.0955207,"Missing"
W16-2506,Q15-1016,0,0.0805488,", we give a comprehensive analysis of the problems that are associated with the evaluation of word vector representations using word similarity tasks.1 We survey existing literature to construct a list of such problems and also summarize existing solutions to some of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are calle"
W16-2506,D12-1091,0,0.0128401,"Missing"
W16-2506,P12-1015,0,0.0446647,"solutions (if available) to address them. 2.1 Semantic or task-specific similarity? Subjectivity of the task The notion of word similarity is subjective and is often confused with relatedness. For example, cup, and coffee are related to each other, but not similar. Coffee refers to a plant (a living organism) or a hot brown drink, whereas cup is a manmade object, which contains liquids, often coffee. Nevertheless, cup and coffee are rated more similar than pairs such as car and train in WS-353 (Finkelstein et al., 2002). Such anomalies are also found in recently constructed datasets like MEN (Bruni et al., 2012). Thus, such datasets unfairly penalize word vector models that capture the fact that cup and coffee are dissimilar. 2.3 No standardized splits & overfitting To obtain generalizable machine learning models, it is necessary to make sure that they do not overfit to a given dataset. Thus, the datasets are usually partitioned into a training, development and test set on which the model is trained, tuned and finally evaluated, respectively (Manning and Schütze, 1999). Existing word similarity datasets are not partitioned into training, development and test sets. Therefore, optimizing the word vecto"
W16-2506,N15-1028,0,0.0179048,"y extend to the word analogy tasks. 31 itly tunes on the test set and overfits the vectors to the task. On the other hand, if researchers decide to perform their own splits of the data, the results obtained across different studies can be incomparable. Furthermore, the average number of word pairs in the word similarity datasets is small (≈ 781, cf. Table 2), and partitioning them further into smaller subsets may produce unstable results. We now present some of the solutions suggested by previous work to avoid overfitting of word vectors to word similarity tasks. Faruqui and Dyer (2014b), and Lu et al. (2015) evaluate the word embeddings exclusively on word similarity and word analogy tasks. Faruqui and Dyer (2014b) tune their embedding on one word similarity task and evaluate them on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks. This evaluation can be performed in two different ways: (1) choose the hyperparameters with best average performance across all tasks, (2) choose the hyperparameters that beat the"
W16-2506,P11-2031,1,0.115822,"ded each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger’s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation ratio of each model with the gold ranking. This problem was solved by Rastogi et al. (2015), which we describe next. Rastogi et al. (2015) observed that the impro"
W16-2506,W13-3512,0,0.0510995,"r refine this hubness problem to show that there exists a power-law relationship between the frequency-rank5 of a word and the frequency-rank of its neighbors. Specifically, they showed that the average rank of the 1000 nearest neighbors of a word follows: nn-rank ≈ 1000 · word-rank0.17 Inability to account for polysemy (3) 3 This shows that pairs of words which have similar frequency will be closer in the vector-space, thus showing higher word similarity than they should according to their word meaning. Even though newer datasets of word similarity sample words from different frequency bins (Luong et al., 2013; Hill et al., 2014), this still does not solve the problem that cosine similarity in the vector-space gets polluted by frequency-based effects. Different distance normalization schemes have been proposed to downplay the frequency/hubness effect when computing nearest neighbors in the vector space (Dinu et al., 2014; Tomašev et al., 2011), Conclusion In this paper we have identified problems associated with word similarity evaluation of word vector models, and reviewed existing solutions wherever possible. Our study suggests that the use of word similarity tasks for evaluation of word vectors"
W16-2506,P14-5004,1,0.41903,"h (1965) who constructed a list of 65 word pairs with annotations of human similarity judgment. They created this dataset to validate the veracity of the distributional hypothesis (Harris, 1954) according to which the meaning of words is evidenced by the context they occur in. They found a positive correlation between contextual similarity and human-annotated similarity of word pairs. Since then, the lack of a standard evaluation method for word vectors has led to the creation of several ad hoc word similarity datasets. Table 2 provides a list of such benchmarks obtained from wordvectors.org (Faruqui and Dyer, 2014a). Introduction Despite the ubiquity of word vector representations in NLP, there is no consensus in the community on what is the best way for evaluating word vectors. The most popular intrinsic evaluation task is the word similarity evaluation. In word similarity evaluation, a list of pairs of words along with their similarity rating (as judged by human annotators) is provided. The task is to measure how well the notion of word similarity according to humans is captured by the word vector representations. Table 1 shows some word pairs along with their similarity judgments from WS-353 (Finkel"
W16-2506,N13-1090,0,0.0600247,"of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are called distributed word embeddings (Collobert and Weston, 2008), and they are task-specific in nature. These embeddings capture task-specific word similarity, for example, if the task is of POS tagging, two nouns cat and man might be considered similar by the model, even"
W16-2506,E14-1049,1,0.384784,"h (1965) who constructed a list of 65 word pairs with annotations of human similarity judgment. They created this dataset to validate the veracity of the distributional hypothesis (Harris, 1954) according to which the meaning of words is evidenced by the context they occur in. They found a positive correlation between contextual similarity and human-annotated similarity of word pairs. Since then, the lack of a standard evaluation method for word vectors has led to the creation of several ad hoc word similarity datasets. Table 2 provides a list of such benchmarks obtained from wordvectors.org (Faruqui and Dyer, 2014a). Introduction Despite the ubiquity of word vector representations in NLP, there is no consensus in the community on what is the best way for evaluating word vectors. The most popular intrinsic evaluation task is the word similarity evaluation. In word similarity evaluation, a list of pairs of words along with their similarity rating (as judged by human annotators) is provided. The task is to measure how well the notion of word similarity according to humans is captured by the word vector representations. Table 1 shows some word pairs along with their similarity judgments from WS-353 (Finkel"
W16-2506,D14-1113,0,0.023745,"texts they occur in. For example, the words bank and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of freque"
W16-2506,D14-1162,0,0.104113,"indings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are called distributed word embeddings (Collobert and Weston, 2008), and they are task-specific in nature. These embeddings capture task-specific word similarity, for example, if the task is of POS tagging, two nouns cat and man might be considered similar by the model, even though they are not semant"
W16-2506,J15-4004,0,0.101034,"Missing"
W16-2506,N15-1058,1,0.07718,"Missing"
W16-2506,N10-1013,0,0.0397994,"mpute similarity between two words given the contexts they occur in. For example, the words bank and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by se"
W16-2506,P15-1173,0,0.0267298,"k and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of frequent words should be evenly distributed through t"
W16-2506,D15-1036,0,0.739851,"n with extrinsic evaluation Word similarity evaluation measures how well the notion of word similarity according to humans is captured in the vector-space word representations. Word vectors that can capture word similarity might be expected to perform well on tasks that require a notion of explicit semantic similarity between words like paraphrasing, entailment. However, it has been shown that no strong correlation is found between the performance of word vectors on word similarity and extrinsic evaluation NLP tasks like text classification, parsing, sentiment analysis (Tsvetkov et al., 2015; Schnabel et al., 2015).3 An absence of strong correlation between the word similarity evaluation and downstream tasks calls for alternative approaches (rAB &lt; r)∧(|ˆ rBT −ˆ rAT |&lt; σpr0 ) =⇒ pval > p0 (2) Here pval is the probability of the test statistic under the null hypothesis that rAT = rBT found using the Steiger’s test. The above conditional ensures that if the empirical difference between the rank correlations of the scores of the competing methods to the gold ratings is less than σpr0 then either the true correlation between the competing methods is greater than r, or the null hypothesis of no difference has"
W16-2506,W14-1601,0,0.0150773,"electing the hyperparameters that perform well across a range of tasks, these methods ensure that the obtained vectors are generalizable. Stratos et al. (2015) divided each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger’s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation"
W16-2506,P15-1124,0,0.00829903,"on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks. This evaluation can be performed in two different ways: (1) choose the hyperparameters with best average performance across all tasks, (2) choose the hyperparameters that beat the baseline vectors on most tasks.2 By selecting the hyperparameters that perform well across a range of tasks, these methods ensure that the obtained vectors are generalizable. Stratos et al. (2015) divided each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferen"
W16-2506,D15-1243,1,0.825403,"llowing: Low correlation with extrinsic evaluation Word similarity evaluation measures how well the notion of word similarity according to humans is captured in the vector-space word representations. Word vectors that can capture word similarity might be expected to perform well on tasks that require a notion of explicit semantic similarity between words like paraphrasing, entailment. However, it has been shown that no strong correlation is found between the performance of word vectors on word similarity and extrinsic evaluation NLP tasks like text classification, parsing, sentiment analysis (Tsvetkov et al., 2015; Schnabel et al., 2015).3 An absence of strong correlation between the word similarity evaluation and downstream tasks calls for alternative approaches (rAB &lt; r)∧(|ˆ rBT −ˆ rAT |&lt; σpr0 ) =⇒ pval > p0 (2) Here pval is the probability of the test statistic under the null hypothesis that rAT = rBT found using the Steiger’s test. The above conditional ensures that if the empirical difference between the rank correlations of the scores of the competing methods to the gold ratings is less than σpr0 then either the true correlation between the competing methods is greater than r, or the null hypothe"
W16-2506,P10-1040,0,0.060644,"iple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of frequent words should be evenly distributed through the space, while large number of rare words should cluster around related, but more frequent words. However, it has been shown that vector-spaces contain hubs, which are vectors that are close to a large number of other vectors in the space (Radovanovi´c et al., 2010). This problem manifests in word vector-spaces in the form of words that have high cosine similarity with a large number of other words (Dinu et al., 2014). Schnabel et"
W16-2506,P12-1092,0,\N,Missing
W16-2506,N15-1070,1,\N,Missing
W16-2520,P14-2131,0,0.0265954,"ks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation that correlates with extrinsic scores is"
W16-2520,P12-1015,0,0.0445806,"ector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaphor). • Finally, we compute the Pearson’s correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extri"
W16-2520,P15-1033,1,0.816876,"in both matrices; this setup is denoted as PTB + SST. WS-353 MEN SimLex PTB PTB + SST QVEC QVEC - CCA QVEC QVEC - CCA POS -0.38 -0.32 0.20 0.23 0.23 0.28 0.23 Parse 0.68 0.51 -0.21 0.39 0.50 0.37 0.63 Table 4: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream syntactic tasks. We extend the setup of Tsvetkov et al. (2015) with two syntactic benchmarks, and evaluate QVEC - CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016), and the second is dependency parsing (Parse), using the stack-LSTM model of Dyer et al. (2015). Although some word similarity tasks obtain high correlations with syntactic applications, these results are inconsistent, and vary from a high negative to a high positive correlation. Conversely, QVEC and QVEC - CCA consistently obtain moderate-to-high positive correlations with the downstream tasks. Comparing performance of QVEC - CCA in PTB and PTB + SST setups sheds light on the importance of linguistic signals captured by the linguistic matrices. Appending supersense-annotated columns to the linguistic matrix which already contains POS-annotated columns does not affect correlations of QV"
W16-2520,P14-5004,1,0.866932,"Missing"
W16-2520,N15-1184,1,0.835181,"vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaph"
W16-2520,N16-1030,1,0.460377,"h are a concatenation of the semantic and syntactic matrices described in §3 for words that occur in both matrices; this setup is denoted as PTB + SST. WS-353 MEN SimLex PTB PTB + SST QVEC QVEC - CCA QVEC QVEC - CCA POS -0.38 -0.32 0.20 0.23 0.23 0.28 0.23 Parse 0.68 0.51 -0.21 0.39 0.50 0.37 0.63 Table 4: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream syntactic tasks. We extend the setup of Tsvetkov et al. (2015) with two syntactic benchmarks, and evaluate QVEC - CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016), and the second is dependency parsing (Parse), using the stack-LSTM model of Dyer et al. (2015). Although some word similarity tasks obtain high correlations with syntactic applications, these results are inconsistent, and vary from a high negative to a high positive correlation. Conversely, QVEC and QVEC - CCA consistently obtain moderate-to-high positive correlations with the downstream tasks. Comparing performance of QVEC - CCA in PTB and PTB + SST setups sheds light on the importance of linguistic signals captured by the linguistic matrices. Appending supersense-annotated columns to the l"
W16-2520,D13-1196,0,0.0549103,"range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation"
W16-2520,D15-1161,1,0.815675,"43 0.02 ··· ··· ··· ··· PTB . JJ In table 3, we show correlations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our sem"
W16-2520,N15-1142,1,0.831815,"43 0.02 ··· ··· ··· ··· PTB . JJ In table 3, we show correlations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our sem"
W16-2520,J93-2004,0,0.0550028,"then describe QVEC - CCA. Both QVEC and QVEC - CCA rely on a matrix of linguistic properties constructed from a manually crafted linguistic resource. Linguistic resources are invaluable as they capture generalizations made by domain experts. However, resource construction is expensive, therefore it is not always possible to find an existing resource that captures exactly the set of optimal lexical properties for a downstream task. Resources that capture more coarse-grained, general properties can be used instead, for example, WordNet for semantic evaluation (Fellbaum, 1998), or Penn Treebank (Marcus et al., 1993, PTB) for syntactic evaluation. Since these properties are not an exact match to the task, the intrinsic evaluation tests for a necessary (but possibly not sufficient) set of generalizations. QVEC . The main idea behind QVEC is to quantify the linguistic content of word embeddings by maximizing the correlation with a manuallyannotated linguistic resource. Let the number of common words in the vocabulary of the word embeddings and the linguistic resource be N . To quantify the semantic content of embeddings, a semantic/syntactic linguistic matrix S ∈ RP ×N is constructed from a semantic/syntac"
W16-2520,H93-1061,0,0.0541212,"the dimensionality of word embeddings. Then, S and X are aligned to maximize the cumulative correlation between the aligned dimensions of the two matrices. Specifically, let A ∈ {0, 1}D×P be a matrix of alignments such that aij = 1 iff xi is aligned to sj , otherwise aij = 0. If r(xi , sj ) is the Pearson’s correlation between vectors xi and sj , then QVEC is defined as: QVEC = A: max P j X X S X aij ≤1 Linguistic Dimension Word Vectors Semantic vectors. To evaluate the semantic content of word vectors, Tsvetkov et al. (2015) exploit supersense annotations in a WordNetannotated corpus—SemCor (Miller et al., 1993). The resulting supersense-dimension matrix has 4,199 rows (supersense-annotated nouns and verbs that occur in SemCor at least 5 times2 ), and 41 columns: 26 for nouns and 15 for verbs. Example vectors are shown in table 1. r(xi , sj ) × aij WORD fish duck chicken i=1 j=1 NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0.67 ··· ··· ··· VB . MOTION 0.00 0.69 0.00 P The constraint j aij ≤ 1, warrants that one distributional dimension is aligned to at most one linguistic dimension. Table 1: Linguistic dimension word vector matrix with semantic vectors, constructed using SemCor. QVEC - CCA . To mea"
W16-2520,D14-1162,0,0.0959606,"relations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the"
W16-2520,D13-1170,0,0.035608,"tic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation that correlates with"
W16-2520,P14-1024,1,0.80936,"ectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaphor). • Finally, we compute the Pearson’s correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. WS-353 MEN SimLex QVEC QVEC - CCA 20NG 0.55 0.76 0.56 0.74 0.77 Metaphor 0.25 0.49 0.44 0.75 0.73 Senti 0.46 0.55 0.51 0.88 0.93 Table 3: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream text classification tasks. In table 4, we evaluate QVEC and QVEC - CCA on syntactic benchmarks. We f"
W16-2520,D15-1243,1,0.582936,"ations Yulia Tsvetkov♠ Manaal Faruqui♠ Chris Dyer♣♠ ♠ Carnegie Mellon University ♣ Google DeepMind {ytsvetko,mfaruqui,cdyer}@cs.cmu.edu Abstract dimensions is an auxiliary mechanism for analyzing how these properties affect the target downstream task. It thus facilitates refinement of word vector models and, consequently, improvement of the target task. Finally, an intrinsic evaluation that approximates a range of related downstream tasks (e.g., semantic text-classification tasks) allows to assess generality (or specificity) of a word vector model, without actually implementing all the tasks. Tsvetkov et al. (2015) proposed an evaluation measure—QVEC—that was shown to correlate well with downstream semantic tasks. Additionally, it helps shed new light on how vector spaces encode meaning thus facilitating the interpretation of word vectors. The crux of the method is to correlate distributional word vectors with linguistic word vectors constructed from rich linguistic resources, annotated by domain experts. Q VEC can easily be adjusted to specific downstream tasks (e.g., part-of-speech tagging) by selecting task-specific linguistic resources (e.g., part-of-speech annotations). However, QVEC suffers from t"
W16-2520,P10-1040,0,0.0694995,"ffective proxy for a range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally effici"
W16-2520,J90-1003,0,\N,Missing
W16-2520,D14-1012,0,\N,Missing
W16-5904,N13-1006,0,0.0257226,"8) is able to incorporate soft constraints defined over a whole distribution of labels by adding the 40 expectation based constraints to the objective(MLE) of the problem. Although this is an appealing method, it can be very expensive to run because the gradient calculations depend on the cross product of model feature space and model constraint space. In fact, Posterior Regularization can be seen as a variational approximation to the objective of GE criterion (Ganchev et al., 2010). PR and GE have been shown to be useful in incorporating soft constraints for various tasks like bilingual NER (Che et al., 2013), cross lingual projection of coreference(Martins, 2015) etc. There has been plenty of work to bias predictions/ learning of structured prediction models in presence of hard constraints, which incorporate discrete penalty associated with label combinations relevant to the constraint features. A key difference of these models from Posterior Regularization is that instead of working with expected counts of output labels, they work with hard count assignments. The constraint driven learning approach of (2007) adds a penalty term to the conditional log probability of the output that can be seen as"
W16-5904,P15-1138,0,0.0265476,"hole distribution of labels by adding the 40 expectation based constraints to the objective(MLE) of the problem. Although this is an appealing method, it can be very expensive to run because the gradient calculations depend on the cross product of model feature space and model constraint space. In fact, Posterior Regularization can be seen as a variational approximation to the objective of GE criterion (Ganchev et al., 2010). PR and GE have been shown to be useful in incorporating soft constraints for various tasks like bilingual NER (Che et al., 2013), cross lingual projection of coreference(Martins, 2015) etc. There has been plenty of work to bias predictions/ learning of structured prediction models in presence of hard constraints, which incorporate discrete penalty associated with label combinations relevant to the constraint features. A key difference of these models from Posterior Regularization is that instead of working with expected counts of output labels, they work with hard count assignments. The constraint driven learning approach of (2007) adds a penalty term to the conditional log probability of the output that can be seen as adding cost deterministically for violating the constra"
W16-5904,petrov-etal-2012-universal,0,0.034222,"Missing"
W16-5904,W96-0213,0,0.714132,"Missing"
W16-5904,D10-1001,0,0.0949825,"Missing"
W16-5904,N03-1028,0,0.10456,"English ConLL dataset easily incorporated as long as their computation de(Tjong Kim Sang and De Meulder, 2003) for both composes along the cliques of our joint models. For NER and PoS tagging models and artificially impov- numerical stability, the constraints in table 2 were erished the data by randomly sampling disjoint task scaled to be in the same range by scaling φ. We performed experiments on jointly modelling two tasks: 1) Named Entity Recognition(NER) and 2) Part of Speech (PoS) tagging. For NER, we follow the standard convention of ‘B-I-O tagging’ (Tjong Kim Sang and De Meulder, 2003; Sha and Pereira, 2003) where ‘B’ and ‘I’ help identify segments of named entities and ‘O’ identifies the words that are not named entities. For PoS, we used the ‘Universal’ PoS tagset, which is largely invariant across several languages (Petrov et al., 2011). The tagset for the two tasks was: 41 Table 3: Performance on NER and Part of Speech tagging. ‘P’, ‘R’, ‘F1’ stand for Precision, Recall and F1 score for Named Entity Recognition task, ‘Acc’ refers to part of speech tagging accuracy. 1x, 2x, and 3x refer to the data sets for the two tasks as described in table 1. ‘Single-task’ refers to independent training of"
W16-5904,W03-0419,0,0.340796,"Missing"
W16-5904,P07-1036,0,\N,Missing
zhang-etal-2014-dual,itamar-itai-2008-using,0,\N,Missing
zhang-etal-2014-dual,N12-1006,0,\N,Missing
zhang-etal-2014-dual,N12-1079,0,\N,Missing
zhang-etal-2014-dual,J93-1004,0,\N,Missing
zhang-etal-2014-dual,tiedemann-2008-synchronizing,0,\N,Missing
zhang-etal-2014-dual,W12-3152,0,\N,Missing
zhang-etal-2014-dual,N10-1113,0,\N,Missing
zhang-etal-2014-dual,J93-2003,0,\N,Missing
zhang-etal-2014-dual,W06-1008,0,\N,Missing
zhang-etal-2014-dual,J03-3002,0,\N,Missing
zhang-etal-2014-dual,C10-1124,0,\N,Missing
zhang-etal-2014-dual,P02-1040,0,\N,Missing
zhang-etal-2014-dual,I11-1125,0,\N,Missing
zhang-etal-2014-dual,W10-0710,0,\N,Missing
zhang-etal-2014-dual,N10-1063,0,\N,Missing
zhang-etal-2014-dual,P07-2045,1,\N,Missing
zhang-etal-2014-dual,P13-1018,1,\N,Missing
zhang-etal-2014-dual,P11-1061,0,\N,Missing
zhang-etal-2014-dual,P05-1074,0,\N,Missing
zhang-etal-2014-dual,P03-1058,0,\N,Missing
zhang-etal-2014-dual,P08-1113,0,\N,Missing
zhang-etal-2014-dual,W12-3153,0,\N,Missing
zhang-etal-2014-dual,2005.mtsummit-papers.11,0,\N,Missing
zhang-etal-2014-dual,D13-1008,1,\N,Missing
zhang-etal-2014-dual,2010.iwslt-papers.14,1,\N,Missing
zhang-etal-2014-dual,I08-2120,0,\N,Missing
zhang-etal-2014-dual,2011.iwslt-papers.3,1,\N,Missing
zhang-etal-2014-dual,W12-3134,0,\N,Missing
zhang-etal-2014-dual,tiedemann-2012-parallel,0,\N,Missing
zhang-etal-2014-dual,N04-1034,0,\N,Missing
zhang-etal-2014-dual,W11-2123,0,\N,Missing
