2010.amta-papers.25,P05-1033,0,0.84181,"xtracted from bi-text, glue rules are used to perform serial combination of phrases. However, this basic method for combining phrases is not sufficient for phrase reordering. In this paper, we extend the HPB model with maximum entropy based bracketing transduction grammar (BTG), which provides content-dependent combination of neighboring phrases in two ways: serial or inverse. Experimental results show that the extended HPB system achieves absolute improvements of 0.9∼1.8 BLEU points over the baseline for large-scale translation tasks. 1 Introduction The hierarchical phrase based (HPB) model (Chiang, 2005), built on weighted synchronous context free grammar (SCFG), provides a powerful mechanism to capture both short and long distance phrase reorderings for statistical machine translation (SMT). It utilizes two types of rules: • Translation rules are learned from wordaligned bilingual corpus. A translation rule can be either a phrasal rule consisting of words or a hierarchical rule consisting of both words and variables. During decoding, phrasal rules perform lexical translation, while hierarchical rules perform both lexical translation and phrase reordering. • Two glue rules are defined to seri"
2010.amta-papers.25,J07-2003,0,0.105788,"across them. The Hierarchical Phrase-based Model The hierarchical phrase-based model is based on a weighted SCFG, which consists of the following rewrite rules: X → hα, γ, ∼i (1) where X is a non-terminal symbol, α/γ is a string consisting of source/target terminals and nonterminals. ∼ describes a one-to-one correspondence between non-terminals in α and γ. The greatest advantage of the HPB model is that it utilizes hierarchical phrases, i.e. phrases that contain X → hzai X1 de X2 , X2 in the X1 i (2) During decoding, this rule swaps the source phrases covered by X1 and X2 on the target side. Chiang (2007) developed a bottom-up decoder using CKY algorithm. For a source sentence, the decoder produces a translation and a parser tree as a byproduct (Figure 1 (a)). Given a source sentence F1J , the goal of decoding is to search the best derivation for S[1,J] 1 . The algorithm produces partial derivations for each cell that spans from j1 to j2 by using translation rules from bottom to top. This guarantees that when the span [j1 , j2 ] is being expanded, all its sub-spans have been already expanded. Finally, we search the best derivation of the span [1, J], from which we can obtain the translation. H"
2010.amta-papers.25,D08-1011,0,0.0115203,"t a content-dependent model under a maximum entropy (ME) framework. The approach yielded significant improvements on phrase reordering over conventional phrase-based SMT systems. In this paper, we extend the HPB model by using BTG rules instead of the monotone glue rules. Analogous to Xiong et al. (2006), we built an ME based classifier to predict whether the neighboring phrases combined serially or inversely. The extended HPB approach can be viewed as a combination of HPB translation and ME based BTG translation. Compared with previous methods of system combination (e.g. (Rosti et al., 2007; He et al., 2008)), the basic difference is that while conventional methods combined translation results after the decoding of different models, our method combines translation models during decoding. Liu et al. (2009) presented a framework for a joint decoding method to combine multiple translation models. They combined the HPB model and the tree-to-string model (Liu et al., 2006). Since these two models are quite different, e.g. the HPB model is formally syntax-based while the tree-tostring model is linguistically syntax-based, it is difficult to combine them to form a joint decoder. Liu et al. (2009) utiliz"
2010.amta-papers.25,N03-1017,0,0.0550657,"Missing"
2010.amta-papers.25,P06-1077,0,0.0160819,"hboring phrases combined serially or inversely. The extended HPB approach can be viewed as a combination of HPB translation and ME based BTG translation. Compared with previous methods of system combination (e.g. (Rosti et al., 2007; He et al., 2008)), the basic difference is that while conventional methods combined translation results after the decoding of different models, our method combines translation models during decoding. Liu et al. (2009) presented a framework for a joint decoding method to combine multiple translation models. They combined the HPB model and the tree-to-string model (Liu et al., 2006). Since these two models are quite different, e.g. the HPB model is formally syntax-based while the tree-tostring model is linguistically syntax-based, it is difficult to combine them to form a joint decoder. Liu et al. (2009) utilized a hypergraph structure to store partial derivations and a modified MERT (Och, 2003) algorithm for training. They reported an absolute improvement of 1.5 BLEU points on a small corpus (6.9M + 8.9M words) for Chinese-to-English translation. It is more straightforward to combine the HPB model and the ME based BTG model. On the one hand, the translation grammar is s"
2010.amta-papers.25,P09-1065,0,0.0266753,"end the HPB model by using BTG rules instead of the monotone glue rules. Analogous to Xiong et al. (2006), we built an ME based classifier to predict whether the neighboring phrases combined serially or inversely. The extended HPB approach can be viewed as a combination of HPB translation and ME based BTG translation. Compared with previous methods of system combination (e.g. (Rosti et al., 2007; He et al., 2008)), the basic difference is that while conventional methods combined translation results after the decoding of different models, our method combines translation models during decoding. Liu et al. (2009) presented a framework for a joint decoding method to combine multiple translation models. They combined the HPB model and the tree-to-string model (Liu et al., 2006). Since these two models are quite different, e.g. the HPB model is formally syntax-based while the tree-tostring model is linguistically syntax-based, it is difficult to combine them to form a joint decoder. Liu et al. (2009) utilized a hypergraph structure to store partial derivations and a modified MERT (Och, 2003) algorithm for training. They reported an absolute improvement of 1.5 BLEU points on a small corpus (6.9M + 8.9M wo"
2010.amta-papers.25,P00-1056,0,0.25252,"Missing"
2010.amta-papers.25,P02-1038,0,0.216213,"e inverted glue rule will provide another option to merge phrases inversely, allowing it to produce a more balanced parser tree. We extended the HPB model by replacing its glue rules with BTG rules, as shown in Table 2. With this extension, the parser tree of the source sentence in Figure 1 (a) is more balanced, as shown in (b). Figure 3 shows a partial derivation. We observed that the translations of source phrases fik and fkj were reversed on the target side. 4.2 The Extended Translation Model Both the HPB model and the ME based BTG model were built within the standard log-linear framework (Och and Ney, 2002): P r(e|f ) ∝ X λi hi (α, γ) (9) i where hi (α, γ) is a feature function and λi is the weight of hi . The HPB model has the following features: translation probabilities p(γ|α) and p(α|γ), lexical weights pw (γ|α) and pw (α|γ), word penalty, phrase penalty, glue rule penalty, and a target ngram language model. When extending the glue rules with ME based BTG, we modify the features of the log-linear model as follows: • An ME based reordering feature was added to predict the order of neighboring phrases: hmebtg (o|X1 , X2 ) = X P (o|X1 , X2 ) (10) To train an ME classifier, we used two kinds of"
2010.amta-papers.25,P03-1021,0,0.0103199,"ion results after the decoding of different models, our method combines translation models during decoding. Liu et al. (2009) presented a framework for a joint decoding method to combine multiple translation models. They combined the HPB model and the tree-to-string model (Liu et al., 2006). Since these two models are quite different, e.g. the HPB model is formally syntax-based while the tree-tostring model is linguistically syntax-based, it is difficult to combine them to form a joint decoder. Liu et al. (2009) utilized a hypergraph structure to store partial derivations and a modified MERT (Och, 2003) algorithm for training. They reported an absolute improvement of 1.5 BLEU points on a small corpus (6.9M + 8.9M words) for Chinese-to-English translation. It is more straightforward to combine the HPB model and the ME based BTG model. On the one hand, the translation grammar is similar as both of the two models are based on CFG. Thus, to combine them together we need only to add into the HPB model an inverted rule, which combines phrases in an inverse order. On the other hand, both use a CKY algorithm for decoding, therefore, making it unnecessary to modify the decoding algorithm. Furthermore"
2010.amta-papers.25,P02-1040,0,0.0777865,"Missing"
2010.amta-papers.25,P07-1040,0,0.0152499,"ion problem and built a content-dependent model under a maximum entropy (ME) framework. The approach yielded significant improvements on phrase reordering over conventional phrase-based SMT systems. In this paper, we extend the HPB model by using BTG rules instead of the monotone glue rules. Analogous to Xiong et al. (2006), we built an ME based classifier to predict whether the neighboring phrases combined serially or inversely. The extended HPB approach can be viewed as a combination of HPB translation and ME based BTG translation. Compared with previous methods of system combination (e.g. (Rosti et al., 2007; He et al., 2008)), the basic difference is that while conventional methods combined translation results after the decoding of different models, our method combines translation models during decoding. Liu et al. (2009) presented a framework for a joint decoding method to combine multiple translation models. They combined the HPB model and the tree-to-string model (Liu et al., 2006). Since these two models are quite different, e.g. the HPB model is formally syntax-based while the tree-tostring model is linguistically syntax-based, it is difficult to combine them to form a joint decoder. Liu et"
2010.amta-papers.25,P96-1021,0,0.395784,"hierarchical rules perform both lexical translation and phrase reordering. • Two glue rules are defined to serially combine neighboring phrases. Glue rules provide a mechanism for finishing translation in cases where no translation rules are available for a source span. However, one disadvantage of the HPB model is that the glue rules only provide monotone combinations of phrases. In some cases, however, the order of phrases maybe inverted. Therefore, we need an additional glue rule to perform inverse combinations of phrases. It is appropriate to use the bracketing transduction grammar (BTG) (Wu, 1996), which provides two options for combining phrases: serial or inverse. Xiong et al. (2006) and Zens and Ney (2006) presented a discriminative phrase reordering model based on BTG. They regarded phrase reordering as a two-class classification problem and built a content-dependent model under a maximum entropy (ME) framework. The approach yielded significant improvements on phrase reordering over conventional phrase-based SMT systems. In this paper, we extend the HPB model by using BTG rules instead of the monotone glue rules. Analogous to Xiong et al. (2006), we built an ME based classifier to"
2010.amta-papers.25,P06-1066,0,0.317393,"glue rules are defined to serially combine neighboring phrases. Glue rules provide a mechanism for finishing translation in cases where no translation rules are available for a source span. However, one disadvantage of the HPB model is that the glue rules only provide monotone combinations of phrases. In some cases, however, the order of phrases maybe inverted. Therefore, we need an additional glue rule to perform inverse combinations of phrases. It is appropriate to use the bracketing transduction grammar (BTG) (Wu, 1996), which provides two options for combining phrases: serial or inverse. Xiong et al. (2006) and Zens and Ney (2006) presented a discriminative phrase reordering model based on BTG. They regarded phrase reordering as a two-class classification problem and built a content-dependent model under a maximum entropy (ME) framework. The approach yielded significant improvements on phrase reordering over conventional phrase-based SMT systems. In this paper, we extend the HPB model by using BTG rules instead of the monotone glue rules. Analogous to Xiong et al. (2006), we built an ME based classifier to predict whether the neighboring phrases combined serially or inversely. The extended HPB a"
2010.amta-papers.25,W06-3108,0,0.0157036,"to serially combine neighboring phrases. Glue rules provide a mechanism for finishing translation in cases where no translation rules are available for a source span. However, one disadvantage of the HPB model is that the glue rules only provide monotone combinations of phrases. In some cases, however, the order of phrases maybe inverted. Therefore, we need an additional glue rule to perform inverse combinations of phrases. It is appropriate to use the bracketing transduction grammar (BTG) (Wu, 1996), which provides two options for combining phrases: serial or inverse. Xiong et al. (2006) and Zens and Ney (2006) presented a discriminative phrase reordering model based on BTG. They regarded phrase reordering as a two-class classification problem and built a content-dependent model under a maximum entropy (ME) framework. The approach yielded significant improvements on phrase reordering over conventional phrase-based SMT systems. In this paper, we extend the HPB model by using BTG rules instead of the monotone glue rules. Analogous to Xiong et al. (2006), we built an ME based classifier to predict whether the neighboring phrases combined serially or inversely. The extended HPB approach can be viewed as"
2010.amta-papers.25,D08-1060,0,0.0254581,"Missing"
2011.mtsummit-papers.44,D09-1105,0,0.0323954,"Missing"
2011.mtsummit-papers.44,P05-1033,0,0.691843,"d the baseline system in BLEU score signiﬁcantly. Moreover, the translation results further proved the effectiveness of our approach. 1 (a) X1 ≘≤ ⓦ⏢ ≘≤ ⓦ⏢ ǃ ⻣䞨≒䫐 ≤ⓦ⏢ X2 Alkali metal, such as aqueous potassium hydroxide , aqueous sodium bicarbonate X1 ≘≤ ⓦ⏢ X2 , ammonia solution P(γ|α),P(α|γ) Pw(γ|α),Pw(α|γ) 0.444444 0.0461205 0.0754714 0.11668 0.0740741 0.0461205 0.5 0.11668 X1 ammonia solution X2 (b) X2 X1 ammonia solution Figure 1: Incorrect reordering. Introduction Reordering is a big challenge for statistical machine translation (SMT). The hierarchical phrasebased translation model (HPB) (Chiang, 2005), which adopts a synchronous context-free grammar (SCFG), is considered to be prominent in capturing global reorderings. However, the HPB model is weak in controlling the reordering process. Thus arbitrary reorderings frequently come up during the decoding process worsening the translation quality. Figure 1(a) shows an example of an incorrect reordering. The non-terminal “X2 ” is reordered 389 with “X1 ” and “d d d d”, but the punctuation “d” indicates that the phrase should be translated monotonously. As shown in Figure 1(b), although the two rules have different feature weights, the decoder"
2011.mtsummit-papers.44,J07-2003,0,0.0368368,"in order to ﬁnd out the most effective model settings, we tested different values on the threshold T hreshold W ord Scope. The results are shown in table 3. From the results, best performance is achieved by setting T hreshold W ord Scope = 3, which was adopted as the ﬁnal setting in the rest of the experiments. It is reasonable that we could not get enough features in a smaller scope and may obtain too much noise in a larger scope. To evaluate the effectiveness of our model, we conducted experiments on four systems: • Baseline: an in-house hierarchical phrasebased machine translation system (Chiang, 2007). • LBR-no-trans: integrated with the lexicalbased reordering model, but did not adopt translations as feature. • LBR-no-de: integrated with the lexical-based reordering model, but was trained with the features merely satisfying the common constraint. • LBR-all: fully integrated with our proposed model into the baseline system. 4.3 Results The experimental results are shown in table 4. We can observe that systems integrated with the lexicalbased reordering model all outperformed the baseline system. The improvements of “LBR-no-de” and “LBR-all” are statistically signiﬁcant at p < 0.01 394 acco"
2011.mtsummit-papers.44,C04-1073,0,0.0390088,"rder. This makes rules in Figure 1(b) ambiguous for the decoder. Generally the HPB model has 8 features (Chiang, 2005), including language model, constituent feature, word penalty, phrase penalty, bi-direction translation weights P (γ|α) and P (α|γ), bi-direction lexical weights Pw (γ|α) and Pw (α|γ), but none of them is responsible for the rationality of reorderings. Although language model evaluates the ﬂuency of target string, it considers the target words only. Various methods have been proposed in order to solve this problem for HPB model. Most of them focused on the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Eisner, 2009; Du and Way, 2010). Those methods reordered source language to target language before training and testing by various proposed syntactic rules. Those ofﬂine rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Shen et al. (2008,2009) proposed a string-todependency language model to exploit long-distance word relations during decoding. He et al. (2010) classiﬁed SCFG rules into different patterns and built a maximum entropy"
2011.mtsummit-papers.44,P02-1038,0,0.01634,"2 where hi (γ, α) is a feature function and λi is the weight of hi . Based on the deﬁciencies of HPB discussed in the early section, we intend to complement the log-linear framework with a feature as a soft constraint to measure the correctness of word order for each hierarchical rule. 3 (4) (2) where X is a non-terminal, γ and α denote source and target strings, which contain both terminals and non-terminals. ∼ is the one-to-one correspondence between terminals and non-terminals in γ and α. Chiang (2005) integrated all the features mentioned in the ﬁrst section into the log-linear framework (Och and Ney, 2002). P (e|f ) ∝ Pre (orderi,j |φi,j )) i,j:1≤i<j≤n • The former research worked on the Japaneseto-English task, while ours works on the Chinese-to-English task. 2.2  Lexical-based Reordering Model Overview of the Model A score Sre is calculated using the lexical-based reordering model for each hierarchical rule r as fol391 Feature Extraction and Model Training The features are extracted from the training set, where the hierarchical rules also come from. We use GIZA++ (Och and Ney, 2003) to obtain word alignments. Given a word aligned sentence pair f, e, where f = {w0 , ..., wn }, we select tra"
2011.mtsummit-papers.44,P03-1021,0,0.0371027,"Missing"
2011.mtsummit-papers.44,2010.eamt-1.32,0,0.0454462,"has 8 features (Chiang, 2005), including language model, constituent feature, word penalty, phrase penalty, bi-direction translation weights P (γ|α) and P (α|γ), bi-direction lexical weights Pw (γ|α) and Pw (α|γ), but none of them is responsible for the rationality of reorderings. Although language model evaluates the ﬂuency of target string, it considers the target words only. Various methods have been proposed in order to solve this problem for HPB model. Most of them focused on the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Eisner, 2009; Du and Way, 2010). Those methods reordered source language to target language before training and testing by various proposed syntactic rules. Those ofﬂine rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Shen et al. (2008,2009) proposed a string-todependency language model to exploit long-distance word relations during decoding. He et al. (2010) classiﬁed SCFG rules into different patterns and built a maximum entropy classiﬁer to select proper translation rules. Hayashi et al. (2010) integrated the me"
2011.mtsummit-papers.44,C10-1050,0,0.0571222,"ble and Eisner, 2009; Du and Way, 2010). Those methods reordered source language to target language before training and testing by various proposed syntactic rules. Those ofﬂine rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Shen et al. (2008,2009) proposed a string-todependency language model to exploit long-distance word relations during decoding. He et al. (2010) classiﬁed SCFG rules into different patterns and built a maximum entropy classiﬁer to select proper translation rules. Hayashi et al. (2010) integrated the method of (Tromble and Eisner, 2009) into the decoder to make this on-line rewriting method as a source language model. Those online methods are involved in the decoding phase as soft constraints to evaluate the word order of translation rules. This paper proposes an on-line method which is based on the lexical information as a new feature for the HPB model. This feature is used to evaluate the correctness of word order in the decoding process. The remainder of this paper is organized as follows. Section 2 introduces the previous related work. In Section 3, we describe the impl"
2011.mtsummit-papers.44,P02-1040,0,0.0810679,"Missing"
2011.mtsummit-papers.44,P08-1066,0,0.0549551,"the ﬂuency of target string, it considers the target words only. Various methods have been proposed in order to solve this problem for HPB model. Most of them focused on the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Eisner, 2009; Du and Way, 2010). Those methods reordered source language to target language before training and testing by various proposed syntactic rules. Those ofﬂine rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Shen et al. (2008,2009) proposed a string-todependency language model to exploit long-distance word relations during decoding. He et al. (2010) classiﬁed SCFG rules into different patterns and built a maximum entropy classiﬁer to select proper translation rules. Hayashi et al. (2010) integrated the method of (Tromble and Eisner, 2009) into the decoder to make this on-line rewriting method as a source language model. Those online methods are involved in the decoding phase as soft constraints to evaluate the word order of translation rules. This paper proposes an on-line method which is based on the lexical info"
2011.mtsummit-papers.44,D09-1008,0,0.0168622,"n 2 introduces the previous related work. In Section 3, we describe the implementation of the lexical-based reordering model and the integration into the decoder. Experiment on the Chinese-toEnglish task is shown in Section 4, followed by a 390 discussion in Section 5. The conclusion and future work are presented in Section 6. 2 Previous Related Work 2.1 Online Reordering Methods Comparing with the ofﬂine method, online method is able to utilize various information during decoding. Shen et al. (2008) proposed a string-todependency target language model to capture long distance word orders and Shen et al. (2009) extended the work by applying more features such as phrase length distribution and context language model. Shen et al. (2008) also intended to build a dependency language model on the source language, but the result reported a decline with this feature. He et al. (2010) divided hierarchical rules into several ﬁxed patterns. For example, the rule <“X1 dd d d X2 ”, “X2 X1 dd dd”> belongs to the pattern <“X1 FX2 ”,“X2 X1 E”>. A maximum entropy classiﬁer is applied to select target rules with proper patterns. This method is insensitive to the terminal order within the rules. Our work is somewhat"
2011.mtsummit-papers.44,P05-1066,0,0.0340153,"in Figure 1(b) ambiguous for the decoder. Generally the HPB model has 8 features (Chiang, 2005), including language model, constituent feature, word penalty, phrase penalty, bi-direction translation weights P (γ|α) and P (α|γ), bi-direction lexical weights Pw (γ|α) and Pw (α|γ), but none of them is responsible for the rationality of reorderings. Although language model evaluates the ﬂuency of target string, it considers the target words only. Various methods have been proposed in order to solve this problem for HPB model. Most of them focused on the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Eisner, 2009; Du and Way, 2010). Those methods reordered source language to target language before training and testing by various proposed syntactic rules. Those ofﬂine rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Shen et al. (2008,2009) proposed a string-todependency language model to exploit long-distance word relations during decoding. He et al. (2010) classiﬁed SCFG rules into different patterns and built a maximum entropy classiﬁer to select p"
2011.mtsummit-papers.44,N03-1017,0,0.0332211,"Missing"
2011.mtsummit-papers.44,W04-3250,0,0.0172716,"reordering model, but did not adopt translations as feature. • LBR-no-de: integrated with the lexical-based reordering model, but was trained with the features merely satisfying the common constraint. • LBR-all: fully integrated with our proposed model into the baseline system. 4.3 Results The experimental results are shown in table 4. We can observe that systems integrated with the lexicalbased reordering model all outperformed the baseline system. The improvements of “LBR-no-de” and “LBR-all” are statistically signiﬁcant at p < 0.01 394 according to the signiﬁcant test method described in (Koehn, 2004). As applying more features to the model, the BLEU score rises accordingly. This proves that the linguistic constraint and the translation feature are effective. We compared the translation results between the baseline system and “LBR-all” to check out the actual inﬂuence of our model. Figure 4 shows some examples of the effectiveness of our method. When analyzing the translation results, we ﬁnd that many reordering mistakes are caused by the ambiguous hierarchical rules which are similar with those in Figure 1. Table 5 lists the ambiguous rules which occur in the examples of Figure 4. 5 Discu"
2011.mtsummit-papers.44,D10-1054,1,0.881739,"blem for HPB model. Most of them focused on the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Eisner, 2009; Du and Way, 2010). Those methods reordered source language to target language before training and testing by various proposed syntactic rules. Those ofﬂine rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Shen et al. (2008,2009) proposed a string-todependency language model to exploit long-distance word relations during decoding. He et al. (2010) classiﬁed SCFG rules into different patterns and built a maximum entropy classiﬁer to select proper translation rules. Hayashi et al. (2010) integrated the method of (Tromble and Eisner, 2009) into the decoder to make this on-line rewriting method as a source language model. Those online methods are involved in the decoding phase as soft constraints to evaluate the word order of translation rules. This paper proposes an on-line method which is based on the lexical information as a new feature for the HPB model. This feature is used to evaluate the correctness of word order in the decoding pro"
2011.mtsummit-papers.44,J03-1002,0,\N,Missing
2011.mtsummit-papers.44,P10-2002,0,\N,Missing
2011.mtsummit-wpt.4,P03-1021,0,0.40321,"tures in the traditional HPB model: phrase translation , inverse phrase translation probability probability , lexical translation probability , inverse lexical translation probability , word penalty, rule penalty and a target ngram language model. The log-linear model (Och and Ney, 2003) is used to combine different features: where is a feature function, is the weight of . In this paper, we add two new features for automatically acquired rules and manually acquired rules into the log-linear model to distinguish different rules. The feature weights are optimized by minimum error rate training (Och, 2003). 2.2 2 2.1 Manually acquired Rules Backgrounds Manually acquired rules are accumulated by human being. Compared with automatically acquired rules, manually acquired rules are of better quality and can capture long sentence structures. Also, there can be constraints for nonterminals in manually acquired rules so that they could match more accurately. Examples of manually acquired rules in our Chinese-to-English patent translation system are shown in Table 1. Hierarchical Phrase-based SMT Hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) is the state-of-the-art SMT model. By utilizin"
2011.mtsummit-wpt.4,P03-1057,0,0.00906221,"omatic Evaluation of MT Quality We evaluate the MT quality in our experiments using the BLEU automatic evaluation metric (Papineni et al., 2002). BLEU measures the similarity between machine translation results and humanmade translation results (called references) by Ngram precision scores and allows multiple reference translations to model the variety of possible translations. BLEU aims to replace subjective evaluation and speed up the development cycle of MT systems. It is now used not only to aid developers but also for automatically tuning of MT systems (e.g., (Och, 2003; Su et al., 1992; Imamura et al., 2003)). In this paper, we use case-insensitive BLEU-4 and there’s only one reference for each input sentence. where denotes the nonterminal, indicates the length constraint of the nonterminal (0 means no length constraint), indicates the nonterminal must/must-not contain these words according to the symbol . indicates whether to match the first valid position or the last one in case there’re several valid positions. The full definition of nonterminals in English . The number of nonterminals in part is simply source side and target side should be the same. With these constraints, a manually acquired"
2011.mtsummit-wpt.4,N03-1017,0,0.00409721,"nslation system (Lü et al., 2007). There’re totally 9283 manually acquired rules. Each of them has at least one matched sentence in the training set. The goal of our experiments was to pick out high quality manually acquired rules which could benefit translation quality. 4.2 Table 3: Examples of short manually acquired rules which caused problems Description of Experiments Baseline Our baseline system was a traditional hierarchical phrase-based system. We obtained word alignments of training data by running GIZA++ (Och and Ney, 2003) and then applied the refinement rule “grow-diag-and-final” (Koehn et al., 2003). After that, we extracted automatically acquired rules according to Chiang 2007. We tuned the feature weights with minimum error rate training (Och, 2003). The traditional hierarchical phrase-based system achieved a BLEU score of 30.55 on the test set. Applying all manually acquired rules Our refined decoder is able to integrate manually acquired rules into the traditional HPB machine translation system. We combined manually acquired rules with automatically acquired rules by assigning manually acquired rules probabilities so The main problem of such rules was no limits on nonterminal length,"
2011.mtsummit-wpt.4,N09-2055,0,0.0379091,"Missing"
2021.findings-emnlp.248,P17-2096,0,0.0812349,"ious state-of-the-art studies. • The novel method extracts the information from the lexicon via the GCN and is not overreliant on the quality of the lexicon. Experimental results in the cross-domain scenario prove that the method can enhance the robustness of the basic neural CWS approaches. 2 Related Work 2010) and 2) neural network methods (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Yang et al., 2017). As the studies of deep learning techniques develop in-depth, the neural CWS methods achieve better performance compared with statistical learning methods (Cai et al., 2017; Zhou et al., 2017; Ma et al., 2018; Yang et al., 2019a; Wang et al., 2019). And neural network architectures gradually replace statistical machine learning methods as the mainstream approaches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this problem, many kinds of research utilize external resources (e.g., pre-trained embeddings, unlabeled data, and lexicons) to improve the performance of the cross-domain CWS (Zhao et al., 2018; Zhang et al., 2018; Ye et al., 2019; Ding et al."
2021.findings-emnlp.248,P15-1168,0,0.0203829,"es a noticeable improvement for CWS. • Experimental results obtained from widely used benchmark datasets demonstrate that L B GCN can improve the performance compared with powerful baseline methods and outperform previous state-of-the-art studies. • The novel method extracts the information from the lexicon via the GCN and is not overreliant on the quality of the lexicon. Experimental results in the cross-domain scenario prove that the method can enhance the robustness of the basic neural CWS approaches. 2 Related Work 2010) and 2) neural network methods (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Yang et al., 2017). As the studies of deep learning techniques develop in-depth, the neural CWS methods achieve better performance compared with statistical learning methods (Cai et al., 2017; Zhou et al., 2017; Ma et al., 2018; Yang et al., 2019a; Wang et al., 2019). And neural network architectures gradually replace statistical machine learning methods as the mainstream approaches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this problem, many kinds of"
2021.findings-emnlp.248,D15-1141,0,0.0466278,"Missing"
2021.findings-emnlp.248,P17-1110,0,0.0160166,"aches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this problem, many kinds of research utilize external resources (e.g., pre-trained embeddings, unlabeled data, and lexicons) to improve the performance of the cross-domain CWS (Zhao et al., 2018; Zhang et al., 2018; Ye et al., 2019; Ding et al., 2020). For example, Huang et al. (2020) try to transfer pre-trained knowledge into the crossdomain CWS in full by leveraging more annotated datasets with different segmentation criteria (Chen et al., 2017). Tian et al. (2020) utilize lexicons and wordhood measures to enhance the robustness in the cross-domain CWS scenario. Graph Neural Network In recent years, the graph neural network has been fully explored and achieved significant progress in several kinds of NLP tasks (Zhou et al., 2020). When dealing with text scenarios, graphs can extract the features from non-structural data by modeling a set of objects (nodes) and their relationships (edges). In particular, we can consider each variable in the text as a node and the dependencies as edges for the sequence labeling task. Marcheggiani and T"
2021.findings-emnlp.248,2020.findings-emnlp.58,0,0.0212389,"riments Datasets and Settings Parameters Hidden states GCN hidden states Bi-gram embeds Learning rate GCN learning rate Batch size Dropout GCN dropout Hidden layers Epochs 768 [128,256,768] 128 [2e-4,1e-4,2e-5] [1e-3,1e-4,1e-5] [64,128,256] [0.1, 0.2, 0.4] [0.1,0.2,0.4] 12 20 Table 2: The crucial hyper-parameters and search ranges. pre-process the unsegmented sentences, which is similar to the previous paper (Cai et al., 2017). The evaluation values for CWS are F-score and Roov . We utilize three mainstream PLMs for training the Transformer encoder, including X LNET- BASE (Yang et al., 2019b; Cui et al., 2020), B ERT- BASE and ROBERTA - WWM (Cui et al., 2019).2 To finetune PLMs, we tune a few crucial hyper-parameters with the development sets for the model. The hyperparameters and search ranges are shown in Table 2. We deploy the model on the same device (GPU environment: Nvidia Tesla V100). 4.2 Experimental Results This section first reports the results of L B GCN with To verify the improvement of our proposed frame- different configurations on five benchmarks and work L B GCN, we do comparative experiments on comparison with existing models. Then it describes both benchmarks (Bakeoff-2005 (Emerso"
2021.findings-emnlp.248,2020.acl-main.595,0,0.0222785,"t al., 2017; Zhou et al., 2017; Ma et al., 2018; Yang et al., 2019a; Wang et al., 2019). And neural network architectures gradually replace statistical machine learning methods as the mainstream approaches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this problem, many kinds of research utilize external resources (e.g., pre-trained embeddings, unlabeled data, and lexicons) to improve the performance of the cross-domain CWS (Zhao et al., 2018; Zhang et al., 2018; Ye et al., 2019; Ding et al., 2020). For example, Huang et al. (2020) try to transfer pre-trained knowledge into the crossdomain CWS in full by leveraging more annotated datasets with different segmentation criteria (Chen et al., 2017). Tian et al. (2020) utilize lexicons and wordhood measures to enhance the robustness in the cross-domain CWS scenario. Graph Neural Network In recent years, the graph neural network has been fully explored and achieved significant progress in several kinds of NLP tasks (Zhou et al., 2020). When dealing with text scenarios, graphs can extract the features from non-structural data by modeling a set"
2021.findings-emnlp.248,2021.naacl-main.436,0,0.015628,"tanding studies have also exploited the learning Bakeoff-2005). However, recent outstanding paradigm in applying pre-trained language modstudies are limited by the small-scale annoels (PLM) for many NLP tasks. Various methods tated corpus. To further improve the perforthat fine-tune PLMs have achieved progress on inmance of CWS methods based on fine-tuning the PLMs, we propose a novel neural framedomain and cross-domain CWS without much manwork, L B GCN, which incorporates a lexiconual effort (Meng et al., 2019; Huang et al., 2020; based graph convolutional network into the Tian et al., 2020; Ke et al., 2021). Transformer encoder. Experimental results Prior research has shown that the problems on five benchmarks and four cross-domain of CWS are segmentation ambiguity and out-ofdatasets show the L B GCN successfully capvocabulary (OOV) words (Zhao et al., 2019). With tures the information of candidate words and helps to improve performance on the benchthe help of the pre-trained knowledge (Devlin et al., marks (Bakeoff-2005 and CTB6) and the 2018; Liu et al., 2019), the fine-tuning CWS methcross-domain datasets (SIGHAN-2010). Furods can effectively alleviate these two issues and ther experiments an"
2021.findings-emnlp.248,P19-1141,0,0.0214752,"s in the cross-domain CWS scenario. Graph Neural Network In recent years, the graph neural network has been fully explored and achieved significant progress in several kinds of NLP tasks (Zhou et al., 2020). When dealing with text scenarios, graphs can extract the features from non-structural data by modeling a set of objects (nodes) and their relationships (edges). In particular, we can consider each variable in the text as a node and the dependencies as edges for the sequence labeling task. Marcheggiani and Titov (2017) present a syntactic GCN to solve the problem of semantic role labeling. Ding et al. (2019) utilize a multi-graph structure to capture the information that the gazetteers offer. In addition, the graph neural network based on the domain lexicon is used to learn the local composition features for medical domain CWS (Du et al., 2020). Chinese Word Segmentation Since Xue (2003) formalizes the CWS as a sequence labeling prob- 3 Proposed Framework lem, most studies follow the character-based paradigm to predict segmentation labels for each The framework of L B GCN is illustrated in Figure 1. character in the sentence. In particular, the adopted It mainly consists of two parts: an encoder-"
2021.findings-emnlp.248,N16-1030,0,0.0608554,", S}, where Tn presents the didate words wi = c1 ...cn , wi ∈ L and additional size of the tag sets (Tn = 4). After linear map- nodes Vd . The entire set of edges is E = Ec ∪ Ed , ping, the framework adopts the function Sof tmax where Ed represents the set of edges between candiand the greedy search for decoding. In previous date words and additional nodes. The 1st character studies, many kinds of research adopt the CRF as c1 in the candidate word connects to the node VB the decoder layer to improve the performance of and VM , and the last character cn connects to the sequence labeling tasks (Lample et al., 2016). How- node VM and VE . For instance, the candidate word ever, the CRF layer has larger time complexity and “水仙花” (daffodils) consists of three characters space complexity for CWS (Duan and Zhao, 2020). “水(water), 仙(fairy) and 花(flower)”. In particuFor practicality, the proposed framework utilizes lar, the character node “水” (water) connects to the 2911 where Q, K, V represents a query and a set of keyvalue pairs through a linear transformation respectively, the matrices W Q ∈ Rdmodel ×dk , W K ∈ Rdmodel ×dk , W V ∈ Rdmodel ×dv are trainable parameters, and dk is the dimension of K. Instead of"
2021.findings-emnlp.248,D14-1093,0,0.031923,"Missing"
2021.findings-emnlp.248,2020.emnlp-main.317,0,0.0195593,"dopts the function Sof tmax where Ed represents the set of edges between candiand the greedy search for decoding. In previous date words and additional nodes. The 1st character studies, many kinds of research adopt the CRF as c1 in the candidate word connects to the node VB the decoder layer to improve the performance of and VM , and the last character cn connects to the sequence labeling tasks (Lample et al., 2016). How- node VM and VE . For instance, the candidate word ever, the CRF layer has larger time complexity and “水仙花” (daffodils) consists of three characters space complexity for CWS (Duan and Zhao, 2020). “水(water), 仙(fairy) and 花(flower)”. In particuFor practicality, the proposed framework utilizes lar, the character node “水” (water) connects to the 2911 where Q, K, V represents a query and a set of keyvalue pairs through a linear transformation respectively, the matrices W Q ∈ Rdmodel ×dk , W K ∈ Rdmodel ×dk , W V ∈ Rdmodel ×dv are trainable parameters, and dk is the dimension of K. Instead of performing a single-head attention function, the Transformer encoder uses the multihead self-attention layer in order to extract contextual features from different representation spaces and utilizes f"
2021.findings-emnlp.248,2021.ccl-1.108,0,0.0903443,"Missing"
2021.findings-emnlp.248,I05-3017,0,0.320822,"Missing"
2021.findings-emnlp.248,D18-1529,0,0.0284409,"Missing"
2021.findings-emnlp.248,D17-1159,0,0.0213233,"Chen et al., 2017). Tian et al. (2020) utilize lexicons and wordhood measures to enhance the robustness in the cross-domain CWS scenario. Graph Neural Network In recent years, the graph neural network has been fully explored and achieved significant progress in several kinds of NLP tasks (Zhou et al., 2020). When dealing with text scenarios, graphs can extract the features from non-structural data by modeling a set of objects (nodes) and their relationships (edges). In particular, we can consider each variable in the text as a node and the dependencies as edges for the sequence labeling task. Marcheggiani and Titov (2017) present a syntactic GCN to solve the problem of semantic role labeling. Ding et al. (2019) utilize a multi-graph structure to capture the information that the gazetteers offer. In addition, the graph neural network based on the domain lexicon is used to learn the local composition features for medical domain CWS (Du et al., 2020). Chinese Word Segmentation Since Xue (2003) formalizes the CWS as a sequence labeling prob- 3 Proposed Framework lem, most studies follow the character-based paradigm to predict segmentation labels for each The framework of L B GCN is illustrated in Figure 1. charact"
2021.findings-emnlp.248,2020.emnlp-main.318,1,0.733193,"., 2018). In particular, recent outhigh performance on several benchmarks (e.g., standing studies have also exploited the learning Bakeoff-2005). However, recent outstanding paradigm in applying pre-trained language modstudies are limited by the small-scale annoels (PLM) for many NLP tasks. Various methods tated corpus. To further improve the perforthat fine-tune PLMs have achieved progress on inmance of CWS methods based on fine-tuning the PLMs, we propose a novel neural framedomain and cross-domain CWS without much manwork, L B GCN, which incorporates a lexiconual effort (Meng et al., 2019; Huang et al., 2020; based graph convolutional network into the Tian et al., 2020; Ke et al., 2021). Transformer encoder. Experimental results Prior research has shown that the problems on five benchmarks and four cross-domain of CWS are segmentation ambiguity and out-ofdatasets show the L B GCN successfully capvocabulary (OOV) words (Zhao et al., 2019). With tures the information of candidate words and helps to improve performance on the benchthe help of the pre-trained knowledge (Devlin et al., marks (Bakeoff-2005 and CTB6) and the 2018; Liu et al., 2019), the fine-tuning CWS methcross-domain datasets (SIGHAN-"
2021.findings-emnlp.248,I17-1019,0,0.0343728,"Missing"
2021.findings-emnlp.248,P14-1028,0,0.0213532,"s framework achieves a noticeable improvement for CWS. • Experimental results obtained from widely used benchmark datasets demonstrate that L B GCN can improve the performance compared with powerful baseline methods and outperform previous state-of-the-art studies. • The novel method extracts the information from the lexicon via the GCN and is not overreliant on the quality of the lexicon. Experimental results in the cross-domain scenario prove that the method can enhance the robustness of the basic neural CWS approaches. 2 Related Work 2010) and 2) neural network methods (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Yang et al., 2017). As the studies of deep learning techniques develop in-depth, the neural CWS methods achieve better performance compared with statistical learning methods (Cai et al., 2017; Zhou et al., 2017; Ma et al., 2018; Yang et al., 2019a; Wang et al., 2019). And neural network architectures gradually replace statistical machine learning methods as the mainstream approaches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this prob"
2021.findings-emnlp.248,C04-1081,0,0.279016,"exicon is used to learn the local composition features for medical domain CWS (Du et al., 2020). Chinese Word Segmentation Since Xue (2003) formalizes the CWS as a sequence labeling prob- 3 Proposed Framework lem, most studies follow the character-based paradigm to predict segmentation labels for each The framework of L B GCN is illustrated in Figure 1. character in the sentence. In particular, the adopted It mainly consists of two parts: an encoder-decoder methods fall into two categories, including 1) statis- layer and a GCN. In the first part, we utilize the tical machine learning methods (Peng et al., 2004; Transformer as the encoder and the Dense as the deTseng et al., 2005; Zhao and Kit, 2008; Zhao et al., coder. The Transformer encoder adopts the PLMs 2909 Bi-gram Encoder Lookup Table xi-1 hi-1 Input xi hi xn hn c1 c2 c1 Lexicon Graph Output ~ h1 B M E S ~ hi-1 B M E S B M E S B M E S Transformer Encoder ~ hi Softmax h1 Dense x1 Decoder ~ hn Graph Convolutional Graph Convolutional c2 B ci-1 ci-1 Lexicon M ci ci+1 cn ci E ReLU ci+1 Graph Embeddings cn Figure 1: The illustration of the proposed framework. Continuous nodes with the same color denote a Chinese word in the pre-defined lexicon. Th"
2021.findings-emnlp.248,2020.findings-emnlp.260,0,0.0522605,"Missing"
2021.findings-emnlp.248,2020.acl-main.734,0,0.131161,"benchmarks (e.g., standing studies have also exploited the learning Bakeoff-2005). However, recent outstanding paradigm in applying pre-trained language modstudies are limited by the small-scale annoels (PLM) for many NLP tasks. Various methods tated corpus. To further improve the perforthat fine-tune PLMs have achieved progress on inmance of CWS methods based on fine-tuning the PLMs, we propose a novel neural framedomain and cross-domain CWS without much manwork, L B GCN, which incorporates a lexiconual effort (Meng et al., 2019; Huang et al., 2020; based graph convolutional network into the Tian et al., 2020; Ke et al., 2021). Transformer encoder. Experimental results Prior research has shown that the problems on five benchmarks and four cross-domain of CWS are segmentation ambiguity and out-ofdatasets show the L B GCN successfully capvocabulary (OOV) words (Zhao et al., 2019). With tures the information of candidate words and helps to improve performance on the benchthe help of the pre-trained knowledge (Devlin et al., marks (Bakeoff-2005 and CTB6) and the 2018; Liu et al., 2019), the fine-tuning CWS methcross-domain datasets (SIGHAN-2010). Furods can effectively alleviate these two issues and t"
2021.findings-emnlp.248,I05-3027,0,0.0581882,"main CWS (Du et al., 2020). Chinese Word Segmentation Since Xue (2003) formalizes the CWS as a sequence labeling prob- 3 Proposed Framework lem, most studies follow the character-based paradigm to predict segmentation labels for each The framework of L B GCN is illustrated in Figure 1. character in the sentence. In particular, the adopted It mainly consists of two parts: an encoder-decoder methods fall into two categories, including 1) statis- layer and a GCN. In the first part, we utilize the tical machine learning methods (Peng et al., 2004; Transformer as the encoder and the Dense as the deTseng et al., 2005; Zhao and Kit, 2008; Zhao et al., coder. The Transformer encoder adopts the PLMs 2909 Bi-gram Encoder Lookup Table xi-1 hi-1 Input xi hi xn hn c1 c2 c1 Lexicon Graph Output ~ h1 B M E S ~ hi-1 B M E S B M E S B M E S Transformer Encoder ~ hi Softmax h1 Dense x1 Decoder ~ hn Graph Convolutional Graph Convolutional c2 B ci-1 ci-1 Lexicon M ci ci+1 cn ci E ReLU ci+1 Graph Embeddings cn Figure 1: The illustration of the proposed framework. Continuous nodes with the same color denote a Chinese word in the pre-defined lexicon. The gray nodes of “B”, “M” and “E” indicate the three additional nodes,"
2021.findings-emnlp.248,O03-4002,0,0.45314,"data by modeling a set of objects (nodes) and their relationships (edges). In particular, we can consider each variable in the text as a node and the dependencies as edges for the sequence labeling task. Marcheggiani and Titov (2017) present a syntactic GCN to solve the problem of semantic role labeling. Ding et al. (2019) utilize a multi-graph structure to capture the information that the gazetteers offer. In addition, the graph neural network based on the domain lexicon is used to learn the local composition features for medical domain CWS (Du et al., 2020). Chinese Word Segmentation Since Xue (2003) formalizes the CWS as a sequence labeling prob- 3 Proposed Framework lem, most studies follow the character-based paradigm to predict segmentation labels for each The framework of L B GCN is illustrated in Figure 1. character in the sentence. In particular, the adopted It mainly consists of two parts: an encoder-decoder methods fall into two categories, including 1) statis- layer and a GCN. In the first part, we utilize the tical machine learning methods (Peng et al., 2004; Transformer as the encoder and the Dense as the deTseng et al., 2005; Zhao and Kit, 2008; Zhao et al., coder. The Transf"
2021.findings-emnlp.248,P17-1078,0,0.0168544,"erimental results obtained from widely used benchmark datasets demonstrate that L B GCN can improve the performance compared with powerful baseline methods and outperform previous state-of-the-art studies. • The novel method extracts the information from the lexicon via the GCN and is not overreliant on the quality of the lexicon. Experimental results in the cross-domain scenario prove that the method can enhance the robustness of the basic neural CWS approaches. 2 Related Work 2010) and 2) neural network methods (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Yang et al., 2017). As the studies of deep learning techniques develop in-depth, the neural CWS methods achieve better performance compared with statistical learning methods (Cai et al., 2017; Zhou et al., 2017; Ma et al., 2018; Yang et al., 2019a; Wang et al., 2019). And neural network architectures gradually replace statistical machine learning methods as the mainstream approaches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this problem, many kinds of research utilize external resources (e.g.,"
2021.findings-emnlp.248,N19-1278,0,0.0326937,"Missing"
2021.findings-emnlp.248,N18-1122,0,0.0501184,"Missing"
2021.findings-emnlp.248,N19-1279,0,0.0158271,"ng methods (Cai et al., 2017; Zhou et al., 2017; Ma et al., 2018; Yang et al., 2019a; Wang et al., 2019). And neural network architectures gradually replace statistical machine learning methods as the mainstream approaches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this problem, many kinds of research utilize external resources (e.g., pre-trained embeddings, unlabeled data, and lexicons) to improve the performance of the cross-domain CWS (Zhao et al., 2018; Zhang et al., 2018; Ye et al., 2019; Ding et al., 2020). For example, Huang et al. (2020) try to transfer pre-trained knowledge into the crossdomain CWS in full by leveraging more annotated datasets with different segmentation criteria (Chen et al., 2017). Tian et al. (2020) utilize lexicons and wordhood measures to enhance the robustness in the cross-domain CWS scenario. Graph Neural Network In recent years, the graph neural network has been fully explored and achieved significant progress in several kinds of NLP tasks (Zhou et al., 2020). When dealing with text scenarios, graphs can extract the features from non-structural da"
2021.findings-emnlp.248,Y98-1020,0,0.78184,"Missing"
2021.findings-emnlp.248,I08-4017,0,0.0609288,"2020). Chinese Word Segmentation Since Xue (2003) formalizes the CWS as a sequence labeling prob- 3 Proposed Framework lem, most studies follow the character-based paradigm to predict segmentation labels for each The framework of L B GCN is illustrated in Figure 1. character in the sentence. In particular, the adopted It mainly consists of two parts: an encoder-decoder methods fall into two categories, including 1) statis- layer and a GCN. In the first part, we utilize the tical machine learning methods (Peng et al., 2004; Transformer as the encoder and the Dense as the deTseng et al., 2005; Zhao and Kit, 2008; Zhao et al., coder. The Transformer encoder adopts the PLMs 2909 Bi-gram Encoder Lookup Table xi-1 hi-1 Input xi hi xn hn c1 c2 c1 Lexicon Graph Output ~ h1 B M E S ~ hi-1 B M E S B M E S B M E S Transformer Encoder ~ hi Softmax h1 Dense x1 Decoder ~ hn Graph Convolutional Graph Convolutional c2 B ci-1 ci-1 Lexicon M ci ci+1 cn ci E ReLU ci+1 Graph Embeddings cn Figure 1: The illustration of the proposed framework. Continuous nodes with the same color denote a Chinese word in the pre-defined lexicon. The gray nodes of “B”, “M” and “E” indicate the three additional nodes, named Begin, Middle,"
2021.findings-emnlp.248,W10-4126,0,0.102005,"Missing"
2021.findings-emnlp.248,D13-1061,0,0.0562079,"Missing"
2021.findings-emnlp.248,D17-1079,0,0.0166175,"art studies. • The novel method extracts the information from the lexicon via the GCN and is not overreliant on the quality of the lexicon. Experimental results in the cross-domain scenario prove that the method can enhance the robustness of the basic neural CWS approaches. 2 Related Work 2010) and 2) neural network methods (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Yang et al., 2017). As the studies of deep learning techniques develop in-depth, the neural CWS methods achieve better performance compared with statistical learning methods (Cai et al., 2017; Zhou et al., 2017; Ma et al., 2018; Yang et al., 2019a; Wang et al., 2019). And neural network architectures gradually replace statistical machine learning methods as the mainstream approaches for CWS. Cross-Domain CWS However, there is an obvious gap in the cross-domain CWS scenario. Neural CWS methods still suffer from the OOV problems. To alleviate this problem, many kinds of research utilize external resources (e.g., pre-trained embeddings, unlabeled data, and lexicons) to improve the performance of the cross-domain CWS (Zhao et al., 2018; Zhang et al., 2018; Ye et al., 2019; Ding et al., 2020). For exampl"
C02-1057,2001.mtsummit-papers.25,0,0.293303,"Missing"
C02-1057,2001.mtsummit-papers.67,0,0.0972107,"-occurrence and so on as indicators of translation quality. Brew C (1994) compares human rankings and automatic measures to decide the translation quality, whose criteria involve word frequency, POS tagging distribution and other text features. Another type of evaluation method involves comparison of the translation result with human translations. Yokoyama (2001) proposed a two-way MT based evaluation method, which compares output Japanese sentences with the original Japanese sentence for the word identification, the correctness of the modification, the syntactic dependency and the parataxis. Yasuda (2001) evaluates the translation output by measuring the similarity between the translation output and translation answer candidates from a parallel corpus. Akiba (2001) uses multiple edit distances to automatically rank machine translation output by translation examples. Another path of machine translation evaluation is based on test suites. Yu (1993) designs a test suite consisting of sentences with various test points. Guessoum (2001) proposes a semi-automatic evaluation method of the grammatical coverage machine translation systems via a database of unfolded grammatical structures. Koh (2001) de"
C02-1057,2001.mtsummit-papers.68,0,0.060773,"Missing"
C02-1057,2001.mtsummit-papers.3,0,\N,Missing
C02-1057,bohan-etal-2000-evaluating,0,\N,Missing
C02-1057,2001.mtsummit-papers.35,0,\N,Missing
C02-1057,H94-1019,0,\N,Missing
C02-1057,C00-1055,0,\N,Missing
C04-1104,P91-1027,0,0.0556205,"nerated by means of linguistic heuristic information and filtered via statistical methods. Evaluation on the acquisition of 20 multi-pattern verbs shows that our experiment achieved the similar precision and recall with former researches. Besides, simple application of the acquired lexicon to a PCFG parser indicates great potentialities of subcategorization information in the fields of NLP. Credits This research is sponsored by National Natural Science Foundation (Grant No. 60373101 and 603750 19), and High-Tech Research and Development Program (Grant No. 2002AA117010-09). Introduction Since (Brent 1991) there have been a considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics. As for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g. (Korhonen 2001). And for auto-acquisition and relevant application, researchers have made great achievements not only in English, e.g. (Briscoe and Carroll 1997), (Korhonen 2003), but also in many other languages, such as Germany (Schult"
C04-1104,J93-2002,0,0.703304,"Missing"
C04-1104,A97-1052,0,0.544317,"gh-Tech Research and Development Program (Grant No. 2002AA117010-09). Introduction Since (Brent 1991) there have been a considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics. As for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g. (Korhonen 2001). And for auto-acquisition and relevant application, researchers have made great achievements not only in English, e.g. (Briscoe and Carroll 1997), (Korhonen 2003), but also in many other languages, such as Germany (Schulte im Walde 2002), Czech (Sarkar and Zeman 2000), and Portuguese (Gamallo et. al 2002). However, relevant theoretical researches on Chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and a few papers on manual acquisition or prescriptive designment of syntactic patterns. Due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such a harmony that satisfies the description granularity for SCF (Han and Zhao 2004). T"
C04-1104,W02-0905,0,0.46107,"Missing"
C04-1104,P03-1009,0,0.0837542,"ment Program (Grant No. 2002AA117010-09). Introduction Since (Brent 1991) there have been a considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics. As for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g. (Korhonen 2001). And for auto-acquisition and relevant application, researchers have made great achievements not only in English, e.g. (Briscoe and Carroll 1997), (Korhonen 2003), but also in many other languages, such as Germany (Schulte im Walde 2002), Czech (Sarkar and Zeman 2000), and Portuguese (Gamallo et. al 2002). However, relevant theoretical researches on Chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and a few papers on manual acquisition or prescriptive designment of syntactic patterns. Due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such a harmony that satisfies the description granularity for SCF (Han and Zhao 2004). The only auto-acqu"
C04-1104,P02-1029,0,0.0986094,"e have been a considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics. As for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g. (Korhonen 2001). And for auto-acquisition and relevant application, researchers have made great achievements not only in English, e.g. (Briscoe and Carroll 1997), (Korhonen 2003), but also in many other languages, such as Germany (Schulte im Walde 2002), Czech (Sarkar and Zeman 2000), and Portuguese (Gamallo et. al 2002). However, relevant theoretical researches on Chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and a few papers on manual acquisition or prescriptive designment of syntactic patterns. Due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such a harmony that satisfies the description granularity for SCF (Han and Zhao 2004). The only auto-acquisition work for Chinese SCF made by (Han and Zhao 2004) describes the pred"
C04-1104,C00-2100,0,0.634205,"erable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics. As for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g. (Korhonen 2001). And for auto-acquisition and relevant application, researchers have made great achievements not only in English, e.g. (Briscoe and Carroll 1997), (Korhonen 2003), but also in many other languages, such as Germany (Schulte im Walde 2002), Czech (Sarkar and Zeman 2000), and Portuguese (Gamallo et. al 2002). However, relevant theoretical researches on Chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and a few papers on manual acquisition or prescriptive designment of syntactic patterns. Due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such a harmony that satisfies the description granularity for SCF (Han and Zhao 2004). The only auto-acquisition work for Chinese SCF made by (Han and Zhao 2004) describes the predefinition of 152 general frames"
C04-1104,dorr-etal-2000-chinese,0,\N,Missing
C04-1104,H91-1067,0,\N,Missing
C10-1074,J96-1002,0,0.0338033,"Feature Beginning CB Negative Beginning FI Feature Inside CI Negative Inside PB Positive Beginning N Negation Word PI Positive Inside O Other Table 1. Basic Tag Set for Review Mining 3.2 Structure Aware Model In this section, we describe how to encode different linguistic structure into model representation based on our CRFs framework. 3.2.1 Using Linear CRFs. For each sentence in a review, our task is to extract all the object features, positive opinions and negative opinions. This task can be modeled as a classification problem. Traditional classification tools, e.g. Maximum Entropy model (Berger et al, 1996), can be employed, where each word or phrase will be treated as an instance. However, they independently consider each word or phrase, and ignore the dependency relationship among them. Actually, the context information plays an important role for review mining. For example, given two continuous words with same part of speech, if the previous word is a positive opinion, the next word is more likely a positive opinion. Another example is that if the previous word is an adjective, and it is an opinion, the next noun word is more likely an object feature. To this end, we formulate the review mini"
C10-1074,H05-1045,0,0.00632958,"is a generative model, which is hard to integrate rich, overlapping features. It may encounter sparse data problem, especially when simultaneously integrating multiple features. Our framework is based on Conditional Random Fields (CRFs). CRFs is a discriminative model, which can easily integrate various features. These are some studies on opinion mining with Conditional Random Fields. For example, with CRFs, Zhao et al (2008) and McDonald et al. (2007) performed sentiment classification in sentence and document level; Breck et al (2007) identified opinion expressions from newswire documents; Choi et al. (2005) determined opinion holders to opinions also from newswire data. None of previous work focuses on jointly extracting object features, positive opinions and negative opinions simultaneously from review data. More importantly, we also show how to encode the linguistic structure, such as conjunction structure and syntactic tree structure, into model representation in our framework. This is significantly different from most of previous studies, which consider the structure information as heuristic rules (Hu and Liu, 2004) or input features (Wilson et al. 2009). Recently, there are some studies on"
C10-1074,esuli-sebastiani-2006-sentiwordnet,0,0.0325539,"ns in the review sentence, with a collected conjunction set, including “and”, “but”, “or”, “however”, “although” etc. For each conjunction, we extract its connected two text sequences. The nearest two words with same part of speech from the two text sequences are connected with the skip-edge. Here, we just consider the noun, adjective, and adverb. For example, in “good pictures and beautiful music”, there are two skip-edges: one connects two adjective words “good” and “beautiful”; the other connects two nouns “pictures” and “music”. We also employ the general sentiment lexicons, SentiWordNet (Esuli and Sebastiani, 2006), to connect opinions. Two nearest opinion words, detected by sentiment lexicon, from two sequences, will also be connected by skip-edge. If the nearest distance exceeds the threshold, this skip edge will be discarded. Here, we consider the threshold as nine. Skip-chain CRFs improve the performance of review mining, because it naturally encodes the conjunction structure into model representation with skip-edges. 3.2.3 Leveraging Syntactic Tree Structure Besides the conjunction structure, the syntactic tree structure also helps for review mining. The tree denotes the syntactic relationship amon"
C10-1074,P97-1023,0,0.0166859,"tinuous words, as discussed above. It views each word in the sentence as a node, and adjacent nodes are connected by an edge. The graphical representation is shown in Figure 2(a). Linear CRFs can make use of dependency relationship among adjacent words. 3.2.2 Leveraging Conjunction Structure We observe that the conjunctions play important roles on review mining: If the words or phrases are connected by conjunction “and”, they mostly belong to the same opinion polarity. If the words or phrases are connected by conjunction “but”, they mostly belong to different opinion polarity, as reported in (Hatzivassiloglou and McKeown, 1997; Ding and Liu, 2007). For example, “This phone has a very cool and useful feature – the speakerphone”, if we only detect “cool”, it is hard to determine its opinion polarity. But if we see “cool” is connected with “useful” by conjunction “and”, we can easily acquire the polarity of “cool” as positive. This conjunction structure not only helps to determine the opinions, but also helps to recognize object features. For example, “I like the special effects and music in this movie”, with word “music” and conjunction “and”, we can easily detect that “special effects” as an object feature. To model"
C10-1074,H05-1043,0,0.90566,"ject (movie) features, such as “movie”, “actor”, with their corresponding positive opinions and negative opinions, are listed in a structured way. The opinions are ranked by their frequencies. This provides a concise view for reviews. To accomplish this goal, we need to do three tasks: 1), extract all the object features and opinions; 2), determine the sentiment polarities for opinions; 3), for each object feature, determine the relevant opinions, i.e. object feature-opinion pairs. For the first two tasks, most previous studies employ linguistic rules or statistical methods (Hu and Liu, 2004; Popescu and Etzioni 2005). They mainly use unsupervised learning methods, which lack an effective way to address infrequent object features and opinions. They are also hard to incorporate rich overlapping features. 653 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653–661, Beijing, August 2010 Actually, there are many useful features, which have not been fully exploited for review mining. Meanwhile, most of previous methods extract object features, opinions, and determine the polarities for opinions separately. In fact, the object features, positive opinions and neg"
C10-1074,N07-1038,0,0.0084283,"ious work focuses on jointly extracting object features, positive opinions and negative opinions simultaneously from review data. More importantly, we also show how to encode the linguistic structure, such as conjunction structure and syntactic tree structure, into model representation in our framework. This is significantly different from most of previous studies, which consider the structure information as heuristic rules (Hu and Liu, 2004) or input features (Wilson et al. 2009). Recently, there are some studies on joint sentiment/topic extraction (Mei et al. 2007; Titov and McDonald, 2008; Snyder and Barzilay, 2007). These methods represent reviews as several coarse-grained topics, which can be considered as clusters of object features. They are hard to indentify the low-frequency object features and opinions. While in this paper, we will extract all the present object features and corresponding opinions with their polarities. Besides, the joint sentiment/topic methods are mainly based on review document for topic extraction. In our framework, we focus on sentence-level review extraction. 3 3.1 Structure Aware Review Mining Problem Definition To produce review summaries, we need to first finish two tasks"
C10-1074,P02-1053,0,0.0155192,"Missing"
C10-1074,J09-3003,0,0.168825,"rk can naturally encode the linguistic structure. Besides the neighbor context with linear-chain CRFs, we propose to use Skip-chain CRFs and Tree CRFs to utilize the conjunction structure and syntactic tree structure. We also propose a new unified model, Skip-Tree CRFs to integrate these structures. Here, “structure-aware” refers to the output structure, which model the relationship among output labels. This is significantly different from the previous input structure methods, which consider the linguistic structure as heuristic rules (Ding and Liu, 2007) or input features for classification (Wilson et al. 2009). Our proposed framework has the following advantages: First, it can employ rich features for review mining. We will analyze the effect of features for review mining in this framework. Second, the framework can utilize the relationship among object features, positive opinions and negative opinions. It jointly extracts these three types of expressions in a unified way. Third, the linguistic structure information can be naturally integrated into model representation, which provides more semantic dependency for output labels. Through extensive experiments on movie review and product review, we sh"
C10-1074,P07-1055,0,0.0181301,"and negative opinions, in a unified framework. Recently, Jin and Ho (2009) propose to use Lexicalized HMM for review mining. Lexicalized HMM is a variant of HMM. It is a generative model, which is hard to integrate rich, overlapping features. It may encounter sparse data problem, especially when simultaneously integrating multiple features. Our framework is based on Conditional Random Fields (CRFs). CRFs is a discriminative model, which can easily integrate various features. These are some studies on opinion mining with Conditional Random Fields. For example, with CRFs, Zhao et al (2008) and McDonald et al. (2007) performed sentiment classification in sentence and document level; Breck et al (2007) identified opinion expressions from newswire documents; Choi et al. (2005) determined opinion holders to opinions also from newswire data. None of previous work focuses on jointly extracting object features, positive opinions and negative opinions simultaneously from review data. More importantly, we also show how to encode the linguistic structure, such as conjunction structure and syntactic tree structure, into model representation in our framework. This is significantly different from most of previous stu"
C10-1074,D08-1013,0,\N,Missing
C10-1074,H05-2017,0,\N,Missing
C10-1074,P08-1036,0,\N,Missing
C10-2044,P08-1009,0,0.0191213,"information to constrain phrases to respect syntactic boundaries. Chiang (2005) introduced a constituent feature to reward phrases that match a syntactic tree but did not yield significant improvement. Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. This led to a substantial improvements. Gimpel and Smith (2008) presented rich contextual features on the source side including constituent syntactical features for phrase-based translation. Cherry (2008) utilized a dependency tree as a soft constraint to detect syntactic cohesion violations for a phrase-based 383 Coling 2010: Poster Volume, pages 383–390, Beijing, August 2010 她1 ta 将2 jiang 成为3 chengwei She1 will2 become3 印度4 yindu X[1,5] X[2,5] X[3,5] X[4,5] X[5,5] 有史以来5 youshiyilai 的6 de the4 first5 female6 president7 X[7,9] X[7,8] X[7,7] 首位7 shouwei in8 女8 nü 总统9 zongtong India’s9 history10 Figure 1: An example of Chinese-English translation. The rule X → hXL de XR , XR in XL i pattern-matches 5 and 3 spans on the left and right of the Chinese word “de”, respectively. Sc ⇒ h她 将 成为 X, She w"
C10-2044,D08-1024,0,0.0305284,"Missing"
C10-2044,P03-1021,0,0.284808,"nd Ney, 2002) to combine various features: X P r(e|f ) ∝ λi hi (α, γ) (3) i where hi (α, γ) is a feature function and λi is the weight of hi . Analogous to the previous phrase-based model, Chiang defined the following features: translation probabilities p(γ|α) and p(α|γ), lexical weights pw (γ|α) and pw (α|γ), word penalty, rule penalty, and a target n-gram language model. In this paper, we integrate a phrase boundary classifier as an additional feature into the loglinear model to provide soft constraint for patternmatching during decoding. The feature weights are optimized by MERT algorithm (Och, 2003). 3 Learning Phrase Boundaries We build a phrase boundary classifier (PBC) within a maximum entropy framework. The PBC predicts a boundary tag for each source word, considering contextual features: Ptag (t|fj , F1J ) = P exp( i λi hi (t, fj , F1J )) P P J t exp( i λi hi (t, fj , F1 ) (4) where, t ∈ {b, m, e, s}, fj is the jth word in source sentence F1J , hi is a feature function and λi is the weight of hi . To build PBC, we first present a method to recognize phrase boundaries and extract training examples from word-aligned bilingual corpus, then we define contextual feature functions. 3.1 Ph"
C10-2044,N09-1025,0,0.0369935,"Missing"
C10-2044,P02-1040,0,0.0788882,"d span forms a larger PM span and the left boundary of f (X1 ) should be updated. As a result, the hpbc score is recomputed: (7) (8) hpbc (F13 ) = hpbc + hpbc + ∆P BC The decoding algorithm is efficient since the computing of the PBC score is a procedure of table-lookup. (13) where, ∆P BC = log(T P M [2, 2]) − log(T P M [1, 2]) (14) 2 We use “*” as a tag of the non-terminal symbol “X1 ” since it has not been derived. 5 Experiments 5.1 Experimental Setup Our experiments were on Chinese-to-English translation. The training corpus (77M+81M) we used are from LDC 3 . The evaluation metric is BLEU (Papineni et al., 2002), as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n = 4. To obtain word alignments, we first ran GIZA++ (Och and Ney, 2002) in both translation directions and then refined it by “grow-diag-final” method (Koehn et al., 2003). For the language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train two 4-gram models on xinhua portion of GigaWord corpus and the English side of the training corpus. The NIST MT03 test set is used to tune the feature weights of the log-linear model by MERT (Och, 2003). We tested our system on the NIST MT06 and MT08"
C10-2044,P05-1033,0,0.545379,"derings are arbitrary because the models are weak on determining phrase boundaries for patternmatching. This paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information. We use phrase boundaries, which indicate the beginning/ending of phrase reordering, as soft constraints for decoding. Experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale Chineseto-English translation. X → hXL de XR , XR in XL i 1 Introduction The hierarchial phrase-based (HPB) model (Chiang, 2005) outperformed previous phrase-based models (Koehn et al., 2003; Och and Ney, 2004) by utilizing hierarchical phrases consisting of both words and variables. Thus the HPB model has generalization ability: a translation rule learned from a phrase pair can be used for other phrase pairs with the same pattern, e.g. reordering information of a short span can be applied for a large span during decoding. Therefore, the model captures both short and long distance phrase reorderings. However, one shortcoming of the HPB model is that it is difficult to determine phrase boundaries for pattern-matching. T"
C10-2044,J07-2003,0,0.0593139,"rained on manually segmented sentences. Learning phrase boundaries is analogous to word boundaries. The basic difference is that the unit for learning word boundaries is character while the unit for learning phrase boundaries is word. In this paper, we adopt the boundary tags presented by Ng and Low (2004) and build a classifier to predict phrase boundaries within maximum entropy framework. We train it directly on a word-aligned bilingual corpus, without any manually annotation and syntactical information. 2.2 The Hierarchical Phrase-based Model We built a hierarchical phrase-based MT system (Chiang, 2007) based on weighted SCFG. The translation knowledge is represented by rewriting rules: X → hα, γ, ∼i (2) where X is a non-terminal, α and γ are source and target strings, respectively. Both of them contain words and possibly co-indexed non-terminals. ∼ describes a one-to-one correspondence between non-terminals in α and γ. Chiang (2007) used the standard log-linear framework (Och and Ney, 2002) to combine various features: X P r(e|f ) ∝ λi hi (α, γ) (3) i where hi (α, γ) is a feature function and λi is the weight of hi . Analogous to the previous phrase-based model, Chiang defined the following"
C10-2044,W08-0302,0,0.0138199,"research showed that phrases should be constrained to some extent for improving translation quality. Most of the existing approaches utilized syntactic information to constrain phrases to respect syntactic boundaries. Chiang (2005) introduced a constituent feature to reward phrases that match a syntactic tree but did not yield significant improvement. Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. This led to a substantial improvements. Gimpel and Smith (2008) presented rich contextual features on the source side including constituent syntactical features for phrase-based translation. Cherry (2008) utilized a dependency tree as a soft constraint to detect syntactic cohesion violations for a phrase-based 383 Coling 2010: Poster Volume, pages 383–390, Beijing, August 2010 她1 ta 将2 jiang 成为3 chengwei She1 will2 become3 印度4 yindu X[1,5] X[2,5] X[3,5] X[4,5] X[5,5] 有史以来5 youshiyilai 的6 de the4 first5 female6 president7 X[7,9] X[7,8] X[7,7] 首位7 shouwei in8 女8 nü 总统9 zongtong India’s9 history10 Figure 1: An example of Chinese-English translation. The rule"
C10-2044,N03-1017,0,0.089456,"rmining phrase boundaries for patternmatching. This paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information. We use phrase boundaries, which indicate the beginning/ending of phrase reordering, as soft constraints for decoding. Experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale Chineseto-English translation. X → hXL de XR , XR in XL i 1 Introduction The hierarchial phrase-based (HPB) model (Chiang, 2005) outperformed previous phrase-based models (Koehn et al., 2003; Och and Ney, 2004) by utilizing hierarchical phrases consisting of both words and variables. Thus the HPB model has generalization ability: a translation rule learned from a phrase pair can be used for other phrase pairs with the same pattern, e.g. reordering information of a short span can be applied for a large span during decoding. Therefore, the model captures both short and long distance phrase reorderings. However, one shortcoming of the HPB model is that it is difficult to determine phrase boundaries for pattern-matching. Therefore, during decoding, a rule may be applied for all possi"
C10-2044,P09-1036,0,0.0356659,"hinese word “de”, respectively. Sc ⇒ h她 将 成为 X, She will become Xi ⇒ h她 将 成为 X[4,5] 的 X[7,9] , She will become X[7,9] in X[4,5] i ⇒ h她 将 成为 k 印度 有史以来 k 的 k 首位 女 总统, She will become the first female president in India’s historyi Figure 2: The correct derivation with adequate pattern-matching of XR . Si ⇒ h她 将 成为 X 总统, She will become X presidenti ⇒ h她 将 成为 X[4,5] 的 X[7,8] 总统, She will become X[7,8] in X[4,5] presidenti ⇒ h她 将 成为 k 印度 有史以来 k 的 k 首位 女 k 总统, She will become the first female in India’s history presidenti Figure 3: A wrong derivation with inadequate pattern-matching of XR . system. Xiong et al. (2009) presented a syntaxdriven bracketing model to predict whether two phrases are translated together or not, using syntactic features learned from training corpus. Although these approaches differ from each other, the main basic idea is the utilization of syntactic information. In this paper, we present a novel approach to learn phrase boundaries for hierarchical phrasebased translation. A phrase boundary indicates the beginning or ending of a phrase reordering. Motivated by Ng and Low (2004) that built a classifier to predict word boundaries for word segmentation, we build a classifier to predic"
C10-2044,W04-3250,0,0.182903,"Missing"
C10-2044,P08-1114,0,0.0574927,"pan [7, 9] and moves it as a whole unit. While in Si , XR matches the span [7, 8] and left the last word [9, 9] be translated separately. Similarly, other incorrect derivations are caused by inadequate pattern-matching of XL and/or XR . Previous research showed that phrases should be constrained to some extent for improving translation quality. Most of the existing approaches utilized syntactic information to constrain phrases to respect syntactic boundaries. Chiang (2005) introduced a constituent feature to reward phrases that match a syntactic tree but did not yield significant improvement. Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. This led to a substantial improvements. Gimpel and Smith (2008) presented rich contextual features on the source side including constituent syntactical features for phrase-based translation. Cherry (2008) utilized a dependency tree as a soft constraint to detect syntactic cohesion violations for a phrase-based 383 Coling 2010: Poster Volume, pages 383–390, Beijing, August 2010 她1 ta 将2 jiang 成为3 chengwei She1 will2"
C10-2044,W04-3236,0,0.262919,"ndia’s history presidenti Figure 3: A wrong derivation with inadequate pattern-matching of XR . system. Xiong et al. (2009) presented a syntaxdriven bracketing model to predict whether two phrases are translated together or not, using syntactic features learned from training corpus. Although these approaches differ from each other, the main basic idea is the utilization of syntactic information. In this paper, we present a novel approach to learn phrase boundaries for hierarchical phrasebased translation. A phrase boundary indicates the beginning or ending of a phrase reordering. Motivated by Ng and Low (2004) that built a classifier to predict word boundaries for word segmentation, we build a classifier to predict phrase boundaries. We classify each source word into one of the 4 boundary tags: “b” indicates the beginning of a phrase, “m” indicates a word appears in the middle of a phrase, “e” indicates the end of a phrase, “s” indicates a single-word phrase. We use phrase boundaries as soft constraints for decoding. To do this, we incorporate our classifier as a feature into the HPB model and propose an efficient decoding algorithm. Compared to the previous work, out approach has the following adv"
C10-2044,P02-1038,0,0.743305,"k. We train it directly on a word-aligned bilingual corpus, without any manually annotation and syntactical information. 2.2 The Hierarchical Phrase-based Model We built a hierarchical phrase-based MT system (Chiang, 2007) based on weighted SCFG. The translation knowledge is represented by rewriting rules: X → hα, γ, ∼i (2) where X is a non-terminal, α and γ are source and target strings, respectively. Both of them contain words and possibly co-indexed non-terminals. ∼ describes a one-to-one correspondence between non-terminals in α and γ. Chiang (2007) used the standard log-linear framework (Och and Ney, 2002) to combine various features: X P r(e|f ) ∝ λi hi (α, γ) (3) i where hi (α, γ) is a feature function and λi is the weight of hi . Analogous to the previous phrase-based model, Chiang defined the following features: translation probabilities p(γ|α) and p(α|γ), lexical weights pw (γ|α) and pw (α|γ), word penalty, rule penalty, and a target n-gram language model. In this paper, we integrate a phrase boundary classifier as an additional feature into the loglinear model to provide soft constraint for patternmatching during decoding. The feature weights are optimized by MERT algorithm (Och, 2003). 3"
C10-2044,J04-4002,0,0.0626196,"aries for patternmatching. This paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information. We use phrase boundaries, which indicate the beginning/ending of phrase reordering, as soft constraints for decoding. Experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale Chineseto-English translation. X → hXL de XR , XR in XL i 1 Introduction The hierarchial phrase-based (HPB) model (Chiang, 2005) outperformed previous phrase-based models (Koehn et al., 2003; Och and Ney, 2004) by utilizing hierarchical phrases consisting of both words and variables. Thus the HPB model has generalization ability: a translation rule learned from a phrase pair can be used for other phrase pairs with the same pattern, e.g. reordering information of a short span can be applied for a large span during decoding. Therefore, the model captures both short and long distance phrase reorderings. However, one shortcoming of the HPB model is that it is difficult to determine phrase boundaries for pattern-matching. Therefore, during decoding, a rule may be applied for all possible source phrases w"
C10-2044,N10-1016,0,\N,Missing
D10-1054,D07-1007,0,0.0354008,"ely to be used, as it occurs more frequently in a training corpus. However, the example is not a noun possessive case because the subphrase covered by X1 is not a noun but a prepositional phrase. Thus, without considering information of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model t"
D10-1054,P07-1005,0,0.0738102,"occurs more frequently in a training corpus. However, the example is not a noun possessive case because the subphrase covered by X1 is not a noun but a prepositional phrase. Thus, without considering information of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexic"
D10-1054,N09-1025,0,0.0588747,"Missing"
D10-1054,P05-1033,0,0.770129,"i Table 1: A classification of grammar rules for the HPB model. PR = phrasal rule, HR = hierarchical rule, GR = glue rule. tween phrases. Therefore, the HPB model outperforms conventional phrase-based models on phrase reorderings. However, HPB translation suffers from a limitation, in that the phrase reorderings lack of contextual information, such as the surrounding words of a phrase and the content of sub-phrases that represented by variables. Consider the following two hierarchical rules in translating a Chinese sentence into English: Introduction The hierarchical phrase-based (HPB) model (Chiang, 2005; Chiang, 2007) has been widely adopted in statistical machine translation (SMT). It utilizes synchronous context free grammar (SCFG) rules to perform translation. Typically, there are three types of rules (see Table 1): phrasal rule, a phrase pair consisting of consecutive words; hierarchical rule, a hierarchical phrase pair consisting of both words and variables; and glue rule, which is used to merge phrases serially. Phrasal rule captures short distance reorderings within phrases, while hierarchical rule captures long distance reorderings beX → hX1  X2 , X1 ’s X2 i (1) X → hX1  X2 , X2 X1"
D10-1054,J07-2003,0,0.238189,"lassification of grammar rules for the HPB model. PR = phrasal rule, HR = hierarchical rule, GR = glue rule. tween phrases. Therefore, the HPB model outperforms conventional phrase-based models on phrase reorderings. However, HPB translation suffers from a limitation, in that the phrase reorderings lack of contextual information, such as the surrounding words of a phrase and the content of sub-phrases that represented by variables. Consider the following two hierarchical rules in translating a Chinese sentence into English: Introduction The hierarchical phrase-based (HPB) model (Chiang, 2005; Chiang, 2007) has been widely adopted in statistical machine translation (SMT). It utilizes synchronous context free grammar (SCFG) rules to perform translation. Typically, there are three types of rules (see Table 1): phrasal rule, a phrase pair consisting of consecutive words; hierarchical rule, a hierarchical phrase pair consisting of both words and variables; and glue rule, which is used to merge phrases serially. Phrasal rule captures short distance reorderings within phrases, while hierarchical rule captures long distance reorderings beX → hX1  X2 , X1 ’s X2 i (1) X → hX1  X2 , X2 X1 i (2) Ú Ûd"
D10-1054,C08-1041,1,0.883396,"ranslation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully applied to improve syntax-based SMT translation (Liu et al., 2008), using more sophisticated syntactical features. Shen et al. (2008) integrated various contextual and linguistic features into an HPB system, using surrounding words and dependency information for building context and dependency language models, respectively. In this paper, we focus on im"
D10-1054,N03-1017,0,0.12174,"Missing"
D10-1054,D08-1010,1,0.809653,"nformation to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully applied to improve syntax-based SMT translation (Liu et al., 2008), using more sophisticated syntactical features. Shen et al. (2008) integrated various contextual and linguistic features into an HPB system, using surrounding words and dependency information for building context and dependency language models, respectively. In this paper, we focus on improving phrase reordering for HPB translation. We classify SCFG rules into several reordering patterns consisting of two variables X and F (or E) 1 , such as X1 F X2 and X2 EX1 . We treat phrase reordering as a classification problem and build a MaxEnt model for each source reordering pattern based on various"
D10-1054,P00-1056,0,0.190088,"ere tuned on NIST MT03 and tested on MT06 and MT08. The evaluation metric was BLEU (Papineni et al., 2002) with case-insensitive matching of n-grams, where n = 4. We evaluated our approach on Chinese-toEnglish translation. The training data contained 77M Chinese words and 81M English words. These data come from 17 corpora: LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08 (HK News, HK Hansards). To obtain word alignments, we first ran GIZA++ (Och and Ney, 2000) in both translation directions and then refined the results using the “grow-diagfinal” method (Koehn et al., 2003). For the language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train two 4-gram models on the Xinhua portion of the GigaWord corpus and the English side of the training corpus. 4.1 Statistical Information of Rules Hierarchical Rules We extracted 162M translation rules from the training corpus. Among them, there were 127M hierarchical rules, which contained 85M hierarchical source phrases. We classified these source phrases into 7 patterns as described in Se"
D10-1054,P02-1038,0,0.392864,"e, POS tags of the boundary and neighboring words on the source side. • Target lexical feature, the boundary words of the target phrases covered by e(X). These features can be extracted together with translation rules from bilingual corpus. However, since the hierarchical rule does not allow for adjacent variables on the source side, we extract features gr by using the method described in Xiong et for Pme al. (2006). We train the classifiers with a MaxEnt trainer (Zhang, 2004). 3 Integrating the MEPR Classifier into the HPB Model The HPB model is built within the standard loglinear framework (Och and Ney, 2002): P r(e|f ) ∝ X λi hi (α, γ) (7) i where hi (α, γ) is a feature function and λi is the weight of hi . The HPB model has the following features: translation probabilities p(γ|α) and p(α|γ), 559 lexical weights pw (γ|α) and pw (α|γ), word penalty, phrase penalty, glue rule penalty, and a target ngram language model. To integrate the MEPR classifiers into the translation model, the features of the log-linear model are changed as follows: • We add the MEPR classifier as a feature function to predict reordering pattern: hme (Tγ |Tα ) = X Pme (Tγ |Tα , α, γ) (8) During decoding, we first classify ea"
D10-1054,P03-1021,0,0.348362,"for each MaxEnt model from large-scale bilingual corpus. • For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule. By doing this, there are two options for the decoder to merge phrases: serial or inverse. We then build a classifier for glue rules to predict reorderings of neighboring phrases, analogous to Xiong et al. (2006). • We integrate the MaxEnt based phrase reordering models as features into the HPB model (Chiang, 2005). The feature weights can be tuned together with other feature functions by MERT algorithm (Och, 2003). Experimental results show that the presented method achieves significant improvement over the baseline. On Chinese-to-English translation tasks of NIST evluation, improvements in BLEU (case-insensitive) are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set, and 2.1 on MT08. The rest of the paper is structured as follows: Section 2 describes the MaxEnt based phrase reordering method. Section 3 integrates the MaxEnt models into the translation model. In Section 4, we report experimental results. We analyze the presented method and experimental results in Section 5 and conclude in Section 6. Source ph"
D10-1054,P02-1040,0,0.0804068,"cause the CKY algorithm guarantees that the sub spans [j1 , k] and [k + 1, j2 ] have been translated before [j1 , j2 ]. 4 Experiments We carried out experiments on four systems: • HPB: replication of the Hiero system (Chiang, 2005); • HPB+MEHR: HPB with MaxEnt based classifier for hierarchical rules, as described in Section 2.1; • HPB+MEGR: HPB with MaxEnt based classifier for glue rules, as described in Section 2.2; • HPB+MER: HPB with MaxEnt based classifier for both hierarchical and glue rules. All systems were tuned on NIST MT03 and tested on MT06 and MT08. The evaluation metric was BLEU (Papineni et al., 2002) with case-insensitive matching of n-grams, where n = 4. We evaluated our approach on Chinese-toEnglish translation. The training data contained 77M Chinese words and 81M English words. These data come from 17 corpora: LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08 (HK News, HK Hansards). To obtain word alignments, we first ran GIZA++ (Och and Ney, 2000) in both translation directions and then refined the results using the “grow-diagfinal”"
D10-1054,P96-1021,0,0.570122,"We then build a classifier for each source pattern to predict phrase reorderings. This is different from He et al. (2008), in which they built a classifier for each ambiguous hierarchical sourceside. Therefore, the training examples for each MaxEnt model is small and the model maybe unstable. Here, we classify source hierarchical phrases into 7 reordering patterns according to the arrangement of words and variables. We can obtain sufficient samples for each MaxEnt model from large-scale bilingual corpus. • For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule. By doing this, there are two options for the decoder to merge phrases: serial or inverse. We then build a classifier for glue rules to predict reorderings of neighboring phrases, analogous to Xiong et al. (2006). • We integrate the MaxEnt based phrase reordering models as features into the HPB model (Chiang, 2005). The feature weights can be tuned together with other feature functions by MERT algorithm (Och, 2003). Experimental results show that the presented method achieves significant improvement over the baseline. On Chinese-to-English translation tasks o"
D10-1054,P06-1066,0,0.507232,"rmation of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully applied to improve syntax-b"
D10-1054,W06-3108,0,0.062027,"without considering information of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully app"
D10-1054,D09-1008,0,\N,Missing
H05-1027,W02-1001,0,0.107516,"he least CER. Therefore, the best estimators are those which minimize the expected error rate on unseen test data. Since the distribution of test data is unknown, we can approximately minimize the error rate on some given training data (Vapnik 1999). Toward this end, we have developed a very simple heuristic training procedure called minimum sample risk, as presented in the next section. 3 Minimum Sample Risk 3.1 Problem Definition We follow the general framework of linear discriminant models described in (Duda et al. 2001). In the rest of the paper we use the following notation, adapted from Collins (2002). • Training data is a set of example input/output pairs. In LM for IME, training samples are represented as {Ai, WiR}, for i = 1…M, where each Ai is an input phonetic string and WiR is the reference transcript of Ai. • We assume some way of generating a set of candidate word strings given A, denoted by GEN(A). In our experiments, GEN(A) consists of top N word strings converted from A using a baseline IME system that uses only a word trigram model. • We assume a set of D+1 features fd(W), for d = 0…D. The features could be arbitrary functions that map W to real values. Using vector notation, w"
H05-1027,W02-1032,1,0.740514,"CER. Therefore, the best estimators are those which minimize the expected error rate on unseen test data. Since the distribution of test data is unknown, we can approximately minimize the error rate on some given training data (Vapnik 1999). Toward this end, we have developed a very simple heuristic training procedure called minimum sample risk, as presented in the next section. 3 Minimum Sample Risk 3.1 Problem Definition We follow the general framework of linear discriminant models described in (Duda et al. 2001). In the rest of the paper we use the following notation, adapted from Collins (2002). • Training data is a set of example input/output pairs. In LM for IME, training samples are represented as {Ai, WiR}, for i = 1…M, where each Ai is an input phonetic string and WiR is the reference transcript of Ai. • We assume some way of generating a set of candidate word strings given A, denoted by GEN(A). In our experiments, GEN(A) consists of top N word strings converted from A using a baseline IME system that uses only a word trigram model. • We assume a set of D+1 features fd(W), for d = 0…D. The features could be arbitrary functions that map W to real values. Using vector notation, w"
H05-1027,P03-1021,0,0.052351,"quation (4) is a step function of λ, thus cannot be optimized directly by regular gradientbased procedures – a grid search has to be used instead. However, there are problems with simple grid search: using a large grid could miss the optimal solution whereas using a fine-grained grid would lead to a very slow algorithm. Secondly, in 211 the case of LM, there are millions of candidate features, some of which are highly correlated. We address these issues respectively in the next two subsections. 3.3 Grid Line Search Our implementation of a grid search is a modified version of that proposed in (Och 2003). The modifications are made to deal with the efficiency issue due to the fact that there is a very large number of features and training samples in our task, compared to only 8 features used in (Och 2003). Unlike a simple grid search where the intervals between any two adjacent grids are equal and fixed, we determine for each feature a sequence of grids with differently sized intervals, each corresponding to a different value of sample risk. As shown in Equation (4), the loss function (i.e. sample risk) over all training samples is the sum of the loss function (i.e. Er(.)) of each training sa"
H05-1027,P05-1034,0,0.036735,"Missing"
H05-1027,H05-1034,1,0.792962,"e 3, but also better generalization properties (fewer test errors), as shown in Figure 4. 4.4 Domain Adaptation Results Though MSR achieves impressive performance in CER reduction over the comparison methods, as described in Section 4.2, the experiments are all performed using newspaper text for both training and testing, which is not a realistic scenario if we are to deploy the model in an application. This section reports the results of additional experiments in which we adapt a model trained on one domain to a different domain, i.e., in a so-called cross-domain LM adaptation paradigm. See (Suzuki and Gao 2005) for a detailed report. The data sets we used stem from five distinct sources of text. The Nikkei newspaper corpus described in Section 4.1 was used as the background domain, on which the word trigram model was trained. We used four adaptation domains: Yomiuri (newspaper corpus), TuneUp (balanced corpus containing newspapers and other sources of text), Encarta (encyclopedia) and Shincho (collection of novels). For each of the four domains, we used an 72,000-sentence subset as adaptation training data, a 5,000-sentence subset as held-out data and another 5,000-sentence subset as test data. Simi"
H05-1054,W98-1120,0,0.0685884,"Missing"
H05-1054,M95-1012,0,0.0352926,"Missing"
H05-1054,M98-1001,0,0.029034,"Missing"
H05-1054,W03-1509,1,0.889564,"Missing"
H05-1054,P00-1015,0,0.0625794,"Missing"
H05-1054,C02-1012,0,0.224382,"ised learning like Error-driven [Aberdeen, et al. 1995], Decision Tree [Sekine, et al. 1998], HMM[Bikel, et al. 1997] and Maximum Entropy[Borthwick, et al. 1999][Mikheev, et al.1998]. Similarly, the models for Chinese NER can also be divided into two categories: Individual Model and Integrated Model. Individual Model[Chen, et al. 1998][Sun, et al. 1994][Zheng, et al. 2000] consists of several submodels, each of them deals with a kind of entities. For example, the recognition of PN may be statistical-based model, while LN and ON may be rulebased model like [Chen, et al. 1998]. Integrated Model[Sun, et al. 2002] [Zhang, et al. 2003][Yu, et al. 1998][Chua, et al. 2002] deals with all kinds of 428 entities in a unified statistical framework. Most of these integrated models can be viewed as a HMM model. The differences among them are the definition of state and the features used in entity model and context model. In fact, a NER model recognizes named entities through mining the intrinsic features in the entities and the contextual features around the entities. Most of existing approaches employ either coarse particle features, like POS and ROLE[Zhang, et al. 2003], or fine particle features like word."
H05-1054,O03-5002,0,0.0465195,"Missing"
H05-1054,A97-1029,0,0.0360009,"Missing"
H05-1054,M98-1021,0,0.0955011,"Missing"
H05-1054,M95-1001,0,0.0633419,"Missing"
H05-1054,M98-1014,0,0.0383546,"efore keywords are accepted as the candidate ONs. 8. An organization name template list: We mainly use organization name templates to recognize the missed nested ONs in the statistical model. Some of these templates are as follows: ON--&gt;LN D* OrgKeyWord ON--&gt;PN D* OrgKeyWord ON--&gt;ON OrgKeyWord D and OrgKeyWord denote words in the middle of ONs and ONs keywords. D* means repeating zero or more times. 5 Back-off Model to Smooth Data sparseness problem still exists. As some parameters were never observed in training corpus, the model will back off to a less powerful model. The escape probability[Black, et al. 1998] was adopted to smooth the statistical model shown as (15). ^ p( W N W1 LW N 1 ) = λ N p( W N W 1LW N 1 ) + (15) λ N 1 p( W N W2 LWN 1 ) + L + λ1 p( W N ) + λ0 p0 431 N where λ N = 1 e N , λ i = ( 1 e i ) ∑e k ,0 &lt; i &lt; N , and ei k = i +1 is the escape probability which can be estimated by equation (16). eN = q( W1W 2 LW N 1 ) f ( W1W 2 LW N 1 ) (16) q(w1w2…wN-1) in (16) denotes the number of different symbol wN that have directly followed the word sequence w1w2…wN-1. 6 Experiments In this chapter, we will conduct experiments to answer the following questions. Will the Hybrid Model be more ef"
H05-1054,W99-0613,0,0.104038,"Missing"
H05-1054,M98-1016,0,0.776795,"Missing"
I05-1048,W03-1727,0,0.0271319,"gs: ‘d’, ‘n’, ‘nr’, ‘t’, and ‘v’ based on these character ’ is a word in the dictionary, which only has two valid part-of-speech tags. But ‘ tags, namely, ‘time’ and ‘person name’. Obviously, the part-of-speech tags: ‘d’, ‘n’ ’ are invalid. Accordingly, the tags ‘aF’, ‘dF’, ‘nF’ , ‘vF’ on ‘ ’ and and ‘v’ of ‘ the tags ‘dL’, ‘nL’, ‘vL’ on ‘ ’ are also invalid. So they should be pruned from the candidates of the character tagging. The second constraint prunes pseudo words in the elementary model. Many studies in dictionary-based segmentation treat unknown words as sequences of single characters [1], [14]. The second constraint ensures that the new word produced by the elementary model must have one or more ‘unattached’ single characters (not subsumed by any other words). For example, the sequence ‘ ’ (program error) will ’ because of the tag ‘nF’ on ‘ ’ and the tag ‘nL’ on combine the pseudo word ‘ ‘ ’. The second constraint will prune ‘ ’ since ‘ ’ (program) and ‘ ’ (error) are already in the dictionary and there is no “unattached” single character in it. Accordingly, the tag ‘nF’ on ‘ ’ and the tag ‘nL’ on ‘ ’ will be deleted from the candidates of character tagging. The following exp"
I05-1048,W03-1711,0,0.0645506,"Missing"
I05-1048,W03-1730,0,0.255808,"teps show the proposed approach solves the various issues related to Chinese morphology by a concise character tagging process where word building is revealed. 就 将 就 将就 将 小 明 明 天 将 就 程 序 错 误 进 行 分 析 小明 明天 将 就 程序 错误 进行 分析 3 Experiments and Discussion We evaluated the proposed character method using the SIGHAN Backoff data, i.e. the one-month People's Daily Corpus of 1998, and the first version of Penn Chinese Treebank [15]. We compared our approach against two state-of-the-art systems: one is based on a bi-gram word segmentation model [7], and the other based on a wordbased hidden Markov model [3]. For simplicity, we only considered three kinds of unknown words (personal name, location name, and organization name) in the all methods. The same corpus and word-dictionary were used to train the above three systems. The training data set was the 5-month People's Daily Corpus of 1998, which contained approximately 6,300,000 words and 46 word part-of-speech tags. The system dictionary contained 100,000 words and the valid part-of-speech tag(s) of each word. On average, there were 1.3 part-of-speech tags for a word in the dictionary. In the following, chr-HMM refers to the proposed elementary"
I05-1048,W03-1728,0,0.0455127,"Missing"
I05-1048,W04-3236,0,0.0590887,"a situation in which the same Chinese sequence may be one word or several words in different contexts. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 542 – 552, 2005. © Springer-Verlag Berlin Heidelberg 2005 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 543 Compared with Chinese words, Chinese characters are relatively less unambiguous. The Chinese character set is very limited. Therefore, unknown characters occur rarely in a sentence. The grammatical advantages of characters have inspired researchers to adopt character features in Chinese morphology and parsing [5], [6], [11], [12]. However, it is difficult to incorporate necessary word features, such as the form of a Chinese word and its fixed part-of-speech tags, in most character-based approaches. For this reason, character-based approaches have not achieved satisfactory performance in large-scale open tests. In this paper, we propose a lexicon-constrained character model to combine the merits of both approaches. We explore how to capture the Chinese word building rules using a statistical method, which reflects the regularities in the word formation process. First, a character hidden Markov method a"
I05-1048,C04-1067,0,0.0822945,"uation in which the same Chinese sequence may be one word or several words in different contexts. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 542 – 552, 2005. © Springer-Verlag Berlin Heidelberg 2005 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 543 Compared with Chinese words, Chinese characters are relatively less unambiguous. The Chinese character set is very limited. Therefore, unknown characters occur rarely in a sentence. The grammatical advantages of characters have inspired researchers to adopt character features in Chinese morphology and parsing [5], [6], [11], [12]. However, it is difficult to incorporate necessary word features, such as the form of a Chinese word and its fixed part-of-speech tags, in most character-based approaches. For this reason, character-based approaches have not achieved satisfactory performance in large-scale open tests. In this paper, we propose a lexicon-constrained character model to combine the merits of both approaches. We explore how to capture the Chinese word building rules using a statistical method, which reflects the regularities in the word formation process. First, a character hidden Markov method assign"
I05-1048,W03-1723,0,0.0249838,"logical result is: ‘ /nr /t /d /d /n /n /v /v ’. The above steps show the proposed approach solves the various issues related to Chinese morphology by a concise character tagging process where word building is revealed. 就 将 就 将就 将 小 明 明 天 将 就 程 序 错 误 进 行 分 析 小明 明天 将 就 程序 错误 进行 分析 3 Experiments and Discussion We evaluated the proposed character method using the SIGHAN Backoff data, i.e. the one-month People's Daily Corpus of 1998, and the first version of Penn Chinese Treebank [15]. We compared our approach against two state-of-the-art systems: one is based on a bi-gram word segmentation model [7], and the other based on a wordbased hidden Markov model [3]. For simplicity, we only considered three kinds of unknown words (personal name, location name, and organization name) in the all methods. The same corpus and word-dictionary were used to train the above three systems. The training data set was the 5-month People's Daily Corpus of 1998, which contained approximately 6,300,000 words and 46 word part-of-speech tags. The system dictionary contained 100,000 words and the valid part-of-speech tag(s) of each word. On average, there were 1.3 part-of-speech tags for a word in the dictionary."
I05-1048,P04-1059,0,0.0208849,"92.3 88.6 90.2 92.5 94.9 95.9 96.7 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 551 From Table 4, it is obvious that word segmentation precision increases significantly, and at the same time, the corresponding recall remains the same or slightly declined. This implies that the chr-HMM retains the correct words by the original system and concurrently decreases significantly its errors. 4 Related Work Although character features are very important in Chinese morphology, research in character-based approach is unpopular. Chooi-Ling Goh et al. [16], Jianfeng Gao et al. [8] and Huaping Zhang [3] adopted character information to handle unknown words; X. Luo [11], Yao Meng [12] and Shengfen Luo [17] each presented characterbased parsing models for Chinese parsing or new-word extraction. T. Nakagawa used word-level information and character-level information for word segmentation [6]. Hwee Tou Ng et al. [5] investigated word-based and character-based approaches and proposed a maximum entropy character-based POS analyzer. Although the character tags proposed in this paper are essentially similar to some of the previous work mentioned above, here our focus is to inte"
I05-1048,W03-1719,0,0.02957,"character tags. The proposed model solves the problems of unknown word detection, word segmentation and part-of-speech tagging using both word and character features. Additionally, our module is a post-processing module, which can be coupled to any existing Chinese morphological system; and it can readily recall some of the unknown words omitted by the system, and as a result, significantly improves the overall performance. Evaluations of the proposed system on SIGHAN open test sets indicate that our method outperforms the best bakeoff results on 3 test sets, and ranks 2nd in the 4th test set [9]. 2 A Lexicon-Constrained Character Model for Chinese Morphology 2.1 An Elementary Model to Describe Chinese Word Building Rules It is recognized that there are some regularities in the process of forming words from Chinese characters. This in general can be captured by word building rules. In this paper, we explore a statistical model to acquire such rules. The following are some definitions used in the proposed model. [Def. 1] character position feature We use four notations to denote the position of a character in a Chinese word. ‘F’ means the first character of the word, ‘L’ the last chara"
I05-1048,W03-1025,0,0.0691428,"Missing"
I05-1048,Y95-1016,0,0.0318951,"d’, ‘n’, ‘nr’, ‘t’, and ‘v’ based on these character ’ is a word in the dictionary, which only has two valid part-of-speech tags. But ‘ tags, namely, ‘time’ and ‘person name’. Obviously, the part-of-speech tags: ‘d’, ‘n’ ’ are invalid. Accordingly, the tags ‘aF’, ‘dF’, ‘nF’ , ‘vF’ on ‘ ’ and and ‘v’ of ‘ the tags ‘dL’, ‘nL’, ‘vL’ on ‘ ’ are also invalid. So they should be pruned from the candidates of the character tagging. The second constraint prunes pseudo words in the elementary model. Many studies in dictionary-based segmentation treat unknown words as sequences of single characters [1], [14]. The second constraint ensures that the new word produced by the elementary model must have one or more ‘unattached’ single characters (not subsumed by any other words). For example, the sequence ‘ ’ (program error) will ’ because of the tag ‘nF’ on ‘ ’ and the tag ‘nL’ on combine the pseudo word ‘ ‘ ’. The second constraint will prune ‘ ’ since ‘ ’ (program) and ‘ ’ (error) are already in the dictionary and there is no “unattached” single character in it. Accordingly, the tag ‘nF’ on ‘ ’ and the tag ‘nL’ on ‘ ’ will be deleted from the candidates of character tagging. The following experimen"
I05-1048,C02-1145,0,0.0251343,"e correct result ‘ /nrF /nrL /tF /tL /dS /dS /nF /nL /nF /nL /vF /vL /vF /vL’ is selected, and the corresponding morphological result is: ‘ /nr /t /d /d /n /n /v /v ’. The above steps show the proposed approach solves the various issues related to Chinese morphology by a concise character tagging process where word building is revealed. 就 将 就 将就 将 小 明 明 天 将 就 程 序 错 误 进 行 分 析 小明 明天 将 就 程序 错误 进行 分析 3 Experiments and Discussion We evaluated the proposed character method using the SIGHAN Backoff data, i.e. the one-month People's Daily Corpus of 1998, and the first version of Penn Chinese Treebank [15]. We compared our approach against two state-of-the-art systems: one is based on a bi-gram word segmentation model [7], and the other based on a wordbased hidden Markov model [3]. For simplicity, we only considered three kinds of unknown words (personal name, location name, and organization name) in the all methods. The same corpus and word-dictionary were used to train the above three systems. The training data set was the 5-month People's Daily Corpus of 1998, which contained approximately 6,300,000 words and 46 word part-of-speech tags. The system dictionary contained 100,000 words and the"
I05-1048,P03-2039,0,0.0323539,"88.1 90.9 82.9 87.1 90.1 92.3 88.6 90.2 92.5 94.9 95.9 96.7 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 551 From Table 4, it is obvious that word segmentation precision increases significantly, and at the same time, the corresponding recall remains the same or slightly declined. This implies that the chr-HMM retains the correct words by the original system and concurrently decreases significantly its errors. 4 Related Work Although character features are very important in Chinese morphology, research in character-based approach is unpopular. Chooi-Ling Goh et al. [16], Jianfeng Gao et al. [8] and Huaping Zhang [3] adopted character information to handle unknown words; X. Luo [11], Yao Meng [12] and Shengfen Luo [17] each presented characterbased parsing models for Chinese parsing or new-word extraction. T. Nakagawa used word-level information and character-level information for word segmentation [6]. Hwee Tou Ng et al. [5] investigated word-based and character-based approaches and proposed a maximum entropy character-based POS analyzer. Although the character tags proposed in this paper are essentially similar to some of the previous work mentioned above,"
I05-1048,W03-1704,0,0.0412161,"it is obvious that word segmentation precision increases significantly, and at the same time, the corresponding recall remains the same or slightly declined. This implies that the chr-HMM retains the correct words by the original system and concurrently decreases significantly its errors. 4 Related Work Although character features are very important in Chinese morphology, research in character-based approach is unpopular. Chooi-Ling Goh et al. [16], Jianfeng Gao et al. [8] and Huaping Zhang [3] adopted character information to handle unknown words; X. Luo [11], Yao Meng [12] and Shengfen Luo [17] each presented characterbased parsing models for Chinese parsing or new-word extraction. T. Nakagawa used word-level information and character-level information for word segmentation [6]. Hwee Tou Ng et al. [5] investigated word-based and character-based approaches and proposed a maximum entropy character-based POS analyzer. Although the character tags proposed in this paper are essentially similar to some of the previous work mentioned above, here our focus is to integrate various word features with the characterbased model in such a way that the probability of the model is undistorted. The"
I05-1048,W03-1726,0,\N,Missing
I05-1087,C02-1011,0,0.0871573,"Missing"
I05-1087,W01-1413,0,0.102608,"Missing"
I05-1087,P95-1050,0,0.0686081,"Missing"
I05-1087,C96-2098,0,0.0875572,"Missing"
I05-1087,P99-1067,0,0.23634,"Missing"
I05-1087,W95-0114,0,0.102302,"Missing"
I05-1087,W97-0119,0,0.469563,"Missing"
I05-1087,P98-1069,0,0.0589631,"Missing"
I05-1087,C98-1066,0,\N,Missing
I05-2003,J93-2003,0,\N,Missing
I05-2003,C04-1069,0,\N,Missing
I05-3006,W99-0613,0,0.132517,"Missing"
I05-3006,bick-2004-named,0,0.0412842,"Missing"
I05-3006,O04-2004,0,0.0493047,"Missing"
I05-3006,P03-1043,0,0.0134333,"eristics for this task. [Pierre 2002] developed an English NER system capable of identifying product names in product views. It employed a simple Boolean classifier for identifying product name, which was constructed from the list of product names. The method is similar to token matching and has a limitation for product NER applications. [Bick et al. 2004] recognized named entities including product names based on constraint grammar based parser for Danish. This rule-based approach is highly dependent on the performance of Danish parser and suffers from its weakness in system portability. [C. Niu et al. 2003] presented a bootstrapping approach for English named entity recognition using successive learners of parsing-based decision 1 Introduction Named entity recognition(NER) plays a significantly important role in information extraction(IE) and many other applications. Previous study on NER is mainly focused either on the proper name identification of person(PER), location(LOC), organization(ORG), time(TIM) and numeral(NUM) expressions almost in news domain, which can be viewed as general NER, or other named entity (NE) recognition in specific domain such as biology. As far as we know, however, t"
I05-3006,M95-1012,0,\N,Missing
I05-3006,C02-1012,0,\N,Missing
I05-3006,O03-5002,0,\N,Missing
I05-3006,A97-1029,0,\N,Missing
I13-1109,D09-1102,0,0.0738981,"Missing"
I13-1109,P05-1049,0,0.0387668,"led examples as nodes in a connected graph, then propagating the label information from any vertex to nearby nodes through weighted edges iteratively, finally get the labels of unlabeled examples after the propagation process converges. The labels of unlabeled examples are determined by considering both the similarity between labeled and unlabeled examples, and the similarity between unlabeled examples (Chen, et al.,2008). LP algorithm has achieved good performance in many applications, such as noun phrase anaphoricity in coreference resolution (Zhou, et al., 2009), word sense disambiguation (Niu, et al., 2005) and entity relation extraction (Chen, et al., 2006). 870 4.1 Graph Building Let X  {xi }in1 be a set of tweets for a given organization, where xi represents ith tweet, n is the total number of tweets. The first l tweets are （xl , yl ) , YL  { y1, ..., yl } are labels. labeled（x1 , y1 )... Here, yi  C , C refers to two known classes (True or False) for this task. The others （xl 1 , yl 1 ) …（xl u , yl u ) are the unlabeled tweets, where YU  { yl 1, ..., yl u } are unknown. For the graph, the nodes represent both labeled and unlabeled tweets. The edge between any two nodes xi and x j"
I13-1109,P06-1017,0,0.0276674,"opagating the label information from any vertex to nearby nodes through weighted edges iteratively, finally get the labels of unlabeled examples after the propagation process converges. The labels of unlabeled examples are determined by considering both the similarity between labeled and unlabeled examples, and the similarity between unlabeled examples (Chen, et al.,2008). LP algorithm has achieved good performance in many applications, such as noun phrase anaphoricity in coreference resolution (Zhou, et al., 2009), word sense disambiguation (Niu, et al., 2005) and entity relation extraction (Chen, et al., 2006). 870 4.1 Graph Building Let X  {xi }in1 be a set of tweets for a given organization, where xi represents ith tweet, n is the total number of tweets. The first l tweets are （xl , yl ) , YL  { y1, ..., yl } are labels. labeled（x1 , y1 )... Here, yi  C , C refers to two known classes (True or False) for this task. The others （xl 1 , yl 1 ) …（xl u , yl u ) are the unlabeled tweets, where YU  { yl 1, ..., yl u } are unknown. For the graph, the nodes represent both labeled and unlabeled tweets. The edge between any two nodes xi and x j is weighted by some distance measure. Based on assum"
I13-1109,W11-1719,0,0.0301508,"age Processing, pages 869–873, Nagoya, Japan, 14-18 October 2013. campaign. They adopt SVM classifier with external resources, including Wordnet, metadata profile, category profile, Google set, and user feedback, to enrich the information of the given organization. Yoshida et al. (2010) classify organization names into “organization-like names” or “general-word-like names”. Kalmar (2010) adopts bootstrapping method to classify the tweets. The research of (Garcí a-Cumbreras et al., 2010) shows the named entities in tweets are appropriate for certain company names. There are some similar works. Perez-Tellez et al. (2011) adopt clustering technique to solve the problem of organization name disambiguation. Focus on identifying relevant tweets for social TV, Dan et al. (2011) propose a bootstrapping algorithm utilizing a small manually labeled dataset, and a large dataset of unlabeled messages. Different from their works, we utilize semisupervised methods to classify the tweets. We aim to transfer related or unrelated information of the given organization among tweets based on a small scale of labeled data. Compared with bootstrapping algorithm, which is based on a local consistency assumption, LP algorithm is b"
I13-1109,P11-1037,0,0.0350842,"is organized as follows: Section 2 describes the related work on name disambiguation. Section 3 gives problem description and an overview of our approach. Section 4 and Section 5 present LP-based and TSVM-based semi-supervised methods to classify tweets. Section 6 gives the experiments and results. Finally section 7 summarizes this paper. 2 Related Work Twitter contains little information in each tweet, with no more than 140 characters. This makes the tasks of analyzing Twitter messages more challenge, and attracts much interest from the research community in recent years (Meij et al., 2012; Liu et al., 2011; Sriram et al., 2011 ). The most related works are WePS-3 Online Reputation Management 1 held in 2010, which aims to identify tweets which are related to a given company (Amigóet al., 2010). In WePS-3, the research of (Yerva et al., 2010) shows the best performance in the evaluation 1 http://nlp.uned.es/weps/ 869 International Joint Conference on Natural Language Processing, pages 869–873, Nagoya, Japan, 14-18 October 2013. campaign. They adopt SVM classifier with external resources, including Wordnet, metadata profile, category profile, Google set, and user feedback, to enrich the informatio"
O05-5010,A97-1029,0,0.0402095,"Missing"
O05-5010,W98-1120,0,0.0670202,"Missing"
O05-5010,W03-1509,0,0.0248746,"Missing"
O05-5010,M95-1012,0,\N,Missing
O05-5010,C02-1012,0,\N,Missing
P06-2026,W01-1413,0,0.0322871,"nsic relationships, e.g., acquiring translations from Japanese to Chinese or from Korean to English. 5) Acquiring translations using anchor text information (Lu et al., 2004). 6) Acquiring translations from the Web. When people use Asia language (Chinese, Japanese, and Korean) to write, they often annotate associated English meanings after terms. With the development of Web and the open of accessible electronic documents, digital library, and scientific articles, these resources will become more and more abundant. Thus, acquiring term translations from the Web is a feasible and effective way. Nagata et al. (2001) proposed an empirical function of the byte distance between Japanese and English terms as an evaluation criterion to extract translations of Japanese words, and the results could be used as a Japanese-English dictionary. Cheng et al. (2004) utilized the Web as the corpus source to translate English unknown queries for CLIR. They proposed context-vector and chi-square methods to determine Chinese translations for unknown query terms via mining of top 100 search-result pages from Web search engines. Zhang and Vines (2004) proposed using a Web search engine to obtain translations of Chinese out-"
P06-2026,P99-1067,0,0.0222929,"(CLIR). 4) As one of the typical application paradigms of the combination of CLIR and Web mining. Automatic acquisition of bilingual translations has been extensively researched in the literature. The methods of acquiring translations are usually summarized as the following six categories. 1) Acquiring translations from parallel corpora. To reduce the workload of manual annotations, researchers have proposed different methods to automatically collect parallel corpora of different language versions from the Web (Kilgarriff, 2003). 2) Acquiring translations from nonparallel corpora (Fung, 1997; Rapp, 1999). It is based on the clue that the context of source term is very similar to that of target translation in a large amount of corpora. 3) Acquiring translations from a combination of translations of constituent words (Li et al., 2003). 4) Acquiring translations using cognate matching (Gey, 2004) 199 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 199–206, c Sydney, July 2006. 2006 Association for Computational Linguistics or transliteration (Seo et al., 2004). This method is very suitable for the translation between two languages with some intrinsic relationships, e.g."
P06-2026,I05-1087,1,0.800685,"hip Economic Indicators (10)”. In Example 1, the item “Three Kingdoms” is suffix redundancy and should be removed. In Example 2, the term “Blue Chip” is in accord with the definition of prefix redundancy information, but this term is a correct translation candidate. Thus, the problem of affix redundancy information is so complex that we need an evaluation method to decide to retain or drop the candidate. To deal with subset redundancy and affix redundancy information, sort-based subset deletion and mutual information methods are respectively proposed. More details refer to our previous paper (Fang et al., 2005). 5 Candidate evaluation based on multifeatures 5.1 Possible features for translation pairs Through analyzing mass Web pages, we obtain the following possible features that have important influences on term translation mining. They include: 1) candidate frequency and its distribution in different Web pages, 2) length ratio between source terms and target candidates (S-T), 3) distance between S-T, and 4) keywords, key symbols and boundary information between S-T. 1) Candidate frequency and its distribution Translation candidate frequency is the most important feature and is the basis of decisio"
P06-2026,W97-0119,0,0.0384083,"n retrieval (CLIR). 4) As one of the typical application paradigms of the combination of CLIR and Web mining. Automatic acquisition of bilingual translations has been extensively researched in the literature. The methods of acquiring translations are usually summarized as the following six categories. 1) Acquiring translations from parallel corpora. To reduce the workload of manual annotations, researchers have proposed different methods to automatically collect parallel corpora of different language versions from the Web (Kilgarriff, 2003). 2) Acquiring translations from nonparallel corpora (Fung, 1997; Rapp, 1999). It is based on the clue that the context of source term is very similar to that of target translation in a large amount of corpora. 3) Acquiring translations from a combination of translations of constituent words (Li et al., 2003). 4) Acquiring translations using cognate matching (Gey, 2004) 199 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 199–206, c Sydney, July 2006. 2006 Association for Computational Linguistics or transliteration (Seo et al., 2004). This method is very suitable for the translation between two languages with some intrinsic relati"
P06-2026,J03-3001,0,\N,Missing
P06-2106,francopoulo-etal-2006-lexical,1,0.861995,"Missing"
P06-2106,W03-1905,1,0.81081,"ess ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and allowing for encoding individual lexical entries as instances of the model (Ide et al., 2003; Bertagna et al., 2004b). In the framework of our project, by situating our work in the context of W3C standards and relying on standardized technologies underlying this community, the original RDF schema for ISLE lexical entries has been made compliant to OWL. The whole data model has been formalized in OWL by using Prot´eg´e 3.2 beta and has been extended to cover the morphological component as well (see Figure 2). Prot´eg´e 3.2 beta has been also used as a tool to instantiate the lexical entries of our sample monolingual lexicons, thus ensuring adherence to the model, encoding coherence an"
P06-2106,bel-etal-2000-simple,1,0.877753,"s: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of the following four research items. There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES1 , PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003) and LIRICS2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources. 2 (4) Evaluation through application classification Figure 1: Relations among research items 1 Introduction 1 (2) Sample lexicons (1) building a description framework of lexical entries (2) building sample lexicons (3) building an upper-layer ontology (4) evaluating the proposed framework through an application Figure 1 illustrates the relations among these research items. Our main aim is the researc"
P06-2106,C94-1091,1,0.485541,"ical operations, which are special lexical entities allowing the user to define multilin3 MILE is based on the experience derived from existing computational lexicons (e.g. LE-PAROLE, SIMPLE, EuroWordNet, etc.). 828 “CL” stands for a classifier. They always follow cardinal numbers in Japanese. Note that different classifiers are used for different nouns. In the above examples, classifier “hiki” is used to count noun “inu (dog)”, while “satsu” for “hon (book)”. The classifier is determined based on the semantic type of the noun. In the Thai language, classifiers are used in various situations (Sornlertlamvanich et al., 1994). The classifier plays an important role in construction with noun to express ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and al"
P06-2106,zhang-etal-2004-distributional,1,0.766907,"the set of postpositions as values of FunctionType instead of conventional function types such as “subj” and “obj”. It might be an user defined data category or language dependent data category. Furthermore, it is preferable to prepare the mapping between Japanese postpositions and conventional function types. This is interesting because it seems more a terminological difference, but the model can be applied also to Japanese. 4 Building sample lexicons 4.1 Swadesh list and basic lexicon The issue involved in defining a basic lexicon for a given language is more complicated than one may think (Zhang et al., 2004). The naive approach of simply taking the most frequent words in a language is flawed in many ways. First, all frequency counts are corpus-based and hence inherit the bias of corpus sampling. For instance, since it is easier to sample written formal texts, words used predominantly in informal contexts are usually underrepresented. Second, frequency of content words is topic-dependent and may vary from corpus to corpus. Last, and most crucially, frequency of a word does not correlate to its conceptual necessity, 4.2 Aligning multilingual lexical entries Since our goal is to build a multilingual"
P06-2106,bertagna-etal-2004-content,1,0.887142,"e morphological, syntactic and semantic layers. Moreover, an intermediate module allows to define mechanisms of linkage and mapping between the syntactic and semantic layers. Within each layer, a basic linguistic information unit is identified; basic units are separated but still interlinked each other across the different layers. Within each of the MLM layers, different types of lexical object are distinguished : fits with as many Asian languages as possible, and contributing to the ISO-TC37/SC4 activities. As a starting point, we employ an existing description framework, the MILE framework (Bertagna et al., 2004a), to describe several lexical entries of several Asian languages. Through building sample lexicons (research item (2)), we will find problems of the existing framework, and extend it so as to fit with Asian languages. In this extension, we need to be careful in keeping consistency with the existing framework. We start with Chinese, Japanese and Thai as target Asian languages and plan to expand the coverage of languages. The research items (2) and (3) also comprise the similar feedback loop. Through building sample lexicons, we refine an upper-layer ontology. An application built in the resea"
P06-2106,Y06-1043,1,\N,Missing
P09-2031,P05-1033,0,0.493,"n various phrases, such as , accept France ’s invitation”. “ While f2 almost always appears in f1 , indicating that the variable X may not be replaced with other words expect “President”. It indicates that R2 is not likely to be useful, although f2 may appears frequently in a corpus. 4. N (p), the number of phrases that contain p as a substring. É {I   Given a monolingual corpus, key phrases can be extracted efficiently according to Algorithm 1. Firstly (line 1), all possible phrases are extracted as candidates of key phrases. This step is analogous to the rule extraction as described in (Chiang, 2005). The basic difference is that there are no word alignment constraints for monolingual phrase extraction, which therefore results in a substantial number of candidate phrases. We use the following restrictions to limit the phrase number: 2.2 C-value C-value, a measurement of automatic term recognition, is proposed by Frantzi and Ananiadou (1996) to extract nested collocations, collocations that substrings of other longer ones. We use C-value for two reasons: on one hand, it uses rich factors besides phrase frequency, e.g. the phrase length, the frequency that a sub-phrase appears in longer phr"
P09-2031,C96-1009,0,0.0494903,"as a substring. É {I   Given a monolingual corpus, key phrases can be extracted efficiently according to Algorithm 1. Firstly (line 1), all possible phrases are extracted as candidates of key phrases. This step is analogous to the rule extraction as described in (Chiang, 2005). The basic difference is that there are no word alignment constraints for monolingual phrase extraction, which therefore results in a substantial number of candidate phrases. We use the following restrictions to limit the phrase number: 2.2 C-value C-value, a measurement of automatic term recognition, is proposed by Frantzi and Ananiadou (1996) to extract nested collocations, collocations that substrings of other longer ones. We use C-value for two reasons: on one hand, it uses rich factors besides phrase frequency, e.g. the phrase length, the frequency that a sub-phrase appears in longer phrases. Thus it is appropriate for extracting hierarchical phrases. On the other hand, the computation of C-value is efficient. Analogous to (Frantzi and Ananiadou, 1996), we use 4 factors (L, F, S, N ) to determine if a phrase p is a key phrase: 1. The length of a candidate phrase is limited to pl; 2. The length of the initial phrase used to crea"
P09-2031,D07-1103,0,0.085662,"Missing"
P09-2031,N03-1017,0,0.0142339,"Missing"
P09-2031,W04-3250,0,0.268048,"Missing"
P09-2031,P08-1066,0,0.0397018,"Missing"
P09-2054,I05-3009,0,0.374292,"Missing"
P09-2054,C08-1130,1,0.621394,"Missing"
W00-1211,A88-1019,0,\N,Missing
Y09-2017,N09-1055,0,0.02138,"lem that some frequent nouns and noun phrases may not be real product features. Directly using frequent nouns as features may lead to the reduction of precision. Popescu and Etzioni (2005) utilize relation-specific extraction patterns with web PMI assessor to assess feature candidates. They consider the product features finding as a domain words extraction process. Using the web as the resource, product features are identified without opinion words. For the identification of opinion words, the method of the nearest vicinity match is simple and effective. Further, Researchers (Su et al., 2008; Du and Tan, 2009) focus on the association between the features and opinion words by mutual reinforcement approach. Ding and Liu (2007) combines the linguistic rules and opinion aggregation function to determine the semantic orientation. Our work is different from theirs: the short length reviews are first separately analyzed for their simple structures. A graph model is built to capture the relationship between product features and opinion words. With an iterative reinforcement, product features and opinion words are found in one stage. 3 Bootstrapping Given the product reviews, the task is to find the produc"
Y09-2017,H05-1043,0,0.0339845,"earch is that of (Hu and Liu, 2004). They adopt association rule mining for extracting nouns as frequent features. Compactness pruning and redundancy pruning are used to filter the incorrect features. Then frequent features are used to find adjacent opinion words (adjectives) that modify them. Infrequent features are extracted with these opinion words. Their method is effective in finding the product features. However, there is a problem that some frequent nouns and noun phrases may not be real product features. Directly using frequent nouns as features may lead to the reduction of precision. Popescu and Etzioni (2005) utilize relation-specific extraction patterns with web PMI assessor to assess feature candidates. They consider the product features finding as a domain words extraction process. Using the web as the resource, product features are identified without opinion words. For the identification of opinion words, the method of the nearest vicinity match is simple and effective. Further, Researchers (Su et al., 2008; Du and Tan, 2009) focus on the association between the features and opinion words by mutual reinforcement approach. Ding and Liu (2007) combines the linguistic rules and opinion aggregatio"
Y09-2017,W02-1028,0,0.0467361,"found in one stage. 3 Bootstrapping Given the product reviews, the task is to find the product features and opinion words. As observed, the opinion words mostly appear around the features in the review sentences. They are highly dependent on each other. So we adopt bootstrapping method to find both of them in a unified process. Bootstrapping (Abney, 2002) refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. It has been proved effective in semantic lexicon construction (Widdows and Dorow, 2002; Thelen and Riloff, 2002). In our task, it also includes the following steps: (i) In the product reviews, choose a small set of features and opinions as “seed words” (ii) Count co-occurrence of candidates and seed words in the product reviews (iii) Use a figure of merit based upon these counts to select new seed words (iv) Return to step (ii) and iterate n times As product features are mostly nouns and noun phrases. Adjectives are normally used to express opinions in reviews. In this paper, we take nouns and noun phrases as the feature candidates, the adjectives as the opinion word candidates, though there are other c"
Y09-2017,C02-1114,0,0.0412275,"res and opinion words are found in one stage. 3 Bootstrapping Given the product reviews, the task is to find the product features and opinion words. As observed, the opinion words mostly appear around the features in the review sentences. They are highly dependent on each other. So we adopt bootstrapping method to find both of them in a unified process. Bootstrapping (Abney, 2002) refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. It has been proved effective in semantic lexicon construction (Widdows and Dorow, 2002; Thelen and Riloff, 2002). In our task, it also includes the following steps: (i) In the product reviews, choose a small set of features and opinions as “seed words” (ii) Count co-occurrence of candidates and seed words in the product reviews (iii) Use a figure of merit based upon these counts to select new seed words (iv) Return to step (ii) and iterate n times As product features are mostly nouns and noun phrases. Adjectives are normally used to express opinions in reviews. In this paper, we take nouns and noun phrases as the feature candidates, the adjectives as the opinion word candidates"
Y09-2017,H05-2017,0,\N,Missing
Y09-2017,P02-1046,0,\N,Missing
Y10-1036,I05-3009,0,0.0651294,"Missing"
Y10-1036,chen-etal-2006-study,0,0.0295783,"sed on the hypothesis that “if a candidate occurs frequently in a few documents of a domain, it is likely a term”. The co-occurrences between the target string and its components or context, referred to as Internal association (e.g. Schone and Jurafsky, 2001) and context dependency (e.g. Sornlertlamvanich et al., 2000), are used for term extraction. There are also studies evaluating the distribution of a term within a domain or across domains through different metrics, such as term representativeness (Hisamitsu and Niwa, 2002), Inter-Domain Entropy (Chang, 2005) and the Lexicon Set Algorithm (Chen et al., 2006). Statistics based techniques can extract common used terms with statistical significance. However, the techniques are very sensitive to term frequency, and thus terms with low frequencies cannot be extracted. The second category is based on trigger words or characters. According to (Feng et al., 2004) and (Yang et al., 2008), characters and words immediately before and after these terms are proven to be useful for term extraction. Accessor Variety Criteria proposed in (Feng et al., 2004) considers the characters that are directly before or after a string as important factors for determining t"
Y10-1036,C02-1125,0,0.0234646,"ly used statistical measurement is TF-IDF (Salton and McGill, 1983; Frank, 1999), which is based on the hypothesis that “if a candidate occurs frequently in a few documents of a domain, it is likely a term”. The co-occurrences between the target string and its components or context, referred to as Internal association (e.g. Schone and Jurafsky, 2001) and context dependency (e.g. Sornlertlamvanich et al., 2000), are used for term extraction. There are also studies evaluating the distribution of a term within a domain or across domains through different metrics, such as term representativeness (Hisamitsu and Niwa, 2002), Inter-Domain Entropy (Chang, 2005) and the Lexicon Set Algorithm (Chen et al., 2006). Statistics based techniques can extract common used terms with statistical significance. However, the techniques are very sensitive to term frequency, and thus terms with low frequencies cannot be extracted. The second category is based on trigger words or characters. According to (Feng et al., 2004) and (Yang et al., 2008), characters and words immediately before and after these terms are proven to be useful for term extraction. Accessor Variety Criteria proposed in (Feng et al., 2004) considers the charac"
Y10-1036,W03-1704,0,0.0905496,"Missing"
Y10-1036,W02-1407,0,0.0841621,"Missing"
Y10-1036,W01-0513,0,0.0428773,"main categories including statistics based measures, trigger words (or characters) based algorithms, domain knowledge based methods and supervised methods. The first category is statistics based measures which identify terms by their statistical significance. The most widely used statistical measurement is TF-IDF (Salton and McGill, 1983; Frank, 1999), which is based on the hypothesis that “if a candidate occurs frequently in a few documents of a domain, it is likely a term”. The co-occurrences between the target string and its components or context, referred to as Internal association (e.g. Schone and Jurafsky, 2001) and context dependency (e.g. Sornlertlamvanich et al., 2000), are used for term extraction. There are also studies evaluating the distribution of a term within a domain or across domains through different metrics, such as term representativeness (Hisamitsu and Niwa, 2002), Inter-Domain Entropy (Chang, 2005) and the Lexicon Set Algorithm (Chen et al., 2006). Statistics based techniques can extract common used terms with statistical significance. However, the techniques are very sensitive to term frequency, and thus terms with low frequencies cannot be extracted. The second category is based on"
Y10-1036,C00-2116,0,0.0397932,"igger words (or characters) based algorithms, domain knowledge based methods and supervised methods. The first category is statistics based measures which identify terms by their statistical significance. The most widely used statistical measurement is TF-IDF (Salton and McGill, 1983; Frank, 1999), which is based on the hypothesis that “if a candidate occurs frequently in a few documents of a domain, it is likely a term”. The co-occurrences between the target string and its components or context, referred to as Internal association (e.g. Schone and Jurafsky, 2001) and context dependency (e.g. Sornlertlamvanich et al., 2000), are used for term extraction. There are also studies evaluating the distribution of a term within a domain or across domains through different metrics, such as term representativeness (Hisamitsu and Niwa, 2002), Inter-Domain Entropy (Chang, 2005) and the Lexicon Set Algorithm (Chen et al., 2006). Statistics based techniques can extract common used terms with statistical significance. However, the techniques are very sensitive to term frequency, and thus terms with low frequencies cannot be extracted. The second category is based on trigger words or characters. According to (Feng et al., 2004"
Y10-1036,C08-1130,1,0.416479,"Missing"
Y10-1036,P09-2054,1,0.883773,"Missing"
Y10-1036,W03-1730,0,0.0439902,"Missing"
Y10-1036,J04-1004,0,\N,Missing
Y11-1010,P09-1114,0,0.0311749,"k in news page and several content blocks in forum and blog page. The content block detecting method is shown below: (4) Where, fi is the feature and wi is its weight. There are many heuristic features (Christian , 2009) such as the variance of the text length of nodes, the ratio of the length of the link to the length of the text in the node, the ratio of the fixed text length and number of stop words inside the DOM node. The remained issue is how to get the weight of each feature. Since there are three related types: News, Forums and Blogs. We can consider this problem as transfer learning (Jing, 2009). We are interested in getting the weight of target webpage type T and we have labeled instance for K auxiliary type A1, …, Ak. Let wk denote the weight vector of the linear classifier for the auxiliary type Ak and wT denote the weight vector for the target type T. we now assume that these weight vectors are related through a common component v: , , for k = 1,2, …, K (5) 93 If we assume that only weight of certain general features can be shared between different web page types, we can force certain dimensions of v to be 0. We use a square matrix F and set Fv=0. The entries of F are set to 0 ex"
Y11-1023,P05-1033,0,0.722892,"opose a novel lexical reordering model based on a maximum entropy classifier. Our model employs the word alignment and translation during the decoding process. Experimental results on the Chinese-to-English task showed that our method outperformed the baseline system in BLEU score significantly. Moreover, the translation results further proved the effectiveness of our approach. Keywords: hierarchical phrase-based model, reordering, maximum entropy model 1 Introduction Reordering is a big challenge for statistical machine translation (SMT). The hierarchical phrasebased (HPB) translation model (Chiang, 2005), which adopts a synchronous context-free grammar (SCFG), is considered to be prominent in capturing global reorderings. However, the HPB model is weak in controlling the reordering process. Arbitrary reorderings frequently come up during the decoding phase, worsening the translation quality, such as the example shown in Figure 1(a). The non-terminal “X2 ” is reordered with “X1 ” and “anshui rongye”, but the punctuation “d” indicates that the phrase should be translated monotonously. This kind of reordering error frequently occurs when target phrases contain similar lexicons, which include bot"
Y11-1023,J07-2003,0,0.0441534,"tu[figure] 6 ) BL Referring now to Figure 7, the user selects to menu options 607 (Figure 6) MEL Referring to FIG.7, user selection of the menu options 607 (Figure 6) RF Referring to FIG. 7, the user's selection of menu option 607 (FIG. 6) Figure 4: The actual influence of our method on translation results. Src: The source sentence. BL: Translation of Baseline system. M EL: Translation of ME-all system. RF : Reference. Then, to evaluate the effectiveness of our model, we conducted experiments on six systems. • Baseline system: an in-house hierarchical phrase-based machine translation system (Chiang, 2007) with 4-gram language model. • Baseline 5-gram: baseline system using 5-gram language model. • Baseline 6-gram: baseline system using 6-gram language model. • MLE: proposed method using maximum likelihood estimation. • ME-no-de: proposed method using maximum entropy model, but was trained with the features merely satisfying the non-linguistic constraint. • ME-all: proposed method using maximum entropy model using all sorts of features. Note that all the MLE and ME systems are trained with 4-gram language model only. 4.3 Results The experimental results are shown in Table 4. We can observe that"
Y11-1023,C04-1073,0,0.0391906,"us sodium bicarbonate P(γ|α) P(α|γ) anshui rongye X2 , ammonia solution Pw(γ|α) Pw(α|γ) X2 X1 ammonia solution 0.0740741 0.5 0.0461205 0.11668 X1 ammonia solution X2 0.444444 0.0754714 0.0461205 0.11668 Figure 1: Incorrect reordering. string, it considers the target words only. As shown in Figure 1(b), the two SCFG rules have different probabilities given by P (γ|α), P (α|γ) and language model, but they do not yield correct result. Various methods have been proposed in order to solve the reordering problem for HPB model. Some of them focus on the sentence rewriting in the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Elsner, 2009; Du and Way, 2010). The main idea of those studies is to rewrite source language in order to make the source word sequence close to the target language before training and testing by various syntactic methodologies. Those offline rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Moreover, offline methods require syntax analysis which does not always produce convincing result on certain languages. The parsing error will"
Y11-1023,P03-1021,0,0.0149297,"2003) was run in both translation directions to obtain the word alignment on the training set, and then the alignment result was refined by “grow-diag-final” method (Koehn, 2003). For the language model, we used the SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002) to train three language models from 4-gram to 6-gram on the target portion of the training set. Considering that long distance language models may influence the word order on the target string, it makes sense to compare our model with those long distance language models. We used the minimum error rate training algorithm (MERT) (Och, 2003) for tuning the feature weights of the log-linear model, and adopted BLEU (Papineni et al., 2002) as our evaluation metric. An open source Maximum Entropy Toolkit (Zhang, 2004) was employed to train our lexical model. We implemented two systems based on maximum likelihood estimation (MLE) and maximum entropy (ME) model, respectively. Our experiments were carried out in two steps. Firstly, in order to find out the most effective model settings, we tested different values of the threshold T hreshold W ord Scope on the both systems. The results are shown in Table 3. From the results, the ME based"
Y11-1023,P02-1038,0,0.0178462,"esearch worked on the Japanese-to-English task, while ours works on the Chinese-to-English task. 2.2 Hierarchical Phrase-Based Model The hierarchical phrase-based (HPB) model (Chiang, 2005), which is based on a synchronous context-free grammar (SCFG), is presented in the form X −→&lt; γ, α, ∼&gt; (2) where X is a non-terminal, γ and α denote source and target strings, which contain both terminals and non-terminals. ∼ is the one-to-one correspondence between terminals and non-terminals in γ and α. Chiang (2005) integrated all the features mentioned in the first section into the log-linear framework (Och and Ney, 2002) X P (e|f ) ∝ λi hi (γ, α) (3) i where hi (γ, α) is a feature function and λi is the weight of hi . One merit of the log-linear framework is that we are able to adopt various features into the HPB model conveniently. Hence we intend to complement the HPB model with our proposed method as a new feature. 218 广|ad 谱|v 抗|v 微生物|n 活性|n 的|u 聚腈|nr 基|Ng 丙烯酸|n 酯|Ng 膜|n guang pu kang weishengwu huoxing de jvqing ji bingxisuan zhi mo a polymeric cyanoacrylate film having a broad spectrum of antimicrobial activity Figure 2: Sentence pair with word alignment. Table 1: Features extracted from “de” and “jvqin"
Y11-1023,2010.eamt-1.32,0,0.0452857,"2 X1 ammonia solution 0.0740741 0.5 0.0461205 0.11668 X1 ammonia solution X2 0.444444 0.0754714 0.0461205 0.11668 Figure 1: Incorrect reordering. string, it considers the target words only. As shown in Figure 1(b), the two SCFG rules have different probabilities given by P (γ|α), P (α|γ) and language model, but they do not yield correct result. Various methods have been proposed in order to solve the reordering problem for HPB model. Some of them focus on the sentence rewriting in the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Elsner, 2009; Du and Way, 2010). The main idea of those studies is to rewrite source language in order to make the source word sequence close to the target language before training and testing by various syntactic methodologies. Those offline rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Moreover, offline methods require syntax analysis which does not always produce convincing result on certain languages. The parsing error will result in wrongly reordered sentences so that the translations cannot be correct. Shen"
Y11-1023,C10-1050,0,0.13874,"the target language before training and testing by various syntactic methodologies. Those offline rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Moreover, offline methods require syntax analysis which does not always produce convincing result on certain languages. The parsing error will result in wrongly reordered sentences so that the translations cannot be correct. Shen et al. (2008, 2009) proposed a string-to-dependency language model to capture longdistance word order. He et al. (2010) classified SCFG rules into different patterns and built a maximum entropy classifier to select proper translation rules during decoding. Analogously, Cui et al. (2010) proposed a joint model for SCFG rule selection. Four sub-models which include context free model and context based model are used to predict proper rules. Both source and target strings are considered simultaneously in the model. Hayashi et al. (2010) integrated the method of (Tromble and Elsner, 2009) into the decoder as a source language model. Those online methods are involved in the decoding phase as a soft constraint to bi"
Y11-1023,P02-1040,0,0.0830793,"ning set, and then the alignment result was refined by “grow-diag-final” method (Koehn, 2003). For the language model, we used the SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002) to train three language models from 4-gram to 6-gram on the target portion of the training set. Considering that long distance language models may influence the word order on the target string, it makes sense to compare our model with those long distance language models. We used the minimum error rate training algorithm (MERT) (Och, 2003) for tuning the feature weights of the log-linear model, and adopted BLEU (Papineni et al., 2002) as our evaluation metric. An open source Maximum Entropy Toolkit (Zhang, 2004) was employed to train our lexical model. We implemented two systems based on maximum likelihood estimation (MLE) and maximum entropy (ME) model, respectively. Our experiments were carried out in two steps. Firstly, in order to find out the most effective model settings, we tested different values of the threshold T hreshold W ord Scope on the both systems. The results are shown in Table 3. From the results, the ME based systems produce better results than the MLE based systems. This indicates that the ME classifier"
Y11-1023,P08-1066,0,0.0598138,"010). The main idea of those studies is to rewrite source language in order to make the source word sequence close to the target language before training and testing by various syntactic methodologies. Those offline rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Moreover, offline methods require syntax analysis which does not always produce convincing result on certain languages. The parsing error will result in wrongly reordered sentences so that the translations cannot be correct. Shen et al. (2008, 2009) proposed a string-to-dependency language model to capture longdistance word order. He et al. (2010) classified SCFG rules into different patterns and built a maximum entropy classifier to select proper translation rules during decoding. Analogously, Cui et al. (2010) proposed a joint model for SCFG rule selection. Four sub-models which include context free model and context based model are used to predict proper rules. Both source and target strings are considered simultaneously in the model. Hayashi et al. (2010) integrated the method of (Tromble and Elsner, 2009) into the decoder as"
Y11-1023,D09-1008,0,0.0175017,"work. In Section 3, we describe the implementation of the maximum entropy based lexical reordering model and the integration into the decoder. Experiment on the Chinese-to-English task is shown in Section 4, followed by a discussion in Section 5. The conclusion and future work are presented in Section 6. 217 2 Previous Related Work 2.1 Online Reordering Methods Comparing with the offline method, online method is able to utilize various useful information during decoding. Shen et al. (2008) proposed a string-to-dependency target language model to capture long distance word orders. Furthermore, Shen et al. (2009) extended the work by applying more features such as phrase length distribution and context language model. Shen et al. (2009) also intended to build a dependency language model on the source language, but the result reported a decline with this feature. He et al. (2010) classified SCFG rules into several fixed patterns. For example, the rule &lt;X1 anshui rongye X2 , X2 X1 ammonia solution&gt; belongs to the pattern &lt;X1 FX2 , X2 X1 E&gt;. A maximum entropy classifier is trained to select rules according to the patterns on the both source and target sides. This method is insensitive to the terminal ord"
Y11-1023,P05-1066,0,0.0381786,"P(γ|α) P(α|γ) anshui rongye X2 , ammonia solution Pw(γ|α) Pw(α|γ) X2 X1 ammonia solution 0.0740741 0.5 0.0461205 0.11668 X1 ammonia solution X2 0.444444 0.0754714 0.0461205 0.11668 Figure 1: Incorrect reordering. string, it considers the target words only. As shown in Figure 1(b), the two SCFG rules have different probabilities given by P (γ|α), P (α|γ) and language model, but they do not yield correct result. Various methods have been proposed in order to solve the reordering problem for HPB model. Some of them focus on the sentence rewriting in the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Elsner, 2009; Du and Way, 2010). The main idea of those studies is to rewrite source language in order to make the source word sequence close to the target language before training and testing by various syntactic methodologies. Those offline rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Moreover, offline methods require syntax analysis which does not always produce convincing result on certain languages. The parsing error will result in wrongly reor"
Y11-1023,W04-3250,0,0.0210116,"d using maximum likelihood estimation. • ME-no-de: proposed method using maximum entropy model, but was trained with the features merely satisfying the non-linguistic constraint. • ME-all: proposed method using maximum entropy model using all sorts of features. Note that all the MLE and ME systems are trained with 4-gram language model only. 4.3 Results The experimental results are shown in Table 4. We can observe that all the proposed methods outperformed the baseline system. The improvements are all statistically significant at p &lt; 0.01 according to the significant test method described in (Koehn, 2004). The fact that our methods yield better results than the long distance n-gram language model illustrates that information on the source side is useful to judge the word order. Moreover, as applying the simple linguistic constraint, the BLEU score rises accordingly. We can observe that the ME-all system outperformed ME-no-de systems at the significance p &lt; 0.05. This proves that the linguistic constraint is effective. We compared the translation results between the baseline system and ME-all system to investigate the actual influence of our method. Figure 4 shows some examples. From the first"
Y11-1023,D09-1105,0,0.0143821,"solution Pw(γ|α) Pw(α|γ) X2 X1 ammonia solution 0.0740741 0.5 0.0461205 0.11668 X1 ammonia solution X2 0.444444 0.0754714 0.0461205 0.11668 Figure 1: Incorrect reordering. string, it considers the target words only. As shown in Figure 1(b), the two SCFG rules have different probabilities given by P (γ|α), P (α|γ) and language model, but they do not yield correct result. Various methods have been proposed in order to solve the reordering problem for HPB model. Some of them focus on the sentence rewriting in the preprocessing stage (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Tromble and Elsner, 2009; Du and Way, 2010). The main idea of those studies is to rewrite source language in order to make the source word sequence close to the target language before training and testing by various syntactic methodologies. Those offline rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Moreover, offline methods require syntax analysis which does not always produce convincing result on certain languages. The parsing error will result in wrongly reordered sentences so that the translations cann"
Y11-1023,D10-1054,1,0.890113,"close to the target language before training and testing by various syntactic methodologies. Those offline rewriting methods are independent of the decoder. Thus other useful information such as word translation generated in the decoding process cannot be utilized. Moreover, offline methods require syntax analysis which does not always produce convincing result on certain languages. The parsing error will result in wrongly reordered sentences so that the translations cannot be correct. Shen et al. (2008, 2009) proposed a string-to-dependency language model to capture longdistance word order. He et al. (2010) classified SCFG rules into different patterns and built a maximum entropy classifier to select proper translation rules during decoding. Analogously, Cui et al. (2010) proposed a joint model for SCFG rule selection. Four sub-models which include context free model and context based model are used to predict proper rules. Both source and target strings are considered simultaneously in the model. Hayashi et al. (2010) integrated the method of (Tromble and Elsner, 2009) into the decoder as a source language model. Those online methods are involved in the decoding phase as a soft constraint to bi"
Y11-1023,N03-1017,0,\N,Missing
Y11-1023,J03-1002,0,\N,Missing
Y11-1023,P10-2002,0,\N,Missing
Y11-1066,W11-1719,0,0.0256031,"s a model for each company. Yoshida et al. (2010) classify organization names into “organization-like names” or “general-word-like names”. They categorize each query in the first stage, and categorize each tweet in the second stage using the rules customized for each class of queries. Kalmar (2010) adopts bootstrapping method to classify the tweets. The research of (García-Cumbreras et al., 2010) shows the named entities in tweets are appropriate for certain company names. Tsagkias et al. (2010) prove that a general classifier can be employed to predict the presence of any company in Twitter. Perez-Tellez et al. (2011) propose term expansion to the ambiguous words and words which highly co-occur with it. Our work is different from theirs: supervised and semi-supervised methods are utilized in different stages for the classification of tweets. For the task, the set of organization names in the training and test corpora are different. The model could not be trained for a certain organization. Therefore, our method aims to utilize both training data and the test data to improve the accuracy of the performance. In detail, the tweets are firstly classified by Maximum Entropy classifier trained on training data."
Y12-1010,I11-1053,0,0.0961119,"spectively. After relation words extraction, we filter out relation words those less than 5 times. We also assign a URI kb/relationWord/word to each 103 relation word and use kb:relationWord to represent relations. Finally, we use Resource Description Framework (RDF) to describe the knowledge base. Due to the limited space, Figure 4 shows a snippet of domain knowledge base. 3.3 Sentence Simplification As discussed above, the characteristic complexity of the sentences in biomedical text challenges the relationship mining task. Recently, researchers have paid attention to simplifying sentences (Bach et al., 2011; Jonnalagadda et al., 2009). However, these approaches usually use syntax information as learning features or to generate rules. This is a chicken and egg problem. Inspired by (Bach et al., 2011), we develop a new sentence simplification model without using syntax parser. Moreover, ours uses domain knowledge to incorporate more constrains to reduce the search space and computational complexity. Benefits of this sentence simplification model are twofold: 1) Sentence structure is simplified, second, 2) Since we can obtain more simple sentences that contain only one-one relationship, it alleviat"
Y12-1010,C92-2082,0,0.189242,"of each sentence, and then adopted four extraction rules to find protein and gene interactions (Fundel et al., 2007). Rinaldi et al., (2007) also utilized dependency parsing and lexicon to extract protein and gene relationships. However, rule-based methods mainly rely on handcraft rules, and suffer from low recall due to the sparseness of extraction rules. In addition, rulebased methods that incorporate syntactic information can be computationally costly in larger corpus. Due to the sparseness issue in handcraft rules, pattern-based methods aim to construct comprehensive rules automatically (Hearst, 1992). Specifically, they are based on the duality of relationships, and usually adopt bootstrapping paradigm. For example, Brin (1998) proposed a pattern-based relation extraction system named DIPRE, which starts with a small set of seed facts for one or more relations of interest. Then it automatically looks for linguistic patterns in underlying sources as indicators of facts. Finally it utilizes these patterns to identify new fact candidates as further hypotheses to populate relationships. Agichtein and Gravano (2000) proposed a system called Snowball, which adopts similar strategy with DIPRE. H"
Y12-1010,W02-1010,0,0.0839779,"Missing"
Y12-1012,1983.tc-1.13,0,0.114072,"Missing"
Y12-1012,2005.mtsummit-papers.11,0,0.015803,"first to apply pivot translation strategies on Chinese-Japanese patent SMT translation. Though similar strategies have been implemented, most of them are applied on language pairs which are from the same nature. As far as we know, no one has applied pivot translation strategies on Chinese-Japanese patent translation. Secondly, we make use of three patent corpora which are independent of each other, due to the fact that multilingual corpora are usually not easy to exploit, while others usually use corpora in which the sentences are aligned to each other across all languages, such as Europarl (Koehn, 2005). Besides, as we use large Chinese-English and English-Japanese corpora to help Chinese-Japanese SMT translation, we figure out approaches to make these pivot translation strategies practicable on such big data set. Finally, we implement three pivot translation strategies and apply minimum bayes risk (MBR) system combination on the translation results to further improve translation quality, which achieves an absolute improvement of 4.25 BLEU4 (Papineni et al., 2002) points over baseline system. The rest of this paper is organized as follows. We describe related work making use of pivot languag"
Y12-1012,N03-1017,0,0.0089507,"ledge, we are the first to apply pivot translation strategies on ChineseJapanese patent translation. We implement three pivot translation strategies and perform a sentence level system combination on different translation results to further improve translation quality. 3 3.1 Direct phrase-based SMT and pivot translation strategies Direct phrase-based SMT Moses 1 is a freely available statistical machine translation system, which is also the most popular open-source platform for researchers working on SMT. Currently, Moses offers two types of translation models: phrase-based translation model (Koehn et al., 2003) and tree-based translation model. We use phrase-based Moses to build up our direct phrase-based SMT system. In phrase-based SMT model, there are mainly three kinds of translation resources: translation rule table, language model and reordering table. Both translation rule table and reordering table are learnt from segmented sentence aligned bilingual corpus. Language model is learnt from target monolingual corpus. We employ the phrase-based Moses which uses different feature functions, such as direct phrase translation probability, inverse phrase translation probability, direct lexical weight"
Y12-1012,W10-3222,0,0.0510083,"Missing"
Y12-1012,2009.mtsummit-wpt.3,0,0.356438,"Missing"
Y12-1012,J03-1002,0,0.00649673,"Missing"
Y12-1012,P02-1040,0,0.0849684,"easy to exploit, while others usually use corpora in which the sentences are aligned to each other across all languages, such as Europarl (Koehn, 2005). Besides, as we use large Chinese-English and English-Japanese corpora to help Chinese-Japanese SMT translation, we figure out approaches to make these pivot translation strategies practicable on such big data set. Finally, we implement three pivot translation strategies and apply minimum bayes risk (MBR) system combination on the translation results to further improve translation quality, which achieves an absolute improvement of 4.25 BLEU4 (Papineni et al., 2002) points over baseline system. The rest of this paper is organized as follows. We describe related work making use of pivot languages (Section 2), and introduce direct SMT system and three kinds of pivot translation strategies, as well as minimum bayes risk system combination (Section3). Then, we present our experimental data and pivot translation strategy results (Section 4). Discussion on our work is in Section 5. The last section draws our conclusion and future work. 118 2 Related work Pivot languages have been used for different purposes. Gollins and Sanderson (2001) used multiple pivot lan"
Y12-1012,W11-2601,0,0.0291059,"Missing"
Y12-1012,I08-1062,0,0.0286274,"nized as follows. We describe related work making use of pivot languages (Section 2), and introduce direct SMT system and three kinds of pivot translation strategies, as well as minimum bayes risk system combination (Section3). Then, we present our experimental data and pivot translation strategy results (Section 4). Discussion on our work is in Section 5. The last section draws our conclusion and future work. 118 2 Related work Pivot languages have been used for different purposes. Gollins and Sanderson (2001) used multiple pivot languages to improve cross language information retrieval. Ramirez et al. (2008) makes use of existing English resources as a pivot language to create a trilingual JapaneseSpanish-English thesaurus. Wang et al. (2006) improved word alignment for scarce-resourced languages pairs using bilingual corpora of pivot languages. Zhao et al. (2008) extracted paraphrase patterns from bilingual parallel corpora with a pivot approach. Concerning the contribution of pivot languages to SMT, researchers have done a lot of work on it. Al-Hunaity et al. (2010) used English as pivot language to enhance Danish-Arabic SMT. Babych et al. (2007) compared the direct translation method with pivo"
Y12-1012,J03-3002,0,0.139898,"Missing"
Y12-1012,P07-1108,0,0.054567,"Missing"
Y12-1012,P08-1089,0,0.0281122,"l data and pivot translation strategy results (Section 4). Discussion on our work is in Section 5. The last section draws our conclusion and future work. 118 2 Related work Pivot languages have been used for different purposes. Gollins and Sanderson (2001) used multiple pivot languages to improve cross language information retrieval. Ramirez et al. (2008) makes use of existing English resources as a pivot language to create a trilingual JapaneseSpanish-English thesaurus. Wang et al. (2006) improved word alignment for scarce-resourced languages pairs using bilingual corpora of pivot languages. Zhao et al. (2008) extracted paraphrase patterns from bilingual parallel corpora with a pivot approach. Concerning the contribution of pivot languages to SMT, researchers have done a lot of work on it. Al-Hunaity et al. (2010) used English as pivot language to enhance Danish-Arabic SMT. Babych et al. (2007) compared the direct translation method with pivot translation strategy and confirmed that better translation quality could be achieved with pivot translation strategy. Bertoldi et al. (2008) provided theoretical formulation of SMT with pivot languages and introduced new methods for training alignment models"
Y12-1012,2008.iwslt-papers.1,0,\N,Missing
Y12-1012,W09-0431,0,\N,Missing
Y12-1012,P06-2112,0,\N,Missing
Y12-1012,J05-4003,0,\N,Missing
Y12-1012,N07-1061,0,\N,Missing
Y12-1012,I11-1154,0,\N,Missing
Y12-1012,P09-1018,0,\N,Missing
Y12-1012,I11-1091,0,\N,Missing
Y12-1012,2010.iwslt-papers.12,0,\N,Missing
Y12-1012,2011.eamt-1.34,0,\N,Missing
Y12-1012,W03-1730,0,\N,Missing
Y12-1012,D08-1076,0,\N,Missing
Y12-1025,W11-1719,0,\N,Missing
Y12-1025,P11-1037,0,\N,Missing
zhang-etal-2010-extracting,N09-1055,0,\N,Missing
zhang-etal-2010-extracting,H05-2017,0,\N,Missing
zhang-etal-2010-extracting,H05-1043,0,\N,Missing
